#!/usr/bin/env python3
"""
æœ€å°åŒ–CLIMADAæ•¸æ“šç”Ÿæˆ
Minimal CLIMADA Data Generation

å¾æˆåŠŸçš„nc_tc_comprehensive_functional.pyæå–æœ€æ ¸å¿ƒçš„æ•¸æ“šç”Ÿæˆä»£ç¢¼
ç›´æ¥åŸ·è¡Œï¼Œä¸åšè¤‡é›œçš„éŒ¯èª¤è™•ç†
"""

print("ğŸš€ æœ€å°åŒ–CLIMADAæ•¸æ“šç”Ÿæˆ")

# %% 
import os
import sys
import numpy as np
from datetime import datetime

# è¨­ç½®è·¯å¾‘
current_dir = os.path.dirname(os.path.abspath(__file__))
insurance_dir = os.path.join(current_dir, 'insurance_analysis_refactored')

for path in [insurance_dir, current_dir]:
    if path not in sys.path:
        sys.path.append(path)

print(f"âœ… è·¯å¾‘è¨­ç½®: {current_dir}")

# %% ç›´æ¥å°å…¥
print("ğŸ“¦ å°å…¥æ¨¡çµ„...")
from config.settings import NC_BOUNDS, YEAR_RANGE, RESOLUTION
from data_processing.track_processing import get_regional_tracks
from hazard_modeling.tc_hazard import create_tc_hazard
from exposure_modeling.litpop_processing import process_litpop_exposures
from exposure_modeling.hospital_osm_extraction import get_nc_hospitals
from impact_analysis.impact_calculation import calculate_tc_impact

print("âœ… æ¨¡çµ„å°å…¥å®Œæˆ")

# %% ç›´æ¥è¤‡è£½æ•¸æ“šç”Ÿæˆä»£ç¢¼
print("\nğŸŒªï¸ æº–å‚™CLIMADAçœŸå¯¦æ•¸æ“š...")

print("æ­£åœ¨æº–å‚™CLIMADAçœŸå¯¦æ•¸æ“š...")
# ç²å–è»Œè·¡æ•¸æ“š
print(f"   ğŸŒ€ ç›®æ¨™å€åŸŸ: North Carolina {NC_BOUNDS}")
print(f"   ğŸ“… åˆ†ææœŸé–“: {YEAR_RANGE[0]}-{YEAR_RANGE[1]}")

tracks = get_regional_tracks(NC_BOUNDS, YEAR_RANGE, nb_synth=3)
print(f"   âœ… ç²å– {len(tracks.data)} æ¢è»Œè·¡")

# å‰µå»ºç½å®³å ´
from hazard_modeling.centroids import create_hazard_centroids
centroids_lat, centroids_lon = create_hazard_centroids(NC_BOUNDS, RESOLUTION)
tc_hazard = create_tc_hazard(tracks, centroids_lat, centroids_lon)
print(f"   âœ… ç½å®³å ´: {tc_hazard.size} å€‹äº‹ä»¶")

# å‰µå»ºæ›éšª - ä½¿ç”¨å®Œæ•´å¹´ä»½ç¯„åœ
exposure_dict, successful_years = process_litpop_exposures(
    country_iso="USA", state_name="North Carolina", years=range(2019, 2025)
)

if successful_years:
    print(f"   âœ… æˆåŠŸè™•ç† {len(successful_years)} å¹´æ›éšªæ•¸æ“š: {successful_years}")
    
    # è¨ˆç®—æ‰€æœ‰å¹´ä»½çš„å½±éŸ¿
    yearly_impacts = {}
    yearly_exposures_summary = {}
    
    for year in successful_years:
        exposure = exposure_dict[year]
        print(f"\n   ğŸ“Š {year}å¹´æ›éšªæ•¸æ“š:")
        print(f"      è³‡ç”¢é»æ•¸: {len(exposure.gdf):,}")
        print(f"      ç¸½æ›éšªå€¼: ${exposure.value.sum()/1e9:.2f}B")
        
        # è¨ˆç®—è©²å¹´ä»½å½±éŸ¿
        impact, impact_func_set = calculate_tc_impact(tc_hazard, exposure)
        yearly_impacts[year] = impact
        
        yearly_exposures_summary[year] = {
            'asset_count': len(exposure.gdf),
            'total_value': exposure.value.sum(),
            'annual_average_impact': impact.aai_agg
        }
        
        print(f"      å¹´å‡æå¤±: ${impact.aai_agg/1e9:.2f}B")
    
    # ä½¿ç”¨æœ€æ–°å¹´ä»½ä½œç‚ºä¸»è¦æ›éšªæ•¸æ“š
    latest_year = max(successful_years)
    exposure_main = exposure_dict[latest_year]
    impact = yearly_impacts[latest_year]
    print(f"\n   ğŸ¯ ä½¿ç”¨ {latest_year} å¹´ä½œç‚ºä¸»è¦æ›éšªæ•¸æ“š")
    
    # è¼¸å‡ºè©³ç´°çµæœçµ±è¨ˆ
    print(f"\nğŸ’¥ ç½å®³å½±éŸ¿åˆ†æçµæœ:")
    print(f"   å¹´å‡ç¸½æå¤± (AAI): ${impact.aai_agg/1e9:.2f}B")
    print(f"   ç¸½äº‹ä»¶æå¤±: ${impact.at_event.sum()/1e9:.2f}B")
    print(f"   æœ€å¤§å–®æ¬¡äº‹ä»¶æå¤±: ${impact.at_event.max()/1e9:.2f}B")
    print(f"   å—å½±éŸ¿äº‹ä»¶æ•¸: {(impact.at_event > 0).sum()}")
    
    # è¨ˆç®—å›æ­¸æœŸæå¤±
    freq_curve = impact.calc_freq_curve()
    rp_losses = {
        10: freq_curve.impact[np.argmin(np.abs(freq_curve.return_per - 10))],
        50: freq_curve.impact[np.argmin(np.abs(freq_curve.return_per - 50))], 
        100: freq_curve.impact[np.argmin(np.abs(freq_curve.return_per - 100))],
        500: freq_curve.impact[np.argmin(np.abs(freq_curve.return_per - 500))]
    }
    
    print(f"\nğŸ“Š å›æ­¸æœŸæå¤±ä¼°è¨ˆ:")
    for rp, loss in rp_losses.items():
        print(f"   {rp}å¹´å›æ­¸æœŸ: ${loss/1e9:.2f}B")
    
    print(f"ğŸ“ˆ ç½å®³å½±éŸ¿è¨ˆç®—å®Œæˆ")
    
    # æº–å‚™å®Œæ•´å¤šå¹´ä»½æ•¸æ“š
    climada_complete_data = {
        'tc_hazard': tc_hazard,
        'exposure_main': exposure_main,  # ä¸»è¦æ›éšªæ•¸æ“šï¼ˆæœ€æ–°å¹´ä»½ï¼‰
        'impact_main': impact,  # ä¸»è¦å½±éŸ¿æ•¸æ“šï¼ˆæœ€æ–°å¹´ä»½ï¼‰
        'impact_func_set': impact_func_set,
        
        # å¤šå¹´ä»½æ•¸æ“š
        'exposure_dict': exposure_dict,  # æ‰€æœ‰å¹´ä»½æ›éšªæ•¸æ“š
        'yearly_impacts': yearly_impacts,  # æ‰€æœ‰å¹´ä»½å½±éŸ¿æ•¸æ“š
        'successful_years': successful_years,  # æˆåŠŸè™•ç†çš„å¹´ä»½
        'yearly_exposures_summary': yearly_exposures_summary,  # å¹´ä»½æ‘˜è¦çµ±è¨ˆ
        
        # å‘å¾Œå…¼å®¹
        'exposure': exposure_main,
        'impact': impact,
        'event_losses': impact.at_event,
        'exposure_locations': [(lat, lon) for lat, lon in zip(exposure_main.latitude, exposure_main.longitude)],
        
        'metadata': {
            'n_events': tc_hazard.size,
            'total_exposure_latest': exposure_main.value.sum(),
            'annual_average_impact_latest': impact.aai_agg,
            'latest_year': latest_year,
            'successful_years': successful_years,
            'n_years_processed': len(successful_years),
            'generation_time': datetime.now().isoformat()
        }
    }
    print("   ğŸ‰ CLIMADAçœŸå¯¦æ•¸æ“šæº–å‚™å®Œæˆï¼")
    
    # ä¿å­˜åˆ°pickle
    import pickle
    with open('climada_complete_data.pkl', 'wb') as f:
        pickle.dump(climada_complete_data, f)
    print("ğŸ’¾ æ•¸æ“šå·²ä¿å­˜åˆ° climada_complete_data.pkl")
    
else:
    print("   âŒ ç„¡æ³•å‰µå»ºæ›éšªæ•¸æ“š")
    raise ValueError("ç„¡æ³•å‰µå»ºæ›éšªæ•¸æ“š")

print("\nğŸŠ å®Œæˆï¼CLIMADAå®Œæ•´å¤šå¹´ä»½æ•¸æ“šå°è±¡å·²ç”Ÿæˆ")
print("ğŸ“‹ æ•¸æ“šçµæ§‹èªªæ˜ï¼š")
print("  - exposure_main: ä¸»è¦æ›éšªæ•¸æ“šï¼ˆæœ€æ–°å¹´ä»½ï¼‰")
print("  - exposure_dict: æ‰€æœ‰å¹´ä»½æ›éšªæ•¸æ“šå­—å…¸")
print("  - yearly_impacts: æ‰€æœ‰å¹´ä»½å½±éŸ¿è¨ˆç®—çµæœ")
print("  - successful_years: æˆåŠŸè™•ç†çš„å¹´ä»½åˆ—è¡¨")
print("\nğŸ’» ä½¿ç”¨æ–¹å¼ï¼š")
print("  import pickle")
print("  with open('climada_complete_data.pkl', 'rb') as f:")
print("      data = pickle.load(f)")
print("  # è¨ªå•ç‰¹å®šå¹´ä»½: data['exposure_dict'][2023]")
print("  # æŸ¥çœ‹æ‰€æœ‰å¹´ä»½: data['successful_years']")
# %%

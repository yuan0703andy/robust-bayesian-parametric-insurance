#!/usr/bin/env python3
"""
05. Robust Bayesian Parametric Insurance Analysis - CPU Optimized with Œµ-Contamination
Á©©ÂÅ•Ë≤ùÊ∞èÂèÉÊï∏Âûã‰øùÈö™ÂàÜÊûê - CPUÂÑ™ÂåñŒµ-Ê±°ÊüìÁâàÊú¨

Complete implementation using CPU-optimized Œµ-contamination Bayesian analysis with 
full insurance_analysis_refactored framework integration.

ÂÆåÊï¥ÂØ¶Áèæ‰ΩøÁî®CPUÂÑ™ÂåñŒµ-Ê±°ÊüìË≤ùÊ∞èÂàÜÊûêÔºåÊï¥ÂêàÊâÄÊúâinsurance_analysis_refactoredÊ°ÜÊû∂„ÄÇ

New Features:
- Integrated Œµ-contamination theory: œÄ(Œ∏) = (1-Œµ)œÄ‚ÇÄ(Œ∏) + Œµq(Œ∏) 
- Complete 4-level hierarchical Bayesian model (‰∏çÁ∞°Âåñ)
- Theoretical contamination estimation + MCMC inference
- Extreme reparameterization for robust convergence
- Dual-process modeling: normal weather + typhoon events

Author: Research Team
Date: 2025-01-16 (Updated with Œµ-contamination framework)
"""

import os
import sys
import argparse
import pickle
import time
import numpy as np
import pandas as pd
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Any

# Force CPU-only execution
os.environ['JAX_PLATFORMS'] = 'cpu'
os.environ['PYTENSOR_FLAGS'] = 'device=cpu,floatX=float32,optimizer=fast_compile,allow_gc=True'

print("=" * 80)
print("05. Robust Bayesian Parametric Insurance - CPU Optimized with Œµ-Contamination")
print("Á©©ÂÅ•Ë≤ùÊ∞èÂèÉÊï∏Âûã‰øùÈö™ÂàÜÊûê - CPUÂÑ™ÂåñŒµ-Ê±°ÊüìÁâàÊú¨")
print("=" * 80)
print("\nüíª CPU-only mode: Stable, fast, and reliable")
print("üõ°Ô∏è Œµ-Contamination framework: œÄ(Œ∏) = (1-Œµ)œÄ‚ÇÄ(Œ∏) + Œµq(Œ∏)")
print("üöÄ Full insurance_analysis_refactored integration")
print("üî¨ Complete 4-level hierarchical Bayesian model (‰∏çÁ∞°Âåñ)")

# Import frameworks
print("\nüì¶ Loading frameworks...")

# Import insurance analysis framework
try:
    from insurance_analysis_refactored.core import (
        ParametricInsuranceEngine,
        SkillScoreEvaluator,
        create_standard_technical_premium_calculator,
        InsuranceProductManager,
        MultiObjectiveOptimizer,
        MarketAcceptabilityAnalyzer,
        TechnicalPremiumVisualizer
    )
    
    # Import specialized modules
    from insurance_analysis_refactored.core.enhanced_spatial_analysis import EnhancedCatInCircleAnalyzer
    from insurance_analysis_refactored.core.saffir_simpson_products import generate_steinmann_2023_products
    
    print("   ‚úÖ Complete insurance analysis framework loaded")
    HAS_INSURANCE_FRAMEWORK = True
except ImportError as e:
    print(f"   ‚ùå Insurance framework error: {e}")
    print("   üí° Will continue with available components")
    # Try individual imports
    try:
        from insurance_analysis_refactored.core import ParametricInsuranceEngine, SkillScoreEvaluator
        HAS_INSURANCE_FRAMEWORK = True
        print("   ‚úÖ Basic insurance framework loaded")
    except ImportError:
        HAS_INSURANCE_FRAMEWORK = False
        print("   ‚ùå Critical insurance framework components missing")
        sys.exit(1)

# Import Bayesian framework (CPU-optimized with Œµ-contamination)
try:
    from bayesian import (
        ModelClassAnalyzer, 
        ModelClassSpec, 
        AnalyzerConfig, 
        MCMCConfig,
        get_cpu_optimized_mcmc_config,
        configure_pymc_environment,
        ProbabilisticLossDistributionGenerator,
        # NEW: Integrated Œµ-contamination framework
        EpsilonContaminationMCMC,
        MCMCConfig as EpsilonMCMCConfig,
        quick_epsilon_contamination_mcmc,
        quick_contamination_analysis
    )
    print("   ‚úÖ CPU-optimized Bayesian framework with Œµ-contamination loaded")
    HAS_BAYESIAN = True
    HAS_EPSILON_CONTAMINATION = True
except ImportError as e:
    print(f"   ‚ö†Ô∏è Bayesian framework limited: {e}")
    HAS_BAYESIAN = False
    HAS_EPSILON_CONTAMINATION = False

# Configure PyMC for CPU
if HAS_BAYESIAN:
    configure_pymc_environment()
    print("   ‚úÖ PyMC configured for CPU execution")

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='CPU-Optimized Robust Bayesian Analysis')
    parser.add_argument('--quick-test', action='store_true',
                       help='Run with minimal samples for testing')
    parser.add_argument('--n-cores', type=int, default=None,
                       help='Number of CPU cores to use')
    parser.add_argument('--max-cores', type=int, default=None,
                       help='Maximum CPU cores to use (no limit if not specified)')
    parser.add_argument('--max-chains', type=int, default=None,
                       help='Maximum MCMC chains to use (auto-scale if not specified)')
    parser.add_argument('--high-performance', action='store_true',
                       help='Enable high-performance mode for workstation/server systems')
    parser.add_argument('--robust-sampling', action='store_true',
                       help='Enable robust sampling mode for difficult convergence (slower but more stable)')
    parser.add_argument('--balanced-mode', action='store_true',
                       help='Enable balanced mode (good convergence + reasonable speed) - RECOMMENDED')
    parser.add_argument('--verbose', action='store_true',
                       help='Enable verbose output')
    return parser.parse_args()

def detect_environment():
    """Detect system capabilities"""
    import multiprocessing
    import platform
    
    n_cores = multiprocessing.cpu_count()
    system = platform.system()
    
    # Try to get memory info
    try:
        import psutil
        memory_gb = psutil.virtual_memory().total / (1024**3)
        has_psutil = True
    except ImportError:
        memory_gb = None
        has_psutil = False
    
    # Determine system class
    if n_cores >= 32:
        system_class = "High-end Workstation/Server"
        recommended_cores = n_cores  # Use all cores
    elif n_cores >= 16:
        system_class = "High-end Desktop"
        recommended_cores = min(n_cores, 12)  # Leave some cores free
    elif n_cores >= 8:
        system_class = "Mid-range System"
        recommended_cores = min(n_cores, 8)
    else:
        system_class = "Entry-level System"
        recommended_cores = min(n_cores, 4)
    
    print(f"\nüîç System Detection:")
    print(f"   OS: {system}")
    print(f"   CPU cores: {n_cores}")
    print(f"   System class: {system_class}")
    print(f"   Python: {platform.python_version()}")
    if has_psutil and memory_gb:
        print(f"   Memory: {memory_gb:.1f} GB")
    
    # Performance recommendations
    if n_cores >= 16:
        print(f"\nüí° High-performance system detected!")
        print(f"   Consider using --high-performance flag")
        print(f"   Optimal MCMC: {min(n_cores//2, 16)} chains √ó {n_cores} cores")
    
    return {
        'n_cores': n_cores,
        'system': system,
        'system_class': system_class,
        'memory_gb': memory_gb,
        'recommended_cores': recommended_cores,
        'high_performance_capable': n_cores >= 16
    }

def load_analysis_data():
    """Load all required analysis data"""
    print("\nüìÇ Loading analysis data...")
    
    data = {}
    required_files = {
        'climada_data': 'results/climada_data/climada_complete_data.pkl',
        'spatial_analysis': 'results/spatial_analysis/cat_in_circle_results.pkl',
        'insurance_products': 'results/insurance_products/products.pkl',
        'traditional_analysis': 'results/traditional_basis_risk_analysis/analysis_results.pkl'
    }
    
    for key, filepath in required_files.items():
        try:
            with open(filepath, 'rb') as f:
                data[key] = pickle.load(f)
            print(f"   ‚úÖ {key.replace('_', ' ').title()} loaded")
        except FileNotFoundError:
            print(f"   ‚ö†Ô∏è {filepath} not found, using fallback")
            data[key] = None
        except Exception as e:
            print(f"   ‚ùå Error loading {filepath}: {e}")
            data[key] = None
    
    # Extract event losses from CLIMADA data
    if data['climada_data']:
        event_losses_array = data['climada_data'].get('event_losses')
        if event_losses_array is not None:
            data['event_losses'] = {i: loss for i, loss in enumerate(event_losses_array)}
            data['observed_losses'] = np.array([loss for loss in event_losses_array if loss > 0])
        else:
            data['event_losses'] = {}
            data['observed_losses'] = np.array([])
    else:
        # Create synthetic data for demonstration
        print("   üîß Creating synthetic event loss data for demonstration...")
        np.random.seed(42)  # Reproducible results
        synthetic_losses = np.random.lognormal(mean=15, sigma=1.5, size=100) * 1e6  # Log-normal losses
        
        # Add some zero-loss events
        all_events = np.concatenate([np.zeros(50), synthetic_losses])
        np.random.shuffle(all_events)
        
        data['event_losses'] = {i: loss for i, loss in enumerate(all_events)}
        data['observed_losses'] = np.array([loss for loss in all_events if loss > 0])
        print(f"   ‚úÖ Generated {len(all_events)} events with {len(data['observed_losses'])} non-zero losses")
    
    print(f"\nüìä Data Summary:")
    print(f"   Event losses: {len(data['event_losses'])} events")
    print(f"   Non-zero losses: {len(data['observed_losses'])} events")
    if len(data['observed_losses']) > 0:
        print(f"   Loss range: ${np.min(data['observed_losses'])/1e6:.1f}M - ${np.max(data['observed_losses'])/1e6:.1f}M")
        print(f"   Mean loss: ${np.mean(data['observed_losses'])/1e6:.1f}M")
    
    return data

def setup_cpu_environment(args, system_info):
    """Setup CPU-optimized environment"""
    print("\nüîß Setting up CPU-optimized environment...")
    
    # Determine cores to use
    if args.high_performance:
        print("üöÄ High-performance mode enabled")
        n_cores = args.n_cores if args.n_cores else system_info['n_cores']  # Use all cores
        max_cores = args.max_cores
        max_chains = args.max_chains
    else:
        n_cores = args.n_cores if args.n_cores else system_info['recommended_cores']
        max_cores = args.max_cores if args.max_cores else 8  # Conservative default
        max_chains = args.max_chains if args.max_chains else 4  # Conservative default
    
    # Get optimized MCMC configuration with new parameters
    mcmc_config_dict = get_cpu_optimized_mcmc_config(
        n_cores=n_cores, 
        quick_test=args.quick_test,
        max_cores=max_cores,
        max_chains=max_chains,
        robust_sampling=args.robust_sampling,
        balanced_mode=args.balanced_mode
    )
    
    # Display sampling mode
    if args.robust_sampling:
        print("üõ°Ô∏è Robust sampling mode enabled:")
        print("   ‚Ä¢ Ultra-conservative settings")
        print("   ‚Ä¢ Slower but maximum stability")
        print("   ‚Ä¢ Designed to eliminate divergences")
    elif args.balanced_mode:
        print("üéØ Balanced mode enabled:")
        print("   ‚Ä¢ Good convergence + reasonable speed")
        print("   ‚Ä¢ Recommended for most use cases")
        print("   ‚Ä¢ Should eliminate divergences quickly")
    
    print(f"üìä CPU Configuration:")
    print(f"   Cores to use: {n_cores}/{system_info['n_cores']}")
    print(f"   MCMC chains: {mcmc_config_dict['n_chains']}")
    print(f"   Samples per chain: {mcmc_config_dict['n_samples']}")
    print(f"   Total samples: {mcmc_config_dict['n_chains'] * mcmc_config_dict['n_samples']:,}")
    print(f"   Target accept: {mcmc_config_dict['target_accept']}")
    print(f"   Quick test mode: {args.quick_test}")
    
    return mcmc_config_dict

def run_parametric_insurance_design(data, args):
    """Run comprehensive parametric insurance design using insurance_analysis_refactored"""
    print("\n" + "=" * 80)
    print("Phase 1: Parametric Insurance Product Design")
    print("ÈöéÊÆµ1ÔºöÂèÉÊï∏Âûã‰øùÈö™Áî¢ÂìÅË®≠Ë®à")
    print("=" * 80)
    
    # Initialize parametric insurance engine
    engine = ParametricInsuranceEngine()
    
    # Get existing products or create new ones
    if data['insurance_products']:
        existing_products = data['insurance_products']
        print(f"   ‚úÖ Using {len(existing_products)} existing products")
    else:
        print("   üîß Creating new Steinmann-style products...")
        try:
            existing_products = generate_steinmann_2023_products()
            print(f"   ‚úÖ Created {len(existing_products)} new products")
        except NameError:
            # Fallback to simple products
            existing_products = [
                {
                    'product_id': f'fallback_{i}',
                    'trigger_type': 'wind_speed',
                    'trigger_threshold': 30 + i * 10,
                    'coverage_amount': 1e8 + i * 5e7,
                    'premium_rate': 0.02 + i * 0.01
                }
                for i in range(5)
            ]
            print(f"   ‚úÖ Created {len(existing_products)} fallback products")
    
    # Initialize product manager
    try:
        product_manager = InsuranceProductManager()
    except NameError:
        product_manager = None
    
    # Enhanced spatial analysis if data available
    spatial_results = None
    if data['spatial_analysis']:
        print("\nüåç Running enhanced spatial analysis...")
        try:
            spatial_analyzer = EnhancedCatInCircleAnalyzer()
            spatial_results = spatial_analyzer.analyze_spatial_basis_risk(
                data['spatial_analysis'], 
                existing_products
            )
            print("   ‚úÖ Spatial analysis complete")
        except (NameError, Exception) as e:
            print(f"   ‚ö†Ô∏è Spatial analysis skipped: {e}")
            spatial_results = None
    
    # Calculate technical premiums
    print("\nüí∞ Calculating technical premiums...")
    premium_calculator = create_standard_technical_premium_calculator()
    
    # Process products with technical premiums
    enhanced_products = []
    for i, product in enumerate(existing_products):
        try:
            # Calculate premium - convert to ParametricProduct if needed
            if isinstance(product, dict):
                # Convert dict to ParametricProduct for calculator
                from insurance_analysis_refactored.core import ParametricProduct, ParametricIndexType, PayoutFunctionType
                parametric_product = ParametricProduct(
                    product_id=product.get('product_id', f'product_{i}'),
                    name=product.get('name', f'Product {i}'),
                    description=product.get('description', 'Generated product'),
                    index_type=ParametricIndexType.CAT_IN_CIRCLE,
                    payout_function_type=PayoutFunctionType.STEP,
                    trigger_thresholds=product.get('trigger_thresholds', [30.0]),
                    payout_amounts=product.get('payout_amounts', [1e8]),
                    max_payout=product.get('max_payout', 1e8)
                )
            else:
                parametric_product = product
            
            # Use fallback hazard indices if no data
            hazard_indices = data['observed_losses'] if len(data['observed_losses']) > 0 else np.array([30, 40, 50, 60, 70])
            
            premium_data = premium_calculator.calculate_technical_premium(
                parametric_product, 
                hazard_indices
            )
            
            # Enhance product with premium data
            enhanced_product = {
                **product,
                'technical_premium': premium_data,
                'product_id': f"product_{i:03d}"
            }
            enhanced_products.append(enhanced_product)
            
        except Exception as e:
            if args.verbose:
                print(f"   ‚ö†Ô∏è Premium calculation failed for product {i}: {e}")
            continue
    
    print(f"   ‚úÖ Technical premiums calculated for {len(enhanced_products)} products")
    
    return {
        'products': enhanced_products,
        'spatial_results': spatial_results,
        'engine': engine,
        'product_manager': product_manager
    }

def run_bayesian_analysis(data, mcmc_config_dict, insurance_results, args):
    """Run CPU-optimized Œµ-contamination Bayesian analysis"""
    print("\n" + "=" * 80)
    print("Phase 2: CPU-Optimized Œµ-Contamination Bayesian Analysis")
    print("ÈöéÊÆµ2ÔºöCPUÂÑ™ÂåñŒµ-Ê±°ÊüìË≤ùÊ∞èÂàÜÊûê")
    print("=" * 80)
    
    if not HAS_BAYESIAN or not HAS_EPSILON_CONTAMINATION or len(data['observed_losses']) < 10:
        print("‚ö†Ô∏è Skipping Œµ-contamination analysis (insufficient data or missing framework)")
        return None
    
    # üõ°Ô∏è Robust Bayesian ÂéüÂâáÔºö‰ΩøÁî®ÂéüÂßãÊï∏ÊìöÔºå‰ΩÜËΩâÊèõÁÇ∫ÂêàÁêÜÂ∞∫Â∫¶
    raw_losses = data['observed_losses'].copy()
    if len(raw_losses) > 0:
        # Âè™ÂÅöÂ∞∫Â∫¶ËΩâÊèõÔºàÈô§‰ª•1MÔºâÔºå‰øùÊåÅÊï∏ÊìöÁöÑÂéüÂßãÂàÜÂ∏ÉÁâπÊÄß
        # ÈÄôÊ®£Êó¢ÈÅøÂÖçÊï∏ÂÄºÂïèÈ°åÔºåÂèà‰∏çÊúÉÈÅéÂ∫¶Êìç‰ΩúÊï∏Êìö
        analysis_data = raw_losses / 1e6  # ËΩâÊèõÁÇ∫ÁôæËê¨ÁæéÂÖÉÂñÆ‰Ωç
        
        print(f"   üìä RobustÊï∏ÊìöËôïÁêÜ:")
        print(f"      ÂéüÂßãÁØÑÂúç: ${np.min(raw_losses)/1e6:.1f}M - ${np.max(raw_losses)/1e6:.1f}M")
        print(f"      ÂàÜÊûêÂñÆ‰Ωç: {np.min(analysis_data):.2f} - {np.max(analysis_data):.2f} (ÁôæËê¨ÁæéÂÖÉ)")
        print(f"      ‰øùÊåÅÂéüÂßãÂàÜÂ∏ÉÁâπÊÄßÔºåÁ¨¶ÂêàRobust BayesianÂéüÂâá")
    else:
        analysis_data = raw_losses
    
    print(f"\nüî¨ Œµ-Contamination Framework Configuration:")
    
    # Step 1: ÁêÜË´ñÊ±°ÊüìÁ®ãÂ∫¶‰º∞Ë®à
    print(f"   Step 1: ÁêÜË´ñÊ±°ÊüìÁ®ãÂ∫¶‰º∞Ë®à...")
    try:
        contamination_estimate = quick_contamination_analysis(analysis_data)
        theoretical_epsilon = contamination_estimate.epsilon_consensus
        epsilon_uncertainty = contamination_estimate.epsilon_uncertainty
        
        print(f"      üí° ÁêÜË´ñÂª∫Ë≠∞: Œµ = {theoretical_epsilon:.3f} ¬± {epsilon_uncertainty:.3f}")
        print(f"      üå™Ô∏è Ëß£Èáã: {theoretical_epsilon:.1%} È¢±È¢®‰∫ã‰ª∂ + {(1-theoretical_epsilon):.1%} Ê≠£Â∏∏Â§©Ê∞£")
        
        # Ê†πÊìöÁêÜË´ñ‰º∞Ë®àË™øÊï¥ŒµÁØÑÂúç
        if args.quick_test:
            epsilon_values = [max(0.01, theoretical_epsilon)]
        else:
            epsilon_values = [
                max(0.01, theoretical_epsilon - 0.03),
                theoretical_epsilon,
                min(0.15, theoretical_epsilon + 0.03)
            ]
    except Exception as e:
        print(f"      ‚ö†Ô∏è ÁêÜË´ñÂàÜÊûêÂ§±Êïó: {e}")
        epsilon_values = [0.05] if args.quick_test else [0.01, 0.05, 0.1]
        theoretical_epsilon = None
    
    # Step 2: MCMCÈöéÂ±§Ê®°ÂûãÈÖçÁΩÆ
    print(f"   Step 2: MCMCÈöéÂ±§Ê®°ÂûãÈÖçÁΩÆ...")
    print(f"      ÂàÜÊûêŒµÂÄº: {epsilon_values}")
    print(f"      MCMCË®≠ÁΩÆ: {mcmc_config_dict['n_samples']} samples, {mcmc_config_dict['n_warmup']} warmup")
    print(f"      üéØ ÁõÆÊ®ôÊé•ÂèóÁéá: {mcmc_config_dict['target_accept']} (Ê•µÈ´òÊî∂ÊñÇÊ®ôÊ∫ñ)")
    
    # Create Œµ-contamination MCMC configuration
    epsilon_mcmc_config = EpsilonMCMCConfig(
        n_samples=mcmc_config_dict["n_samples"],
        n_warmup=mcmc_config_dict["n_warmup"],
        n_chains=mcmc_config_dict["n_chains"],
        target_accept=mcmc_config_dict["target_accept"],
        max_treedepth=mcmc_config_dict.get("max_treedepth", 20),
        standardize_data=True,
        log_transform=False
    )
    
    # Step 3: Âü∑Ë°åÁ∂úÂêàŒµ-contaminationÂàÜÊûê
    print(f"\nüöÄ Âü∑Ë°åÁ∂úÂêàŒµ-contamination MCMCÂàÜÊûê...")
    print(f"   Êï∏ÊìöÈªûÊï∏: {len(analysis_data)}")
    print(f"   üõ°Ô∏è ÂÆåÊï¥4Â±§ÈöéÂ±§Œµ-contaminationÊ®°Âûã (‰∏çÁ∞°Âåñ)")
    print(f"   üìä ÁêÜË´ñÊ°ÜÊû∂: œÄ(Œ∏) = (1-Œµ)œÄ‚ÇÄ(Œ∏) + Œµq(Œ∏)")
    
    start_time = time.time()
    
    try:
        # ‰ΩøÁî®Á∂úÂêàÂàÜÊûêÂáΩÊï∏
        mcmc_sampler = EpsilonContaminationMCMC(epsilon_mcmc_config)
        comprehensive_results = mcmc_sampler.comprehensive_contamination_analysis(
            analysis_data, 
            epsilon_values=epsilon_values,
            include_theory=True
        )
        
        elapsed = time.time() - start_time
        
        # ÊèêÂèñÁµêÊûú
        mcmc_analysis = comprehensive_results.get('mcmc_analysis', {})
        best_model = mcmc_analysis.get('best_model')
        successful_models = mcmc_analysis.get('successful_models', [])
        
        print(f"\n‚úÖ Œµ-Contamination ÂàÜÊûêÂÆåÊàê!")
        print(f"   Âü∑Ë°åÊôÇÈñì: {elapsed:.1f} seconds")
        print(f"   ÊàêÂäüÊî∂ÊñÇ: {len(successful_models)}/{len(epsilon_values)}")
        
        if best_model:
            print(f"   üèÜ ÊúÄ‰Ω≥Ê®°Âûã: Œµ = {best_model.epsilon_value:.3f}")
            print(f"   üìà DICË©ïÂàÜ: {best_model.dic:.2f}")
            print(f"   üî¨ Êî∂ÊñÇÂìÅË≥™: R-hat = {best_model.rhat_max:.4f}")
            print(f"   üìä ÊúâÊïàÊ®£Êú¨: ESS = {best_model.ess_min:.0f}")
        
        # ‰øùÂ≠òÁ∂úÂêàÂ†±Âëä
        if args.verbose:
            print(f"\nüìÑ Á∂úÂêàÂàÜÊûêÂ†±Âëä:")
            print(comprehensive_results.get('comprehensive_report', 'Â†±ÂëäÁîüÊàêÂ§±Êïó'))
        
        # ÂâµÂª∫ÂÖºÂÆπÁöÑÁµêÊûúÊ†ºÂºè
        bayesian_results = {
            'comprehensive_results': comprehensive_results,
            'best_model': best_model,
            'successful_models': successful_models,
            'theoretical_epsilon': theoretical_epsilon,
            'epsilon_uncertainty': epsilon_uncertainty if 'epsilon_uncertainty' in locals() else None,
            'execution_time': elapsed,
            'epsilon_contamination_framework': True,
            'model_type': 'Œµ-contamination_hierarchical'
        }
        
        return bayesian_results
        
    except Exception as e:
        print(f"\n‚ö†Ô∏è Œµ-Contamination ÂàÜÊûêÂ§±Êïó: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        
        # Â¶ÇÊûúŒµ-contaminationÂ§±ÊïóÔºåÂòóË©¶Âü∫Êú¨ÂàÜÊûê
        print(f"\nüîÑ Fallback to basic Bayesian analysis...")
        return run_fallback_bayesian_analysis(data, mcmc_config_dict, args, analysis_data)


def run_fallback_bayesian_analysis(data, mcmc_config_dict, args, analysis_data):
    """Fallback basic Bayesian analysis if Œµ-contamination fails"""
    try:
        # Create MCMC configuration
        mcmc_config = MCMCConfig(
            n_samples=mcmc_config_dict["n_samples"],
            n_warmup=mcmc_config_dict["n_warmup"],
            n_chains=mcmc_config_dict["n_chains"],
            cores=mcmc_config_dict["cores"],
            target_accept=mcmc_config_dict["target_accept"]
        )
        
        # Create analyzer configuration
        analyzer_config = AnalyzerConfig(
            mcmc_config=mcmc_config,
            use_mpe=False,
            parallel_execution=False,
            max_workers=1,
            model_selection_criterion='dic',
            calculate_ranges=True,
            calculate_weights=True
        )
        
        # Model specification
        model_class_spec = ModelClassSpec(
            enable_epsilon_contamination=False,  # Âü∫Êú¨Ê®°Âûã
            epsilon_values=[],
            contamination_distribution="none"
        )
        
        print(f"   Âü∫Êú¨BayesianÊ®°ÂûãÈÖçÁΩÆÂÆåÊàê")
        
        # Create analyzer
        analyzer = ModelClassAnalyzer(model_class_spec, analyzer_config)
        
        # Run analysis
        start_time = time.time()
        ensemble_results = analyzer.analyze_model_class(analysis_data)
        elapsed = time.time() - start_time
        
        print(f"   ‚úÖ Âü∫Êú¨BayesianÂàÜÊûêÂÆåÊàê ({elapsed:.1f}s)")
        
        return {
            'ensemble_results': ensemble_results,
            'best_model': ensemble_results.best_model if ensemble_results else None,
            'execution_time': elapsed,
            'epsilon_contamination_framework': False,
            'model_type': 'basic_hierarchical'
        }
        
    except Exception as e:
        print(f"   ‚ùå Âü∫Êú¨BayesianÂàÜÊûê‰πüÂ§±Êïó: {e}")
        return None

def run_skill_evaluation(data, insurance_results, bayesian_results, args):
    """Run comprehensive skill score evaluation"""
    print("\n" + "=" * 80)
    print("Phase 3: Skill Score Evaluation")
    print("ÈöéÊÆµ3ÔºöÊäÄËÉΩË©ïÂàÜË©ï‰º∞")
    print("=" * 80)
    
    skill_evaluator = SkillScoreEvaluator()
    
    # Get predictions from different sources
    predictions_sources = {}
    
    # 1. Insurance product predictions
    if insurance_results and insurance_results['products']:
        # Use first product as baseline
        baseline_product = insurance_results['products'][0]
        if 'technical_premium' in baseline_product:
            premium_value = baseline_product['technical_premium'].get('expected_premium', 0)
            predictions_sources['insurance_premium'] = np.full(
                len(data['observed_losses']), premium_value
            )
    
    # 2. Œµ-contamination Bayesian model predictions
    if bayesian_results:
        if bayesian_results.get('epsilon_contamination_framework', False):
            # New Œµ-contamination framework results
            best_model = bayesian_results.get('best_model')
            if best_model and hasattr(best_model, 'posterior_samples'):
                posterior_samples = best_model.posterior_samples
                if 'theta' in posterior_samples:
                    predictions_sources['epsilon_contamination_model'] = np.full(
                        len(data['observed_losses']), 
                        np.mean(posterior_samples['theta'])
                    )
                    
                # Add theoretical epsilon prediction
                theoretical_epsilon = bayesian_results.get('theoretical_epsilon')
                if theoretical_epsilon:
                    predictions_sources['theoretical_contamination'] = np.full(
                        len(data['observed_losses']), 
                        theoretical_epsilon * np.max(data['observed_losses']) if len(data['observed_losses']) > 0 else 0
                    )
        else:
            # Fallback basic Bayesian results
            ensemble_results = bayesian_results.get('ensemble_results')
            if ensemble_results and hasattr(ensemble_results, 'individual_results'):
                best_model_result = ensemble_results.individual_results[ensemble_results.best_model]
                if hasattr(best_model_result, 'posterior_samples'):
                    posterior_samples = best_model_result.posterior_samples
                    if 'theta' in posterior_samples:
                        predictions_sources['basic_bayesian_model'] = np.full(
                            len(data['observed_losses']), 
                            np.mean(posterior_samples['theta'])
                        )
    
    # 3. Simple statistical predictions
    if len(data['observed_losses']) > 0:
        predictions_sources['mean_baseline'] = np.full(
            len(data['observed_losses']), 
            np.mean(data['observed_losses'])
        )
        predictions_sources['median_baseline'] = np.full(
            len(data['observed_losses']), 
            np.median(data['observed_losses'])
        )
    
    # Evaluate each prediction source
    skill_results = {}
    for source_name, predictions in predictions_sources.items():
        if len(predictions) == len(data['observed_losses']):
            # Calculate basic skill scores
            rmse = np.sqrt(np.mean((predictions - data['observed_losses'])**2))
            mae = np.mean(np.abs(predictions - data['observed_losses']))
            correlation = np.corrcoef(predictions, data['observed_losses'])[0, 1] if len(predictions) > 1 else 0
            
            skill_results[source_name] = {
                'RMSE': rmse,
                'MAE': mae,
                'Correlation': correlation,
                'RMSE_normalized': rmse / np.mean(data['observed_losses']) if np.mean(data['observed_losses']) > 0 else np.inf
            }
            
            print(f"\nüìä {source_name.replace('_', ' ').title()} Skill Scores:")
            print(f"   RMSE: ${rmse/1e6:.1f}M")
            print(f"   MAE: ${mae/1e6:.1f}M")
            print(f"   Correlation: {correlation:.4f}")
            print(f"   Normalized RMSE: {skill_results[source_name]['RMSE_normalized']:.4f}")
    
    return skill_results

def run_market_analysis(insurance_results, args):
    """Run market acceptability analysis"""
    print("\n" + "=" * 80)
    print("Phase 4: Market Acceptability Analysis")
    print("ÈöéÊÆµ4ÔºöÂ∏ÇÂ†¥Êé•ÂèóÂ∫¶ÂàÜÊûê")
    print("=" * 80)
    
    if not insurance_results or not insurance_results['products']:
        print("‚ö†Ô∏è Skipping market analysis (no products available)")
        return None
    
    # Initialize market analyzer
    market_analyzer = MarketAcceptabilityAnalyzer()
    
    # Analyze products
    products = insurance_results['products'][:10]  # Limit for performance
    
    market_results = []
    for product in products:
        try:
            # Market analysis for each product
            market_score = market_analyzer.analyze_product_acceptability(product)
            market_results.append({
                'product_id': product.get('product_id', 'unknown'),
                'market_score': market_score,
                'product_summary': {
                    'trigger_type': product.get('trigger_type', 'unknown'),
                    'coverage_amount': product.get('coverage_amount', 0)
                }
            })
        except Exception as e:
            if args.verbose:
                print(f"   ‚ö†Ô∏è Market analysis failed for product: {e}")
            continue
    
    print(f"‚úÖ Market analysis complete for {len(market_results)} products")
    
    # Summary statistics
    if market_results:
        scores = [r['market_score'] for r in market_results if r['market_score'] is not None]
        if scores:
            print(f"   Mean market score: {np.mean(scores):.3f}")
            print(f"   Best market score: {np.max(scores):.3f}")
    
    return market_results

def save_comprehensive_results(data, insurance_results, bayesian_results, 
                              skill_results, market_results, args, mcmc_config_dict):
    """Save all analysis results"""
    print("\n" + "=" * 80)
    print("Phase 5: Save Comprehensive Results")
    print("ÈöéÊÆµ5ÔºöÂÑ≤Â≠òÁ∂úÂêàÁµêÊûú")
    print("=" * 80)
    
    results_dir = Path('results/robust_bayesian_cpu_comprehensive')
    results_dir.mkdir(parents=True, exist_ok=True)
    
    # Compile comprehensive results
    final_results = {
        'analysis_metadata': {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'version': 'CPU-Optimized v1.0',
            'quick_test': args.quick_test,
            'n_cores_used': mcmc_config_dict.get('cores', 'unknown'),
            'framework_versions': {
                'has_bayesian': HAS_BAYESIAN,
                'has_insurance': HAS_INSURANCE_FRAMEWORK
            }
        },
        'data_summary': {
            'n_events': len(data['event_losses']),
            'n_nonzero_losses': len(data['observed_losses']),
            'mean_loss': np.mean(data['observed_losses']) if len(data['observed_losses']) > 0 else 0,
            'max_loss': np.max(data['observed_losses']) if len(data['observed_losses']) > 0 else 0
        },
        'insurance_analysis': {
            'n_products': len(insurance_results['products']) if insurance_results else 0,
            'spatial_analysis_available': insurance_results['spatial_results'] is not None if insurance_results else False
        },
        'bayesian_analysis': {
            'completed': bayesian_results is not None,
            'framework_type': bayesian_results.get('model_type', 'unknown') if bayesian_results else None,
            'epsilon_contamination_used': bayesian_results.get('epsilon_contamination_framework', False) if bayesian_results else False,
            'best_model': (bayesian_results.get('best_model').epsilon_value if hasattr(bayesian_results.get('best_model', {}), 'epsilon_value') 
                          else bayesian_results.get('best_model')) if bayesian_results else None,
            'theoretical_epsilon': bayesian_results.get('theoretical_epsilon') if bayesian_results else None,
            'epsilon_uncertainty': bayesian_results.get('epsilon_uncertainty') if bayesian_results else None,
            'execution_time': bayesian_results.get('execution_time') if bayesian_results else None,
            'successful_models': len(bayesian_results.get('successful_models', [])) if bayesian_results else 0,
            'n_models_evaluated': (len(bayesian_results.get('successful_models', [])) if bayesian_results.get('epsilon_contamination_framework') 
                                  else len(bayesian_results.get('ensemble_results', {}).get('individual_results', {})) if bayesian_results else 0)
        },
        'skill_evaluation': skill_results,
        'market_analysis': {
            'completed': market_results is not None,
            'n_products_analyzed': len(market_results) if market_results else 0
        }
    }
    
    # Save main results
    with open(results_dir / 'comprehensive_results.pkl', 'wb') as f:
        pickle.dump({
            'final_results': final_results,
            'insurance_results': insurance_results,
            'bayesian_results': bayesian_results,
            'skill_results': skill_results,
            'market_results': market_results,
            'data': data
        }, f)
    
    # Save summary report
    with open(results_dir / 'analysis_report.txt', 'w') as f:
        f.write("CPU-Optimized Robust Bayesian Analysis Report\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"Analysis completed: {final_results['analysis_metadata']['timestamp']}\n")
        f.write(f"Version: {final_results['analysis_metadata']['version']}\n")
        f.write(f"Quick test mode: {final_results['analysis_metadata']['quick_test']}\n\n")
        
        f.write("Data Summary:\n")
        f.write(f"  Total events: {final_results['data_summary']['n_events']}\n")
        f.write(f"  Non-zero losses: {final_results['data_summary']['n_nonzero_losses']}\n")
        f.write(f"  Mean loss: ${final_results['data_summary']['mean_loss']/1e6:.1f}M\n")
        f.write(f"  Max loss: ${final_results['data_summary']['max_loss']/1e6:.1f}M\n\n")
        
        f.write("Analysis Components:\n")
        f.write(f"  Insurance products: {final_results['insurance_analysis']['n_products']}\n")
        f.write(f"  Bayesian analysis: {'‚úÖ' if final_results['bayesian_analysis']['completed'] else '‚ùå'}\n")
        f.write(f"  Market analysis: {'‚úÖ' if final_results['market_analysis']['completed'] else '‚ùå'}\n")
        
        if final_results['bayesian_analysis']['completed']:
            f.write(f"  Best Bayesian model: {final_results['bayesian_analysis']['best_model']}\n")
            f.write(f"  Execution time: {final_results['bayesian_analysis']['execution_time']:.1f}s\n")
        
        f.write("\nSkill Score Summary:\n")
        for source, scores in skill_results.items():
            f.write(f"  {source}:\n")
            f.write(f"    RMSE: ${scores['RMSE']/1e6:.1f}M\n")
            f.write(f"    Correlation: {scores['Correlation']:.4f}\n")
    
    print(f"‚úÖ Results saved to: {results_dir}")
    
    return results_dir, final_results

def main():
    """Main analysis workflow"""
    print("üöÄ Starting CPU-Optimized Robust Bayesian Analysis")
    
    # Parse arguments
    args = parse_arguments()
    
    # Detect system capabilities
    system_info = detect_environment()
    
    # Setup CPU environment
    mcmc_config_dict = setup_cpu_environment(args, system_info)
    
    # Load data
    data = load_analysis_data()
    
    # Phase 1: Parametric insurance design
    insurance_results = run_parametric_insurance_design(data, args)
    
    # Phase 2: Bayesian analysis
    bayesian_results = run_bayesian_analysis(data, mcmc_config_dict, insurance_results, args)
    
    # Phase 3: Skill evaluation
    skill_results = run_skill_evaluation(data, insurance_results, bayesian_results, args)
    
    # Phase 4: Market analysis
    market_results = run_market_analysis(insurance_results, args)
    
    # Phase 5: Save results
    results_dir, final_results = save_comprehensive_results(
        data, insurance_results, bayesian_results, skill_results, 
        market_results, args, mcmc_config_dict
    )
    
    # Final summary
    print("\n" + "=" * 80)
    print("üéâ CPU-Optimized Œµ-Contamination Analysis Complete!")
    print("=" * 80)
    print(f"‚úÖ Results directory: {results_dir}")
    print(f"‚úÖ Insurance products: {final_results['insurance_analysis']['n_products']}")
    
    # Enhanced Bayesian analysis summary
    bayesian_info = final_results['bayesian_analysis']
    if bayesian_info['completed']:
        print(f"‚úÖ Bayesian analysis: Success ({bayesian_info['framework_type']})")
        
        if bayesian_info['epsilon_contamination_used']:
            print(f"üõ°Ô∏è Œµ-Contamination Framework:")
            print(f"   ‚Ä¢ Theoretical Œµ: {bayesian_info['theoretical_epsilon']:.3f} ¬± {bayesian_info.get('epsilon_uncertainty', 0):.3f}")
            print(f"   ‚Ä¢ Best model Œµ: {bayesian_info['best_model']}")
            print(f"   ‚Ä¢ Successful models: {bayesian_info['successful_models']}/{bayesian_info['n_models_evaluated']}")
            print(f"   ‚Ä¢ Mathematical framework: œÄ(Œ∏) = (1-Œµ)œÄ‚ÇÄ(Œ∏) + Œµq(Œ∏)")
        else:
            print(f"üîÑ Fallback Bayesian Analysis (basic hierarchical)")
            
        print(f"‚è±Ô∏è Execution time: {bayesian_info['execution_time']:.1f} seconds")
    else:
        print(f"‚ö†Ô∏è Bayesian analysis: Skipped")
    
    print(f"‚úÖ Market analysis: {'Success' if final_results['market_analysis']['completed'] else 'Skipped'}")
    print(f"‚úÖ CPU cores used: {mcmc_config_dict['cores']}")
    
    print("\nüí° Key advantages of Œµ-contamination framework:")
    print("   ‚Ä¢ Complete robust Bayesian theory implementation")
    print("   ‚Ä¢ Dual-process modeling: normal weather + typhoon events") 
    print("   ‚Ä¢ Theoretical validation + MCMC inference")
    print("   ‚Ä¢ 4-level hierarchical model (‰∏çÁ∞°Âåñ)")
    print("   ‚Ä¢ Extreme reparameterization for convergence")
    print("   ‚Ä¢ CPU-optimized for stability and reliability")
    print("=" * 80)
    
    return final_results

if __name__ == "__main__":
    main()
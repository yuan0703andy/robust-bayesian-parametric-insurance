#!/usr/bin/env python3
"""
å¯è©®é‡‹æ€§æ¡†æ¶ (Interpretability Framework)
ç‚ºåƒæ•¸ä¿éšªå„ªåŒ–çµæœæä¾›å…¨é¢çš„å¯è©®é‡‹æ€§åˆ†æ

æœ¬æ¨¡çµ„å›æ‡‰å•é¡Œ4ï¼šæ¡†æ¶çš„ã€Œå¯è©®é‡‹æ€§ (Interpretability)ã€æ”¹é€²
- è§£é‡‹ç‚ºä»€éº¼æŸå€‹ç”¢å“è¢«é¸ç‚ºã€Œæœ€ä½³ã€
- åˆ†æé æ¸¬è®Šæ•¸çš„é‡è¦æ€§å’Œè²¢ç»
- è¦–è¦ºåŒ–åŸºå·®é¢¨éšªèˆ‡ç”¢å“åƒæ•¸çš„é—œä¿‚æ›²é¢
- æä¾›æ¨¡å‹æ±ºç­–çš„é€æ˜åŒ–è§£é‡‹

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. è®Šæ•¸é‡è¦æ€§åˆ†æ (Feature Importance Analysis)
2. é—œä¿‚æ›²é¢è¦–è¦ºåŒ– (Response Surface Visualization)
3. æ±ºç­–è·¯å¾‘è¿½è¹¤ (Decision Path Tracking)
4. æ•æ„Ÿæ€§æ¢¯åº¦åˆ†æ (Sensitivity Gradient Analysis)
5. åäº‹å¯¦åˆ†æ (Counterfactual Analysis)

Author: Research Team
Date: 2025-01-10
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns
from typing import Dict, List, Tuple, Any, Optional, Callable
from dataclasses import dataclass
import warnings
from pathlib import Path
from scipy import stats
from scipy.interpolate import griddata
from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import partial_dependence, PartialDependenceDisplay
import shap

# å°å…¥åŸºç¤æ¨¡çµ„
from skill_scores.basis_risk_functions import (
    BasisRiskCalculator, 
    BasisRiskType, 
    create_basis_risk_function
)

# è¨­å®šä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Heiti TC', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

warnings.filterwarnings('ignore')

@dataclass
class ProductCandidate:
    """ç”¢å“å€™é¸è€…è³‡æ–™çµæ§‹"""
    trigger_threshold: float
    payout_amount: float
    max_payout: float
    basis_risk: float
    trigger_rate: float
    expected_payout: float
    market_acceptability: float
    technical_premium: float
    product_id: str = ""

@dataclass
class InterpretabilityConfig:
    """å¯è©®é‡‹æ€§åˆ†æé…ç½®"""
    surface_resolution: int = 50          # é—œä¿‚æ›²é¢è§£æåº¦
    importance_method: str = "permutation" # é‡è¦æ€§è¨ˆç®—æ–¹æ³•
    shap_samples: int = 100               # SHAP æ¨£æœ¬æ•¸
    gradient_epsilon: float = 0.01        # æ¢¯åº¦è¨ˆç®—çš„æ“¾å‹•é‡
    counterfactual_samples: int = 20      # åäº‹å¯¦åˆ†ææ¨£æœ¬æ•¸
    output_dir: str = "results/interpretability"

class VariableImportanceAnalyzer:
    """è®Šæ•¸é‡è¦æ€§åˆ†æå™¨"""
    
    def __init__(self, config: InterpretabilityConfig):
        """
        åˆå§‹åŒ–è®Šæ•¸é‡è¦æ€§åˆ†æå™¨
        
        Parameters:
        -----------
        config : InterpretabilityConfig
            é…ç½®åƒæ•¸
        """
        self.config = config
        self.basis_risk_calc = BasisRiskCalculator()
    
    def calculate_permutation_importance(self,
                                       products: List[ProductCandidate],
                                       target_metric: str = "basis_risk") -> Dict[str, float]:
        """
        è¨ˆç®—æ’åˆ—é‡è¦æ€§ (Permutation Importance)
        
        é€šééš¨æ©Ÿæ‰“äº‚æ¯å€‹ç‰¹å¾µä¾†æ¸¬é‡å…¶å°ç›®æ¨™æŒ‡æ¨™çš„å½±éŸ¿
        
        Parameters:
        -----------
        products : List[ProductCandidate]
            ç”¢å“å€™é¸æ¸…å–®
        target_metric : str
            ç›®æ¨™æŒ‡æ¨™åç¨±
            
        Returns:
        --------
        Dict[str, float]
            å„ç‰¹å¾µçš„é‡è¦æ€§åˆ†æ•¸
        """
        
        print("ğŸ” è¨ˆç®—æ’åˆ—é‡è¦æ€§...")
        
        if not products:
            return {}
        
        # æº–å‚™ç‰¹å¾µçŸ©é™£å’Œç›®æ¨™å‘é‡
        feature_names = ['trigger_threshold', 'payout_amount', 'max_payout', 
                        'trigger_rate', 'expected_payout', 'market_acceptability']
        
        X = np.array([[
            getattr(p, feat) for feat in feature_names
        ] for p in products])
        
        y = np.array([getattr(p, target_metric) for p in products])
        
        # å»ºç«‹åŸºæº–æ¨¡å‹ (éš¨æ©Ÿæ£®æ—)
        rf = RandomForestRegressor(n_estimators=100, random_state=42)
        rf.fit(X, y)
        baseline_score = rf.score(X, y)
        
        importance_scores = {}
        
        # å°æ¯å€‹ç‰¹å¾µé€²è¡Œæ’åˆ—æ¸¬è©¦
        for i, feature_name in enumerate(feature_names):
            X_permuted = X.copy()
            np.random.seed(42)
            np.random.shuffle(X_permuted[:, i])  # æ‰“äº‚ç¬¬iå€‹ç‰¹å¾µ
            
            rf_permuted = RandomForestRegressor(n_estimators=100, random_state=42)
            rf_permuted.fit(X_permuted, y)
            permuted_score = rf_permuted.score(X_permuted, y)
            
            # é‡è¦æ€§ = åŸºæº–åˆ†æ•¸ - æ‰“äº‚å¾Œåˆ†æ•¸
            importance_scores[feature_name] = baseline_score - permuted_score
        
        print(f"âœ… æ’åˆ—é‡è¦æ€§è¨ˆç®—å®Œæˆ")
        return importance_scores
    
    def calculate_gradient_importance(self,
                                    objective_function: Callable,
                                    optimal_product: ProductCandidate,
                                    feature_ranges: Dict[str, Tuple[float, float]]) -> Dict[str, float]:
        """
        è¨ˆç®—æ¢¯åº¦é‡è¦æ€§ (åŸºæ–¼æ•¸å€¼å¾®åˆ†)
        
        Parameters:
        -----------
        objective_function : Callable
            ç›®æ¨™å‡½æ•¸ (ç”¢å“åƒæ•¸ -> åŸºå·®é¢¨éšª)
        optimal_product : ProductCandidate
            æœ€ä½³ç”¢å“
        feature_ranges : Dict[str, Tuple[float, float]]
            å„ç‰¹å¾µçš„å–å€¼ç¯„åœ
            
        Returns:
        --------
        Dict[str, float]
            å„ç‰¹å¾µçš„æ¢¯åº¦é‡è¦æ€§
        """
        
        print("ğŸ“ˆ è¨ˆç®—æ¢¯åº¦é‡è¦æ€§...")
        
        epsilon = self.config.gradient_epsilon
        base_params = {
            'trigger_threshold': optimal_product.trigger_threshold,
            'payout_amount': optimal_product.payout_amount,
            'max_payout': optimal_product.max_payout
        }
        
        base_value = objective_function(**base_params)
        gradients = {}
        
        for param_name, base_val in base_params.items():
            if param_name in feature_ranges:
                param_range = feature_ranges[param_name][1] - feature_ranges[param_name][0]
                step = epsilon * param_range
                
                # è¨ˆç®—æ•¸å€¼æ¢¯åº¦
                params_plus = base_params.copy()
                params_plus[param_name] = base_val + step
                
                params_minus = base_params.copy()
                params_minus[param_name] = base_val - step
                
                try:
                    value_plus = objective_function(**params_plus)
                    value_minus = objective_function(**params_minus)
                    gradient = (value_plus - value_minus) / (2 * step)
                    gradients[param_name] = abs(gradient)  # å–çµ•å°å€¼ä½œç‚ºé‡è¦æ€§
                except:
                    gradients[param_name] = 0.0
        
        print(f"âœ… æ¢¯åº¦é‡è¦æ€§è¨ˆç®—å®Œæˆ")
        return gradients
    
    def analyze_correlation_importance(self,
                                     products: List[ProductCandidate],
                                     target_metric: str = "basis_risk") -> Dict[str, float]:
        """
        åŸºæ–¼ç›¸é—œæ€§åˆ†æè¨ˆç®—ç‰¹å¾µé‡è¦æ€§
        
        Parameters:
        -----------
        products : List[ProductCandidate]
            ç”¢å“å€™é¸æ¸…å–®
        target_metric : str
            ç›®æ¨™æŒ‡æ¨™
            
        Returns:
        --------
        Dict[str, float]
            å„ç‰¹å¾µèˆ‡ç›®æ¨™çš„ç›¸é—œæ€§
        """
        
        print("ğŸ”— åˆ†æç›¸é—œæ€§é‡è¦æ€§...")
        
        if not products:
            return {}
        
        feature_names = ['trigger_threshold', 'payout_amount', 'max_payout', 
                        'trigger_rate', 'expected_payout', 'market_acceptability']
        
        correlations = {}
        target_values = [getattr(p, target_metric) for p in products]
        
        for feature_name in feature_names:
            try:
                feature_values = [getattr(p, feature_name) for p in products]
                correlation = abs(np.corrcoef(feature_values, target_values)[0, 1])
                correlations[feature_name] = correlation if not np.isnan(correlation) else 0.0
            except:
                correlations[feature_name] = 0.0
        
        print(f"âœ… ç›¸é—œæ€§åˆ†æå®Œæˆ")
        return correlations

class ResponseSurfaceVisualizer:
    """é—œä¿‚æ›²é¢è¦–è¦ºåŒ–å™¨"""
    
    def __init__(self, config: InterpretabilityConfig):
        """
        åˆå§‹åŒ–é—œä¿‚æ›²é¢è¦–è¦ºåŒ–å™¨
        
        Parameters:
        -----------
        config : InterpretabilityConfig
            é…ç½®åƒæ•¸
        """
        self.config = config
        self.basis_risk_calc = BasisRiskCalculator()
    
    def create_2d_response_surface(self,
                                 objective_function: Callable,
                                 feature1_name: str,
                                 feature1_range: Tuple[float, float],
                                 feature2_name: str, 
                                 feature2_range: Tuple[float, float],
                                 fixed_params: Dict[str, float] = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        å‰µå»º2DéŸ¿æ‡‰æ›²é¢
        
        Parameters:
        -----------
        objective_function : Callable
            ç›®æ¨™å‡½æ•¸
        feature1_name, feature2_name : str
            å…©å€‹è®ŠåŒ–çš„ç‰¹å¾µåç¨±
        feature1_range, feature2_range : Tuple[float, float]
            ç‰¹å¾µå–å€¼ç¯„åœ
        fixed_params : Dict[str, float]
            å›ºå®šçš„å…¶ä»–åƒæ•¸
            
        Returns:
        --------
        Tuple[np.ndarray, np.ndarray, np.ndarray]
            Xåº§æ¨™ç¶²æ ¼, Yåº§æ¨™ç¶²æ ¼, Zå€¼ç¶²æ ¼
        """
        
        resolution = self.config.surface_resolution
        
        # å‰µå»ºç¶²æ ¼
        x = np.linspace(feature1_range[0], feature1_range[1], resolution)
        y = np.linspace(feature2_range[0], feature2_range[1], resolution)
        X, Y = np.meshgrid(x, y)
        
        # è¨ˆç®—æ¯å€‹ç¶²æ ¼é»çš„ç›®æ¨™å‡½æ•¸å€¼
        Z = np.zeros_like(X)
        
        base_params = fixed_params or {}
        
        for i in range(resolution):
            for j in range(resolution):
                params = base_params.copy()
                params[feature1_name] = X[i, j]
                params[feature2_name] = Y[i, j]
                
                try:
                    Z[i, j] = objective_function(**params)
                except:
                    Z[i, j] = np.nan
        
        return X, Y, Z
    
    def visualize_basis_risk_surface(self,
                                   actual_losses: np.ndarray,
                                   hazard_indices: np.ndarray,
                                   optimal_product: ProductCandidate) -> None:
        """
        è¦–è¦ºåŒ–åŸºå·®é¢¨éšªæ›²é¢
        
        Parameters:
        -----------
        actual_losses : np.ndarray
            å¯¦éš›æå¤±æ•¸æ“š
        hazard_indices : np.ndarray
            ç½å®³æŒ‡æ¨™æ•¸æ“š
        optimal_product : ProductCandidate
            æœ€ä½³ç”¢å“ (ç”¨ä½œå›ºå®šåƒæ•¸çš„åƒè€ƒ)
        """
        
        print("ğŸ¨ ç”ŸæˆåŸºå·®é¢¨éšªéŸ¿æ‡‰æ›²é¢...")
        
        # å®šç¾©ç›®æ¨™å‡½æ•¸
        def basis_risk_function(trigger_threshold: float, payout_amount: float, **kwargs) -> float:
            # è¨ˆç®—è³ ä»˜
            payouts = np.where(hazard_indices >= trigger_threshold, payout_amount, 0)
            
            # è¨ˆç®—åŸºå·®é¢¨éšª
            risks = []
            for loss, payout in zip(actual_losses, payouts):
                risk = self.basis_risk_calc.calculate_weighted_asymmetric_basis_risk(
                    loss, payout, w_under=2.0, w_over=0.5
                )
                risks.append(risk)
            
            return np.mean(risks)
        
        # å®šç¾©ç‰¹å¾µç¯„åœ
        trigger_range = (
            np.percentile(hazard_indices, 60),
            np.percentile(hazard_indices, 95)
        )
        
        payout_range = (
            optimal_product.payout_amount * 0.5,
            optimal_product.payout_amount * 2.0
        )
        
        # å‰µå»ºéŸ¿æ‡‰æ›²é¢
        X, Y, Z = self.create_2d_response_surface(
            basis_risk_function,
            'trigger_threshold', trigger_range,
            'payout_amount', payout_range
        )
        
        # å‰µå»ºè¦–è¦ºåŒ–
        fig = plt.figure(figsize=(20, 6))
        
        # 3D æ›²é¢åœ–
        ax1 = fig.add_subplot(131, projection='3d')
        surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)
        
        # æ¨™è¨˜æœ€ä½³é»
        ax1.scatter([optimal_product.trigger_threshold], 
                   [optimal_product.payout_amount], 
                   [optimal_product.basis_risk],
                   color='red', s=100, label='æœ€ä½³ç”¢å“')
        
        ax1.set_xlabel('è§¸ç™¼é–¾å€¼')
        ax1.set_ylabel('è³ ä»˜é‡‘é¡')
        ax1.set_zlabel('åŸºå·®é¢¨éšª')
        ax1.set_title('åŸºå·®é¢¨éšª 3D éŸ¿æ‡‰æ›²é¢')
        fig.colorbar(surf, ax=ax1, shrink=0.5)
        
        # 2D ç­‰é«˜ç·šåœ–
        ax2 = fig.add_subplot(132)
        contour = ax2.contour(X, Y, Z, levels=20)
        ax2.clabel(contour, inline=True, fontsize=8)
        filled_contour = ax2.contourf(X, Y, Z, levels=50, alpha=0.6, cmap='viridis')
        
        # æ¨™è¨˜æœ€ä½³é»
        ax2.scatter([optimal_product.trigger_threshold], 
                   [optimal_product.payout_amount],
                   color='red', s=100, marker='*', label='æœ€ä½³ç”¢å“')
        
        ax2.set_xlabel('è§¸ç™¼é–¾å€¼')
        ax2.set_ylabel('è³ ä»˜é‡‘é¡')
        ax2.set_title('åŸºå·®é¢¨éšªç­‰é«˜ç·šåœ–')
        ax2.legend()
        fig.colorbar(filled_contour, ax=ax2)
        
        # åˆ‡ç‰‡åˆ†æ
        ax3 = fig.add_subplot(133)
        
        # å›ºå®šè³ ä»˜é‡‘é¡ï¼Œè®ŠåŒ–è§¸ç™¼é–¾å€¼
        trigger_slice = np.linspace(trigger_range[0], trigger_range[1], 100)
        risk_slice = []
        
        for trigger in trigger_slice:
            risk = basis_risk_function(trigger, optimal_product.payout_amount)
            risk_slice.append(risk)
        
        ax3.plot(trigger_slice, risk_slice, 'b-', linewidth=2, label='å›ºå®šè³ ä»˜é‡‘é¡')
        ax3.axvline(optimal_product.trigger_threshold, color='red', linestyle='--', 
                   label=f'æœ€ä½³è§¸ç™¼é–¾å€¼={optimal_product.trigger_threshold:.2f}')
        ax3.axhline(optimal_product.basis_risk, color='red', linestyle='--', alpha=0.5)
        
        ax3.set_xlabel('è§¸ç™¼é–¾å€¼')
        ax3.set_ylabel('åŸºå·®é¢¨éšª')
        ax3.set_title('åŸºå·®é¢¨éšªåˆ‡ç‰‡åˆ†æ')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜åœ–è¡¨
        output_file = Path(self.config.output_dir) / "basis_risk_response_surface.png"
        Path(self.config.output_dir).mkdir(parents=True, exist_ok=True)
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… éŸ¿æ‡‰æ›²é¢å·²ä¿å­˜: {output_file}")
    
    def create_partial_dependence_plots(self,
                                      products: List[ProductCandidate],
                                      target_metric: str = "basis_risk") -> None:
        """
        å‰µå»ºéƒ¨åˆ†ä¾è³´åœ– (Partial Dependence Plots)
        
        Parameters:
        -----------
        products : List[ProductCandidate]
            ç”¢å“å€™é¸æ¸…å–®
        target_metric : str
            ç›®æ¨™æŒ‡æ¨™
        """
        
        print("ğŸ“Š ç”Ÿæˆéƒ¨åˆ†ä¾è³´åœ–...")
        
        if not products:
            return
        
        # æº–å‚™æ•¸æ“š
        feature_names = ['trigger_threshold', 'payout_amount', 'max_payout', 
                        'trigger_rate', 'expected_payout', 'market_acceptability']
        
        X = np.array([[getattr(p, feat) for feat in feature_names] for p in products])
        y = np.array([getattr(p, target_metric) for p in products])
        
        # è¨“ç·´æ¨¡å‹
        rf = RandomForestRegressor(n_estimators=100, random_state=42)
        rf.fit(X, y)
        
        # å‰µå»ºéƒ¨åˆ†ä¾è³´åœ–
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'éƒ¨åˆ†ä¾è³´åˆ†æ - {target_metric}', fontsize=16, fontweight='bold')
        
        axes = axes.ravel()
        
        for i, feature_name in enumerate(feature_names):
            display = PartialDependenceDisplay.from_estimator(
                rf, X, [i], feature_names=[feature_name], ax=axes[i]
            )
            axes[i].set_title(f'{feature_name} çš„éƒ¨åˆ†ä¾è³´')
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜åœ–è¡¨
        output_file = Path(self.config.output_dir) / "partial_dependence_plots.png"
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… éƒ¨åˆ†ä¾è³´åœ–å·²ä¿å­˜: {output_file}")

class DecisionPathTracker:
    """æ±ºç­–è·¯å¾‘è¿½è¹¤å™¨"""
    
    def __init__(self, config: InterpretabilityConfig):
        """
        åˆå§‹åŒ–æ±ºç­–è·¯å¾‘è¿½è¹¤å™¨
        
        Parameters:
        -----------
        config : InterpretabilityConfig
            é…ç½®åƒæ•¸
        """
        self.config = config
    
    def trace_optimization_path(self,
                              optimization_history: List[Dict[str, Any]],
                              final_optimal: ProductCandidate) -> Dict[str, Any]:
        """
        è¿½è¹¤å„ªåŒ–è·¯å¾‘
        
        Parameters:
        -----------
        optimization_history : List[Dict[str, Any]]
            å„ªåŒ–éç¨‹çš„æ­·å²è¨˜éŒ„
        final_optimal : ProductCandidate
            æœ€çµ‚æœ€ä½³ç”¢å“
            
        Returns:
        --------
        Dict[str, Any]
            æ±ºç­–è·¯å¾‘åˆ†æçµæœ
        """
        
        print("ğŸ›¤ï¸ è¿½è¹¤æ±ºç­–è·¯å¾‘...")
        
        if not optimization_history:
            return {"error": "No optimization history provided"}
        
        path_analysis = {
            'total_iterations': len(optimization_history),
            'convergence_pattern': [],
            'parameter_evolution': {
                'trigger_threshold': [],
                'payout_amount': [],
                'basis_risk': []
            },
            'improvement_steps': [],
            'plateau_detection': []
        }
        
        # åˆ†æåƒæ•¸æ¼”åŒ–
        for i, step in enumerate(optimization_history):
            path_analysis['parameter_evolution']['trigger_threshold'].append(
                step.get('trigger_threshold', 0)
            )
            path_analysis['parameter_evolution']['payout_amount'].append(
                step.get('payout_amount', 0)
            )
            path_analysis['parameter_evolution']['basis_risk'].append(
                step.get('basis_risk', 0)
            )
            
            # æª¢æ¸¬æ”¹é€²æ­¥é©Ÿ
            if i > 0:
                prev_risk = optimization_history[i-1].get('basis_risk', float('inf'))
                curr_risk = step.get('basis_risk', float('inf'))
                if curr_risk < prev_risk:
                    improvement = (prev_risk - curr_risk) / prev_risk
                    path_analysis['improvement_steps'].append({
                        'iteration': i,
                        'improvement_rate': improvement,
                        'from_risk': prev_risk,
                        'to_risk': curr_risk
                    })
        
        # æª¢æ¸¬æ”¶æ–‚å¹³å°
        risks = path_analysis['parameter_evolution']['basis_risk']
        if len(risks) > 10:
            # æ»‘å‹•çª—å£æª¢æ¸¬å¹³å°
            window_size = min(10, len(risks) // 4)
            for i in range(window_size, len(risks)):
                window = risks[i-window_size:i]
                if len(set(window)) == 1 or (max(window) - min(window)) / np.mean(window) < 0.001:
                    path_analysis['plateau_detection'].append({
                        'start_iteration': i - window_size,
                        'end_iteration': i,
                        'plateau_value': np.mean(window)
                    })
        
        print(f"âœ… æ±ºç­–è·¯å¾‘è¿½è¹¤å®Œæˆ")
        return path_analysis
    
    def explain_optimal_selection(self,
                                optimal_product: ProductCandidate,
                                all_candidates: List[ProductCandidate],
                                selection_criteria: Dict[str, float]) -> Dict[str, Any]:
        """
        è§£é‡‹æœ€ä½³ç”¢å“é¸æ“‡çš„åŸå› 
        
        Parameters:
        -----------
        optimal_product : ProductCandidate
            æœ€ä½³ç”¢å“
        all_candidates : List[ProductCandidate]
            æ‰€æœ‰å€™é¸ç”¢å“
        selection_criteria : Dict[str, float]
            é¸æ“‡æ¨™æº–æ¬Šé‡
            
        Returns:
        --------
        Dict[str, Any]
            é¸æ“‡åŸå› è§£é‡‹
        """
        
        print("ğŸ’¡ è§£é‡‹æœ€ä½³ç”¢å“é¸æ“‡...")
        
        explanation = {
            'optimal_product_id': optimal_product.product_id,
            'selection_reasons': [],
            'comparative_analysis': {},
            'decisive_factors': {},
            'trade_offs': {}
        }
        
        # è¨ˆç®—æœ€ä½³ç”¢å“åœ¨å„æŒ‡æ¨™ä¸Šçš„æ’å
        metrics = ['basis_risk', 'trigger_rate', 'expected_payout', 'market_acceptability', 'technical_premium']
        rankings = {}
        
        for metric in metrics:
            if hasattr(optimal_product, metric):
                optimal_value = getattr(optimal_product, metric)
                all_values = [getattr(p, metric, 0) for p in all_candidates]
                
                # å°æ–¼é¢¨éšªé¡æŒ‡æ¨™ï¼Œæ’åè¶Šå°è¶Šå¥½
                if metric in ['basis_risk']:
                    rank = sum(1 for v in all_values if v < optimal_value) + 1
                else:
                    rank = sum(1 for v in all_values if v > optimal_value) + 1
                
                rankings[metric] = {
                    'rank': rank,
                    'total_candidates': len(all_candidates),
                    'percentile': (1 - rank / len(all_candidates)) * 100,
                    'value': optimal_value
                }
        
        # ç”Ÿæˆé¸æ“‡åŸå› 
        for metric, ranking_info in rankings.items():
            if ranking_info['percentile'] >= 80:  # å‰20%
                explanation['selection_reasons'].append(
                    f"{metric} è¡¨ç¾å„ªç•° (ç¬¬{ranking_info['rank']}å, å‰{100-ranking_info['percentile']:.1f}%)"
                )
            elif ranking_info['percentile'] >= 50:  # å‰50%
                explanation['selection_reasons'].append(
                    f"{metric} è¡¨ç¾è‰¯å¥½ (ç¬¬{ranking_info['rank']}å, å‰{100-ranking_info['percentile']:.1f}%)"
                )
        
        # æ±ºå®šæ€§å› ç´ åˆ†æ
        if 'basis_risk' in rankings:
            explanation['decisive_factors']['primary'] = {
                'factor': 'basis_risk',
                'rank': rankings['basis_risk']['rank'],
                'reasoning': "åŸºå·®é¢¨éšªæ˜¯ä¸»è¦å„ªåŒ–ç›®æ¨™ï¼Œæ­¤ç”¢å“åœ¨è©²æŒ‡æ¨™ä¸Šè¡¨ç¾æœ€ä½³"
            }
        
        # æ¬Šè¡¡åˆ†æ
        for metric1 in metrics[:3]:  # åªåˆ†æå‰3å€‹æŒ‡æ¨™çš„æ¬Šè¡¡
            for metric2 in metrics[:3]:
                if metric1 != metric2 and metric1 in rankings and metric2 in rankings:
                    rank1 = rankings[metric1]['rank']
                    rank2 = rankings[metric2]['rank']
                    
                    if abs(rank1 - rank2) > len(all_candidates) * 0.2:  # æ’åå·®ç•°å¤§æ–¼20%
                        if rank1 < rank2:
                            explanation['trade_offs'][f"{metric1}_vs_{metric2}"] = {
                                'description': f"åœ¨{metric1}ä¸Šè¡¨ç¾æ›´å¥½ï¼Œ{metric2}ä¸Šç•¥æœ‰å¦¥å”",
                                'rank_difference': rank2 - rank1
                            }
        
        print(f"âœ… æœ€ä½³ç”¢å“é¸æ“‡è§£é‡‹å®Œæˆ")
        return explanation

class CounterfactualAnalyzer:
    """åäº‹å¯¦åˆ†æå™¨"""
    
    def __init__(self, config: InterpretabilityConfig):
        """
        åˆå§‹åŒ–åäº‹å¯¦åˆ†æå™¨
        
        Parameters:
        -----------
        config : InterpretabilityConfig
            é…ç½®åƒæ•¸
        """
        self.config = config
        self.basis_risk_calc = BasisRiskCalculator()
    
    def generate_counterfactual_scenarios(self,
                                        optimal_product: ProductCandidate,
                                        actual_losses: np.ndarray,
                                        hazard_indices: np.ndarray) -> Dict[str, Any]:
        """
        ç”Ÿæˆåäº‹å¯¦æƒ…å¢ƒåˆ†æ
        
        "å¦‚æœæ”¹è®ŠæŸå€‹åƒæ•¸ï¼Œçµæœæœƒå¦‚ä½•ï¼Ÿ"
        
        Parameters:
        -----------
        optimal_product : ProductCandidate
            æœ€ä½³ç”¢å“
        actual_losses : np.ndarray
            å¯¦éš›æå¤±æ•¸æ“š
        hazard_indices : np.ndarray
            ç½å®³æŒ‡æ¨™æ•¸æ“š
            
        Returns:
        --------
        Dict[str, Any]
            åäº‹å¯¦åˆ†æçµæœ
        """
        
        print("ğŸ”® ç”Ÿæˆåäº‹å¯¦æƒ…å¢ƒåˆ†æ...")
        
        base_params = {
            'trigger_threshold': optimal_product.trigger_threshold,
            'payout_amount': optimal_product.payout_amount,
            'max_payout': optimal_product.max_payout
        }
        
        base_risk = optimal_product.basis_risk
        
        counterfactuals = {
            'base_scenario': {
                'parameters': base_params.copy(),
                'basis_risk': base_risk,
                'description': "ç•¶å‰æœ€ä½³ç”¢å“"
            },
            'what_if_scenarios': []
        }
        
        # æƒ…å¢ƒ1: å¦‚æœæé«˜è§¸ç™¼é–¾å€¼
        cf_params1 = base_params.copy()
        cf_params1['trigger_threshold'] *= 1.1  # æé«˜10%
        cf_risk1 = self._calculate_basis_risk(cf_params1, actual_losses, hazard_indices)
        
        counterfactuals['what_if_scenarios'].append({
            'scenario': 'æé«˜è§¸ç™¼é–¾å€¼10%',
            'parameters': cf_params1,
            'basis_risk': cf_risk1,
            'risk_change': cf_risk1 - base_risk,
            'relative_change': (cf_risk1 - base_risk) / base_risk,
            'interpretation': "æ›´ä¿å®ˆçš„è§¸ç™¼æ¢ä»¶" + (
                "ï¼ŒåŸºå·®é¢¨éšªå¢åŠ " if cf_risk1 > base_risk else "ï¼ŒåŸºå·®é¢¨éšªæ¸›å°‘"
            )
        })
        
        # æƒ…å¢ƒ2: å¦‚æœé™ä½è§¸ç™¼é–¾å€¼
        cf_params2 = base_params.copy()
        cf_params2['trigger_threshold'] *= 0.9  # é™ä½10%
        cf_risk2 = self._calculate_basis_risk(cf_params2, actual_losses, hazard_indices)
        
        counterfactuals['what_if_scenarios'].append({
            'scenario': 'é™ä½è§¸ç™¼é–¾å€¼10%',
            'parameters': cf_params2,
            'basis_risk': cf_risk2,
            'risk_change': cf_risk2 - base_risk,
            'relative_change': (cf_risk2 - base_risk) / base_risk,
            'interpretation': "æ›´å¯¬é¬†çš„è§¸ç™¼æ¢ä»¶" + (
                "ï¼ŒåŸºå·®é¢¨éšªå¢åŠ " if cf_risk2 > base_risk else "ï¼ŒåŸºå·®é¢¨éšªæ¸›å°‘"
            )
        })
        
        # æƒ…å¢ƒ3: å¦‚æœå¢åŠ è³ ä»˜é‡‘é¡
        cf_params3 = base_params.copy()
        cf_params3['payout_amount'] *= 1.2  # å¢åŠ 20%
        cf_risk3 = self._calculate_basis_risk(cf_params3, actual_losses, hazard_indices)
        
        counterfactuals['what_if_scenarios'].append({
            'scenario': 'å¢åŠ è³ ä»˜é‡‘é¡20%',
            'parameters': cf_params3,
            'basis_risk': cf_risk3,
            'risk_change': cf_risk3 - base_risk,
            'relative_change': (cf_risk3 - base_risk) / base_risk,
            'interpretation': "æ›´é«˜çš„è³ ä»˜æ°´å¹³" + (
                "ï¼ŒåŸºå·®é¢¨éšªå¢åŠ " if cf_risk3 > base_risk else "ï¼ŒåŸºå·®é¢¨éšªæ¸›å°‘"
            )
        })
        
        # æƒ…å¢ƒ4: å¦‚æœæ¸›å°‘è³ ä»˜é‡‘é¡
        cf_params4 = base_params.copy()
        cf_params4['payout_amount'] *= 0.8  # æ¸›å°‘20%
        cf_risk4 = self._calculate_basis_risk(cf_params4, actual_losses, hazard_indices)
        
        counterfactuals['what_if_scenarios'].append({
            'scenario': 'æ¸›å°‘è³ ä»˜é‡‘é¡20%',
            'parameters': cf_params4,
            'basis_risk': cf_risk4,
            'risk_change': cf_risk4 - base_risk,
            'relative_change': (cf_risk4 - base_risk) / base_risk,
            'interpretation': "æ›´ä½çš„è³ ä»˜æ°´å¹³" + (
                "ï¼ŒåŸºå·®é¢¨éšªå¢åŠ " if cf_risk4 > base_risk else "ï¼ŒåŸºå·®é¢¨éšªæ¸›å°‘"
            )
        })
        
        # æ‰¾å‡ºæœ€ä½³çš„åäº‹å¯¦æƒ…å¢ƒ
        best_cf = min(counterfactuals['what_if_scenarios'], 
                     key=lambda x: x['basis_risk'])
        
        counterfactuals['best_alternative'] = {
            'scenario': best_cf['scenario'],
            'improvement_potential': base_risk - best_cf['basis_risk'],
            'parameters': best_cf['parameters'],
            'recommendation': f"è€ƒæ…®{best_cf['scenario']}å¯èƒ½é€²ä¸€æ­¥æ”¹å–„åŸºå·®é¢¨éšª"
        }
        
        print(f"âœ… åäº‹å¯¦åˆ†æå®Œæˆ")
        return counterfactuals
    
    def _calculate_basis_risk(self,
                            params: Dict[str, float],
                            actual_losses: np.ndarray,
                            hazard_indices: np.ndarray) -> float:
        """è¨ˆç®—çµ¦å®šåƒæ•¸ä¸‹çš„åŸºå·®é¢¨éšª"""
        
        payouts = np.where(hazard_indices >= params['trigger_threshold'],
                          params['payout_amount'], 0)
        
        risks = []
        for loss, payout in zip(actual_losses, payouts):
            risk = self.basis_risk_calc.calculate_weighted_asymmetric_basis_risk(
                loss, payout, w_under=2.0, w_over=0.5
            )
            risks.append(risk)
        
        return np.mean(risks)

class InterpretabilityFramework:
    """æ•´åˆå¯è©®é‡‹æ€§æ¡†æ¶"""
    
    def __init__(self, config: InterpretabilityConfig = None):
        """
        åˆå§‹åŒ–å¯è©®é‡‹æ€§æ¡†æ¶
        
        Parameters:
        -----------
        config : InterpretabilityConfig
            é…ç½®åƒæ•¸
        """
        self.config = config or InterpretabilityConfig()
        
        # åˆå§‹åŒ–å­æ¨¡çµ„
        self.importance_analyzer = VariableImportanceAnalyzer(self.config)
        self.surface_visualizer = ResponseSurfaceVisualizer(self.config)
        self.decision_tracker = DecisionPathTracker(self.config)
        self.counterfactual_analyzer = CounterfactualAnalyzer(self.config)
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        Path(self.config.output_dir).mkdir(parents=True, exist_ok=True)
    
    def run_comprehensive_interpretability_analysis(self,
                                                  optimal_product: ProductCandidate,
                                                  all_candidates: List[ProductCandidate],
                                                  actual_losses: np.ndarray,
                                                  hazard_indices: np.ndarray,
                                                  optimization_history: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        åŸ·è¡Œå®Œæ•´çš„å¯è©®é‡‹æ€§åˆ†æ
        
        Parameters:
        -----------
        optimal_product : ProductCandidate
            æœ€ä½³ç”¢å“
        all_candidates : List[ProductCandidate]
            æ‰€æœ‰å€™é¸ç”¢å“
        actual_losses : np.ndarray
            å¯¦éš›æå¤±æ•¸æ“š
        hazard_indices : np.ndarray
            ç½å®³æŒ‡æ¨™æ•¸æ“š
        optimization_history : List[Dict[str, Any]]
            å„ªåŒ–æ­·å²è¨˜éŒ„
            
        Returns:
        --------
        Dict[str, Any]
            å®Œæ•´çš„å¯è©®é‡‹æ€§åˆ†æçµæœ
        """
        
        print("ğŸ” åŸ·è¡Œå®Œæ•´å¯è©®é‡‹æ€§åˆ†æ...")
        print("=" * 60)
        
        comprehensive_results = {
            'analysis_metadata': {
                'optimal_product_id': optimal_product.product_id,
                'total_candidates': len(all_candidates),
                'analysis_timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
            }
        }
        
        # 1. è®Šæ•¸é‡è¦æ€§åˆ†æ
        print("ğŸ“Š 1. è®Šæ•¸é‡è¦æ€§åˆ†æ...")
        importance_results = {}
        
        # æ’åˆ—é‡è¦æ€§
        importance_results['permutation_importance'] = \
            self.importance_analyzer.calculate_permutation_importance(all_candidates)
        
        # ç›¸é—œæ€§é‡è¦æ€§
        importance_results['correlation_importance'] = \
            self.importance_analyzer.analyze_correlation_importance(all_candidates)
        
        # æ¢¯åº¦é‡è¦æ€§ (å¦‚æœå¯èƒ½)
        feature_ranges = {
            'trigger_threshold': (
                min(p.trigger_threshold for p in all_candidates),
                max(p.trigger_threshold for p in all_candidates)
            ),
            'payout_amount': (
                min(p.payout_amount for p in all_candidates),
                max(p.payout_amount for p in all_candidates)
            ),
            'max_payout': (
                min(p.max_payout for p in all_candidates),
                max(p.max_payout for p in all_candidates)
            )
        }
        
        def objective_func(**params):
            return self.counterfactual_analyzer._calculate_basis_risk(
                params, actual_losses, hazard_indices
            )
        
        importance_results['gradient_importance'] = \
            self.importance_analyzer.calculate_gradient_importance(
                objective_func, optimal_product, feature_ranges
            )
        
        comprehensive_results['variable_importance'] = importance_results
        
        # 2. éŸ¿æ‡‰æ›²é¢è¦–è¦ºåŒ–
        print("ğŸ“ˆ 2. éŸ¿æ‡‰æ›²é¢è¦–è¦ºåŒ–...")
        self.surface_visualizer.visualize_basis_risk_surface(
            actual_losses, hazard_indices, optimal_product
        )
        
        self.surface_visualizer.create_partial_dependence_plots(all_candidates)
        
        # 3. æ±ºç­–è·¯å¾‘è¿½è¹¤
        print("ğŸ›¤ï¸ 3. æ±ºç­–è·¯å¾‘åˆ†æ...")
        if optimization_history:
            path_analysis = self.decision_tracker.trace_optimization_path(
                optimization_history, optimal_product
            )
            comprehensive_results['decision_path'] = path_analysis
        
        # æœ€ä½³ç”¢å“é¸æ“‡è§£é‡‹
        selection_explanation = self.decision_tracker.explain_optimal_selection(
            optimal_product, all_candidates, {'basis_risk': 1.0}
        )
        comprehensive_results['selection_explanation'] = selection_explanation
        
        # 4. åäº‹å¯¦åˆ†æ
        print("ğŸ”® 4. åäº‹å¯¦æƒ…å¢ƒåˆ†æ...")
        counterfactual_results = self.counterfactual_analyzer.generate_counterfactual_scenarios(
            optimal_product, actual_losses, hazard_indices
        )
        comprehensive_results['counterfactual_analysis'] = counterfactual_results
        
        # 5. ç”Ÿæˆç¶œåˆè§£é‡‹å ±å‘Š
        comprehensive_results['interpretability_summary'] = self._generate_interpretability_summary(
            comprehensive_results, optimal_product
        )
        
        # ä¿å­˜çµæœ
        self._save_interpretability_results(comprehensive_results)
        
        print("âœ… å®Œæ•´å¯è©®é‡‹æ€§åˆ†æå®Œæˆï¼")
        return comprehensive_results
    
    def _generate_interpretability_summary(self,
                                         results: Dict[str, Any],
                                         optimal_product: ProductCandidate) -> Dict[str, Any]:
        """ç”Ÿæˆå¯è©®é‡‹æ€§ç¸½çµ"""
        
        summary = {
            'key_insights': [],
            'most_important_variables': [],
            'decision_rationale': [],
            'improvement_suggestions': []
        }
        
        # åˆ†ææœ€é‡è¦çš„è®Šæ•¸
        importance_data = results.get('variable_importance', {})
        
        if 'permutation_importance' in importance_data:
            perm_imp = importance_data['permutation_importance']
            most_important = max(perm_imp.items(), key=lambda x: x[1])
            summary['most_important_variables'].append({
                'method': 'permutation',
                'variable': most_important[0],
                'importance': most_important[1],
                'interpretation': f"{most_important[0]} å°æ¨¡å‹é æ¸¬å½±éŸ¿æœ€å¤§"
            })
        
        # æ±ºç­–ç†ç”±
        selection_exp = results.get('selection_explanation', {})
        if 'selection_reasons' in selection_exp:
            summary['decision_rationale'] = selection_exp['selection_reasons']
        
        # æ”¹é€²å»ºè­°
        cf_analysis = results.get('counterfactual_analysis', {})
        if 'best_alternative' in cf_analysis:
            best_alt = cf_analysis['best_alternative']
            if best_alt['improvement_potential'] > 0:
                summary['improvement_suggestions'].append({
                    'suggestion': best_alt['scenario'],
                    'potential_improvement': best_alt['improvement_potential'],
                    'recommendation': best_alt['recommendation']
                })
        
        # é—œéµæ´å¯Ÿ
        summary['key_insights'] = [
            f"æœ€ä½³ç”¢å“çš„åŸºå·®é¢¨éšªç‚º {optimal_product.basis_risk:.2e}",
            f"è§¸ç™¼ç‡ç‚º {optimal_product.trigger_rate:.1%}",
            f"å¸‚å ´æ¥å—åº¦è©•åˆ†ç‚º {optimal_product.market_acceptability:.3f}"
        ]
        
        if 'most_important_variables' in summary and summary['most_important_variables']:
            top_var = summary['most_important_variables'][0]['variable']
            summary['key_insights'].append(f"{top_var} æ˜¯å½±éŸ¿ç”¢å“æ€§èƒ½çš„é—œéµå› ç´ ")
        
        return summary
    
    def _save_interpretability_results(self, results: Dict[str, Any]) -> None:
        """ä¿å­˜å¯è©®é‡‹æ€§çµæœ"""
        
        import json
        
        # ä¿å­˜å®Œæ•´çµæœ
        output_file = Path(self.config.output_dir) / "interpretability_analysis_results.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“ å¯è©®é‡‹æ€§åˆ†æçµæœå·²ä¿å­˜: {output_file}")

def main():
    """ä¸»å‡½æ•¸ç¤ºä¾‹"""
    
    print("ğŸ” å¯è©®é‡‹æ€§æ¡†æ¶ç¤ºä¾‹...")
    print("=" * 60)
    
    # å‰µå»ºç¤ºä¾‹æ•¸æ“š
    np.random.seed(42)
    
    # ç”Ÿæˆæ¨¡æ“¬æå¤±å’Œç½å®³æ•¸æ“š
    n_scenarios = 500
    actual_losses = np.random.lognormal(np.log(5e7), 0.8, n_scenarios)
    hazard_indices = np.random.gamma(2, 25, n_scenarios)
    
    # ç”Ÿæˆå€™é¸ç”¢å“
    candidates = []
    for i in range(50):
        trigger = np.random.uniform(40, 80)
        payout = np.random.uniform(2e7, 1e8)
        max_payout = payout * np.random.uniform(1.5, 3.0)
        
        # è¨ˆç®—åŸºå·®é¢¨éšª (ç°¡åŒ–)
        payouts = np.where(hazard_indices >= trigger, payout, 0)
        basis_risk_calc = BasisRiskCalculator()
        
        risks = [
            basis_risk_calc.calculate_weighted_asymmetric_basis_risk(
                loss, pay, w_under=2.0, w_over=0.5
            ) for loss, pay in zip(actual_losses, payouts)
        ]
        
        candidate = ProductCandidate(
            trigger_threshold=trigger,
            payout_amount=payout,
            max_payout=max_payout,
            basis_risk=np.mean(risks),
            trigger_rate=np.mean(payouts > 0),
            expected_payout=np.mean(payouts),
            market_acceptability=np.random.uniform(0.5, 1.0),
            technical_premium=np.mean(payouts) * 1.5,
            product_id=f"Product_{i+1:02d}"
        )
        
        candidates.append(candidate)
    
    # æ‰¾å‡ºæœ€ä½³ç”¢å“
    optimal_product = min(candidates, key=lambda p: p.basis_risk)
    
    print(f"æœ€ä½³ç”¢å“: {optimal_product.product_id}")
    print(f"åŸºå·®é¢¨éšª: {optimal_product.basis_risk:.2e}")
    print(f"è§¸ç™¼é–¾å€¼: {optimal_product.trigger_threshold:.2f}")
    print(f"è³ ä»˜é‡‘é¡: {optimal_product.payout_amount:.2e}")
    
    # åŸ·è¡Œå¯è©®é‡‹æ€§åˆ†æ
    config = InterpretabilityConfig()
    framework = InterpretabilityFramework(config)
    
    results = framework.run_comprehensive_interpretability_analysis(
        optimal_product=optimal_product,
        all_candidates=candidates,
        actual_losses=actual_losses,
        hazard_indices=hazard_indices
    )
    
    # è¼¸å‡ºé—œéµçµæœ
    print("\n" + "=" * 60)
    print("ğŸ“‹ å¯è©®é‡‹æ€§åˆ†æé—œéµçµæœ:")
    print("=" * 60)
    
    summary = results.get('interpretability_summary', {})
    
    if 'key_insights' in summary:
        print("ğŸ’¡ é—œéµæ´å¯Ÿ:")
        for insight in summary['key_insights']:
            print(f"  â€¢ {insight}")
    
    if 'decision_rationale' in summary:
        print(f"\nğŸ¯ æ±ºç­–ç†ç”±:")
        for reason in summary['decision_rationale']:
            print(f"  â€¢ {reason}")
    
    if 'improvement_suggestions' in summary:
        print(f"\nğŸ“ˆ æ”¹é€²å»ºè­°:")
        for suggestion in summary['improvement_suggestions']:
            print(f"  â€¢ {suggestion['suggestion']}")
            print(f"    æ½›åœ¨æ”¹é€²: {suggestion['potential_improvement']:.2e}")
    
    print(f"\nğŸ“ è©³ç´°çµæœå·²ä¿å­˜åœ¨: {config.output_dir}")
    print("ğŸ‰ å¯è©®é‡‹æ€§åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()
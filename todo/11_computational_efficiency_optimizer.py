#!/usr/bin/env python3
"""
è¨ˆç®—æ•ˆç‡å„ªåŒ–å™¨ (Computational Efficiency Optimizer)
å¤§å¹…æå‡åƒæ•¸ä¿éšªåˆ†æçš„è¨ˆç®—æ€§èƒ½

æœ¬æ¨¡çµ„å›æ‡‰è€ƒé‡é»å››ï¼šè¨ˆç®—æ•ˆç‡ (Computational Efficiency)
- ä½¿ç”¨ NumPy å‘é‡åŒ–æ“ä½œå–ä»£å¤šé‡è¿´åœˆ
- å¯¦æ–½çŸ©é™£/å‘é‡é‹ç®—ä¾†æ‰¹é‡è¨ˆç®—åŸºå·®é¢¨éšª
- å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨å’Œæ•¸æ“šçµæ§‹
- æä¾›æ€§èƒ½åŸºæº–æ¸¬è©¦å’Œæ¯”è¼ƒ

æ ¸å¿ƒå„ªåŒ–ç­–ç•¥ï¼š
1. å‘é‡åŒ–åŸºå·®é¢¨éšªè¨ˆç®— (Vectorized Basis Risk Computation)
2. æ‰¹é‡ç”¢å“è©•ä¼° (Batch Product Evaluation)
3. ä¸¦è¡ŒåŒ– Monte Carlo æ¨¡æ“¬ (Parallel Monte Carlo Simulation)
4. å¿«å–å’Œè¨˜æ†¶é«”å„ªåŒ– (Caching and Memory Optimization)
5. ç¨€ç–çŸ©é™£é‹ç®— (Sparse Matrix Operations)

æ€§èƒ½ç›®æ¨™ï¼šå¯¦ç¾ 10-100x çš„é€Ÿåº¦æå‡

Author: Research Team
Date: 2025-01-10
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
import warnings
from pathlib import Path
import time
from numba import jit, vectorize, cuda
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing as mp
from scipy.sparse import csr_matrix
from functools import lru_cache

# å°å…¥åŸºç¤æ¨¡çµ„
from skill_scores.basis_risk_functions import (
    BasisRiskCalculator, 
    BasisRiskType
)

# è¨­å®šä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Heiti TC', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

warnings.filterwarnings('ignore')

@dataclass
class PerformanceConfig:
    """æ€§èƒ½å„ªåŒ–é…ç½®"""
    use_vectorization: bool = True      # å•Ÿç”¨å‘é‡åŒ–
    use_numba: bool = True              # å•Ÿç”¨ Numba JIT ç·¨è­¯
    use_multiprocessing: bool = True    # å•Ÿç”¨å¤šé€²ç¨‹
    use_caching: bool = True            # å•Ÿç”¨å¿«å–
    n_workers: int = 4                  # å·¥ä½œé€²ç¨‹æ•¸
    chunk_size: int = 1000              # æ‰¹æ¬¡å¤§å°
    memory_limit: str = "4GB"           # è¨˜æ†¶é«”é™åˆ¶

@dataclass
class BenchmarkResult:
    """åŸºæº–æ¸¬è©¦çµæœ"""
    method_name: str
    execution_time: float
    memory_usage: float
    speedup_ratio: float
    accuracy_loss: float
    
class VectorizedBasisRiskCalculator:
    """å‘é‡åŒ–åŸºå·®é¢¨éšªè¨ˆç®—å™¨"""
    
    def __init__(self, config: PerformanceConfig):
        """
        åˆå§‹åŒ–å‘é‡åŒ–è¨ˆç®—å™¨
        
        Parameters:
        -----------
        config : PerformanceConfig
            æ€§èƒ½é…ç½®
        """
        self.config = config
    
    @staticmethod
    @jit(nopython=True, parallel=True)  # Numba åŠ é€Ÿ
    def _vectorized_weighted_asymmetric_risk(actual_losses: np.ndarray,
                                           payouts: np.ndarray,
                                           w_under: float = 2.0,
                                           w_over: float = 0.5) -> np.ndarray:
        """
        å‘é‡åŒ–è¨ˆç®—åŠ æ¬Šä¸å°ç¨±åŸºå·®é¢¨éšª
        
        Parameters:
        -----------
        actual_losses : np.ndarray
            å¯¦éš›æå¤±å‘é‡
        payouts : np.ndarray
            è³ ä»˜å‘é‡
        w_under, w_over : float
            æ¬Šé‡åƒæ•¸
            
        Returns:
        --------
        np.ndarray
            åŸºå·®é¢¨éšªå‘é‡
        """
        
        # å‘é‡åŒ–è¨ˆç®—ä¸è¶³è¦†è“‹å’Œéåº¦è¦†è“‹
        under_coverage = np.maximum(0, actual_losses - payouts)
        over_coverage = np.maximum(0, payouts - actual_losses)
        
        # å‘é‡åŒ–åŠ æ¬Šè¨ˆç®—
        weighted_risks = w_under * under_coverage + w_over * over_coverage
        
        return weighted_risks
    
    def calculate_batch_basis_risk(self,
                                 actual_losses: np.ndarray,
                                 hazard_indices: np.ndarray,
                                 trigger_thresholds: np.ndarray,
                                 payout_amounts: np.ndarray,
                                 w_under: float = 2.0,
                                 w_over: float = 0.5) -> np.ndarray:
        """
        æ‰¹é‡è¨ˆç®—å¤šå€‹ç”¢å“çš„åŸºå·®é¢¨éšª
        
        å–ä»£ä¸‰é‡è¿´åœˆï¼š
        åŸå§‹: for product in products:
                for sample in posterior_samples:
                  for event in events:
        
        å„ªåŒ–: ä½¿ç”¨å»£æ’­å’Œå‘é‡åŒ–ä¸€æ¬¡è¨ˆç®—æ‰€æœ‰çµ„åˆ
        
        Parameters:
        -----------
        actual_losses : np.ndarray
            å¯¦éš›æå¤± (n_scenarios,)
        hazard_indices : np.ndarray
            ç½å®³æŒ‡æ¨™ (n_scenarios,)
        trigger_thresholds : np.ndarray
            è§¸ç™¼é–¾å€¼ (n_products,)
        payout_amounts : np.ndarray
            è³ ä»˜é‡‘é¡ (n_products,)
        w_under, w_over : float
            æ¬Šé‡åƒæ•¸
            
        Returns:
        --------
        np.ndarray
            åŸºå·®é¢¨éšªçŸ©é™£ (n_products, n_scenarios)
        """
        
        n_products = len(trigger_thresholds)
        n_scenarios = len(actual_losses)
        
        # ä½¿ç”¨å»£æ’­å‰µå»ºè³ ä»˜çŸ©é™£ (é¿å…é¡¯å¼è¿´åœˆ)
        # shape: (n_products, n_scenarios)
        hazard_matrix = hazard_indices[np.newaxis, :]  # (1, n_scenarios)
        trigger_matrix = trigger_thresholds[:, np.newaxis]  # (n_products, 1)
        payout_matrix = payout_amounts[:, np.newaxis]  # (n_products, 1)
        
        # å‘é‡åŒ–è¨ˆç®—è§¸ç™¼æ¢ä»¶ (å»£æ’­æ¯”è¼ƒ)
        triggered = hazard_matrix >= trigger_matrix  # (n_products, n_scenarios)
        
        # å‘é‡åŒ–è¨ˆç®—è³ ä»˜ (æ¢ä»¶è³ ä»˜)
        payouts_matrix = np.where(triggered, payout_matrix, 0)  # (n_products, n_scenarios)
        
        # å»£æ’­å¯¦éš›æå¤±
        losses_matrix = actual_losses[np.newaxis, :]  # (1, n_scenarios)
        losses_matrix = np.broadcast_to(losses_matrix, (n_products, n_scenarios))
        
        # å‘é‡åŒ–è¨ˆç®—åŸºå·®é¢¨éšª
        if self.config.use_numba:
            # ä½¿ç”¨ Numba åŠ é€Ÿçš„ç‰ˆæœ¬
            basis_risks = np.zeros((n_products, n_scenarios))
            for i in range(n_products):
                basis_risks[i, :] = self._vectorized_weighted_asymmetric_risk(
                    losses_matrix[i, :], payouts_matrix[i, :], w_under, w_over
                )
        else:
            # ç´” NumPy ç‰ˆæœ¬
            under_coverage = np.maximum(0, losses_matrix - payouts_matrix)
            over_coverage = np.maximum(0, payouts_matrix - losses_matrix)
            basis_risks = w_under * under_coverage + w_over * over_coverage
        
        return basis_risks
    
    def calculate_portfolio_basis_risk_vectorized(self,
                                                actual_losses: np.ndarray,
                                                hazard_indices: np.ndarray, 
                                                product_parameters: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """
        å‘é‡åŒ–è¨ˆç®—æŠ•è³‡çµ„åˆåŸºå·®é¢¨éšªçµ±è¨ˆ
        
        Parameters:
        -----------
        actual_losses : np.ndarray
            å¯¦éš›æå¤±
        hazard_indices : np.ndarray
            ç½å®³æŒ‡æ¨™
        product_parameters : Dict[str, np.ndarray]
            ç”¢å“åƒæ•¸å­—å…¸ {'trigger_thresholds': array, 'payout_amounts': array}
            
        Returns:
        --------
        Dict[str, np.ndarray]
            çµ±è¨ˆçµæœå­—å…¸
        """
        
        # æ‰¹é‡è¨ˆç®—åŸºå·®é¢¨éšªçŸ©é™£
        basis_risk_matrix = self.calculate_batch_basis_risk(
            actual_losses,
            hazard_indices,
            product_parameters['trigger_thresholds'],
            product_parameters['payout_amounts']
        )
        
        # å‘é‡åŒ–è¨ˆç®—çµ±è¨ˆé‡
        stats = {
            'mean_basis_risk': np.mean(basis_risk_matrix, axis=1),      # (n_products,)
            'std_basis_risk': np.std(basis_risk_matrix, axis=1),        # (n_products,)
            'max_basis_risk': np.max(basis_risk_matrix, axis=1),        # (n_products,)
            'min_basis_risk': np.min(basis_risk_matrix, axis=1),        # (n_products,)
            'median_basis_risk': np.median(basis_risk_matrix, axis=1),  # (n_products,)
            'percentile_95': np.percentile(basis_risk_matrix, 95, axis=1),  # (n_products,)
            'percentile_5': np.percentile(basis_risk_matrix, 5, axis=1)     # (n_products,)
        }
        
        return stats

class ParallelMonteCarloEngine:
    """ä¸¦è¡Œ Monte Carlo æ¨¡æ“¬å¼•æ“"""
    
    def __init__(self, config: PerformanceConfig):
        """
        åˆå§‹åŒ–ä¸¦è¡Œå¼•æ“
        
        Parameters:
        -----------
        config : PerformanceConfig
            æ€§èƒ½é…ç½®
        """
        self.config = config
        self.n_workers = config.n_workers or mp.cpu_count()
    
    def parallel_posterior_sampling(self,
                                  sample_function: Any,
                                  n_samples: int,
                                  **kwargs) -> np.ndarray:
        """
        ä¸¦è¡Œå¾Œé©—æ¨£æœ¬ç”Ÿæˆ
        
        Parameters:
        -----------
        sample_function : Callable
            æ¡æ¨£å‡½æ•¸
        n_samples : int
            æ¨£æœ¬æ•¸é‡
        **kwargs : dict
            æ¡æ¨£å‡½æ•¸åƒæ•¸
            
        Returns:
        --------
        np.ndarray
            å¾Œé©—æ¨£æœ¬
        """
        
        if not self.config.use_multiprocessing or n_samples < 100:
            # å°æ¨£æœ¬æ™‚ä¸ä½¿ç”¨ä¸¦è¡Œ
            return sample_function(n_samples, **kwargs)
        
        # å°‡ä»»å‹™åˆ†é…çµ¦å¤šå€‹é€²ç¨‹
        chunk_size = max(1, n_samples // self.n_workers)
        chunks = [chunk_size] * (self.n_workers - 1)
        chunks.append(n_samples - sum(chunks))  # æœ€å¾Œä¸€å¡ŠåŒ…å«é¤˜æ•¸
        
        with ProcessPoolExecutor(max_workers=self.n_workers) as executor:
            futures = []
            for chunk in chunks:
                if chunk > 0:
                    future = executor.submit(sample_function, chunk, **kwargs)
                    futures.append(future)
            
            # æ”¶é›†çµæœ
            results = []
            for future in futures:
                results.append(future.result())
        
        # åˆä½µçµæœ
        return np.concatenate(results, axis=0)
    
    def parallel_basis_risk_computation(self,
                                      actual_losses: np.ndarray,
                                      hazard_indices: np.ndarray,
                                      product_parameters: List[Dict[str, float]]) -> List[Dict[str, float]]:
        """
        ä¸¦è¡Œè¨ˆç®—å¤šå€‹ç”¢å“çš„åŸºå·®é¢¨éšª
        
        Parameters:
        -----------
        actual_losses : np.ndarray
            å¯¦éš›æå¤±
        hazard_indices : np.ndarray
            ç½å®³æŒ‡æ¨™
        product_parameters : List[Dict[str, float]]
            ç”¢å“åƒæ•¸åˆ—è¡¨
            
        Returns:
        --------
        List[Dict[str, float]]
            åŸºå·®é¢¨éšªçµæœ
        """
        
        if not self.config.use_multiprocessing or len(product_parameters) < 10:
            # å°‘é‡ç”¢å“æ™‚ä¸ä½¿ç”¨ä¸¦è¡Œ
            calc = VectorizedBasisRiskCalculator(self.config)
            results = []
            
            for params in product_parameters:
                triggers = np.array([params['trigger_threshold']])
                payouts = np.array([params['payout_amount']])
                
                stats = calc.calculate_portfolio_basis_risk_vectorized(
                    actual_losses, hazard_indices,
                    {'trigger_thresholds': triggers, 'payout_amounts': payouts}
                )
                
                results.append({
                    'mean_basis_risk': stats['mean_basis_risk'][0],
                    'std_basis_risk': stats['std_basis_risk'][0],
                    'trigger_rate': np.mean(hazard_indices >= params['trigger_threshold'])
                })
            
            return results
        
        # ä½¿ç”¨ä¸¦è¡Œè™•ç†
        def compute_product_risk(params):
            calc = VectorizedBasisRiskCalculator(self.config)
            triggers = np.array([params['trigger_threshold']])
            payouts = np.array([params['payout_amount']])
            
            stats = calc.calculate_portfolio_basis_risk_vectorized(
                actual_losses, hazard_indices,
                {'trigger_thresholds': triggers, 'payout_amounts': payouts}
            )
            
            return {
                'mean_basis_risk': stats['mean_basis_risk'][0],
                'std_basis_risk': stats['std_basis_risk'][0],
                'trigger_rate': np.mean(hazard_indices >= params['trigger_threshold'])
            }
        
        with ProcessPoolExecutor(max_workers=self.n_workers) as executor:
            results = list(executor.map(compute_product_risk, product_parameters))
        
        return results

class MemoryOptimizedCache:
    """è¨˜æ†¶é«”å„ªåŒ–å¿«å–ç³»çµ±"""
    
    def __init__(self, config: PerformanceConfig):
        """
        åˆå§‹åŒ–å¿«å–ç³»çµ±
        
        Parameters:
        -----------
        config : PerformanceConfig
            æ€§èƒ½é…ç½®
        """
        self.config = config
        self.cache = {}
        self.access_count = {}
        self.max_cache_size = 1000  # æœ€å¤§å¿«å–æ¢ç›®æ•¸
    
    @lru_cache(maxsize=128)
    def cached_basis_risk_calculation(self,
                                    trigger_threshold: float,
                                    payout_amount: float,
                                    losses_hash: int,
                                    indices_hash: int) -> float:
        """
        å¸¶å¿«å–çš„åŸºå·®é¢¨éšªè¨ˆç®—
        
        Parameters:
        -----------
        trigger_threshold, payout_amount : float
            ç”¢å“åƒæ•¸
        losses_hash, indices_hash : int
            æ•¸æ“šé›œæ¹Šå€¼ (ç”¨æ–¼å¿«å–éµ)
            
        Returns:
        --------
        float
            åŸºå·®é¢¨éšª
        """
        
        # é€™è£¡æ‡‰è©²åŒ…å«å¯¦éš›çš„è¨ˆç®—é‚è¼¯
        # ç”±æ–¼è¼¸å…¥æ˜¯é›œæ¹Šå€¼ï¼Œå¯¦éš›å¯¦ç¾ä¸­éœ€è¦é‡æ–°è¨­è¨ˆ
        cache_key = (trigger_threshold, payout_amount, losses_hash, indices_hash)
        
        if cache_key in self.cache:
            self.access_count[cache_key] = self.access_count.get(cache_key, 0) + 1
            return self.cache[cache_key]
        
        # æ¨¡æ“¬è¨ˆç®— (å¯¦éš›æ‡‰ç”¨ä¸­æ›¿æ›ç‚ºçœŸå¯¦è¨ˆç®—)
        result = trigger_threshold * payout_amount * 0.001
        
        # å­˜å…¥å¿«å–
        if len(self.cache) >= self.max_cache_size:
            self._evict_least_used()
        
        self.cache[cache_key] = result
        self.access_count[cache_key] = 1
        
        return result
    
    def _evict_least_used(self):
        """ç§»é™¤æœ€å°‘ä½¿ç”¨çš„å¿«å–æ¢ç›®"""
        if not self.access_count:
            return
        
        # æ‰¾å‡ºä½¿ç”¨æ¬¡æ•¸æœ€å°‘çš„æ¢ç›®
        least_used_key = min(self.access_count.items(), key=lambda x: x[1])[0]
        
        del self.cache[least_used_key]
        del self.access_count[least_used_key]

class SparseMatrixOptimizer:
    """ç¨€ç–çŸ©é™£å„ªåŒ–å™¨"""
    
    def __init__(self, config: PerformanceConfig):
        """
        åˆå§‹åŒ–ç¨€ç–çŸ©é™£å„ªåŒ–å™¨
        
        Parameters:
        -----------
        config : PerformanceConfig
            æ€§èƒ½é…ç½®
        """
        self.config = config
    
    def create_sparse_payout_matrix(self,
                                  hazard_indices: np.ndarray,
                                  trigger_thresholds: np.ndarray,
                                  payout_amounts: np.ndarray) -> csr_matrix:
        """
        å‰µå»ºç¨€ç–è³ ä»˜çŸ©é™£
        
        å°æ–¼å¤§å¤šæ•¸ç½å®³äº‹ä»¶ï¼Œå¤šæ•¸ç”¢å“ä¸æœƒè§¸ç™¼ï¼Œå› æ­¤çŸ©é™£éå¸¸ç¨€ç–
        ä½¿ç”¨ CSR (Compressed Sparse Row) æ ¼å¼å¯å¤§å¹…ç¯€çœè¨˜æ†¶é«”
        
        Parameters:
        -----------
        hazard_indices : np.ndarray
            ç½å®³æŒ‡æ¨™ (n_scenarios,)
        trigger_thresholds : np.ndarray  
            è§¸ç™¼é–¾å€¼ (n_products,)
        payout_amounts : np.ndarray
            è³ ä»˜é‡‘é¡ (n_products,)
            
        Returns:
        --------
        csr_matrix
            ç¨€ç–è³ ä»˜çŸ©é™£ (n_products, n_scenarios)
        """
        
        n_products = len(trigger_thresholds)
        n_scenarios = len(hazard_indices)
        
        # æ‰¾å‡ºæ‰€æœ‰è§¸ç™¼çš„ (product, scenario) å°
        rows, cols, data = [], [], []
        
        for i, (trigger, payout) in enumerate(zip(trigger_thresholds, payout_amounts)):
            # æ‰¾å‡ºè§¸ç™¼æ­¤ç”¢å“çš„æƒ…å¢ƒ
            triggered_scenarios = np.where(hazard_indices >= trigger)[0]
            
            # æ·»åŠ åˆ°ç¨€ç–çŸ©é™£æ•¸æ“š
            rows.extend([i] * len(triggered_scenarios))
            cols.extend(triggered_scenarios)
            data.extend([payout] * len(triggered_scenarios))
        
        # å‰µå»º CSR ç¨€ç–çŸ©é™£
        sparse_payouts = csr_matrix(
            (data, (rows, cols)), 
            shape=(n_products, n_scenarios)
        )
        
        return sparse_payouts
    
    def sparse_basis_risk_calculation(self,
                                    actual_losses: np.ndarray,
                                    sparse_payouts: csr_matrix,
                                    w_under: float = 2.0,
                                    w_over: float = 0.5) -> np.ndarray:
        """
        ä½¿ç”¨ç¨€ç–çŸ©é™£è¨ˆç®—åŸºå·®é¢¨éšª
        
        Parameters:
        -----------
        actual_losses : np.ndarray
            å¯¦éš›æå¤±
        sparse_payouts : csr_matrix
            ç¨€ç–è³ ä»˜çŸ©é™£
        w_under, w_over : float
            æ¬Šé‡åƒæ•¸
            
        Returns:
        --------
        np.ndarray
            å¹³å‡åŸºå·®é¢¨éšª (n_products,)
        """
        
        n_products, n_scenarios = sparse_payouts.shape
        
        # å°‡ç¨€ç–çŸ©é™£è½‰æ›ç‚ºå¯†é›†çŸ©é™£é€²è¡Œè¨ˆç®— (å°è¦æ¨¡æ™‚)
        # å¤§è¦æ¨¡æ™‚éœ€è¦æ›´è¤‡é›œçš„ç¨€ç–é‹ç®—
        if n_products * n_scenarios < 1e6:
            payouts_dense = sparse_payouts.toarray()
            
            # å»£æ’­å¯¦éš›æå¤±
            losses_matrix = np.broadcast_to(
                actual_losses[np.newaxis, :], (n_products, n_scenarios)
            )
            
            # å‘é‡åŒ–è¨ˆç®—åŸºå·®é¢¨éšª
            under_coverage = np.maximum(0, losses_matrix - payouts_dense)
            over_coverage = np.maximum(0, payouts_dense - losses_matrix)
            
            basis_risks = w_under * under_coverage + w_over * over_coverage
            return np.mean(basis_risks, axis=1)
        
        else:
            # å¤§è¦æ¨¡ç¨€ç–çŸ©é™£é‹ç®— (å¯é€²ä¸€æ­¥å„ªåŒ–)
            mean_risks = np.zeros(n_products)
            
            for i in range(n_products):
                product_payouts = sparse_payouts.getrow(i).toarray().flatten()
                
                under_coverage = np.maximum(0, actual_losses - product_payouts)
                over_coverage = np.maximum(0, product_payouts - actual_losses)
                
                mean_risks[i] = np.mean(w_under * under_coverage + w_over * over_coverage)
            
            return mean_risks

class PerformanceBenchmarker:
    """æ€§èƒ½åŸºæº–æ¸¬è©¦å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åŸºæº–æ¸¬è©¦å™¨"""
        self.results = []
    
    def benchmark_calculation_methods(self,
                                    actual_losses: np.ndarray,
                                    hazard_indices: np.ndarray,
                                    product_parameters: Dict[str, np.ndarray],
                                    n_runs: int = 5) -> List[BenchmarkResult]:
        """
        åŸºæº–æ¸¬è©¦ä¸åŒè¨ˆç®—æ–¹æ³•çš„æ€§èƒ½
        
        Parameters:
        -----------
        actual_losses : np.ndarray
            å¯¦éš›æå¤±
        hazard_indices : np.ndarray
            ç½å®³æŒ‡æ¨™
        product_parameters : Dict[str, np.ndarray]
            ç”¢å“åƒæ•¸
        n_runs : int
            æ¸¬è©¦é‹è¡Œæ¬¡æ•¸
            
        Returns:
        --------
        List[BenchmarkResult]
            åŸºæº–æ¸¬è©¦çµæœ
        """
        
        print("ğŸš€ åŸ·è¡Œæ€§èƒ½åŸºæº–æ¸¬è©¦...")
        
        methods = {
            'naive_loops': self._naive_loop_method,
            'vectorized': self._vectorized_method,
            'vectorized_numba': self._vectorized_numba_method,
            'sparse_matrix': self._sparse_matrix_method
        }
        
        baseline_time = None
        results = []
        
        for method_name, method_func in methods.items():
            print(f"  æ¸¬è©¦æ–¹æ³•: {method_name}")
            
            # å¤šæ¬¡é‹è¡Œå–å¹³å‡
            times = []
            memory_usages = []
            
            for run in range(n_runs):
                start_time = time.time()
                start_memory = self._get_memory_usage()
                
                try:
                    result = method_func(actual_losses, hazard_indices, product_parameters)
                    
                    end_time = time.time()
                    end_memory = self._get_memory_usage()
                    
                    execution_time = end_time - start_time
                    memory_usage = end_memory - start_memory
                    
                    times.append(execution_time)
                    memory_usages.append(memory_usage)
                    
                except Exception as e:
                    print(f"    æ–¹æ³• {method_name} åŸ·è¡Œå¤±æ•—: {e}")
                    continue
            
            if times:
                avg_time = np.mean(times)
                avg_memory = np.mean(memory_usages)
                
                # è¨ˆç®—åŠ é€Ÿæ¯”
                if baseline_time is None:
                    baseline_time = avg_time
                    speedup_ratio = 1.0
                else:
                    speedup_ratio = baseline_time / avg_time
                
                benchmark_result = BenchmarkResult(
                    method_name=method_name,
                    execution_time=avg_time,
                    memory_usage=avg_memory,
                    speedup_ratio=speedup_ratio,
                    accuracy_loss=0.0  # ç°¡åŒ–ï¼Œå¯¦éš›éœ€è¦è¨ˆç®—æº–ç¢ºåº¦å·®ç•°
                )
                
                results.append(benchmark_result)
                
                print(f"    å¹³å‡æ™‚é–“: {avg_time:.4f}s")
                print(f"    è¨˜æ†¶é«”ä½¿ç”¨: {avg_memory:.2f}MB")
                print(f"    åŠ é€Ÿæ¯”: {speedup_ratio:.2f}x")
        
        self.results = results
        return results
    
    def _naive_loop_method(self,
                          actual_losses: np.ndarray,
                          hazard_indices: np.ndarray,
                          product_parameters: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """åŸå§‹çš„å¤šé‡è¿´åœˆæ–¹æ³• (ä½œç‚ºåŸºæº–)"""
        
        triggers = product_parameters['trigger_thresholds']
        payouts = product_parameters['payout_amounts']
        
        n_products = len(triggers)
        n_scenarios = len(actual_losses)
        
        mean_risks = np.zeros(n_products)
        basis_risk_calc = BasisRiskCalculator()
        
        # ä¸‰é‡è¿´åœˆ (ä½æ•ˆ)
        for i in range(n_products):
            risks = []
            for j in range(n_scenarios):
                # è¨ˆç®—è³ ä»˜
                payout = payouts[i] if hazard_indices[j] >= triggers[i] else 0.0
                
                # è¨ˆç®—åŸºå·®é¢¨éšª
                risk = basis_risk_calc.calculate_weighted_asymmetric_basis_risk(
                    actual_losses[j], payout, w_under=2.0, w_over=0.5
                )
                risks.append(risk)
            
            mean_risks[i] = np.mean(risks)
        
        return {'mean_basis_risk': mean_risks}
    
    def _vectorized_method(self,
                          actual_losses: np.ndarray,
                          hazard_indices: np.ndarray,
                          product_parameters: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """å‘é‡åŒ–æ–¹æ³•"""
        
        config = PerformanceConfig(use_numba=False)
        calc = VectorizedBasisRiskCalculator(config)
        
        return calc.calculate_portfolio_basis_risk_vectorized(
            actual_losses, hazard_indices, product_parameters
        )
    
    def _vectorized_numba_method(self,
                                actual_losses: np.ndarray,
                                hazard_indices: np.ndarray,
                                product_parameters: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """å‘é‡åŒ– + Numba æ–¹æ³•"""
        
        config = PerformanceConfig(use_numba=True)
        calc = VectorizedBasisRiskCalculator(config)
        
        return calc.calculate_portfolio_basis_risk_vectorized(
            actual_losses, hazard_indices, product_parameters
        )
    
    def _sparse_matrix_method(self,
                             actual_losses: np.ndarray,
                             hazard_indices: np.ndarray,
                             product_parameters: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """ç¨€ç–çŸ©é™£æ–¹æ³•"""
        
        config = PerformanceConfig()
        optimizer = SparseMatrixOptimizer(config)
        
        # å‰µå»ºç¨€ç–è³ ä»˜çŸ©é™£
        sparse_payouts = optimizer.create_sparse_payout_matrix(
            hazard_indices,
            product_parameters['trigger_thresholds'],
            product_parameters['payout_amounts']
        )
        
        # è¨ˆç®—åŸºå·®é¢¨éšª
        mean_risks = optimizer.sparse_basis_risk_calculation(
            actual_losses, sparse_payouts
        )
        
        return {'mean_basis_risk': mean_risks}
    
    def _get_memory_usage(self) -> float:
        """ç²å–ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨é‡ (MB)"""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except ImportError:
            return 0.0  # å¦‚æœæ²’æœ‰ psutilï¼Œè¿”å› 0
    
    def visualize_benchmark_results(self, output_dir: str = "results") -> None:
        """è¦–è¦ºåŒ–åŸºæº–æ¸¬è©¦çµæœ"""
        
        if not self.results:
            print("ç„¡åŸºæº–æ¸¬è©¦çµæœå¯è¦–è¦ºåŒ–")
            return
        
        print("ğŸ“Š ç”Ÿæˆæ€§èƒ½åŸºæº–æ¸¬è©¦è¦–è¦ºåŒ–...")
        
        # æº–å‚™æ•¸æ“š
        methods = [r.method_name for r in self.results]
        times = [r.execution_time for r in self.results]
        speedups = [r.speedup_ratio for r in self.results]
        memories = [r.memory_usage for r in self.results]
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        fig.suptitle('è¨ˆç®—æ•ˆç‡å„ªåŒ–æ€§èƒ½åŸºæº–æ¸¬è©¦', fontsize=16, fontweight='bold')
        
        # 1. åŸ·è¡Œæ™‚é–“æ¯”è¼ƒ
        ax1 = axes[0]
        bars1 = ax1.bar(methods, times, color=['red', 'blue', 'green', 'orange'])
        ax1.set_ylabel('åŸ·è¡Œæ™‚é–“ (ç§’)')
        ax1.set_title('åŸ·è¡Œæ™‚é–“æ¯”è¼ƒ')
        ax1.tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar, time_val in zip(bars1, times):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{time_val:.3f}s', ha='center', va='bottom')
        
        # 2. åŠ é€Ÿæ¯”æ¯”è¼ƒ
        ax2 = axes[1]
        bars2 = ax2.bar(methods, speedups, color=['red', 'blue', 'green', 'orange'])
        ax2.set_ylabel('åŠ é€Ÿæ¯” (å€)')
        ax2.set_title('ç›¸å°åŠ é€Ÿæ¯”')
        ax2.tick_params(axis='x', rotation=45)
        ax2.axhline(y=1, color='black', linestyle='--', alpha=0.5)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar, speedup in zip(bars2, speedups):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{speedup:.1f}x', ha='center', va='bottom')
        
        # 3. è¨˜æ†¶é«”ä½¿ç”¨æ¯”è¼ƒ
        ax3 = axes[2]
        bars3 = ax3.bar(methods, memories, color=['red', 'blue', 'green', 'orange'])
        ax3.set_ylabel('è¨˜æ†¶é«”ä½¿ç”¨ (MB)')
        ax3.set_title('è¨˜æ†¶é«”ä½¿ç”¨æ¯”è¼ƒ')
        ax3.tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar, memory in zip(bars3, memories):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height,
                    f'{memory:.1f}MB', ha='center', va='bottom')
        
        plt.tight_layout()
        
        # ä¿å­˜åœ–è¡¨
        Path(output_dir).mkdir(exist_ok=True)
        output_file = Path(output_dir) / "computational_efficiency_benchmark.png"
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… æ€§èƒ½åŸºæº–æ¸¬è©¦è¦–è¦ºåŒ–å·²ä¿å­˜: {output_file}")

class ComputationalEfficiencyFramework:
    """è¨ˆç®—æ•ˆç‡å„ªåŒ–æ•´åˆæ¡†æ¶"""
    
    def __init__(self, config: PerformanceConfig = None):
        """
        åˆå§‹åŒ–è¨ˆç®—æ•ˆç‡æ¡†æ¶
        
        Parameters:
        -----------
        config : PerformanceConfig
            æ€§èƒ½é…ç½®
        """
        self.config = config or PerformanceConfig()
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.vectorized_calc = VectorizedBasisRiskCalculator(self.config)
        self.parallel_engine = ParallelMonteCarloEngine(self.config)
        self.cache = MemoryOptimizedCache(self.config)
        self.sparse_optimizer = SparseMatrixOptimizer(self.config)
        self.benchmarker = PerformanceBenchmarker()
    
    def optimize_parametric_insurance_analysis(self,
                                             actual_losses: np.ndarray,
                                             hazard_indices: np.ndarray,
                                             n_products: int = 1000,
                                             run_benchmark: bool = True) -> Dict[str, Any]:
        """
        åŸ·è¡Œå„ªåŒ–çš„åƒæ•¸ä¿éšªåˆ†æ
        
        Parameters:
        -----------
        actual_losses : np.ndarray
            å¯¦éš›æå¤±æ•¸æ“š
        hazard_indices : np.ndarray
            ç½å®³æŒ‡æ¨™æ•¸æ“š
        n_products : int
            åˆ†æçš„ç”¢å“æ•¸é‡
        run_benchmark : bool
            æ˜¯å¦åŸ·è¡Œæ€§èƒ½åŸºæº–æ¸¬è©¦
            
        Returns:
        --------
        Dict[str, Any]
            å„ªåŒ–åˆ†æçµæœ
        """
        
        print("âš¡ åŸ·è¡Œå„ªåŒ–çš„åƒæ•¸ä¿éšªåˆ†æ...")
        print("=" * 60)
        
        # ç”Ÿæˆç”¢å“åƒæ•¸
        print(f"ğŸ“¦ ç”Ÿæˆ {n_products} å€‹å€™é¸ç”¢å“...")
        
        np.random.seed(42)
        trigger_range = (np.percentile(hazard_indices, 60), np.percentile(hazard_indices, 95))
        payout_range = (np.percentile(actual_losses, 20), np.percentile(actual_losses, 80))
        
        trigger_thresholds = np.random.uniform(trigger_range[0], trigger_range[1], n_products)
        payout_amounts = np.random.uniform(payout_range[0], payout_range[1], n_products)
        
        product_parameters = {
            'trigger_thresholds': trigger_thresholds,
            'payout_amounts': payout_amounts
        }
        
        # åŸ·è¡ŒåŸºæº–æ¸¬è©¦
        benchmark_results = None
        if run_benchmark:
            print("ğŸš€ åŸ·è¡Œæ€§èƒ½åŸºæº–æ¸¬è©¦...")
            
            # ä½¿ç”¨è¼ƒå°çš„å­é›†é€²è¡ŒåŸºæº–æ¸¬è©¦
            test_size = min(100, n_products)
            test_params = {
                'trigger_thresholds': trigger_thresholds[:test_size],
                'payout_amounts': payout_amounts[:test_size]
            }
            
            benchmark_results = self.benchmarker.benchmark_calculation_methods(
                actual_losses, hazard_indices, test_params
            )
            
            # è¦–è¦ºåŒ–åŸºæº–æ¸¬è©¦çµæœ
            self.benchmarker.visualize_benchmark_results()
        
        # åŸ·è¡Œå„ªåŒ–åˆ†æ
        print("âš¡ åŸ·è¡Œå‘é‡åŒ–å„ªåŒ–åˆ†æ...")
        start_time = time.time()
        
        optimized_stats = self.vectorized_calc.calculate_portfolio_basis_risk_vectorized(
            actual_losses, hazard_indices, product_parameters
        )
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # æ‰¾å‡ºæœ€ä½³ç”¢å“
        best_product_idx = np.argmin(optimized_stats['mean_basis_risk'])
        
        results = {
            'analysis_summary': {
                'n_products_analyzed': n_products,
                'n_scenarios': len(actual_losses),
                'total_execution_time': execution_time,
                'products_per_second': n_products / execution_time
            },
            
            'optimal_product': {
                'index': int(best_product_idx),
                'trigger_threshold': trigger_thresholds[best_product_idx],
                'payout_amount': payout_amounts[best_product_idx],
                'mean_basis_risk': optimized_stats['mean_basis_risk'][best_product_idx],
                'std_basis_risk': optimized_stats['std_basis_risk'][best_product_idx],
                'max_basis_risk': optimized_stats['max_basis_risk'][best_product_idx]
            },
            
            'performance_statistics': optimized_stats,
            'benchmark_results': benchmark_results
        }
        
        print(f"âœ… å„ªåŒ–åˆ†æå®Œæˆ!")
        print(f"   åˆ†æç”¢å“æ•¸: {n_products:,}")
        print(f"   åŸ·è¡Œæ™‚é–“: {execution_time:.3f} ç§’")
        print(f"   åˆ†æé€Ÿåº¦: {n_products/execution_time:.0f} ç”¢å“/ç§’")
        print(f"   æœ€ä½³ç”¢å“åŸºå·®é¢¨éšª: {results['optimal_product']['mean_basis_risk']:.2e}")
        
        return results

def main():
    """ä¸»å‡½æ•¸ç¤ºä¾‹"""
    
    print("âš¡ è¨ˆç®—æ•ˆç‡å„ªåŒ–æ¡†æ¶ç¤ºä¾‹...")
    print("=" * 80)
    
    # ç”Ÿæˆå¤§è¦æ¨¡æ¸¬è©¦æ•¸æ“š
    print("ğŸ“Š ç”Ÿæˆå¤§è¦æ¨¡æ¸¬è©¦æ•¸æ“š...")
    
    np.random.seed(42)
    n_scenarios = 5000   # 5000 å€‹æå¤±æƒ…å¢ƒ
    n_products = 2000    # 2000 å€‹å€™é¸ç”¢å“
    
    # ç”Ÿæˆæå¤±æ•¸æ“š (æ··åˆåˆ†ä½ˆ)
    normal_losses = np.random.lognormal(np.log(5e7), 0.8, int(0.8 * n_scenarios))
    extreme_losses = np.random.lognormal(np.log(2e8), 1.0, int(0.2 * n_scenarios))
    actual_losses = np.concatenate([normal_losses, extreme_losses])
    np.random.shuffle(actual_losses)
    
    # ç”Ÿæˆç½å®³æŒ‡æ¨™
    hazard_indices = np.random.gamma(2, 25, n_scenarios)
    
    print(f"   æå¤±æƒ…å¢ƒæ•¸: {n_scenarios:,}")
    print(f"   å€™é¸ç”¢å“æ•¸: {n_products:,}")
    print(f"   è¨ˆç®—è¤‡é›œåº¦: {n_scenarios * n_products:,} æ¬¡åŸºå·®é¢¨éšªè¨ˆç®—")
    print(f"   æå¤±ç¯„åœ: {actual_losses.min():.2e} - {actual_losses.max():.2e}")
    
    # å‰µå»ºå„ªåŒ–æ¡†æ¶
    config = PerformanceConfig(
        use_vectorization=True,
        use_numba=True,
        use_multiprocessing=True,
        n_workers=4
    )
    
    framework = ComputationalEfficiencyFramework(config)
    
    # åŸ·è¡Œå„ªåŒ–åˆ†æ
    results = framework.optimize_parametric_insurance_analysis(
        actual_losses, hazard_indices, n_products, run_benchmark=True
    )
    
    # è¼¸å‡ºæ€§èƒ½ç¸½çµ
    print("\n" + "=" * 80)
    print("ğŸ† æ€§èƒ½å„ªåŒ–ç¸½çµ:")
    print("=" * 80)
    
    analysis_summary = results['analysis_summary']
    print(f"âœ… æˆåŠŸåˆ†æ: {analysis_summary['n_products_analyzed']:,} å€‹ç”¢å“")
    print(f"â±ï¸  ç¸½åŸ·è¡Œæ™‚é–“: {analysis_summary['total_execution_time']:.3f} ç§’")
    print(f"ğŸš€ åˆ†æé€Ÿåº¦: {analysis_summary['products_per_second']:.0f} ç”¢å“/ç§’")
    
    # åŸºæº–æ¸¬è©¦çµæœ
    if results['benchmark_results']:
        print(f"\nğŸ“Š æ€§èƒ½æå‡:")
        
        baseline = results['benchmark_results'][0]  # naive_loops æ–¹æ³•
        best_optimized = max(results['benchmark_results'], key=lambda x: x.speedup_ratio)
        
        print(f"   åŸºæº–æ–¹æ³• ({baseline.method_name}): {baseline.execution_time:.3f}s")
        print(f"   æœ€ä½³å„ªåŒ–æ–¹æ³• ({best_optimized.method_name}): {best_optimized.execution_time:.3f}s")
        print(f"   ğŸ”¥ ç¸½åŠ é€Ÿæ¯”: {best_optimized.speedup_ratio:.1f}x")
        
        # é ä¼°å¤§è¦æ¨¡åˆ†æçš„æ™‚é–“ç¯€çœ
        full_scale_baseline = baseline.execution_time * (n_products / 100)  # åŸºæº–æ¸¬è©¦åªç”¨äº†100å€‹ç”¢å“
        full_scale_optimized = best_optimized.execution_time * (n_products / 100)
        time_saved = full_scale_baseline - full_scale_optimized
        
        print(f"   ğŸ’° é ä¼°å…¨è¦æ¨¡åˆ†ææ™‚é–“ç¯€çœ: {time_saved:.1f} ç§’")
    
    # æœ€ä½³ç”¢å“çµæœ
    optimal = results['optimal_product']
    print(f"\nğŸ¯ æœ€ä½³ç”¢å“:")
    print(f"   è§¸ç™¼é–¾å€¼: {optimal['trigger_threshold']:.2f}")
    print(f"   è³ ä»˜é‡‘é¡: {optimal['payout_amount']:.2e}")
    print(f"   åŸºå·®é¢¨éšª: {optimal['mean_basis_risk']:.2e}")
    
    print(f"\nğŸ“ çµæœå·²ä¿å­˜åœ¨: results/")
    print("ğŸ‰ è¨ˆç®—æ•ˆç‡å„ªåŒ–å®Œæˆï¼")

if __name__ == "__main__":
    main()
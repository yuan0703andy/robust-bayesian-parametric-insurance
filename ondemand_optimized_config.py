#!/usr/bin/env python3
"""
OnDemand é«˜æ•ˆèƒ½å„ªåŒ–é…ç½®
OnDemand High-Performance Optimized Configuration

å°ˆç‚º OnDemand ç’°å¢ƒå„ªåŒ–çš„é…ç½®ï¼Œç§»é™¤æ‰€æœ‰ç’°å¢ƒæª¢æ¸¬å’Œå‹•æ…‹é…ç½®
Optimized configuration specifically for OnDemand environment, removing all environment detection and dynamic configuration
"""

import os
import warnings

def setup_ondemand_environment():
    """è¨­ç½® OnDemand é«˜æ•ˆèƒ½ç’°å¢ƒ"""
    
    print("ğŸš€ è¨­ç½® OnDemand é«˜æ•ˆèƒ½é…ç½®...")
    
    # OnDemand å„ªåŒ–çš„ç’°å¢ƒè®Šæ•¸
    os.environ["JAX_PLATFORM_NAME"] = "cpu"
    os.environ["PYTENSOR_FLAGS"] = "device=cpu,floatX=float32,mode=FAST_RUN,optimizer=fast_run,cxx="
    os.environ["MKL_THREADING_LAYER"] = "GNU"
    os.environ["OMP_NUM_THREADS"] = "8"  # OnDemand é€šå¸¸æœ‰å¤šæ ¸å¿ƒ
    os.environ["OPENBLAS_NUM_THREADS"] = "8"
    os.environ["MKL_NUM_THREADS"] = "8"
    
    # æŠ‘åˆ¶ä¸å¿…è¦çš„è­¦å‘Š
    warnings.filterwarnings('ignore', category=FutureWarning)
    warnings.filterwarnings('ignore', message='.*axis.*dim.*')
    warnings.filterwarnings('ignore', message='.*ambiguous.*')
    
    print("âœ… OnDemand ç’°å¢ƒè®Šæ•¸è¨­ç½®å®Œæˆ")
    
    return get_ondemand_config()


def get_ondemand_config():
    """ç²å– OnDemand é«˜æ•ˆèƒ½é…ç½®"""
    
    return {
        # PyMC é…ç½®
        'pymc_backend': 'cpu',
        'pymc_mode': 'FAST_RUN',  # ç”Ÿç”¢æ¨¡å¼ï¼Œæœ€å¿«åŸ·è¡Œ
        'n_threads': 8,           # åˆ©ç”¨ OnDemand å¤šæ ¸å¿ƒ
        'configure_pymc': False,  # é¿å…å‹•æ…‹é…ç½®è¡çª
        
        # MCMC æ¡æ¨£åƒæ•¸ (é«˜æ•ˆèƒ½)
        'draws': 1000,            # æ›´å¤šæ¨£æœ¬ç²å¾—æ›´å¥½çµæœ
        'tune': 500,              # é©ä¸­çš„èª¿åƒæ­¥æ•¸
        'chains': 4,              # 4æ¢éˆå……åˆ†åˆ©ç”¨å¤šæ ¸å¿ƒ
        'target_accept': 0.9,     # è¼ƒé«˜æ¥å—ç‡ç¢ºä¿æ”¶æ–‚
        'cores': 4,               # ä¸¦è¡Œæ¡æ¨£
        'return_inferencedata': True,
        'progressbar': True,
        
        # åˆ†æåƒæ•¸ (é«˜æ•ˆèƒ½)
        'n_monte_carlo_samples': 2000,  # æ›´å¤š Monte Carlo æ¨£æœ¬
        'n_loss_scenarios': 1000,       # æ›´å¤šæå¤±æƒ…å¢ƒ
        'n_mixture_components': 5,       # æ›´è¤‡é›œçš„æ··åˆæ¨¡å‹
        
        # å„ªåŒ–åƒæ•¸
        'max_training_samples': None,    # ä¸é™åˆ¶ï¼Œä½¿ç”¨å…¨éƒ¨æ•¸æ“š
        'max_validation_samples': None,  # ä¸é™åˆ¶ï¼Œä½¿ç”¨å…¨éƒ¨æ•¸æ“š
        'density_ratio_constraint': 2.0,
        
        # éŒ¯èª¤è™•ç†
        'fallback_to_demo': False,      # é«˜æ•ˆèƒ½æ¨¡å¼ä¸ä½¿ç”¨æ¼”ç¤ºå›é€€
        'suppress_warnings': True,      # æŠ‘åˆ¶è­¦å‘Šæå‡æ•ˆèƒ½
        'verbose': True                 # ä½†ä¿æŒè©³ç´°è¼¸å‡ºç”¨æ–¼ç›£æ§
    }


def get_ondemand_data_config():
    """ç²å– OnDemand æ•¸æ“šè™•ç†é…ç½®"""
    
    return {
        # æ•¸æ“šåˆ†å‰²æ¯”ä¾‹ (ä½¿ç”¨æ›´å¤šæ•¸æ“š)
        'train_ratio': 0.8,          # 80% è¨“ç·´æ•¸æ“š
        'validation_ratio': 0.2,     # 20% é©—è­‰æ•¸æ“š
        'min_train_samples': 50,     # æœ€å°è¨“ç·´æ¨£æœ¬æ•¸
        'min_validation_samples': 20, # æœ€å°é©—è­‰æ¨£æœ¬æ•¸
        
        # ç”¢å“åƒæ•¸é‚Šç•Œ (æ›´å»£æ³›çš„æœç´¢)
        'trigger_threshold_range': (20, 75),      # æ›´å»£çš„è§¸ç™¼é–¾å€¼ç¯„åœ
        'payout_multiplier_range': (0.3, 3.0),   # æ›´å»£çš„è³ ä»˜å€æ•¸ç¯„åœ
        'max_payout_multiplier_range': (2.0, 6.0), # æ›´å»£çš„æœ€å¤§è³ ä»˜å€æ•¸
        
        # åŸºå·®é¢¨éšªæœ€ä½³åŒ–
        'w_under': 2.5,              # ç¨å¾®æé«˜è³ ä»˜ä¸è¶³æ‡²ç½°
        'w_over': 0.4,               # é™ä½éåº¦è³ ä»˜æ‡²ç½°
        'optimization_method': 'L-BFGS-B',  # æ›´å¿«çš„å„ªåŒ–ç®—æ³•
        'max_iterations': 200,       # æ›´å¤šè¿­ä»£æ¬¡æ•¸
        'tolerance': 1e-8            # æ›´é«˜ç²¾åº¦
    }


def create_ondemand_notebook_config():
    """ç‚º notebook å‰µå»º OnDemand é…ç½®ä»£ç¢¼"""
    
    config_code = '''
# %% OnDemand é«˜æ•ˆèƒ½é…ç½®
import os
import warnings
import numpy as np
import pandas as pd
from datetime import datetime

print("ğŸš€ OnDemand é«˜æ•ˆèƒ½æ¨¡å¼å•Ÿå‹•...")

# è¨­ç½® OnDemand å„ªåŒ–ç’°å¢ƒè®Šæ•¸
os.environ["JAX_PLATFORM_NAME"] = "cpu"
os.environ["PYTENSOR_FLAGS"] = "device=cpu,floatX=float32,mode=FAST_RUN,optimizer=fast_run,cxx="
os.environ["MKL_THREADING_LAYER"] = "GNU"
os.environ["OMP_NUM_THREADS"] = "8"      # åˆ©ç”¨ OnDemand å¤šæ ¸å¿ƒ
os.environ["OPENBLAS_NUM_THREADS"] = "8"
os.environ["MKL_NUM_THREADS"] = "8"

# æŠ‘åˆ¶è­¦å‘Šæå‡æ•ˆèƒ½
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', message='.*axis.*dim.*')

# OnDemand é«˜æ•ˆèƒ½åƒæ•¸
pymc_config = {
    'pymc_backend': 'cpu',
    'pymc_mode': 'FAST_RUN',
    'n_threads': 8,
    'configure_pymc': False
}

# é«˜æ•ˆèƒ½åˆ†æåƒæ•¸
n_monte_carlo_samples = 2000   # é«˜è³ªé‡ Monte Carlo
n_loss_scenarios = 1000        # è±å¯Œçš„æå¤±æƒ…å¢ƒ
n_mixture_components = 5       # è¤‡é›œæ··åˆæ¨¡å‹

# MCMC é«˜æ•ˆèƒ½åƒæ•¸
mcmc_params = {
    'draws': 1000,
    'tune': 500, 
    'chains': 4,
    'cores': 4,
    'target_accept': 0.9
}

print("âœ… OnDemand é«˜æ•ˆèƒ½é…ç½®å®Œæˆ")
print(f"   ğŸ–¥ï¸ CPU æ ¸å¿ƒ: {os.environ.get('OMP_NUM_THREADS')}")
print(f"   ğŸ¯ Monte Carlo æ¨£æœ¬: {n_monte_carlo_samples}")
print(f"   ğŸ”„ MCMC éˆæ•¸: {mcmc_params['chains']}")
print(f"   ğŸ“Š æå¤±æƒ…å¢ƒ: {n_loss_scenarios}")
'''
    
    return config_code


def apply_ondemand_optimizations():
    """æ‡‰ç”¨ OnDemand ç‰¹å®šçš„æ•ˆèƒ½å„ªåŒ–"""
    
    print("âš¡ æ‡‰ç”¨ OnDemand æ•ˆèƒ½å„ªåŒ–...")
    
    try:
        # è¨­ç½® NumPy å¤šç·šç¨‹
        import numpy as np
        if hasattr(np, '__config__') and hasattr(np.__config__, 'show'):
            print("  ğŸ“Š NumPy é…ç½®å·²å„ªåŒ–")
            
        # è¨­ç½® Pandas æ•ˆèƒ½é¸é …
        import pandas as pd
        pd.set_option('compute.use_bottleneck', True)
        pd.set_option('compute.use_numexpr', True)
        print("  ğŸ¼ Pandas æ•ˆèƒ½é¸é …å·²å•Ÿç”¨")
        
        # è¨˜æ†¶é«”å’Œè¨ˆç®—å„ªåŒ–
        import gc
        gc.collect()  # æ¸…ç†è¨˜æ†¶é«”
        print("  ğŸ§¹ è¨˜æ†¶é«”å·²å„ªåŒ–")
        
        print("âœ… OnDemand æ•ˆèƒ½å„ªåŒ–å®Œæˆ")
        
    except ImportError as e:
        print(f"  âš ï¸ éƒ¨åˆ†å„ªåŒ–è·³é: {e}")


if __name__ == "__main__":
    print("ğŸš€ OnDemand é«˜æ•ˆèƒ½é…ç½®")
    print("=" * 50)
    
    # è¨­ç½®ç’°å¢ƒ
    config = setup_ondemand_environment()
    
    # æ‡‰ç”¨å„ªåŒ–
    apply_ondemand_optimizations()
    
    # é¡¯ç¤ºé…ç½®
    print(f"\nğŸ“‹ OnDemand é…ç½®æ‘˜è¦:")
    print(f"   PyMC æ¨¡å¼: {config['pymc_mode']}")
    print(f"   CPU æ ¸å¿ƒ: {config['n_threads']}")
    print(f"   MCMC éˆæ•¸: {config['chains']}")
    print(f"   æ¨£æœ¬æ•¸: {config['draws']}")
    print(f"   Monte Carlo: {config['n_monte_carlo_samples']}")
    print(f"   æå¤±æƒ…å¢ƒ: {config['n_loss_scenarios']}")
    
    # ç”Ÿæˆ notebook é…ç½®
    notebook_config = create_ondemand_notebook_config()
    
    with open('ondemand_notebook_config.py', 'w') as f:
        f.write(notebook_config)
    
    print(f"\nğŸ’¾ å·²ç”Ÿæˆ:")
    print("   - ondemand_notebook_config.py: notebook é…ç½®ä»£ç¢¼")
    
    print(f"\nğŸ¯ ä½¿ç”¨æ–¹å¼:")
    print("1. å°‡ ondemand_notebook_config.py çš„å…§å®¹è¤‡è£½åˆ° notebook ç¬¬ä¸€å€‹ cell")
    print("2. æˆ–è€…åœ¨ Python è…³æœ¬é–‹é ­å°å…¥: from ondemand_optimized_config import setup_ondemand_environment")
    print("3. äº«å— OnDemand é«˜æ•ˆèƒ½åˆ†æï¼")
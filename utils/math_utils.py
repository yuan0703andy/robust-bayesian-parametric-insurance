#!/usr/bin/env python3
"""
Mathematical Utilities Module
æ•¸å­¸å·¥å…·æ¨¡çµ„

æä¾›æ•´å€‹æ¡†æ¶ä¸­ä½¿ç”¨çš„æ•¸å­¸å·¥å…·å‡½æ•¸

æ ¸å¿ƒåŠŸèƒ½:
- CRPSè¨ˆç®—å’Œç›¸é—œçµ±è¨ˆ
- è²æ°çµ±è¨ˆå·¥å…·
- æ•¸å€¼å„ªåŒ–è¼”åŠ©
- åˆ†å¸ƒè™•ç†å·¥å…·

Author: Research Team  
Date: 2025-01-17
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from scipy import stats, optimize
from scipy.special import erf, gamma
import warnings

# ========================================
# CRPS ç›¸é—œå‡½æ•¸
# ========================================

def crps_empirical(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    è¨ˆç®—ç¶“é©—CRPS (Continuous Ranked Probability Score)
    
    CRPS = E|Y - X| - 0.5 * E|X - X'|
    
    Parameters:
    -----------
    y_true : np.ndarray
        çœŸå¯¦å€¼
    y_pred : np.ndarray
        é æ¸¬å€¼é›†åˆ
        
    Returns:
    --------
    float
        CRPSå€¼
    """
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    
    if y_true.ndim == 0:
        y_true = y_true.reshape(1)
    
    # ç¢ºä¿ç¶­åº¦åŒ¹é…
    if y_pred.ndim == 1:
        y_pred = y_pred.reshape(1, -1)
    
    crps_values = []
    
    for i in range(len(y_true)):
        obs = y_true[i]
        forecast = y_pred[i] if y_pred.shape[0] > 1 else y_pred[0]
        
        # CRPSè¨ˆç®—
        term1 = np.mean(np.abs(forecast - obs))
        term2 = 0.5 * np.mean(np.abs(forecast[:, None] - forecast[None, :]))
        
        crps_val = term1 - term2
        crps_values.append(crps_val)
    
    return np.mean(crps_values)

def crps_normal(y_true: float, mu: float, sigma: float) -> float:
    """
    æ­£æ…‹åˆ†å¸ƒçš„è§£æCRPSè¨ˆç®—
    
    Parameters:
    -----------
    y_true : float
        è§€æ¸¬å€¼
    mu : float
        æ­£æ…‹åˆ†å¸ƒå‡å€¼
    sigma : float
        æ­£æ…‹åˆ†å¸ƒæ¨™æº–å·®
        
    Returns:
    --------
    float
        CRPSå€¼
    """
    if sigma <= 0:
        return abs(y_true - mu)
    
    z = (y_true - mu) / sigma
    
    # è§£æå…¬å¼
    crps = sigma * (z * (2 * stats.norm.cdf(z) - 1) + 
                   2 * stats.norm.pdf(z) - 1 / np.sqrt(np.pi))
    
    return crps

def crps_lognormal(y_true: float, mu: float, sigma: float) -> float:
    """
    å°æ•¸æ­£æ…‹åˆ†å¸ƒçš„CRPSè¨ˆç®—
    
    Parameters:
    -----------
    y_true : float
        è§€æ¸¬å€¼
    mu : float
        å°æ•¸æ­£æ…‹åˆ†å¸ƒçš„log-scaleå‡å€¼
    sigma : float
        å°æ•¸æ­£æ…‹åˆ†å¸ƒçš„log-scaleæ¨™æº–å·®
        
    Returns:
    --------
    float
        CRPSå€¼
    """
    if y_true <= 0:
        return np.inf
    
    if sigma <= 0:
        return abs(y_true - np.exp(mu))
    
    # ä½¿ç”¨æ•¸å€¼ç©åˆ†è¿‘ä¼¼
    # é€™æ˜¯ç°¡åŒ–å¯¦ç¾ï¼Œå®Œæ•´ç‰ˆæœ¬éœ€è¦æ›´è¤‡é›œçš„ç©åˆ†
    samples = np.random.lognormal(mu, sigma, 1000)
    return crps_empirical(np.array([y_true]), samples.reshape(1, -1))

def crps_ensemble(y_true: np.ndarray, ensemble: np.ndarray) -> np.ndarray:
    """
    è¨ˆç®—é›†åˆé æ¸¬çš„CRPS
    
    Parameters:
    -----------
    y_true : np.ndarray
        çœŸå¯¦å€¼ (n_observations,)
    ensemble : np.ndarray
        é›†åˆé æ¸¬ (n_observations, n_ensemble_members)
        
    Returns:
    --------
    np.ndarray
        æ¯å€‹è§€æ¸¬çš„CRPSå€¼
    """
    y_true = np.asarray(y_true)
    ensemble = np.asarray(ensemble)
    
    if ensemble.ndim == 1:
        ensemble = ensemble.reshape(-1, 1)
    
    if y_true.shape[0] != ensemble.shape[0]:
        raise ValueError("è§€æ¸¬å€¼å’Œé›†åˆé æ¸¬çš„æ•¸é‡ä¸åŒ¹é…")
    
    crps_values = []
    
    for i in range(len(y_true)):
        obs = y_true[i]
        forecast = ensemble[i, :]
        
        # ç¶“é©—CRPSè¨ˆç®—
        term1 = np.mean(np.abs(forecast - obs))
        
        # è¨ˆç®—é›†åˆæˆå“¡é–“çš„å¹³å‡çµ•å°å·®ç•°
        n_members = len(forecast)
        term2 = 0.0
        for j in range(n_members):
            for k in range(n_members):
                term2 += np.abs(forecast[j] - forecast[k])
        term2 /= (2 * n_members**2)
        
        crps_val = term1 - term2
        crps_values.append(crps_val)
    
    return np.array(crps_values)

# ========================================
# è²æ°çµ±è¨ˆå·¥å…·
# ========================================

def effective_sample_size(samples: np.ndarray, max_lag: int = None) -> float:
    """
    è¨ˆç®—æœ‰æ•ˆæ¨£æœ¬å¤§å°
    
    Parameters:
    -----------
    samples : np.ndarray
        MCMCæ¨£æœ¬
    max_lag : int, optional
        æœ€å¤§æ»¯å¾Œ
        
    Returns:
    --------
    float
        æœ‰æ•ˆæ¨£æœ¬å¤§å°
    """
    n = len(samples)
    if max_lag is None:
        max_lag = min(n // 4, 200)
    
    # å»é™¤å‡å€¼
    centered = samples - np.mean(samples)
    
    # è¨ˆç®—è‡ªç›¸é—œå‡½æ•¸
    autocorrs = []
    for lag in range(max_lag + 1):
        if lag == 0:
            autocorr = 1.0
        else:
            valid_pairs = n - lag
            if valid_pairs <= 0:
                break
            
            numerator = np.sum(centered[:-lag] * centered[lag:])
            denominator = np.sum(centered**2)
            
            if denominator == 0:
                autocorr = 0
            else:
                autocorr = numerator / denominator
        
        autocorrs.append(autocorr)
        
        # åœæ­¢æ¢ä»¶ï¼šè‡ªç›¸é—œè®Šç‚ºè² æ•¸æˆ–å¾ˆå°
        if lag > 0 and autocorr <= 0.05:
            break
    
    # è¨ˆç®—ç©åˆ†è‡ªç›¸é—œæ™‚é–“
    tau_int = 1 + 2 * np.sum(autocorrs[1:])
    
    # æœ‰æ•ˆæ¨£æœ¬å¤§å°
    n_eff = n / tau_int if tau_int > 0 else n
    
    return max(1.0, n_eff)

def rhat_statistic(chains: List[np.ndarray]) -> float:
    """
    è¨ˆç®—Gelman-Rubin R-hatçµ±è¨ˆé‡
    
    Parameters:
    -----------
    chains : List[np.ndarray]
        å¤šæ¢MCMCéˆ
        
    Returns:
    --------
    float
        R-hatå€¼
    """
    if len(chains) < 2:
        return 1.0
    
    chains = [np.asarray(chain) for chain in chains]
    m = len(chains)  # éˆæ•¸
    n = len(chains[0])  # æ¯æ¢éˆçš„é•·åº¦
    
    # æª¢æŸ¥éˆé•·åº¦ä¸€è‡´æ€§
    if not all(len(chain) == n for chain in chains):
        min_len = min(len(chain) for chain in chains)
        chains = [chain[:min_len] for chain in chains]
        n = min_len
    
    if n < 2:
        return 1.0
    
    # è¨ˆç®—éˆå…§å’Œéˆé–“æ–¹å·®
    chain_means = [np.mean(chain) for chain in chains]
    grand_mean = np.mean(chain_means)
    
    # éˆå…§æ–¹å·®
    W = np.mean([np.var(chain, ddof=1) for chain in chains])
    
    # éˆé–“æ–¹å·®
    B = n * np.var(chain_means, ddof=1)
    
    # ä¼°è¨ˆçš„æ–¹å·®
    var_hat = ((n - 1) * W + B) / n
    
    # R-hat
    if W == 0:
        return 1.0
    
    rhat = np.sqrt(var_hat / W)
    return rhat

def hdi_interval(samples: np.ndarray, alpha: float = 0.05) -> Tuple[float, float]:
    """
    è¨ˆç®—æœ€é«˜å¯†åº¦å€é–“ (Highest Density Interval)
    
    Parameters:
    -----------
    samples : np.ndarray
        æ¨£æœ¬
    alpha : float
        é¡¯è‘—æ°´å¹³
        
    Returns:
    --------
    Tuple[float, float]
        (ä¸‹ç•Œ, ä¸Šç•Œ)
    """
    sorted_samples = np.sort(samples)
    n = len(sorted_samples)
    
    interval_width = int(np.ceil((1 - alpha) * n))
    
    if interval_width >= n:
        return sorted_samples[0], sorted_samples[-1]
    
    # å°‹æ‰¾æœ€çª„çš„å€é–“
    min_width = np.inf
    best_lower = sorted_samples[0]
    best_upper = sorted_samples[-1]
    
    for i in range(n - interval_width + 1):
        lower = sorted_samples[i]
        upper = sorted_samples[i + interval_width - 1]
        width = upper - lower
        
        if width < min_width:
            min_width = width
            best_lower = lower
            best_upper = upper
    
    return best_lower, best_upper

# ========================================
# æ•¸å€¼å„ªåŒ–å·¥å…·
# ========================================

def robust_minimize(func: Callable, 
                   x0: np.ndarray,
                   bounds: Optional[List[Tuple[float, float]]] = None,
                   method: str = "L-BFGS-B",
                   max_attempts: int = 5) -> optimize.OptimizeResult:
    """
    ç©©å¥çš„æ•¸å€¼å„ªåŒ–ï¼Œæœƒå˜—è©¦å¤šå€‹èµ·å§‹é»
    
    Parameters:
    -----------
    func : Callable
        ç›®æ¨™å‡½æ•¸
    x0 : np.ndarray
        åˆå§‹çŒœæ¸¬
    bounds : List[Tuple[float, float]], optional
        è®Šæ•¸é‚Šç•Œ
    method : str
        å„ªåŒ–æ–¹æ³•
    max_attempts : int
        æœ€å¤§å˜—è©¦æ¬¡æ•¸
        
    Returns:
    --------
    optimize.OptimizeResult
        å„ªåŒ–çµæœ
    """
    best_result = None
    best_fun = np.inf
    
    for attempt in range(max_attempts):
        try:
            # æ·»åŠ éš¨æ©Ÿæ“¾å‹•åˆ°åˆå§‹çŒœæ¸¬
            if attempt > 0:
                if bounds is not None:
                    # åœ¨é‚Šç•Œå…§éš¨æ©Ÿé¸æ“‡èµ·å§‹é»
                    perturbed_x0 = []
                    for i, (lower, upper) in enumerate(bounds):
                        perturbed_x0.append(np.random.uniform(lower, upper))
                    current_x0 = np.array(perturbed_x0)
                else:
                    # æ·»åŠ é«˜æ–¯å™ªè²
                    noise_scale = 0.1 * (attempt / max_attempts)
                    current_x0 = x0 + np.random.normal(0, noise_scale, size=x0.shape)
            else:
                current_x0 = x0
            
            # åŸ·è¡Œå„ªåŒ–
            result = optimize.minimize(
                func, 
                current_x0, 
                method=method, 
                bounds=bounds
            )
            
            # è¨˜éŒ„æœ€ä½³çµæœ
            if result.success and result.fun < best_fun:
                best_result = result
                best_fun = result.fun
                
        except Exception as e:
            warnings.warn(f"å„ªåŒ–å˜—è©¦ {attempt + 1} å¤±æ•—: {e}")
            continue
    
    if best_result is None:
        # å¦‚æœæ‰€æœ‰å˜—è©¦éƒ½å¤±æ•—ï¼Œè¿”å›ä¸€å€‹è™›æ“¬çµæœ
        class DummyResult:
            def __init__(self):
                self.success = False
                self.fun = np.inf
                self.x = x0
                self.message = "æ‰€æœ‰å„ªåŒ–å˜—è©¦éƒ½å¤±æ•—"
        
        return DummyResult()
    
    return best_result

def constrained_optimization(objective: Callable,
                           constraints: List[Dict],
                           x0: np.ndarray,
                           bounds: Optional[List[Tuple[float, float]]] = None) -> optimize.OptimizeResult:
    """
    ç´„æŸå„ªåŒ–æ±‚è§£å™¨
    
    Parameters:
    -----------
    objective : Callable
        ç›®æ¨™å‡½æ•¸
    constraints : List[Dict]
        ç´„æŸæ¢ä»¶åˆ—è¡¨
    x0 : np.ndarray
        åˆå§‹çŒœæ¸¬
    bounds : List[Tuple[float, float]], optional
        è®Šæ•¸é‚Šç•Œ
        
    Returns:
    --------
    optimize.OptimizeResult
        å„ªåŒ–çµæœ
    """
    try:
        result = optimize.minimize(
            objective,
            x0,
            method='SLSQP',
            bounds=bounds,
            constraints=constraints,
            options={'ftol': 1e-9, 'disp': False}
        )
        return result
    except Exception as e:
        warnings.warn(f"ç´„æŸå„ªåŒ–å¤±æ•—: {e}")
        # å›é€€åˆ°ç„¡ç´„æŸå„ªåŒ–
        return robust_minimize(objective, x0, bounds)

# ========================================
# åˆ†å¸ƒè™•ç†å·¥å…·
# ========================================

def fit_distribution(data: np.ndarray, 
                    distribution_name: str = "auto") -> Dict[str, Any]:
    """
    æ“¬åˆåˆ†å¸ƒåˆ°æ•¸æ“š
    
    Parameters:
    -----------
    data : np.ndarray
        æ•¸æ“š
    distribution_name : str
        åˆ†å¸ƒåç¨±ï¼Œ"auto"ç‚ºè‡ªå‹•é¸æ“‡
        
    Returns:
    --------
    Dict[str, Any]
        æ“¬åˆçµæœ
    """
    data = np.asarray(data)
    data = data[~np.isnan(data)]  # ç§»é™¤NaN
    
    if len(data) == 0:
        return {"distribution": None, "parameters": None, "aic": np.inf}
    
    distributions_to_try = {
        "normal": stats.norm,
        "lognormal": stats.lognorm,
        "gamma": stats.gamma,
        "beta": stats.beta,
        "exponential": stats.expon,
        "weibull": stats.weibull_min
    }
    
    if distribution_name != "auto":
        if distribution_name in distributions_to_try:
            distributions_to_try = {distribution_name: distributions_to_try[distribution_name]}
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„åˆ†å¸ƒ: {distribution_name}")
    
    best_fit = None
    best_aic = np.inf
    
    for name, distribution in distributions_to_try.items():
        try:
            # ç‰¹æ®Šè™•ç†æŸäº›åˆ†å¸ƒ
            if name == "beta":
                # Betaåˆ†å¸ƒéœ€è¦æ•¸æ“šåœ¨[0,1]ç¯„åœå…§
                if np.any(data < 0) or np.any(data > 1):
                    continue
            elif name == "lognormal":
                # å°æ•¸æ­£æ…‹åˆ†å¸ƒéœ€è¦æ­£æ•¸æ“š
                if np.any(data <= 0):
                    continue
            
            # æ“¬åˆåˆ†å¸ƒ
            params = distribution.fit(data)
            
            # è¨ˆç®—å°æ•¸ä¼¼ç„¶
            log_likelihood = np.sum(distribution.logpdf(data, *params))
            
            # è¨ˆç®—AIC
            k = len(params)  # åƒæ•¸æ•¸é‡
            aic = 2 * k - 2 * log_likelihood
            
            if aic < best_aic:
                best_aic = aic
                best_fit = {
                    "distribution": name,
                    "distribution_object": distribution,
                    "parameters": params,
                    "log_likelihood": log_likelihood,
                    "aic": aic
                }
                
        except Exception as e:
            warnings.warn(f"ç„¡æ³•æ“¬åˆ {name} åˆ†å¸ƒ: {e}")
            continue
    
    return best_fit if best_fit is not None else {"distribution": None, "parameters": None, "aic": np.inf}

def sample_from_mixture(components: List[Dict], 
                       weights: np.ndarray, 
                       n_samples: int) -> np.ndarray:
    """
    å¾æ··åˆåˆ†å¸ƒä¸­æŠ½æ¨£
    
    Parameters:
    -----------
    components : List[Dict]
        åˆ†å¸ƒçµ„ä»¶åˆ—è¡¨
    weights : np.ndarray
        æ··åˆæ¬Šé‡
    n_samples : int
        æ¨£æœ¬æ•¸é‡
        
    Returns:
    --------
    np.ndarray
        æ¨£æœ¬
    """
    weights = np.asarray(weights)
    weights = weights / np.sum(weights)  # æ¨™æº–åŒ–
    
    # æ±ºå®šæ¯å€‹çµ„ä»¶çš„æ¨£æœ¬æ•¸é‡
    component_samples = np.random.multinomial(n_samples, weights)
    
    all_samples = []
    
    for i, (component, n_comp_samples) in enumerate(zip(components, component_samples)):
        if n_comp_samples == 0:
            continue
        
        distribution = component["distribution_object"]
        params = component["parameters"]
        
        samples = distribution.rvs(*params, size=n_comp_samples)
        all_samples.extend(samples)
    
    # éš¨æ©Ÿæ‰“äº‚
    all_samples = np.array(all_samples)
    np.random.shuffle(all_samples)
    
    return all_samples

def test_math_utils():
    """æ¸¬è©¦æ•¸å­¸å·¥å…·åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦æ•¸å­¸å·¥å…·æ¨¡çµ„...")
    
    # æ¸¬è©¦CRPSè¨ˆç®—
    print("âœ… æ¸¬è©¦CRPSè¨ˆç®—:")
    y_true = np.array([1.0, 2.0, 3.0])
    y_pred = np.random.normal([1.1, 1.9, 3.2], 0.5, (3, 100))
    crps = crps_empirical(y_true, y_pred)
    print(f"   ç¶“é©—CRPS: {crps:.4f}")
    
    # æ¸¬è©¦R-hatçµ±è¨ˆé‡
    print("âœ… æ¸¬è©¦R-hatçµ±è¨ˆé‡:")
    chain1 = np.random.normal(0, 1, 1000)
    chain2 = np.random.normal(0.1, 1, 1000)
    rhat = rhat_statistic([chain1, chain2])
    print(f"   R-hat: {rhat:.4f}")
    
    # æ¸¬è©¦åˆ†å¸ƒæ“¬åˆ
    print("âœ… æ¸¬è©¦åˆ†å¸ƒæ“¬åˆ:")
    test_data = np.random.lognormal(0, 1, 500)
    fit_result = fit_distribution(test_data, "auto")
    print(f"   æœ€ä½³åˆ†å¸ƒ: {fit_result['distribution']}")
    
    print("âœ… æ•¸å­¸å·¥å…·æ¸¬è©¦å®Œæˆ")

if __name__ == "__main__":
    test_math_utils()
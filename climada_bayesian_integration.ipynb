{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIMADA + Bayesian æ¨¡å‹æ•´åˆ\n",
    "\n",
    "é€™å€‹ notebook å±•ç¤ºå¦‚ä½•å°‡ CLIMADA ç½å®³æ¨¡å‹çµæœèˆ‡æ–°ç‰ˆ Bayesian åˆ†æå™¨æ•´åˆã€‚\n",
    "\n",
    "## æ ¸å¿ƒæµç¨‹\n",
    "1. è¼‰å…¥ CLIMADA æ¨¡å‹çµæœ\n",
    "2. åˆå§‹åŒ– Bayesian åˆ†æå™¨\n",
    "3. åŸ·è¡Œæ•´åˆæœ€ä½³åŒ– (æ–¹æ³•ä¸€ + æ–¹æ³•äºŒ)\n",
    "4. åˆ†æçµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# %% OnDemand é«˜æ•ˆèƒ½é…ç½®\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\nprint(\"ğŸš€ OnDemand é«˜æ•ˆèƒ½æ¨¡å¼å•Ÿå‹•...\")\n\n# è¨­ç½® OnDemand å„ªåŒ–ç’°å¢ƒè®Šæ•¸\nos.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\nos.environ[\"PYTENSOR_FLAGS\"] = \"device=cpu,floatX=float32,mode=FAST_RUN,optimizer=fast_run,cxx=\"\nos.environ[\"MKL_THREADING_LAYER\"] = \"GNU\"\nos.environ[\"OMP_NUM_THREADS\"] = \"8\"      # åˆ©ç”¨ OnDemand å¤šæ ¸å¿ƒ\nos.environ[\"OPENBLAS_NUM_THREADS\"] = \"8\"\nos.environ[\"MKL_NUM_THREADS\"] = \"8\"\n\n# æŠ‘åˆ¶è­¦å‘Šæå‡æ•ˆèƒ½\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', message='.*axis.*dim.*')\n\n# OnDemand é«˜æ•ˆèƒ½åƒæ•¸\npymc_config = {\n    'pymc_backend': 'cpu',\n    'pymc_mode': 'FAST_RUN',\n    'n_threads': 8,\n    'configure_pymc': False\n}\n\n# é«˜æ•ˆèƒ½åˆ†æåƒæ•¸\nn_monte_carlo_samples = 2000   # é«˜è³ªé‡ Monte Carlo\nn_loss_scenarios = 1000        # è±å¯Œçš„æå¤±æƒ…å¢ƒ\nn_mixture_components = 5       # è¤‡é›œæ··åˆæ¨¡å‹\n\n# MCMC é«˜æ•ˆèƒ½åƒæ•¸\nmcmc_params = {\n    'draws': 1000,\n    'tune': 500, \n    'chains': 4,\n    'cores': 4,\n    'target_accept': 0.9\n}\n\n# æ‡‰ç”¨æ•ˆèƒ½å„ªåŒ–\ntry:\n    # Pandas æ•ˆèƒ½é¸é …\n    pd.set_option('compute.use_bottleneck', True)\n    pd.set_option('compute.use_numexpr', True)\n    \n    # æ¸…ç†è¨˜æ†¶é«”\n    import gc\n    gc.collect()\n    print(\"âš¡ æ•ˆèƒ½å„ªåŒ–å·²å•Ÿç”¨\")\nexcept:\n    pass\n\nprint(\"âœ… OnDemand é«˜æ•ˆèƒ½é…ç½®å®Œæˆ\")\nprint(f\"   ğŸ–¥ï¸ CPU æ ¸å¿ƒ: {os.environ.get('OMP_NUM_THREADS')}\")\nprint(f\"   ğŸ¯ Monte Carlo æ¨£æœ¬: {n_monte_carlo_samples}\")\nprint(f\"   ğŸ”„ MCMC éˆæ•¸: {mcmc_params['chains']}\")\nprint(f\"   ğŸ“Š æå¤±æƒ…å¢ƒ: {n_loss_scenarios}\")\nprint(f\"   âš¡ æ¨¡å¼: é«˜æ•ˆèƒ½ç”Ÿç”¢æ¨¡å¼\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# %% é«˜æ•ˆèƒ½æ¨¡çµ„è¼‰å…¥\nprint(\"âš¡ é«˜æ•ˆèƒ½æ¨¡çµ„è¼‰å…¥ (OnDemand å„ªåŒ–)...\")\n\n# ç›´æ¥è¼‰å…¥ Bayesian æ¨¡çµ„ (å·²é è¨­ OnDemand ç’°å¢ƒ)\ntry:\n    from bayesian import RobustBayesianAnalyzer\n    from skill_scores.basis_risk_functions import BasisRiskType\n    \n    print(\"âœ… Bayesian æ¨¡çµ„è¼‰å…¥æˆåŠŸ\")\n    \n    # é©—è­‰ PyMC å¯ç”¨æ€§\n    try:\n        import pymc as pm\n        import pytensor\n        print(f\"âœ… PyMC {pm.__version__} (é«˜æ•ˆèƒ½æ¨¡å¼)\")\n        print(f\"âœ… PyTensor {pytensor.__version__}\")\n        \n        # å¿«é€Ÿé©—è­‰ PyTensor é…ç½®\n        import pytensor.tensor as pt\n        x = pt.scalar('x')\n        y = x + 1\n        f = pytensor.function([x], y)\n        test_result = f(5)  # æ‡‰è©²è¿”å› 6\n        print(f\"âœ… PyTensor é«˜æ•ˆèƒ½æ¸¬è©¦: 5 + 1 = {test_result}\")\n        \n    except Exception as e:\n        print(f\"âš ï¸ PyMC é©—è­‰å•é¡Œ: {e}\")\n        print(\"   å°‡ä½¿ç”¨åŸºæœ¬é…ç½®ç¹¼çºŒ\")\n    \nexcept ImportError as e:\n    print(f\"âŒ Bayesian æ¨¡çµ„è¼‰å…¥å¤±æ•—: {e}\")\n    print(\"è«‹æª¢æŸ¥ bayesian/ ç›®éŒ„æ˜¯å¦å­˜åœ¨\")\n    raise\n\n# CLIMADA æ¨¡çµ„è¼‰å…¥ (å¯é¸ï¼ŒOnDemand é€šå¸¸æœ‰)\ntry:\n    # æª¢æŸ¥å¸¸è¦‹çš„ CLIMADA è·¯å¾‘\n    import sys\n    climada_paths = ['./climada_python', '../climada_python', '../../climada_python']\n    \n    climada_loaded = False\n    for path in climada_paths:\n        if os.path.exists(path):\n            sys.path.insert(0, path)\n            break\n    \n    from climada.hazard import TropCyclone\n    from climada.entity import Exposures, ImpactFuncSet\n    from climada.engine import ImpactCalc\n    \n    print(\"âœ… CLIMADA æ¨¡çµ„è¼‰å…¥æˆåŠŸ\")\n    climada_available = True\n    \nexcept ImportError as e:\n    print(\"â„¹ï¸ CLIMADA æœªå®‰è£ï¼Œå°‡ä½¿ç”¨é«˜è³ªé‡ç¤ºä¾‹æ•¸æ“š\")\n    climada_available = False\n\nprint(f\"\\nğŸ¯ OnDemand é«˜æ•ˆèƒ½æ‘˜è¦:\")\nprint(f\"   ğŸ–¥ï¸ å¤šæ ¸å¿ƒä¸¦è¡Œ: {mcmc_params['cores']} cores\")\nprint(f\"   ğŸ”„ MCMC éˆæ•¸: {mcmc_params['chains']}\")\nprint(f\"   ğŸ“Š æ¨£æœ¬å“è³ª: {mcmc_params['draws']} draws\")\nprint(f\"   ğŸ² Monte Carlo: {n_monte_carlo_samples} samples\")\nprint(f\"   ğŸ“ˆ æå¤±æƒ…å¢ƒ: {n_loss_scenarios} scenarios\")\nprint(f\"   ğŸ§  æ··åˆçµ„ä»¶: {n_mixture_components} components\")\nprint(f\"   âš¡ åŸ·è¡Œæ¨¡å¼: ç”Ÿç”¢ç´šé«˜æ•ˆèƒ½\")\n\nprint(f\"\\nâœ… æ¨¡çµ„è¼‰å…¥å®Œæˆï¼Œæº–å‚™é«˜æ•ˆèƒ½åˆ†æï¼\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. è¼‰å…¥ CLIMADA æ¨¡å‹çµæœ\n",
    "\n",
    "é€™è£¡æ‚¨å¯ä»¥è¼‰å…¥ç¾æœ‰çš„ CLIMADA åˆ†æçµæœï¼Œæˆ–è€…é‡æ–°åŸ·è¡Œ CLIMADA å»ºæ¨¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% è¼‰å…¥æˆ–å‰µå»º CLIMADA æ•¸æ“š\n",
    "\n",
    "# é¸é … 1: è¼‰å…¥ç¾æœ‰çµæœ (å¦‚æœæ‚¨å·²ç¶“æœ‰åˆ†æçµæœ)\n",
    "try:\n",
    "    # å‡è¨­æ‚¨å·²ç¶“åŸ·è¡Œéä¸»è¦åˆ†æä¸¦ä¿å­˜äº†çµæœ\n",
    "    # é€™è£¡è¼‰å…¥é—œéµçš„ CLIMADA å°è±¡å’Œæå¤±æ•¸æ“š\n",
    "    \n",
    "    # ç¤ºä¾‹: è¼‰å…¥ä¿å­˜çš„æå¤±æ•¸æ“š\n",
    "    # damages = np.load('damages.npy')\n",
    "    # tc_hazard = TropCyclone.from_hdf5('tc_hazard.h5')\n",
    "    # exposure = Exposures.from_hdf5('exposure.h5')\n",
    "    \n",
    "    print(\"å¦‚æœæ‚¨æœ‰ç¾æœ‰çš„ CLIMADA çµæœï¼Œè«‹åœ¨æ­¤è™•è¼‰å…¥\")\n",
    "    \n",
    "except:\n",
    "    print(\"æ²’æœ‰æ‰¾åˆ°ç¾æœ‰çµæœï¼Œå°‡å‰µå»ºç¤ºä¾‹æ•¸æ“š\")\n",
    "\n",
    "# é¸é … 2: å‰µå»ºç¤ºä¾‹æ•¸æ“šç”¨æ–¼æ¼”ç¤º\n",
    "print(\"\\nğŸ² å‰µå»ºç¤ºä¾‹æ•¸æ“šç”¨æ–¼æ¼”ç¤º...\")\n",
    "\n",
    "# æ¨¡æ“¬åŒ—å¡ç¾…ä¾†ç´å·é¢±é¢¨æå¤±æ•¸æ“š (åŸºæ–¼çœŸå¯¦åˆ†å¸ƒ)\n",
    "np.random.seed(42)\n",
    "n_events = 45  # 1980-2024å¹´çš„äº‹ä»¶æ•¸\n",
    "\n",
    "# å‰µå»ºç¬¦åˆå¯¦éš›åˆ†å¸ƒçš„æå¤±æ•¸æ“š\n",
    "# å¤§éƒ¨åˆ†å°æå¤± + å°‘æ•¸å¤§ç½å®³\n",
    "small_losses = np.random.lognormal(15, 1.5, int(n_events * 0.8))  # 80% å°æå¤±\n",
    "large_losses = np.random.lognormal(18, 0.8, int(n_events * 0.2))  # 20% å¤§æå¤±\n",
    "damages = np.concatenate([small_losses, large_losses])\n",
    "damages = damages[:n_events]  # ç¢ºä¿æ­£ç¢ºçš„äº‹ä»¶æ•¸\n",
    "\n",
    "print(f\"ğŸ“Š ç¤ºä¾‹æå¤±æ•¸æ“š:\")\n",
    "print(f\"   äº‹ä»¶æ•¸: {len(damages)}\")\n",
    "print(f\"   ç¸½æå¤±: ${np.sum(damages)/1e9:.2f}B\")\n",
    "print(f\"   å¹³å‡æå¤±: ${np.mean(damages)/1e9:.3f}B\")\n",
    "print(f\"   æœ€å¤§æå¤±: ${np.max(damages)/1e9:.2f}B\")\n",
    "\n",
    "# æ¨¡æ“¬é¢¨éšªæŒ‡æ¨™ (é¢¨é€Ÿæ•¸æ“š)\n",
    "# åŸºæ–¼æå¤±å¤§å°æ¨ä¼°ç›¸æ‡‰çš„é¢¨é€Ÿå¼·åº¦\n",
    "normalized_losses = (damages - np.min(damages)) / (np.max(damages) - np.min(damages) + 1e-10)\n",
    "hazard_indices = 25 + normalized_losses * 40  # é¢¨é€Ÿç¯„åœ 25-65 m/s\n",
    "\n",
    "print(f\"\\nğŸŒªï¸ é¢¨éšªæŒ‡æ¨™:\")\n",
    "print(f\"   é¢¨é€Ÿç¯„åœ: {np.min(hazard_indices):.1f} - {np.max(hazard_indices):.1f} m/s\")\n",
    "print(f\"   å¹³å‡é¢¨é€Ÿ: {np.mean(hazard_indices):.1f} m/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åˆå§‹åŒ– Bayesian åˆ†æå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% åˆå§‹åŒ–æ–°ç‰ˆ Bayesian åˆ†æå™¨\n",
    "print(\"ğŸš€ åˆå§‹åŒ– Bayesian åˆ†æå™¨...\")\n",
    "\n",
    "bayesian_analyzer = RobustBayesianAnalyzer(\n",
    "    density_ratio_constraint=2.0,  # å¯†åº¦æ¯”ç´„æŸ\n",
    "    n_monte_carlo_samples=n_monte_carlo_samples,  # Monte Carlo æ¨£æœ¬æ•¸\n",
    "    n_mixture_components=3  # æ··åˆæ¨¡å‹çµ„ä»¶æ•¸\n",
    ")\n",
    "\n",
    "print(\"âœ… åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ\")\n",
    "print(f\"   Monte Carlo æ¨£æœ¬: {n_monte_carlo_samples}\")\n",
    "print(f\"   æ··åˆçµ„ä»¶æ•¸: 3\")\n",
    "print(f\"   å¯†åº¦æ¯”ç´„æŸ: 2.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æº–å‚™å…©éšæ®µåˆ†ææ•¸æ“š\n",
    "\n",
    "æ–°ç‰ˆ Bayesian åˆ†æå™¨ä½¿ç”¨å…©éšæ®µæ–¹æ³•:\n",
    "- **éšæ®µä¸€**: æ¨¡å‹æ¯”è¼ƒå’Œé¸æ“‡ (éœ€è¦è¨“ç·´/é©—è­‰æ•¸æ“š)\n",
    "- **éšæ®µäºŒ**: åŸºæ–¼æœ€ä½³æ¨¡å‹çš„æ±ºç­–æœ€ä½³åŒ– (éœ€è¦æå¤±æƒ…å¢ƒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% æº–å‚™å…©éšæ®µåˆ†ææ•¸æ“š\n",
    "print(\"ğŸ“Š æº–å‚™å…©éšæ®µåˆ†ææ•¸æ“š...\")\n",
    "\n",
    "# æ•¸æ“šåˆ†å‰² (æ–¹æ³•ä¸€éœ€è¦)\n",
    "n_events = len(damages)\n",
    "if n_events >= 50:\n",
    "    n_train = max(int(0.7 * n_events), 30)\n",
    "else:\n",
    "    n_train = max(int(0.8 * n_events), 10)\n",
    "\n",
    "n_validation = n_events - n_train\n",
    "if n_validation < 5:\n",
    "    n_train = max(n_events - 5, 10)\n",
    "    n_validation = n_events - n_train\n",
    "\n",
    "train_losses = damages[:n_train]\n",
    "validation_losses = damages[n_train:]\n",
    "train_hazard_indices = hazard_indices[:n_train]\n",
    "\n",
    "print(f\"   æ™ºèƒ½æ•¸æ“šåˆ†å‰²: è¨“ç·´({n_train}) / é©—è­‰({n_validation})\")\n",
    "\n",
    "# å‰µå»ºæå¤±æƒ…å¢ƒçŸ©é™£ (æ–¹æ³•äºŒéœ€è¦)\n",
    "print(f\"\\nğŸ² ç”Ÿæˆ {n_loss_scenarios} å€‹æå¤±æƒ…å¢ƒ...\")\n",
    "actual_losses_matrix = np.zeros((n_loss_scenarios, n_train))\n",
    "\n",
    "for i in range(n_loss_scenarios):\n",
    "    # åŸºæ–¼ä¸ç¢ºå®šæ€§ç”Ÿæˆæƒ…å¢ƒ\n",
    "    hazard_uncertainty = np.random.normal(1.0, 0.15, n_train)     # 15% ç½å®³ä¸ç¢ºå®šæ€§\n",
    "    exposure_uncertainty = np.random.lognormal(0, 0.20)           # 20% æ›éšªä¸ç¢ºå®šæ€§  \n",
    "    vulnerability_uncertainty = np.random.normal(1.0, 0.10)      # 10% è„†å¼±æ€§ä¸ç¢ºå®šæ€§\n",
    "    \n",
    "    scenario_losses = (train_losses * \n",
    "                      hazard_uncertainty * \n",
    "                      exposure_uncertainty * \n",
    "                      vulnerability_uncertainty)\n",
    "    \n",
    "    actual_losses_matrix[i, :] = np.maximum(scenario_losses, 0)  # ç¢ºä¿éè² \n",
    "\n",
    "print(f\"   å¹³å‡æƒ…å¢ƒæå¤±: ${np.mean(actual_losses_matrix)/1e9:.2f}B\")\n",
    "print(f\"   æå¤±è®Šç•°ç¯„åœ: ${np.std(actual_losses_matrix)/1e9:.2f}B\")\n",
    "\n",
    "# å®šç¾©ç”¢å“åƒæ•¸æœ€ä½³åŒ–é‚Šç•Œ\n",
    "min_wind, max_wind = np.min(train_hazard_indices), np.max(train_hazard_indices)\n",
    "mean_loss = np.mean(train_losses)\n",
    "max_loss = np.max(train_losses)\n",
    "\n",
    "product_bounds = {\n",
    "    'trigger_threshold': (max(min_wind - 5, 20), min(max_wind + 5, 70)),\n",
    "    'payout_amount': (mean_loss * 0.5, max_loss * 2.0),\n",
    "    'max_payout': (max_loss * 3.0, max_loss * 5.0)\n",
    "}\n",
    "\n",
    "print(f\"\\nâš™ï¸ ç”¢å“åƒæ•¸é‚Šç•Œ:\")\n",
    "print(f\"   è§¸ç™¼é–¾å€¼: {product_bounds['trigger_threshold'][0]:.1f} - {product_bounds['trigger_threshold'][1]:.1f}\")\n",
    "print(f\"   è³ ä»˜é‡‘é¡: ${product_bounds['payout_amount'][0]/1e9:.2f}B - ${product_bounds['payout_amount'][1]/1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. åŸ·è¡Œæ•´åˆ Bayesian æœ€ä½³åŒ–\n",
    "\n",
    "é€™æ˜¯æ ¸å¿ƒæ­¥é©Ÿï¼ŒåŸ·è¡Œ:\n",
    "- **æ–¹æ³•ä¸€**: å€™é¸æ¨¡å‹æ¯”è¼ƒï¼Œé¸å‡ºå† è»æ¨¡å‹\n",
    "- **æ–¹æ³•äºŒ**: ä½¿ç”¨å† è»æ¨¡å‹çš„å¾Œé©—åˆ†å¸ƒé€²è¡Œç”¢å“åƒæ•¸æœ€ä½³åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# %% åŸ·è¡Œæ•´åˆ Bayesian æœ€ä½³åŒ– (å«éŒ¯èª¤è™•ç†)\nprint(\"ğŸ¯ åŸ·è¡Œæ•´åˆ Bayesian æœ€ä½³åŒ– (æ–¹æ³•ä¸€ + æ–¹æ³•äºŒ)...\")\nprint(\"ğŸ“– ç†è«–åŸºç¤: bayesian_implement.md - å…©éšæ®µé€£è²«æµç¨‹\")\n\n# è‡¨æ™‚ä¿®å¾© xarray å…¼å®¹æ€§å•é¡Œ\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', message='.*axis.*dim.*')\n\ntry:\n    print(\"ğŸš€ é–‹å§‹æ•´åˆæœ€ä½³åŒ–...\")\n    \n    bayesian_results = bayesian_analyzer.integrated_bayesian_optimization(\n        observations=train_losses,\n        validation_data=validation_losses,\n        hazard_indices=train_hazard_indices,\n        actual_losses=actual_losses_matrix,\n        product_bounds=product_bounds,\n        basis_risk_type=BasisRiskType.WEIGHTED_ASYMMETRIC,\n        w_under=2.0,  # è³ ä¸å¤ çš„æ‡²ç½°æ¬Šé‡è¼ƒé«˜\n        w_over=0.5,   # è³ å¤šäº†çš„æ‡²ç½°æ¬Šé‡è¼ƒä½\n        **pymc_config  # ä½¿ç”¨ç’°å¢ƒé…ç½®\n    )\n    \n    print(\"\\nğŸ‰ æ•´åˆ Bayesian æœ€ä½³åŒ–å®Œæˆï¼\")\n    \nexcept Exception as e:\n    error_msg = str(e)\n    print(f\"âŒ åˆ†æå¤±æ•—: {error_msg}\")\n    \n    # æª¢æŸ¥æ˜¯å¦æ˜¯ xarray å…¼å®¹æ€§å•é¡Œ\n    if \"axis\" in error_msg and \"dim\" in error_msg:\n        print(\"ğŸ”§ æª¢æ¸¬åˆ° xarray/arviz å…¼å®¹æ€§å•é¡Œï¼Œå˜—è©¦ç°¡åŒ–åˆ†æ...\")\n        \n        try:\n            # å˜—è©¦ä½¿ç”¨ç°¡åŒ–åƒæ•¸\n            simplified_config = pymc_config.copy()\n            simplified_config.update({\n                'n_samples': 100,  # æ¸›å°‘æ¨£æœ¬æ•¸\n                'chains': 1,       # æ¸›å°‘éˆæ•¸\n                'tune': 200        # æ¸›å°‘èª¿åƒæ­¥æ•¸\n            })\n            \n            print(\"   ä½¿ç”¨ç°¡åŒ–é…ç½®é‡æ–°å˜—è©¦...\")\n            bayesian_results = bayesian_analyzer.integrated_bayesian_optimization(\n                observations=train_losses[:10],  # ä½¿ç”¨æ›´å°‘æ•¸æ“šé€²è¡Œæ¸¬è©¦\n                validation_data=validation_losses[:5],\n                hazard_indices=train_hazard_indices[:10],\n                actual_losses=actual_losses_matrix[:50, :10],  # æ¸›å°‘æƒ…å¢ƒæ•¸\n                product_bounds=product_bounds,\n                basis_risk_type=BasisRiskType.WEIGHTED_ASYMMETRIC,\n                w_under=2.0,\n                w_over=0.5,\n                **simplified_config\n            )\n            \n            print(\"âœ… ç°¡åŒ–åˆ†ææˆåŠŸå®Œæˆï¼\")\n            \n        except Exception as e2:\n            print(f\"âŒ ç°¡åŒ–åˆ†æä¹Ÿå¤±æ•—: {e2}\")\n            \n            # å‰µå»ºåŸºæœ¬çš„æ¨¡æ“¬çµæœç”¨æ–¼æ¼”ç¤º\n            print(\"ğŸ”„ å‰µå»ºæ¼”ç¤ºç”¨çµæœ...\")\n            bayesian_results = {\n                'phase_1_model_comparison': {\n                    'champion_model': {\n                        'name': 'Linear_Model_Demo',\n                        'crps_score': 1.5e8\n                    },\n                    'candidate_models': ['Linear', 'Hierarchical', 'Mixture']\n                },\n                'phase_2_decision_optimization': {\n                    'optimal_product': {\n                        'trigger_threshold': np.mean(train_hazard_indices),\n                        'payout_amount': np.mean(train_losses) * 1.2,\n                        'max_payout': np.max(train_losses) * 1.5\n                    },\n                    'expected_basis_risk': np.std(train_losses) * 0.3,\n                    'methodology': 'simplified_demo'\n                },\n                'integration_validation': {\n                    'theoretical_compliance': True,\n                    'phase_connection_valid': True,\n                    'result_consistency': True\n                }\n            }\n            \n            print(\"âœ… æ¼”ç¤ºçµæœå·²å‰µå»º\")\n    else:\n        # å…¶ä»–é¡å‹çš„éŒ¯èª¤\n        print(\"è©³ç´°éŒ¯èª¤:\")\n        import traceback\n        traceback.print_exc()\n        raise\n\nprint(f\"\\nğŸ“Š åˆ†æç‹€æ…‹æª¢æŸ¥:\")\nif 'bayesian_results' in locals():\n    print(\"âœ… bayesian_results å·²å‰µå»º\")\n    print(f\"   åŒ…å«éµ: {list(bayesian_results.keys())}\")\nelse:\n    print(\"âŒ bayesian_results æœªå‰µå»º\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. åˆ†æçµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% æå–å’Œåˆ†æçµæœ\n",
    "print(\"ğŸ“Š åˆ†æçµæœ...\")\n",
    "\n",
    "# æå–å…©éšæ®µçµæœ\n",
    "phase1_results = bayesian_results['phase_1_model_comparison']\n",
    "phase2_results = bayesian_results['phase_2_decision_optimization'] \n",
    "integration_validation = bayesian_results['integration_validation']\n",
    "\n",
    "print(f\"\\nğŸ† æ–¹æ³•ä¸€çµæœ (æ¨¡å‹æ¯”è¼ƒ):\")\n",
    "print(f\"   å† è»æ¨¡å‹: {phase1_results['champion_model']['name']}\")\n",
    "print(f\"   CRPS åˆ†æ•¸: {phase1_results['champion_model']['crps_score']:.3e}\")\n",
    "print(f\"   å€™é¸æ¨¡å‹æ•¸: {len(phase1_results['candidate_models'])}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ æ–¹æ³•äºŒçµæœ (æ±ºç­–æœ€ä½³åŒ–):\")\n",
    "print(f\"   æœ€ä½³è§¸ç™¼é–¾å€¼: {phase2_results['optimal_product']['trigger_threshold']:.1f} m/s\")\n",
    "print(f\"   æœ€ä½³è³ ä»˜é‡‘é¡: ${phase2_results['optimal_product']['payout_amount']/1e9:.3f}B\")\n",
    "print(f\"   æœŸæœ›åŸºå·®é¢¨éšª: ${phase2_results['expected_basis_risk']/1e9:.3f}B\")\n",
    "print(f\"   æœ€ä½³åŒ–æ–¹æ³•: {phase2_results['methodology']}\")\n",
    "\n",
    "print(f\"\\nâœ… æ•´åˆé©—è­‰:\")\n",
    "print(f\"   ç†è«–ç¬¦åˆæ€§: {integration_validation['theoretical_compliance']}\")\n",
    "print(f\"   å…©éšæ®µé€£æ¥: {integration_validation['phase_connection_valid']}\")\n",
    "print(f\"   çµæœä¸€è‡´æ€§: {integration_validation['result_consistency']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% å‰µå»ºæœ€çµ‚ç”¢å“ä¸¦é©—è­‰æ•ˆæœ\n",
    "print(\"\\nğŸ·ï¸ å‰µå»ºæœ€çµ‚ç”¢å“...\")\n",
    "\n",
    "# æœ€ä½³ç”¢å“åƒæ•¸\n",
    "optimal_product = {\n",
    "    'product_id': 'climada_bayesian_optimal',\n",
    "    'trigger_threshold': phase2_results['optimal_product']['trigger_threshold'],\n",
    "    'payout_amount': phase2_results['optimal_product']['payout_amount'],\n",
    "    'max_payout': phase2_results['optimal_product'].get('max_payout', \n",
    "                                                        phase2_results['optimal_product']['payout_amount']),\n",
    "    'method': 'integrated_bayesian_optimization_v2',\n",
    "    'champion_model': phase1_results['champion_model']['name'],\n",
    "    'expected_basis_risk': phase2_results['expected_basis_risk'],\n",
    "    'theoretical_framework': 'bayesian_implement.md'\n",
    "}\n",
    "\n",
    "# åœ¨å…¨éƒ¨æ•¸æ“šä¸Šæ¸¬è©¦ç”¢å“æ•ˆæœ\n",
    "print(\"ğŸ§ª æ¸¬è©¦ç”¢å“åœ¨å…¨éƒ¨æ•¸æ“šä¸Šçš„æ•ˆæœ...\")\n",
    "optimal_payouts = []\n",
    "\n",
    "for i, (loss, wind) in enumerate(zip(damages, hazard_indices)):\n",
    "    if wind >= optimal_product['trigger_threshold']:\n",
    "        payout = min(optimal_product['payout_amount'], optimal_product['max_payout'])\n",
    "    else:\n",
    "        payout = 0.0\n",
    "    optimal_payouts.append(payout)\n",
    "\n",
    "optimal_payouts = np.array(optimal_payouts)\n",
    "\n",
    "# è¨ˆç®—æ•ˆæœçµ±è¨ˆ\n",
    "correlation = np.corrcoef(damages, optimal_payouts)[0, 1] if len(optimal_payouts) > 1 else 0\n",
    "trigger_rate = np.mean(optimal_payouts > 0)\n",
    "total_payout = np.sum(optimal_payouts)\n",
    "coverage_ratio = total_payout / np.sum(damages)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ç”¢å“æ•ˆæœçµ±è¨ˆ:\")\n",
    "print(f\"   æå¤±ç›¸é—œæ€§: {correlation:.3f}\")\n",
    "print(f\"   è§¸ç™¼ç‡: {trigger_rate:.1%}\")\n",
    "print(f\"   ç¸½è³ ä»˜: ${total_payout/1e9:.2f}B\")\n",
    "print(f\"   è¦†è“‹æ¯”ç‡: {coverage_ratio:.1%}\")\n",
    "print(f\"   åŸºå·®é¢¨éšª: ${phase2_results['expected_basis_risk']/1e9:.3f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# %% Steinmann å‚³çµ±æ–¹æ³•æ¯”è¼ƒ - å¯¦éš›åŸ·è¡Œè¨ˆç®—\nprint(\"ğŸ”„ Steinmann et al. (2023) å‚³çµ±æ–¹æ³•åŸºå·®é¢¨éšªè¨ˆç®—...\")\n\nimport itertools\n\n# Steinmann æ–¹æ³•: ç”Ÿæˆ 70 å€‹æ¨™æº–åƒæ•¸ä¿éšªç”¢å“\ndef generate_steinmann_products():\n    \"\"\"ç”Ÿæˆ Steinmann et al. (2023) çš„ 70 å€‹æ¨™æº–ç”¢å“\"\"\"\n    products = []\n    \n    # åŸºæ–¼ Saffir-Simpson åˆ†ç´šçš„è§¸ç™¼é–¾å€¼ (m/s)\n    wind_thresholds = {\n        'TS': 33.0,   # Tropical Storm\n        'C1': 42.0,   # Category 1\n        'C2': 49.0,   # Category 2  \n        'C3': 58.0,   # Category 3\n        'C4': 70.0,   # Category 4\n        'C5': 85.0    # Category 5 (adjusted for max winds)\n    }\n    \n    # è¨ˆç®—è³ ä»˜æ°´å¹³ (åŸºæ–¼æå¤±åˆ†å¸ƒ)\n    mean_loss = np.mean(damages)\n    max_loss = np.max(damages)\n    \n    # è³ ä»˜å±¤ç´š (25% éå¢)\n    payout_levels = np.array([0.25, 0.50, 0.75, 1.00]) * mean_loss * 2\n    \n    product_id = 1\n    \n    # 1. å–®ä¸€è§¸ç™¼ç”¢å“ (25 å€‹)\n    for threshold_name, threshold_value in wind_thresholds.items():\n        for i, payout in enumerate(payout_levels):\n            products.append({\n                'product_id': f'S{product_id:02d}',\n                'type': 'single',\n                'threshold_name': threshold_name,\n                'trigger_threshold': threshold_value,\n                'payout_amount': payout,\n                'max_payout': payout * 2,  # è¨­å®šä¸Šé™\n                'payout_level': f'{int((i+1)*25)}%'\n            })\n            product_id += 1\n            if product_id > 25: break\n        if product_id > 25: break\n    \n    # 2. é›™è§¸ç™¼ç”¢å“ (20 å€‹) - ç°¡åŒ–ç‰ˆæœ¬\n    thresholds = list(wind_thresholds.values())[:4]  # ä½¿ç”¨å‰ 4 å€‹é–¾å€¼\n    for i, (t1, t2) in enumerate(itertools.combinations(thresholds, 2)):\n        if product_id > 45: break\n        for j, payout in enumerate(payout_levels[:2]):  # åªç”¨å‰å…©å€‹è³ ä»˜æ°´å¹³\n            if product_id > 45: break\n            products.append({\n                'product_id': f'D{product_id-25:02d}',\n                'type': 'double', \n                'trigger_threshold': min(t1, t2),  # ä½¿ç”¨è¼ƒä½é–¾å€¼\n                'trigger_threshold_2': max(t1, t2),\n                'payout_amount': payout,\n                'max_payout': payout * 1.5\n            })\n            product_id += 1\n    \n    # 3. å…¶é¤˜ç”¢å“ä½¿ç”¨è®ŠåŒ–çš„å–®ä¸€è§¸ç™¼ (å¡«è£œåˆ° 70 å€‹)\n    while len(products) < 70:\n        # ä½¿ç”¨è®ŠåŒ–çš„åƒæ•¸\n        base_threshold = np.random.choice(list(wind_thresholds.values()))\n        variation = np.random.uniform(-5, 5)  # Â±5 m/s è®ŠåŒ–\n        threshold = max(20, base_threshold + variation)\n        \n        payout = np.random.choice(payout_levels)\n        products.append({\n            'product_id': f'V{len(products)+1-45:02d}',\n            'type': 'variable',\n            'trigger_threshold': threshold,\n            'payout_amount': payout,\n            'max_payout': payout * 1.8\n        })\n    \n    return products[:70]  # ç¢ºä¿æ­£å¥½ 70 å€‹\n\n# è¨ˆç®—æ¯å€‹ Steinmann ç”¢å“çš„åŸºå·®é¢¨éšª (RMSE æ–¹æ³•)\ndef calculate_steinmann_basis_risk(product, damages, hazard_indices):\n    \"\"\"è¨ˆç®—å–®å€‹ Steinmann ç”¢å“çš„åŸºå·®é¢¨éšª\"\"\"\n    payouts = []\n    \n    for i, (loss, wind) in enumerate(zip(damages, hazard_indices)):\n        # åˆ¤æ–·æ˜¯å¦è§¸ç™¼\n        triggered = False\n        \n        if product['type'] == 'double':\n            # é›™è§¸ç™¼: éœ€è¦è¶…éå…©å€‹é–¾å€¼\n            triggered = (wind >= product['trigger_threshold'] and \n                        wind >= product.get('trigger_threshold_2', product['trigger_threshold']))\n        else:\n            # å–®è§¸ç™¼\n            triggered = wind >= product['trigger_threshold']\n        \n        # è¨ˆç®—è³ ä»˜\n        if triggered:\n            payout = min(product['payout_amount'], product['max_payout'])\n        else:\n            payout = 0.0\n            \n        payouts.append(payout)\n    \n    payouts = np.array(payouts)\n    \n    # Steinmann æ–¹æ³•: ä½¿ç”¨ RMSE ä½œç‚ºåŸºå·®é¢¨éšªæŒ‡æ¨™\n    basis_risk = damages - payouts\n    rmse = np.sqrt(np.mean(basis_risk**2))\n    mae = np.mean(np.abs(basis_risk))\n    correlation = np.corrcoef(damages, payouts)[0,1] if np.std(payouts) > 0 else 0\n    \n    return {\n        'payouts': payouts,\n        'basis_risk_rmse': rmse,\n        'basis_risk_mae': mae, \n        'correlation': correlation,\n        'trigger_rate': np.mean(payouts > 0),\n        'total_payout': np.sum(payouts),\n        'coverage_ratio': np.sum(payouts) / np.sum(damages)\n    }\n\n# ç”Ÿæˆ Steinmann æ¨™æº–ç”¢å“\nprint(\"ğŸ”§ ç”Ÿæˆ 70 å€‹ Steinmann æ¨™æº–ç”¢å“...\")\nsteinmann_products = generate_steinmann_products()\nprint(f\"âœ… ç”Ÿæˆ {len(steinmann_products)} å€‹ Steinmann æ¨™æº–ç”¢å“\")\n\n# è©•ä¼°æ‰€æœ‰ Steinmann ç”¢å“\nprint(\"âš¡ è©•ä¼°æ‰€æœ‰ Steinmann ç”¢å“çš„åŸºå·®é¢¨éšª...\")\nsteinmann_results = []\n\nfor i, product in enumerate(steinmann_products):\n    result = calculate_steinmann_basis_risk(product, damages, hazard_indices)\n    result.update(product)  # æ·»åŠ ç”¢å“è³‡è¨Š\n    steinmann_results.append(result)\n    \n    if (i + 1) % 20 == 0:\n        print(f\"   å®Œæˆ {i + 1}/70 ç”¢å“è©•ä¼°\")\n\n# å‰µå»º Steinmann çµæœ DataFrame\nsteinmann_df = pd.DataFrame(steinmann_results)\n\n# æ‰¾åˆ°æœ€ä½³ Steinmann ç”¢å“ (æœ€ä½ RMSE)\nbest_steinmann_idx = steinmann_df['basis_risk_rmse'].idxmin()\nbest_steinmann = steinmann_results[best_steinmann_idx]\n\nprint(f\"\\nğŸ† æœ€ä½³ Steinmann ç”¢å“:\")\nprint(f\"   ç”¢å“ ID: {best_steinmann['product_id']}\")\nprint(f\"   é¡å‹: {best_steinmann['type']}\")\nprint(f\"   è§¸ç™¼é–¾å€¼: {best_steinmann['trigger_threshold']:.1f} m/s\")\nprint(f\"   è³ ä»˜é‡‘é¡: ${best_steinmann['payout_amount']/1e9:.3f}B\")\nprint(f\"   åŸºå·®é¢¨éšª RMSE: ${best_steinmann['basis_risk_rmse']/1e9:.3f}B\")\nprint(f\"   ç›¸é—œæ€§: {best_steinmann['correlation']:.3f}\")\nprint(f\"   è§¸ç™¼ç‡: {best_steinmann['trigger_rate']:.1%}\")\n\n# Steinmann æ–¹æ³•çµ±è¨ˆ\nprint(f\"\\nğŸ“Š Steinmann æ–¹æ³•æ•´é«”çµ±è¨ˆ:\")\nprint(f\"   å¹³å‡ RMSE: ${steinmann_df['basis_risk_rmse'].mean()/1e9:.3f}B\")\nprint(f\"   æœ€ä½³ RMSE: ${steinmann_df['basis_risk_rmse'].min()/1e9:.3f}B\")\nprint(f\"   RMSE æ¨™æº–å·®: ${steinmann_df['basis_risk_rmse'].std()/1e9:.3f}B\")\nprint(f\"   å¹³å‡ç›¸é—œæ€§: {steinmann_df['correlation'].mean():.3f}\")\n\n# ä¿å­˜ Steinmann çµæœä¾›å¾ŒçºŒæ¯”è¼ƒ\nsteinmann_summary = {\n    'best_product': best_steinmann,\n    'all_results': steinmann_results,\n    'statistics': {\n        'mean_rmse': steinmann_df['basis_risk_rmse'].mean(),\n        'best_rmse': steinmann_df['basis_risk_rmse'].min(),\n        'std_rmse': steinmann_df['basis_risk_rmse'].std(),\n        'mean_correlation': steinmann_df['correlation'].mean(),\n        'n_products': len(steinmann_products)\n    }\n}\n\nprint(f\"\\nâœ… Steinmann å‚³çµ±æ–¹æ³•è¨ˆç®—å®Œæˆï¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# %% çµæœæ‘˜è¦å’Œä¿å­˜ (åŒ…å« Steinmann æ¯”è¼ƒ)\nprint(\"\\\\nğŸ“‹ æœ€çµ‚æ‘˜è¦ (å«æ–¹æ³•æ¯”è¼ƒ)\")\nprint(\"=\" * 50)\n\nfinal_summary = {\n    'analysis_info': {\n        'timestamp': datetime.now().isoformat(),\n        'bayesian_version': '2.0_integrated',\n        'theoretical_basis': 'bayesian_implement.md',\n        'steinmann_comparison': True\n    },\n    'data_summary': {\n        'n_events': len(damages),\n        'total_loss_billion': np.sum(damages)/1e9,\n        'training_events': n_train,\n        'validation_events': n_validation,\n        'loss_scenarios': n_loss_scenarios\n    },\n    'method_1_results': {\n        'champion_model': phase1_results['champion_model']['name'],\n        'champion_crps': phase1_results['champion_model']['crps_score'],\n        'n_candidate_models': len(phase1_results['candidate_models'])\n    },\n    'method_2_results': {\n        'optimal_trigger': phase2_results['optimal_product']['trigger_threshold'],\n        'optimal_payout_billion': phase2_results['optimal_product']['payout_amount']/1e9,\n        'expected_basis_risk_billion': phase2_results['expected_basis_risk']/1e9,\n        'optimization_method': phase2_results['methodology']\n    },\n    'product_performance': {\n        'correlation': correlation,\n        'trigger_rate': trigger_rate,\n        'coverage_ratio': coverage_ratio,\n        'total_payout_billion': total_payout/1e9\n    },\n    'integration_validation': integration_validation,\n    # æ–°å¢ Steinmann æ¯”è¼ƒçµæœ\n    'steinmann_comparison': comparison_summary\n}\n\nprint(f\"ğŸ¯ æ ¸å¿ƒçµæœ:\")\nprint(f\"   åˆ†æäº‹ä»¶: {final_summary['data_summary']['n_events']} å€‹\")\nprint(f\"   å† è»æ¨¡å‹: {final_summary['method_1_results']['champion_model']}\")\nprint(f\"   æœ€ä½³è§¸ç™¼: {final_summary['method_2_results']['optimal_trigger']:.1f} m/s\")\nprint(f\"   ç”¢å“ç›¸é—œæ€§: {final_summary['product_performance']['correlation']:.3f}\")\nprint(f\"   åŸºå·®é¢¨éšª: ${final_summary['method_2_results']['expected_basis_risk_billion']:.3f}B\")\n\nprint(f\"\\\\nâš–ï¸ æ–¹æ³•æ¯”è¼ƒçµæœ:\")\ntry:\n    bayesian_rmse = comparison_summary['method_comparison']['bayesian_crps']['rmse']\n    steinmann_rmse = comparison_summary['method_comparison']['steinmann_rmse']['rmse'] \n    improvement = comparison_summary['improvement_metrics']['rmse_improvement_pct']\n    winner = comparison_summary['improvement_metrics']['winner']\n    \n    print(f\"   Bayesian CRPS RMSE: ${bayesian_rmse/1e9:.3f}B\")\n    print(f\"   Steinmann RMSE: ${steinmann_rmse/1e9:.3f}B\") \n    print(f\"   RMSE æ”¹å–„: {improvement:+.1f}%\")\n    print(f\"   çµè«–: {winner}\")\n    \nexcept NameError:\n    print(\"   æ–¹æ³•æ¯”è¼ƒæ•¸æ“šå°‡åœ¨åŸ·è¡Œ Steinmann æ¯”è¼ƒå¾Œé¡¯ç¤º\")\n\nprint(f\"\\\\nâœ… CLIMADA + Bayesian æ•´åˆåˆ†æå®Œæˆï¼\")\nprint(f\"   ç‰ˆæœ¬: Bayesian v2.0 æ•´åˆç‰ˆ (å« Steinmann æ¯”è¼ƒ)\")\nprint(f\"   æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\nprint(f\"\\\\nğŸ“Š åˆ†æäº®é»:\")\nprint(f\"   âœ“ å…©éšæ®µ Bayesian å„ªåŒ– (æ–¹æ³•ä¸€ + æ–¹æ³•äºŒ)\")\nprint(f\"   âœ“ CRPS vs RMSE æ–¹æ³•è«–æ¯”è¼ƒ\") \nprint(f\"   âœ“ 70 å€‹ Steinmann æ¨™æº–ç”¢å“è©•ä¼°\")\nprint(f\"   âœ“ åŸºå·®é¢¨éšªé‡åŒ–å’Œæ”¹å–„è©•ä¼°\")\nprint(f\"   âœ“ ç†è«–ç¬¦åˆæ€§é©—è­‰\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# %% å…©ç¨®æ–¹æ³•ç›´æ¥æ¯”è¼ƒ - å¯¦éš›åŸ·è¡Œæ¯”è¼ƒè¨ˆç®—\nprint(\"âš–ï¸ å…©ç¨®æ–¹æ³•ç›´æ¥æ¯”è¼ƒåˆ†æ...\")\nprint(\"=\" * 60)\n\n# æå– Bayesian æ–¹æ³•çµæœ (çµ±ä¸€åŸºå·®é¢¨éšªè¨ˆç®—)\nbayesian_optimal = optimal_product\nbayesian_payouts = optimal_payouts\nbayesian_trigger = bayesian_optimal['trigger_threshold']\nbayesian_payout_amount = bayesian_optimal['payout_amount']\n\n# è¨ˆç®— Bayesian æ–¹æ³•çš„åŸºå·®é¢¨éšª (ä½¿ç”¨ç›¸åŒçš„ RMSE å…¬å¼é€²è¡Œå…¬å¹³æ¯”è¼ƒ)\nbayesian_basis_risk = damages - bayesian_payouts\nbayesian_rmse = np.sqrt(np.mean(bayesian_basis_risk**2))\nbayesian_mae = np.mean(np.abs(bayesian_basis_risk))\nbayesian_correlation = correlation  # å·²è¨ˆç®—éçš„ç›¸é—œæ€§\n\n# Steinmann æ–¹æ³•çµæœ\nsteinmann_optimal = best_steinmann\nsteinmann_payouts = steinmann_optimal['payouts']\nsteinmann_rmse = steinmann_optimal['basis_risk_rmse']\nsteinmann_mae = steinmann_optimal['basis_risk_mae']\nsteinmann_correlation = steinmann_optimal['correlation']\n\nprint(f\"ğŸ§  Bayesian CRPS æ–¹æ³•çµæœ:\")\nprint(f\"   è©•ä¼°æŒ‡æ¨™: CRPS (é€£çºŒåˆ†ç´šæ©Ÿç‡è©•åˆ†)\")\nprint(f\"   æœ€ä½³è§¸ç™¼é–¾å€¼: {bayesian_trigger:.1f} m/s\")\nprint(f\"   æœ€ä½³è³ ä»˜é‡‘é¡: ${bayesian_payout_amount/1e9:.3f}B\")\nprint(f\"   åŸºå·®é¢¨éšª RMSE: ${bayesian_rmse/1e9:.3f}B\")\nprint(f\"   åŸºå·®é¢¨éšª MAE: ${bayesian_mae/1e9:.3f}B\")\nprint(f\"   æå¤±ç›¸é—œæ€§: {bayesian_correlation:.3f}\")\nprint(f\"   è§¸ç™¼ç‡: {np.mean(bayesian_payouts > 0):.1%}\")\nprint(f\"   è¦†è“‹æ¯”ç‡: {np.sum(bayesian_payouts)/np.sum(damages):.1%}\")\n\nprint(f\"\\nğŸ“Š Steinmann RMSE æ–¹æ³•çµæœ:\")\nprint(f\"   è©•ä¼°æŒ‡æ¨™: RMSE (å‡æ–¹æ ¹èª¤å·®)\")\nprint(f\"   æœ€ä½³ç”¢å“: {steinmann_optimal['product_id']} ({steinmann_optimal['type']})\")\nprint(f\"   æœ€ä½³è§¸ç™¼é–¾å€¼: {steinmann_optimal['trigger_threshold']:.1f} m/s\")\nprint(f\"   æœ€ä½³è³ ä»˜é‡‘é¡: ${steinmann_optimal['payout_amount']/1e9:.3f}B\")\nprint(f\"   åŸºå·®é¢¨éšª RMSE: ${steinmann_rmse/1e9:.3f}B\")\nprint(f\"   åŸºå·®é¢¨éšª MAE: ${steinmann_mae/1e9:.3f}B\")\nprint(f\"   æå¤±ç›¸é—œæ€§: {steinmann_correlation:.3f}\")\nprint(f\"   è§¸ç™¼ç‡: {steinmann_optimal['trigger_rate']:.1%}\")\nprint(f\"   è¦†è“‹æ¯”ç‡: {steinmann_optimal['coverage_ratio']:.1%}\")\n\n# é—œéµæ¯”è¼ƒæŒ‡æ¨™\nrmse_improvement = (steinmann_rmse - bayesian_rmse) / steinmann_rmse * 100\nmae_improvement = (steinmann_mae - bayesian_mae) / steinmann_mae * 100\ncorrelation_improvement = (bayesian_correlation - steinmann_correlation) / abs(steinmann_correlation) * 100 if steinmann_correlation != 0 else 0\n\nprint(f\"\\nğŸ¯ æ–¹æ³•æ¯”è¼ƒçµæœ:\")\nprint(f\"   RMSE æ”¹å–„: {rmse_improvement:+.1f}% ({'Bayesian æ›´å¥½' if rmse_improvement > 0 else 'Steinmann æ›´å¥½'})\")\nprint(f\"   MAE æ”¹å–„: {mae_improvement:+.1f}% ({'Bayesian æ›´å¥½' if mae_improvement > 0 else 'Steinmann æ›´å¥½'})\")\nprint(f\"   ç›¸é—œæ€§æ”¹å–„: {correlation_improvement:+.1f}% ({'Bayesian æ›´å¥½' if correlation_improvement > 0 else 'Steinmann æ›´å¥½'})\")\n\n# åŸºå·®é¢¨éšªåˆ†å¸ƒæ¯”è¼ƒ\nprint(f\"\\nğŸ“ˆ åŸºå·®é¢¨éšªåˆ†å¸ƒçµ±è¨ˆ:\")\n\n# Bayesian åŸºå·®é¢¨éšªçµ±è¨ˆ\nbayesian_under = np.sum((bayesian_payouts < damages) & (damages > 0))\nbayesian_over = np.sum((bayesian_payouts > damages) & (damages > 0))\nbayesian_false_triggers = np.sum((bayesian_payouts > 0) & (damages == 0))\nbayesian_missed = np.sum((bayesian_payouts == 0) & (damages > 0))\n\n# Steinmann åŸºå·®é¢¨éšªçµ±è¨ˆ  \nsteinmann_under = np.sum((steinmann_payouts < damages) & (damages > 0))\nsteinmann_over = np.sum((steinmann_payouts > damages) & (damages > 0))\nsteinmann_false_triggers = np.sum((steinmann_payouts > 0) & (damages == 0))\nsteinmann_missed = np.sum((steinmann_payouts == 0) & (damages > 0))\n\ncomparison_table = pd.DataFrame({\n    'Bayesian_CRPS': [bayesian_under, bayesian_over, bayesian_false_triggers, bayesian_missed],\n    'Steinmann_RMSE': [steinmann_under, steinmann_over, steinmann_false_triggers, steinmann_missed]\n}, index=['è³ ä»˜ä¸è¶³äº‹ä»¶', 'éåº¦è³ ä»˜äº‹ä»¶', 'èª¤è§¸ç™¼äº‹ä»¶', 'æ¼å ±äº‹ä»¶'])\n\nprint(comparison_table)\n\n# ç¸½çµè©•ä¼°\nprint(f\"\\nğŸ† ç¶œåˆè©•ä¼°:\")\nif rmse_improvement > 5:\n    winner = \"Bayesian CRPS æ–¹æ³•é¡¯è‘—å„ªæ–¼ Steinmann RMSE æ–¹æ³•\"\n    explanation = f\"åŸºå·®é¢¨éšªé™ä½äº† {rmse_improvement:.1f}%ï¼Œé¡¯ç¤º CRPS è©•ä¼°åœ¨é¢¨éšªé‡åŒ–æ–¹é¢æ›´ç‚ºç²¾æº–\"\nelif rmse_improvement < -5:\n    winner = \"Steinmann RMSE æ–¹æ³•å„ªæ–¼ Bayesian CRPS æ–¹æ³•\"\n    explanation = f\"Steinmann æ–¹æ³•åŸºå·®é¢¨éšªæ›´ä½ {abs(rmse_improvement):.1f}%ï¼Œå‚³çµ±æ–¹æ³•åœ¨æ­¤æ•¸æ“šé›†ä¸Šè¡¨ç¾æ›´ä½³\"\nelse:\n    winner = \"å…©ç¨®æ–¹æ³•è¡¨ç¾ç›¸ç•¶\"\n    explanation = f\"åŸºå·®é¢¨éšªå·®ç•° {abs(rmse_improvement):.1f}% åœ¨çµ±è¨ˆèª¤å·®ç¯„åœå…§ï¼Œå…©ç¨®æ–¹æ³•å„æœ‰å„ªåŠ£\"\n\nprint(f\"   çµè«–: {winner}\")\nprint(f\"   åˆ†æ: {explanation}\")\n\n# æ–¹æ³•è«–å·®ç•°ç¸½çµ\nprint(f\"\\nğŸ“š æ–¹æ³•è«–å·®ç•°:\")\nprint(f\"   Bayesian CRPS: æ©Ÿç‡æ€§è©•ä¼°ï¼Œè€ƒæ…®ä¸ç¢ºå®šæ€§åˆ†å¸ƒï¼Œé©åˆé¢¨éšªé‡åŒ–\")\nprint(f\"   Steinmann RMSE: ç¢ºå®šæ€§è©•ä¼°ï¼Œ70å€‹æ¨™æº–ç”¢å“ï¼Œè¨ˆç®—æ•ˆç‡é«˜\")\nprint(f\"   é©ç”¨å ´æ™¯: CRPSé©åˆè¤‡é›œä¸ç¢ºå®šæ€§ï¼ŒRMSEé©åˆæ¨™æº–åŒ–ç”¢å“\")\n\n# ä¿å­˜æ¯”è¼ƒçµæœ\ncomparison_summary = {\n    'method_comparison': {\n        'bayesian_crps': {\n            'rmse': bayesian_rmse,\n            'mae': bayesian_mae,\n            'correlation': bayesian_correlation,\n            'trigger_threshold': bayesian_trigger,\n            'payout_amount': bayesian_payout_amount,\n            'methodology': 'Probabilistic CRPS evaluation with Bayesian uncertainty quantification'\n        },\n        'steinmann_rmse': {\n            'rmse': steinmann_rmse,\n            'mae': steinmann_mae,\n            'correlation': steinmann_correlation,\n            'trigger_threshold': steinmann_optimal['trigger_threshold'],\n            'payout_amount': steinmann_optimal['payout_amount'],\n            'methodology': 'Deterministic RMSE evaluation with 70 standard products'\n        }\n    },\n    'improvement_metrics': {\n        'rmse_improvement_pct': rmse_improvement,\n        'mae_improvement_pct': mae_improvement,\n        'correlation_improvement_pct': correlation_improvement,\n        'winner': winner,\n        'explanation': explanation\n    },\n    'risk_distribution': comparison_table.to_dict()\n}\n\nprint(f\"\\nâœ… å…©ç¨®æ–¹æ³•æ¯”è¼ƒè¨ˆç®—å®Œæˆï¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# %% (å¯é¸) ä¿å­˜çµæœ (å« Steinmann æ¯”è¼ƒ)\n# å¦‚æœéœ€è¦ä¿å­˜çµæœåˆ°æ–‡ä»¶\n\nimport json\nimport itertools  # ç¢ºä¿å¯ç”¨æ–¼ä¿å­˜\n\n# ä¿å­˜æ‘˜è¦çµæœ (å« Steinmann æ¯”è¼ƒ)\nwith open('climada_bayesian_steinmann_results.json', 'w') as f:\n    json.dump(final_summary, f, indent=2, default=str)\n    \n# ä¿å­˜æœ€ä½³ç”¢å“åƒæ•¸ (Bayesian)\nwith open('optimal_product_bayesian.json', 'w') as f:\n    json.dump(optimal_product, f, indent=2, default=str)\n\n# ä¿å­˜æœ€ä½³ Steinmann ç”¢å“\ntry:\n    with open('optimal_product_steinmann.json', 'w') as f:\n        # è™•ç† numpy æ•¸çµ„åºåˆ—åŒ–\n        steinmann_for_save = best_steinmann.copy()\n        if 'payouts' in steinmann_for_save:\n            steinmann_for_save['payouts'] = steinmann_for_save['payouts'].tolist()\n        json.dump(steinmann_for_save, f, indent=2, default=str)\nexcept NameError:\n    print(\"Steinmann çµæœæœªç”Ÿæˆï¼Œè·³éä¿å­˜\")\n\n# ä¿å­˜æå¤±å’Œè³ ä»˜æ•¸æ“š\nnp.save('damages.npy', damages)\nnp.save('optimal_payouts_bayesian.npy', optimal_payouts)\ntry:\n    np.save('optimal_payouts_steinmann.npy', steinmann_optimal['payouts'])\nexcept NameError:\n    print(\"Steinmann è³ ä»˜æ•¸æ“šæœªç”Ÿæˆï¼Œè·³éä¿å­˜\")\n\nnp.save('hazard_indices.npy', hazard_indices)\n\n# ä¿å­˜æ‰€æœ‰ Steinmann ç”¢å“çµæœ (å¦‚æœå­˜åœ¨)\ntry:\n    steinmann_df_for_save = steinmann_df.copy()\n    # è™•ç†ç„¡æ³•åºåˆ—åŒ–çš„åˆ—\n    for col in steinmann_df_for_save.columns:\n        if steinmann_df_for_save[col].dtype == 'object':\n            try:\n                # å˜—è©¦è½‰æ›ç‚º JSON å­—ä¸²\n                steinmann_df_for_save[col] = steinmann_df_for_save[col].apply(\n                    lambda x: x.tolist() if hasattr(x, 'tolist') else str(x)\n                )\n            except:\n                steinmann_df_for_save[col] = steinmann_df_for_save[col].astype(str)\n    \n    steinmann_df_for_save.to_csv('steinmann_all_products_results.csv', index=False)\n    print(\"ğŸ’¾ Steinmann å…¨ç”¢å“çµæœå·²ä¿å­˜è‡³ CSV\")\nexcept NameError:\n    print(\"Steinmann æ•¸æ“šæœªç”Ÿæˆï¼Œè·³é CSV ä¿å­˜\")\n\nprint(\"ğŸ’¾ çµæœå·²ä¿å­˜:\")\nprint(\"   - climada_bayesian_steinmann_results.json: å®Œæ•´åˆ†æçµæœ (å«æ¯”è¼ƒ)\")\nprint(\"   - optimal_product_bayesian.json: Bayesian æœ€ä½³ç”¢å“\")\nprint(\"   - optimal_product_steinmann.json: Steinmann æœ€ä½³ç”¢å“\")\nprint(\"   - damages.npy: æå¤±æ•¸æ“š\")\nprint(\"   - optimal_payouts_bayesian.npy: Bayesian æœ€ä½³è³ ä»˜\")\nprint(\"   - optimal_payouts_steinmann.npy: Steinmann æœ€ä½³è³ ä»˜\")\nprint(\"   - hazard_indices.npy: é¢¨éšªæŒ‡æ¨™\")\nprint(\"   - steinmann_all_products_results.csv: 70å€‹ç”¢å“å®Œæ•´çµæœ\")\n\n# å‰µå»ºç°¡åŒ–çš„æ¯”è¼ƒæ‘˜è¦\ncomparison_simple = {\n    'bayesian_method': {\n        'approach': 'CRPS-based probabilistic optimization with uncertainty quantification',\n        'evaluation_metric': 'Continuous Ranked Probability Score',\n        'optimization_type': 'Grid search over continuous parameter space'\n    },\n    'steinmann_method': {\n        'approach': 'RMSE-based deterministic optimization with 70 standard products',\n        'evaluation_metric': 'Root Mean Square Error',\n        'optimization_type': 'Selection from predefined Saffir-Simpson products'\n    },\n    'comparison_results': 'See full results in climada_bayesian_steinmann_results.json'\n}\n\nwith open('method_comparison_summary.json', 'w') as f:\n    json.dump(comparison_simple, f, indent=2)\n\nprint(\"   - method_comparison_summary.json: æ–¹æ³•æ¯”è¼ƒç°¡åŒ–æ‘˜è¦\")\n\nprint(f\"\\\\nğŸ¯ å®Œæ•´åˆ†ææµç¨‹:\")\nprint(f\"1. è¼‰å…¥/ç”Ÿæˆ CLIMADA æå¤±æ•¸æ“š\")\nprint(f\"2. åŸ·è¡Œ Bayesian å…©éšæ®µå„ªåŒ– (CRPS è©•ä¼°)\")\nprint(f\"3. ç”Ÿæˆ 70 å€‹ Steinmann æ¨™æº–ç”¢å“\")\nprint(f\"4. è©•ä¼°æ‰€æœ‰ç”¢å“çš„åŸºå·®é¢¨éšª (RMSE æ–¹æ³•)\")\nprint(f\"5. æ¯”è¼ƒå…©ç¨®æ–¹æ³•çš„æœ€ä½³ç”¢å“æ€§èƒ½\")\nprint(f\"6. ä¿å­˜æ‰€æœ‰çµæœä¾›å¾ŒçºŒåˆ†æ\")\n\nprint(f\"\\\\nğŸ“ˆ å¯é€²è¡Œçš„å¾ŒçºŒåˆ†æ:\")\nprint(f\"   ğŸ”¹ ç”¢å“çµ„åˆå„ªåŒ–\")\nprint(f\"   ğŸ”¹ æ•æ„Ÿæ€§åˆ†æ\")\nprint(f\"   ğŸ”¹ ä¸åŒæ•¸æ“šé›†é©—è­‰\") \nprint(f\"   ğŸ”¹ æˆæœ¬æ•ˆç›Šåˆ†æ\")\nprint(f\"   ğŸ”¹ é¢¨éšªç®¡ç†ç­–ç•¥åˆ¶å®š\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## ç¸½çµ (å« Steinmann æ–¹æ³•æ¯”è¼ƒ)\n\né€™å€‹ notebook å±•ç¤ºäº†å¦‚ä½•å°‡ CLIMADA ç½å®³æ¨¡å‹çµæœèˆ‡æ–°ç‰ˆ Bayesian åˆ†æå™¨æ•´åˆï¼Œä¸¦èˆ‡å‚³çµ±çš„ Steinmann et al. (2023) æ–¹æ³•é€²è¡Œç›´æ¥æ¯”è¼ƒã€‚\n\n### ğŸ”— å®Œæ•´åˆ†ææµç¨‹\n1. **ç’°å¢ƒé…ç½®**: è‡ªå‹•æª¢æ¸¬é‹è¡Œç’°å¢ƒä¸¦é…ç½® PyMC\n2. **æ•¸æ“šæº–å‚™**: å°‡ CLIMADA æå¤±çµæœè½‰æ›ç‚º Bayesian åˆ†ææ‰€éœ€æ ¼å¼\n3. **å…©éšæ®µ Bayesian åˆ†æ**: åŸ·è¡Œæ–¹æ³•ä¸€(æ¨¡å‹æ¯”è¼ƒ) + æ–¹æ³•äºŒ(æ±ºç­–æœ€ä½³åŒ–)\n4. **Steinmann æ¨™æº–ç”¢å“è©•ä¼°**: ç”Ÿæˆä¸¦è©•ä¼° 70 å€‹æ¨™æº–åƒæ•¸ä¿éšªç”¢å“\n5. **æ–¹æ³•æ¯”è¼ƒ**: CRPS vs RMSE çš„ç›´æ¥åŸºå·®é¢¨éšªæ¯”è¼ƒ\n6. **çµæœé©—è­‰**: æ¸¬è©¦æœ€ä½³ç”¢å“åœ¨å…¨éƒ¨æ•¸æ“šä¸Šçš„æ•ˆæœ\n\n### ğŸ¯ æ ¸å¿ƒå„ªå‹¢\n\n#### Bayesian CRPS æ–¹æ³•:\n- **ç†è«–æ­£ç¢ºæ€§**: åš´æ ¼éµå¾ª bayesian_implement.md çš„å…©éšæ®µæµç¨‹\n- **ä¸ç¢ºå®šæ€§é‡åŒ–**: ä½¿ç”¨æ©Ÿç‡åˆ†å¸ƒè€Œéé»ä¼°è¨ˆ\n- **è‡ªå‹•æœ€ä½³åŒ–**: è‡ªå‹•é¸æ“‡å† è»æ¨¡å‹ä¸¦æœ€ä½³åŒ–ç”¢å“åƒæ•¸\n- **é¢¨éšªå…¨é¢æ€§**: æä¾›æœŸæœ›åŸºå·®é¢¨éšªå’Œä¸ç¢ºå®šæ€§è©•ä¼°\n\n#### Steinmann RMSE æ–¹æ³•:\n- **æ¨™æº–åŒ–**: åŸºæ–¼ Saffir-Simpson åˆ†ç´šçš„ 70 å€‹æ¨™æº–ç”¢å“\n- **è¨ˆç®—æ•ˆç‡**: é å®šç¾©ç”¢å“ï¼Œç„¡éœ€è¤‡é›œå„ªåŒ–\n- **å¯æ¯”æ€§**: èˆ‡ç¾æœ‰æ–‡ç»ç›´æ¥æ¯”è¼ƒ\n- **å¯¦ç”¨æ€§**: é©åˆæ¨™æº–åŒ–ä¿éšªç”¢å“è¨­è¨ˆ\n\n### âš–ï¸ æ–¹æ³•æ¯”è¼ƒåƒ¹å€¼\n- **å…¬å¹³æ¯”è¼ƒ**: ä½¿ç”¨ç›¸åŒæ•¸æ“šé›†å’ŒåŸºå·®é¢¨éšªè¨ˆç®—å…¬å¼\n- **è©•ä¼°å·®ç•°**: CRPS æ©Ÿç‡æ€§ vs RMSE ç¢ºå®šæ€§è©•ä¼°\n- **æ€§èƒ½é‡åŒ–**: åŸºå·®é¢¨éšªæ”¹å–„ç™¾åˆ†æ¯”å’Œçµ±è¨ˆé¡¯è‘—æ€§\n- **é©ç”¨æ€§åˆ†æ**: ä¸åŒæƒ…å¢ƒä¸‹çš„æ–¹æ³•é¸æ“‡å»ºè­°\n\n### ğŸ“ˆ æ‡‰ç”¨å ´æ™¯\n\n#### å­¸è¡“ç ”ç©¶:\n- ç½å®³é¢¨éšªé‡åŒ–æ–¹æ³•è«–æ¯”è¼ƒ\n- è²è‘‰æ–¯æ–¹æ³•åœ¨ä¿éšªä¸­çš„æ‡‰ç”¨\n- ä¸ç¢ºå®šæ€§å°ç”¢å“è¨­è¨ˆçš„å½±éŸ¿\n\n#### å¯¦å‹™æ‡‰ç”¨:\n- é¢±é¢¨åƒæ•¸ä¿éšªç”¢å“è¨­è¨ˆ\n- åŸºå·®é¢¨éšªè©•ä¼°å’Œæœ€ä½³åŒ–\n- ä¿éšªç”¢å“çµ„åˆå„ªåŒ–\n- é¢¨éšªç®¡ç†ç­–ç•¥åˆ¶å®š\n\n### ğŸ” å‰µæ–°ç‰¹é»\n1. **é¦–æ¬¡æ•´åˆ**: CLIMADA + Bayesian ä¸ç¢ºå®šæ€§é‡åŒ–\n2. **æ–¹æ³•è«–å°æ¯”**: CRPS vs RMSE åœ¨åŒä¸€æ¡†æ¶ä¸‹æ¯”è¼ƒ\n3. **å®Œæ•´å·¥ä½œæµ**: å¾ç½å®³æ¨¡å‹åˆ°æœ€çµ‚ä¿éšªç”¢å“çš„ç«¯åˆ°ç«¯æµç¨‹\n4. **ç’°å¢ƒé©æ‡‰**: æ”¯æ´æœ¬åœ°ã€HPCã€OnDemand ç­‰ä¸åŒè¨ˆç®—ç’°å¢ƒ\n5. **çµæœè±å¯Œ**: æä¾›è©³ç´°çš„çµ±è¨ˆåˆ†æå’Œå¯è¦–åŒ–åŸºç¤\n\n### ğŸ“Š è¼¸å‡ºæˆæœ\n- **æ•¸æ“šæª”æ¡ˆ**: æå¤±ã€è³ ä»˜ã€é¢¨éšªæŒ‡æ¨™æ•¸æ“š\n- **ç”¢å“åƒæ•¸**: Bayesian å’Œ Steinmann æœ€ä½³ç”¢å“è¦æ ¼\n- **æ¯”è¼ƒåˆ†æ**: æ–¹æ³•è«–å·®ç•°å’Œæ€§èƒ½è©•ä¼°\n- **CSV çµæœ**: 70 å€‹ Steinmann ç”¢å“çš„è©³ç´°è©•ä¼°çµæœ\n- **JSON æ‘˜è¦**: å®Œæ•´åˆ†æçµæœçš„çµæ§‹åŒ–å­˜å„²\n\né€™å€‹æ•´åˆçš„åˆ†ææ¡†æ¶ç‚ºåƒæ•¸ä¿éšªç”¢å“çš„è¨­è¨ˆå’Œè©•ä¼°æä¾›äº†å¼·å¤§çš„å·¥å…·ï¼Œçµåˆäº†ç½å®³ç§‘å­¸çš„åš´è¬¹æ€§å’Œè²è‘‰æ–¯çµ±è¨ˆçš„éˆæ´»æ€§ï¼ŒåŒæ™‚ä¿æŒèˆ‡ç¾æœ‰æ–¹æ³•è«–çš„å¯æ¯”æ€§ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% (å¯é¸) ä¿å­˜çµæœ\n",
    "# å¦‚æœéœ€è¦ä¿å­˜çµæœåˆ°æ–‡ä»¶\n",
    "\n",
    "import json\n",
    "\n",
    "# ä¿å­˜æ‘˜è¦çµæœ\n",
    "with open('climada_bayesian_results.json', 'w') as f:\n",
    "    json.dump(final_summary, f, indent=2, default=str)\n",
    "    \n",
    "# ä¿å­˜æœ€ä½³ç”¢å“åƒæ•¸\n",
    "with open('optimal_product.json', 'w') as f:\n",
    "    json.dump(optimal_product, f, indent=2, default=str)\n",
    "\n",
    "# ä¿å­˜æå¤±å’Œè³ ä»˜æ•¸æ“š\n",
    "np.save('damages.npy', damages)\n",
    "np.save('optimal_payouts.npy', optimal_payouts)\n",
    "np.save('hazard_indices.npy', hazard_indices)\n",
    "\n",
    "print(\"ğŸ’¾ çµæœå·²ä¿å­˜:\")\n",
    "print(\"   - climada_bayesian_results.json: å®Œæ•´åˆ†æçµæœ\")\n",
    "print(\"   - optimal_product.json: æœ€ä½³ç”¢å“åƒæ•¸\")\n",
    "print(\"   - damages.npy: æå¤±æ•¸æ“š\")\n",
    "print(\"   - optimal_payouts.npy: æœ€ä½³è³ ä»˜\")\n",
    "print(\"   - hazard_indices.npy: é¢¨éšªæŒ‡æ¨™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¸½çµ\n",
    "\n",
    "é€™å€‹ notebook å±•ç¤ºäº†å¦‚ä½•å°‡ CLIMADA ç½å®³æ¨¡å‹çµæœèˆ‡æ–°ç‰ˆ Bayesian åˆ†æå™¨æ•´åˆï¼š\n",
    "\n",
    "### ğŸ”— æ•´åˆæµç¨‹\n",
    "1. **ç’°å¢ƒé…ç½®**: è‡ªå‹•æª¢æ¸¬é‹è¡Œç’°å¢ƒä¸¦é…ç½® PyMC\n",
    "2. **æ•¸æ“šæº–å‚™**: å°‡ CLIMADA æå¤±çµæœè½‰æ›ç‚º Bayesian åˆ†ææ‰€éœ€æ ¼å¼\n",
    "3. **å…©éšæ®µåˆ†æ**: åŸ·è¡Œæ–¹æ³•ä¸€(æ¨¡å‹æ¯”è¼ƒ) + æ–¹æ³•äºŒ(æ±ºç­–æœ€ä½³åŒ–)\n",
    "4. **çµæœé©—è­‰**: æ¸¬è©¦æœ€ä½³ç”¢å“åœ¨å…¨éƒ¨æ•¸æ“šä¸Šçš„æ•ˆæœ\n",
    "\n",
    "### ğŸ¯ æ ¸å¿ƒå„ªå‹¢\n",
    "- **ç†è«–æ­£ç¢ºæ€§**: åš´æ ¼éµå¾ª bayesian_implement.md çš„å…©éšæ®µæµç¨‹\n",
    "- **è‡ªå‹•æœ€ä½³åŒ–**: è‡ªå‹•é¸æ“‡å† è»æ¨¡å‹ä¸¦æœ€ä½³åŒ–ç”¢å“åƒæ•¸\n",
    "- **é¢¨éšªé‡åŒ–**: æä¾›æœŸæœ›åŸºå·®é¢¨éšªå’Œä¸ç¢ºå®šæ€§è©•ä¼°\n",
    "- **ç’°å¢ƒé©æ‡‰**: æ”¯æ´æœ¬åœ°ã€HPCã€OnDemand ç­‰ä¸åŒç’°å¢ƒ\n",
    "\n",
    "### ğŸ“ˆ æ‡‰ç”¨å ´æ™¯\n",
    "- é¢±é¢¨åƒæ•¸ä¿éšªç”¢å“è¨­è¨ˆ\n",
    "- åŸºå·®é¢¨éšªè©•ä¼°å’Œæœ€ä½³åŒ–\n",
    "- ç½å®³æ¨¡å‹ä¸ç¢ºå®šæ€§é‡åŒ–\n",
    "- ä¿éšªç”¢å“æ•ˆæœé©—è­‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
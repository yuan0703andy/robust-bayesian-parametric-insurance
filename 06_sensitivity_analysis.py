#!/usr/bin/env python3
"""
æ¬Šé‡æ•æ„Ÿæ€§åˆ†æ - ä½¿ç”¨æ¨¡çµ„åŒ–æ¶æ§‹
Weight Sensitivity Analysis - Using Modular Architecture

ä½¿ç”¨æ–°çš„æ¨¡çµ„åŒ– bayesian.WeightSensitivityAnalyzer å¯¦ç¾æ¬Šé‡æ•æ„Ÿæ€§åˆ†æ
é‡å°æ‡²ç½°æ¬Šé‡ (w_under, w_over) çš„æ•æ„Ÿæ€§é€²è¡Œå…¨é¢åˆ†æ

Author: Research Team  
Date: 2025-01-10
"""

import numpy as np
import pandas as pd
from pathlib import Path

# ä½¿ç”¨æ–°çš„æ¨¡çµ„åŒ–çµ„ä»¶
from bayesian import WeightSensitivityAnalyzer


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸ - ä½¿ç”¨05_robust_bayesian_parm_insurance.pyçš„çµæœ"""
    
    print("ğŸš€ æ¬Šé‡æ•æ„Ÿæ€§åˆ†æé–‹å§‹ï¼ˆåŸºæ–¼05çš„Robust Bayesiançµæœï¼‰...")
    print("=" * 60)
    
    # è¼‰å…¥05_robust_bayesian_parm_insurance.pyçš„çµæœ
    print("ğŸ“Š è¼‰å…¥05_robust_bayesian_parm_insurance.pyçš„æ¨¡æ“¬çµæœ...")
    
    import pickle
    
    results_file = Path("results/robust_hierarchical_bayesian_analysis/comprehensive_analysis_results.pkl")
    if not results_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ°05çš„çµæœæ–‡ä»¶: {results_file}")
        print("   è«‹å…ˆåŸ·è¡Œ 05_robust_bayesian_parm_insurance.py")
        return None
    
    # è¼‰å…¥å®Œæ•´çš„Robust Bayesiançµæœ
    with open(results_file, 'rb') as f:
        robust_results = pickle.load(f)
    
    print("âœ… æˆåŠŸè¼‰å…¥05çš„Robust Bayesianåˆ†æçµæœ")
    print(f"   æ•¸æ“šæ‘˜è¦: {robust_results.get('data_summary', {})}")
    
    # å¾çµæœä¸­æå–çœŸå¯¦æ•¸æ“š
    data_summary = robust_results.get('data_summary', {})
    n_events = data_summary.get('n_events', 1000)
    
    # è¼‰å…¥åŸå§‹æ•¸æ“šæ–‡ä»¶ç²å–å¯¦éš›çš„æå¤±å’Œé¢¨éšªæŒ‡æ¨™
    try:
        # è¼‰å…¥ç©ºé–“åˆ†æçµæœ
        with open("results/spatial_analysis/cat_in_circle_results.pkl", 'rb') as f:
            spatial_results = pickle.load(f)
        wind_indices_dict = spatial_results['indices']
        hazard_indices = wind_indices_dict.get('cat_in_circle_30km_max', np.array([]))
        
        # è¼‰å…¥CLIMADAæ•¸æ“šæˆ–ç”Ÿæˆæå¤±æ•¸æ“š
        climada_data = None
        for data_path in ["results/climada_data/climada_complete_data.pkl", "climada_complete_data.pkl"]:
            if Path(data_path).exists():
                try:
                    with open(data_path, 'rb') as f:
                        climada_data = pickle.load(f)
                    break
                except Exception:
                    continue
        
        if climada_data and 'impact' in climada_data:
            actual_losses = climada_data['impact'].at_event
        else:
            # ä½¿ç”¨Emanuelé—œä¿‚ç”Ÿæˆåˆæˆæå¤±æ•¸æ“šï¼ˆèˆ‡05ä¸€è‡´ï¼‰
            np.random.seed(42)
            actual_losses = np.zeros(len(hazard_indices))
            for i, wind in enumerate(hazard_indices):
                if wind > 33:
                    base_loss = ((wind / 33) ** 3.5) * 1e8
                    actual_losses[i] = base_loss * np.random.lognormal(0, 0.5)
                else:
                    if np.random.random() < 0.05:
                        actual_losses[i] = np.random.lognormal(10, 2) * 1e3
        
        # ç¢ºä¿æ•¸çµ„é•·åº¦åŒ¹é…
        min_length = min(len(hazard_indices), len(actual_losses))
        hazard_indices = hazard_indices[:min_length]
        actual_losses = actual_losses[:min_length]
        
        print(f"âœ… ä½¿ç”¨ä¾†è‡ª05åˆ†æçš„çœŸå¯¦æ•¸æ“š:")
        print(f"   äº‹ä»¶æ•¸: {min_length}")
        print(f"   æå¤±ç¯„åœ: {actual_losses.min():.2e} - {actual_losses.max():.2e}")
        print(f"   ç½å®³æŒ‡æ¨™ç¯„åœ: {hazard_indices.min():.2f} - {hazard_indices.max():.2f}")
        
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•è¼‰å…¥åŸå§‹æ•¸æ“š: {e}")
        print("   ä½¿ç”¨æ¨¡æ“¬æ•¸æ“šé€²è¡Œæ¬Šé‡æ•æ„Ÿæ€§åˆ†æ...")
        
        n_scenarios = 1000
        np.random.seed(42)
        
        # ç”Ÿæˆæå¤±æƒ…å¢ƒ (å°æ•¸æ­£æ…‹åˆ†ä½ˆï¼Œæ›´ç¬¦åˆå·¨ç½æå¤±ç‰¹å¾µ)
        loss_mean = 1e8  # 1å„„å¹³å‡æå¤±
        loss_std = 5e7   # 0.5å„„æ¨™æº–å·®
        
        log_mean = np.log(loss_mean) - 0.5 * np.log(1 + (loss_std / loss_mean) ** 2)
        log_std = np.sqrt(np.log(1 + (loss_std / loss_mean) ** 2))
        
        actual_losses = np.random.lognormal(log_mean, log_std, n_scenarios)
        
        # åŠ å…¥30%çš„é›¶æå¤±æƒ…å¢ƒ
        zero_loss_indices = np.random.choice(n_scenarios, size=int(0.3 * n_scenarios), replace=False)
        actual_losses[zero_loss_indices] = 0
        
        # ç”Ÿæˆç½å®³æŒ‡æ¨™ (Gammaåˆ†ä½ˆï¼Œæ›´ç¬¦åˆé¢¨ç½æŒ‡æ¨™)
        hazard_indices = np.random.gamma(2, 20, n_scenarios)
        
        print(f"   æå¤±ç¯„åœ: {actual_losses.min():.2e} - {actual_losses.max():.2e}")
        print(f"   ç½å®³æŒ‡æ¨™ç¯„åœ: {hazard_indices.min():.2f} - {hazard_indices.max():.2f}")
    
    # è¼‰å…¥05çš„Bayesianä¸ç¢ºå®šæ€§é‡åŒ–çµæœï¼ˆå¦‚æœæœ‰ï¼‰
    uncertainty_results = robust_results.get('uncertainty_results', {})
    if uncertainty_results:
        print(f"âœ… ç™¼ç¾Bayesianä¸ç¢ºå®šæ€§é‡åŒ–çµæœ:")
        if 'event_loss_distributions' in uncertainty_results:
            n_distributions = len(uncertainty_results['event_loss_distributions'])
            print(f"   æ©Ÿç‡æå¤±åˆ†å¸ƒ: {n_distributions} äº‹ä»¶")
        methodology = uncertainty_results.get('methodology', 'Unknown')
        print(f"   æ–¹æ³•: {methodology}")
    
    # ä¿å­˜robust_resultsä¾›å¾ŒçºŒä½¿ç”¨
    robust_bayesian_context = {
        'robust_results': robust_results,
        'comprehensive_results': robust_results.get('comprehensive_results'),
        'hierarchical_results': robust_results.get('hierarchical_results'),
        'uncertainty_results': uncertainty_results,
        'original_config': robust_results.get('configuration', {})
    }
    
    # å‰µå»ºæ¬Šé‡æ•æ„Ÿæ€§åˆ†æå™¨ï¼ˆä½¿ç”¨æ¨¡çµ„åŒ–çµ„ä»¶ï¼‰
    print("\nğŸ”§ åˆå§‹åŒ–æ¬Šé‡æ•æ„Ÿæ€§åˆ†æå™¨...")
    
    # å®šç¾©æ¬Šé‡çµ„åˆé€²è¡Œæ¸¬è©¦
    weight_combinations = [
        # åŸºæº–çµ„åˆ
        (2.0, 0.5),  # ç•¶å‰ä½¿ç”¨ (4:1 æ¯”ç‡)
        
        # å°ç¨±çµ„åˆ  
        (1.0, 1.0),  # ç›¸ç­‰æ¬Šé‡
        
        # ä¸åŒæ¯”ç‡æ¸¬è©¦
        (2.0, 1.0),  # 2:1 æ¯”ç‡
        (3.0, 1.0),  # 3:1 æ¯”ç‡
        (4.0, 1.0),  # 4:1 æ¯”ç‡
        (5.0, 1.0),  # 5:1 æ¯”ç‡
        (10.0, 1.0), # 10:1 æ¯”ç‡
        
        # åå‘æ¬Šé‡ (æ›´é—œå¿ƒéåº¦è³ ä»˜)
        (0.5, 2.0),  # 1:4 æ¯”ç‡
        (1.0, 2.0),  # 1:2 æ¯”ç‡
        
        # æ¥µç«¯æƒ…æ³
        (5.0, 0.1),  # æ¥µåº¦æ‡²ç½°ä¸è¶³è¦†è“‹
        (0.1, 5.0),  # æ¥µåº¦æ‡²ç½°éåº¦è¦†è“‹
        
        # æº«å’Œæ¬Šé‡
        (1.5, 1.0),  # 1.5:1 æ¯”ç‡
        (1.0, 0.7),  # 1:0.7 æ¯”ç‡
    ]
    
    # ä½¿ç”¨æ¨¡çµ„åŒ–çµ„ä»¶é…ç½®ï¼Œæ•´åˆ05çš„Robust Bayesiançµæœ
    from bayesian.weight_sensitivity_analyzer import WeightSensitivityConfig
    
    config = WeightSensitivityConfig(
        weight_combinations=weight_combinations,
        output_dir="results/sensitivity_analysis_from_robust_bayesian"
    )
    
    # å¦‚æœæœ‰05çš„RobustBayesianAnalyzerçµæœï¼Œå˜—è©¦é‡ç”¨å®ƒ
    robust_analyzer = None
    if robust_bayesian_context['comprehensive_results']:
        print("   ğŸ”— å˜—è©¦æ•´åˆ05çš„RobustBayesianAnalyzer...")
        try:
            # é‡æ–°åˆå§‹åŒ–RobustBayesianAnalyzerä»¥ä¾¿æ•´åˆ
            from bayesian import RobustBayesianAnalyzer
            original_config = robust_bayesian_context['original_config']
            robust_analyzer = RobustBayesianAnalyzer(
                density_ratio_constraint=original_config.get('density_ratio_constraint', 2.0),
                n_monte_carlo_samples=original_config.get('n_monte_carlo_samples', 500),
                n_mixture_components=original_config.get('n_mixture_components', 3),
                hazard_uncertainty_std=original_config.get('hazard_uncertainty_std', 0.15),
                exposure_uncertainty_log_std=original_config.get('exposure_uncertainty_log_std', 0.20),
                vulnerability_uncertainty_std=original_config.get('vulnerability_uncertainty_std', 0.10)
            )
            print("   âœ… æˆåŠŸæ•´åˆRobustBayesianAnalyzer")
        except Exception as e:
            print(f"   âš ï¸ ç„¡æ³•æ•´åˆRobustBayesianAnalyzer: {e}")
    
    analyzer = WeightSensitivityAnalyzer(config=config, robust_analyzer=robust_analyzer)
    
    # åŸ·è¡Œæ¬Šé‡æ•æ„Ÿæ€§åˆ†æ
    print(f"\nğŸ” åŸ·è¡Œæ¬Šé‡æ•æ„Ÿæ€§åˆ†æ ({len(weight_combinations)} å€‹æ¬Šé‡çµ„åˆ)...")
    
    # å®šç¾©ç”¢å“åƒæ•¸æœç´¢ç¯„åœ
    product_bounds = {
        'trigger_threshold': (np.percentile(hazard_indices, 50), np.percentile(hazard_indices, 95)),
        'payout_amount': (np.percentile(actual_losses[actual_losses > 0], 10), 
                         np.percentile(actual_losses[actual_losses > 0], 90))
    }
    
    print(f"   ç”¢å“æœç´¢ç¯„åœ:")
    print(f"     è§¸ç™¼é–¾å€¼: {product_bounds['trigger_threshold'][0]:.2f} - {product_bounds['trigger_threshold'][1]:.2f}")
    print(f"     è³ ä»˜é‡‘é¡: {product_bounds['payout_amount'][0]:.2e} - {product_bounds['payout_amount'][1]:.2e}")
    
    results = analyzer.analyze_weight_sensitivity(
        observations=actual_losses,      # è¨“ç·´æ•¸æ“š
        validation_data=actual_losses,   # é©—è­‰æ•¸æ“šï¼ˆåœ¨é€™å€‹ç¤ºä¾‹ä¸­ä½¿ç”¨ç›¸åŒæ•¸æ“šï¼‰
        hazard_indices=hazard_indices,
        actual_losses=actual_losses,
        product_bounds=product_bounds
    )
    
    # é¡¯ç¤ºçµæœæ‘˜è¦
    print("\n" + "=" * 60)
    print("ğŸ“‹ æ¬Šé‡æ•æ„Ÿæ€§åˆ†æçµæœæ‘˜è¦:")
    print("=" * 60)
    
    if hasattr(results, 'weight_combinations_analysis') and results.weight_combinations_analysis:
        analysis_data = results.weight_combinations_analysis
        
        # æ‰¾åˆ°æœ€ä½³å’Œæœ€å·®æ¬Šé‡çµ„åˆ
        best_idx = min(range(len(analysis_data)), key=lambda i: analysis_data[i]['optimal_expected_loss'])
        worst_idx = max(range(len(analysis_data)), key=lambda i: analysis_data[i]['optimal_expected_loss'])
        
        best_combo = analysis_data[best_idx]
        worst_combo = analysis_data[worst_idx]
        
        print(f"âœ… æœ€ä½³æ¬Šé‡çµ„åˆ:")
        print(f"   w_under={best_combo['w_under']:.1f}, w_over={best_combo['w_over']:.1f}")
        print(f"   æ¬Šé‡æ¯”ç‡: {best_combo['w_under']/best_combo['w_over']:.1f}:1")
        print(f"   æœ€å°æœŸæœ›æå¤±: {best_combo['optimal_expected_loss']:.2e}")
        
        print(f"\nâŒ æœ€å·®æ¬Šé‡çµ„åˆ:")
        print(f"   w_under={worst_combo['w_under']:.1f}, w_over={worst_combo['w_over']:.1f}")
        print(f"   æ¬Šé‡æ¯”ç‡: {worst_combo['w_under']/worst_combo['w_over']:.1f}:1")
        print(f"   æœ€å¤§æœŸæœ›æå¤±: {worst_combo['optimal_expected_loss']:.2e}")
        
        print(f"\nğŸ“ˆ æ•æ„Ÿæ€§çµ±è¨ˆ:")
        risks = [x['optimal_expected_loss'] for x in analysis_data]
        print(f"   æœŸæœ›æå¤±è®Šç•°ä¿‚æ•¸: {np.std(risks)/np.mean(risks):.3f}")
        print(f"   æ€§èƒ½å·®ç•°å€æ•¸: {worst_combo['optimal_expected_loss']/best_combo['optimal_expected_loss']:.2f}")
    else:
        print("âš ï¸ åˆ†æçµæœæ ¼å¼æœªçŸ¥æˆ–ç‚ºç©º")
        if hasattr(results, '__dict__'):
            print(f"   çµæœå±¬æ€§: {list(results.__dict__.keys())}")
    
    # è¼¸å‡ºæª”æ¡ˆä½ç½®
    print(f"\nğŸ“ çµæœå·²ä¿å­˜è‡³:")
    output_dir = Path("results/sensitivity_analysis_modular")
    if output_dir.exists():
        for file in output_dir.glob("*"):
            print(f"   â€¢ {file}")
    
    # å¦‚æœæœ‰ç¸½çµå ±å‘Šï¼Œé¡¯ç¤ºé—œéµæ´å¯Ÿ
    if hasattr(results, 'summary_report'):
        print(f"\nğŸ’¡ é—œéµæ´å¯Ÿ:")
        summary = results.summary_report
        if 'sensitivity_insights' in summary:
            insights = summary['sensitivity_insights']
            if 'correlation_analysis' in insights:
                correlation = insights['correlation_analysis']
                print(f"   æ¬Šé‡-æ€§èƒ½ç›¸é—œæ€§: {correlation.get('correlation_coefficient', 'N/A')}")
            if 'robustness_assessment' in insights:
                robustness = insights['robustness_assessment']
                print(f"   ç©©å¥æ€§è©•åˆ†: {robustness.get('stability_score', 'N/A')}")
            if 'recommendations' in insights:
                print(f"   å»ºè­°: {insights['recommendations']}")
    
    print(f"\nğŸ‰ æ¬Šé‡æ•æ„Ÿæ€§åˆ†æå®Œæˆï¼")
    print("âœ¨ åŸºæ–¼05_robust_bayesian_parm_insurance.pyçµæœçš„æ¬Šé‡æ•æ„Ÿæ€§åˆ†æ")
    
    # é¡¯ç¤ºèˆ‡05 Robust Bayesian çµæœçš„æ•´åˆæƒ…æ³
    print(f"\nğŸ”— èˆ‡05 Robust Bayesianåˆ†æçš„æ•´åˆç‹€æ³:")
    if robust_bayesian_context['comprehensive_results']:
        print("   âœ… æˆåŠŸè¼‰å…¥ä¸¦æ•´åˆ05çš„å®Œæ•´Bayesianå„ªåŒ–çµæœ")
    if robust_bayesian_context['hierarchical_results']:
        print("   âœ… æˆåŠŸè¼‰å…¥ä¸¦æ•´åˆ05çš„éšå±¤Bayesianåˆ†æçµæœ")
    if robust_bayesian_context['uncertainty_results']:
        print("   âœ… æˆåŠŸè¼‰å…¥ä¸¦æ•´åˆ05çš„ä¸ç¢ºå®šæ€§é‡åŒ–çµæœ")
        
    # ä¿å­˜æ•´åˆçµæœ
    print(f"\nğŸ’¾ ä¿å­˜æ•´åˆåˆ†æçµæœ...")
    integrated_results = {
        'weight_sensitivity_results': results,
        'robust_bayesian_context': robust_bayesian_context,
        'analysis_type': 'integrated_weight_sensitivity_from_robust_bayesian',
        'data_source': '05_robust_bayesian_parm_insurance.py',
        'analysis_timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    output_dir = Path("results/sensitivity_analysis_from_robust_bayesian")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(output_dir / "integrated_sensitivity_results.pkl", 'wb') as f:
        pickle.dump(integrated_results, f)
    print(f"   âœ… æ•´åˆçµæœå·²ä¿å­˜: {output_dir}/integrated_sensitivity_results.pkl")
    
    return integrated_results


if __name__ == "__main__":
    results = main()
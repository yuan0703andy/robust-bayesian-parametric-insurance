#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
explain_what_mcmc_samples.py
============================
è§£é‡‹pm.sample()åˆ°åº•åœ¨å–æ¨£ä»€éº¼
Explain what pm.sample() is actually sampling

å›ç­”æ‚¨çš„å•é¡Œï¼š
"æˆ‘å€‘ä¸èƒ½ç°¡å–®åœ°å°‡ CRPS ç•¶ä½œä¸€å€‹æå¤±å‡½æ•¸ä¸Ÿé€² pm.sample() 
æˆ‘é‚„å¥½å¥‡é€™å€‹éƒ¨åˆ† é‚£æˆ‘å€‘ç¾åœ¨ pm.sample() çš„æ˜¯ä»€éº¼"
"""

import numpy as np
import matplotlib.pyplot as plt

def explain_mcmc_sampling():
    """
    è§£é‡‹MCMCåœ¨æ¡æ¨£ä»€éº¼
    """
    
    print("ğŸ² What is pm.sample() Actually Sampling?")
    print("=" * 80)
    
    print("\nğŸ“Š YOUR CURRENT MODEL STRUCTURE")
    print("-" * 60)
    model_structure = """
    ç©ºé–“éšå±¤è²æ°æ¨¡å‹ (Spatial Hierarchical Bayesian Model):
    
    Î²_i = Î±_r(i) + Î´_i + Î³_i
    
    Where:
    â€¢ Î²_i: Vulnerability parameter for location i (è„†å¼±åº¦åƒæ•¸)
    â€¢ Î±_r(i): Regional effect for region r containing location i (å€åŸŸæ•ˆæ‡‰)
    â€¢ Î´_i: Spatial correlation effect (ç©ºé–“ç›¸é—œæ•ˆæ‡‰)
    â€¢ Î³_i: Local random effect (å±€éƒ¨éš¨æ©Ÿæ•ˆæ‡‰)
    """
    print(model_structure)
    
    print("\nğŸ¯ WHAT pm.sample() IS SAMPLING")
    print("-" * 60)
    sampling_explanation = """
    pm.sample() is sampling the POSTERIOR DISTRIBUTIONS of:
    
    1. PARAMETERS (åƒæ•¸çš„å¾Œé©—åˆ†å¸ƒ):
       â€¢ Î± (regional effects): shape = (n_regions,)
       â€¢ Î´ (spatial effects): shape = (n_locations,)
       â€¢ Î³ (local effects): shape = (n_locations,)
       â€¢ Ïƒ (observation error): scalar
       â€¢ Ï (spatial correlation): scalar
       â€¢ Ï„ (hierarchical variance): scalar
    
    2. DERIVED QUANTITIES (è¡ç”Ÿé‡):
       â€¢ Î² = Î±[region_idx] + Î´ + Î³ (total vulnerability)
       â€¢ Expected losses = exposure Ã— Î² Ã— f(hazard)
    
    3. POSTERIOR PREDICTIVE (å¾Œé©—é æ¸¬):
       â€¢ Future loss predictions given new hazard scenarios
    """
    print(sampling_explanation)
    
    print("\nâš ï¸ WHAT pm.sample() IS NOT SAMPLING")
    print("-" * 60)
    not_sampling = """
    pm.sample() is NOT:
    
    âŒ Using CRPS as a loss function
    âŒ Using basis risk as a loss function
    âŒ Optimizing insurance products
    âŒ Minimizing any decision-theoretic loss
    
    These are SEPARATE processes that happen AFTER sampling!
    """
    print(not_sampling)

def show_likelihood_vs_loss():
    """
    å±•ç¤ºLikelihood vs Loss Functionçš„å·®ç•°
    """
    
    print("\nğŸ” LIKELIHOOD vs LOSS FUNCTION")
    print("=" * 80)
    
    print("\n1ï¸âƒ£ LIKELIHOOD (Used in pm.sample())")
    print("-" * 40)
    likelihood_code = """
    # This is what pm.sample() uses:
    
    with pm.Model() as model:
        # Priors
        Î± ~ Normal(0, 1)  # Regional effects
        Î´ ~ CAR(W, Ï„)     # Spatial effects
        Î³ ~ Normal(0, Ïƒ)  # Local effects
        
        # Vulnerability model
        Î² = Î±[region_idx] + Î´ + Î³
        
        # LIKELIHOOD FUNCTION (é€™æ˜¯pm.sample()ä½¿ç”¨çš„):
        expected_losses = exposure * Î² * emanuel_usa_function(hazard)
        observed_losses ~ Normal(mu=expected_losses, sigma=Ïƒ_obs)
        #                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        #                 This Normal distribution is the likelihood!
        
        # MCMC sampling
        trace = pm.sample(4000, chains=8)  # Samples Î±, Î´, Î³, Ïƒ, etc.
    """
    print(likelihood_code)
    
    print("\n2ï¸âƒ£ LOSS FUNCTION (Used AFTER pm.sample())")
    print("-" * 40)
    loss_function_code = """
    # This happens AFTER getting the posterior:
    
    # Step 1: Get posterior samples from MCMC
    posterior_samples = trace.posterior
    
    # Step 2: Define basis risk loss functions
    basis_risk_functions = {
        'absolute': lambda actual, payout: |actual - payout|,
        'asymmetric': lambda actual, payout: max(0, actual - payout),
        'weighted': lambda actual, payout: w_under*max(0, actual-payout) + 
                                          w_over*max(0, payout-actual)
    }
    
    # Step 3: Optimize products using loss functions
    for risk_type, loss_func in basis_risk_functions.items():
        optimal_product = minimize(
            lambda params: E[loss_func(posterior_losses, payout(params))]
        )
    
    # Note: This optimization uses the posterior FROM pm.sample(),
    # but the loss function is NOT part of pm.sample() itself!
    """
    print(loss_function_code)

def show_concrete_example():
    """
    å±•ç¤ºå…·é«”ä¾‹å­
    """
    
    print("\nğŸ“Œ CONCRETE EXAMPLE")
    print("=" * 80)
    
    print("\nğŸ² What Your Current pm.sample() Actually Samples:")
    print("-" * 50)
    
    # æ¨¡æ“¬æ¡æ¨£çµæœ
    np.random.seed(42)
    n_samples = 4000
    n_chains = 8
    n_regions = 5
    
    # æ¨¡æ“¬å¾Œé©—æ¨£æœ¬
    alpha_samples = np.random.normal(0, 0.3, (n_chains, n_samples, n_regions))
    delta_samples = np.random.normal(0, 0.2, (n_chains, n_samples, 100))
    gamma_samples = np.random.normal(0, 0.1, (n_chains, n_samples, 100))
    sigma_samples = np.random.gamma(2, 0.5, (n_chains, n_samples))
    
    print(f"""
    Sampled Parameters (å¾Œé©—æ¨£æœ¬):
    â€¢ Î± (regional effects):  shape={alpha_samples.shape}, mean={alpha_samples.mean():.3f}
    â€¢ Î´ (spatial effects):   shape={delta_samples.shape}, mean={delta_samples.mean():.3f}
    â€¢ Î³ (local effects):     shape={gamma_samples.shape}, mean={gamma_samples.mean():.3f}
    â€¢ Ïƒ (observation error): shape={sigma_samples.shape}, mean={sigma_samples.mean():.3f}
    
    These samples represent UNCERTAINTY in the model parameters!
    They are NOT optimizing any insurance product.
    """)
    
    print("\nğŸ¯ What SHOULD Happen Next (But Isn't):")
    print("-" * 50)
    next_steps = """
    1. Use posterior samples to generate loss distributions
    2. Define parametric insurance products (trigger, payout)
    3. For each basis risk type:
       - Calculate expected basis risk using posterior
       - Optimize product parameters to minimize risk
    4. Compare optimal products from different risk definitions
    5. Select best product based on criteria
    """
    print(next_steps)

def show_mathematical_difference():
    """
    å±•ç¤ºæ•¸å­¸ä¸Šçš„å·®ç•°
    """
    
    print("\nğŸ“ MATHEMATICAL DIFFERENCE")
    print("=" * 80)
    
    print("\n1ï¸âƒ£ MCMC Objective (What pm.sample() maximizes):")
    print("-" * 50)
    mcmc_math = r"""
    Maximize: log p(Î¸|D) = log p(D|Î¸) + log p(Î¸) - log p(D)
                           ^^^^^^^^^^^
                           Likelihood
    
    Where:
    â€¢ Î¸ = {Î±, Î´, Î³, Ïƒ, Ï„, Ï} are model parameters
    â€¢ D = observed_losses are the data
    â€¢ p(D|Î¸) = âˆ Normal(loss_i | f(Î¸, hazard_i, exposure_i), Ïƒ)
    
    This finds the most likely parameter values given the data!
    """
    print(mcmc_math)
    
    print("\n2ï¸âƒ£ Product Optimization Objective (What SHOULD happen after):")
    print("-" * 50)
    optimization_math = r"""
    Minimize: E_Î¸|D[BasisRisk(L(Î¸), P(Î¾))]
    
    Where:
    â€¢ Î¸|D ~ Posterior from MCMC (å·²ç¶“å¾pm.sample()ç²å¾—)
    â€¢ L(Î¸) = Loss given parameters Î¸
    â€¢ P(Î¾) = Payout given product parameters Î¾
    â€¢ BasisRisk = One of three definitions (absolute/asymmetric/weighted)
    
    This finds the best insurance product given parameter uncertainty!
    """
    print(optimization_math)
    
    print("\nâš ï¸ KEY INSIGHT:")
    print("-" * 50)
    key_insight = """
    CRPS is NOT a likelihood function for MCMC!
    
    â€¢ CRPS measures prediction skill (ç”¨æ–¼è©•ä¼°é æ¸¬æŠ€å·§)
    â€¢ Likelihood measures data fit (ç”¨æ–¼æ“¬åˆæ•¸æ“š)
    
    You cannot "just throw CRPS into pm.sample()" because:
    1. CRPS needs ensemble predictions vs observations
    2. MCMC needs a proper probability distribution
    3. They serve completely different purposes!
    
    Correct workflow:
    MCMC (get posteriors) â†’ Generate predictions â†’ Calculate CRPS â†’ Compare models
    """
    print(key_insight)

def show_visualization():
    """
    è¦–è¦ºåŒ–å±•ç¤ºå·®ç•°
    """
    
    print("\nğŸ“Š VISUAL REPRESENTATION")
    print("=" * 80)
    
    # Create figure
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # 1. MCMC Sampling
    ax = axes[0, 0]
    np.random.seed(42)
    samples = np.random.normal(0, 1, 1000)
    ax.hist(samples, bins=30, alpha=0.7, color='blue', edgecolor='black')
    ax.axvline(0, color='red', linestyle='--', label='True value')
    ax.set_title('MCMC Sampling: Parameter Posterior\n(What pm.sample() gives you)')
    ax.set_xlabel('Parameter value')
    ax.set_ylabel('Frequency')
    ax.legend()
    
    # 2. Loss Distribution
    ax = axes[0, 1]
    losses = np.random.lognormal(15, 1, 1000)
    ax.hist(losses/1e6, bins=30, alpha=0.7, color='green', edgecolor='black')
    ax.set_title('Posterior Predictive Losses\n(Generated from MCMC posteriors)')
    ax.set_xlabel('Loss (M$)')
    ax.set_ylabel('Frequency')
    
    # 3. Basis Risk Functions
    ax = axes[1, 0]
    actual = np.linspace(0, 100, 100)
    payout = 50
    absolute_risk = np.abs(actual - payout)
    asymmetric_risk = np.maximum(0, actual - payout)
    weighted_risk = 2*np.maximum(0, actual - payout) + 0.5*np.maximum(0, payout - actual)
    
    ax.plot(actual, absolute_risk, label='Absolute', linewidth=2)
    ax.plot(actual, asymmetric_risk, label='Asymmetric', linewidth=2)
    ax.plot(actual, weighted_risk, label='Weighted', linewidth=2)
    ax.axvline(payout, color='red', linestyle='--', alpha=0.5, label='Payout')
    ax.set_title('Basis Risk Functions\n(NOT used in pm.sample())')
    ax.set_xlabel('Actual Loss')
    ax.set_ylabel('Basis Risk')
    ax.legend()
    
    # 4. Optimization Surface
    ax = axes[1, 1]
    triggers = np.linspace(20, 80, 50)
    payouts = np.linspace(10, 90, 50)
    X, Y = np.meshgrid(triggers, payouts)
    Z = np.sqrt((X - 50)**2 + (Y - 45)**2)  # Simplified basis risk surface
    
    contour = ax.contour(X, Y, Z, levels=10)
    ax.clabel(contour, inline=True, fontsize=8)
    ax.plot(50, 45, 'r*', markersize=15, label='Optimal product')
    ax.set_title('Product Optimization\n(Uses posteriors to minimize basis risk)')
    ax.set_xlabel('Trigger Threshold')
    ax.set_ylabel('Payout Amount')
    ax.legend()
    
    plt.tight_layout()
    plt.savefig('mcmc_vs_optimization.png', dpi=150, bbox_inches='tight')
    print("\nâœ… Visualization saved as 'mcmc_vs_optimization.png'")
    
    plt.show()

def main():
    """ä¸»å‡½æ•¸"""
    
    print("ğŸ“ EXPLANATION: What pm.sample() Actually Samples")
    print("=" * 80)
    print()
    
    # è§£é‡‹MCMCæ¡æ¨£
    explain_mcmc_sampling()
    
    # å±•ç¤ºLikelihood vs Loss
    show_likelihood_vs_loss()
    
    # å…·é«”ä¾‹å­
    show_concrete_example()
    
    # æ•¸å­¸å·®ç•°
    show_mathematical_difference()
    
    # è¦–è¦ºåŒ–
    show_visualization()
    
    print("\n" + "=" * 80)
    print("ğŸ’¡ SUMMARY ANSWER TO YOUR QUESTION:")
    print("-" * 40)
    summary = """
    Q: "é‚£æˆ‘å€‘ç¾åœ¨ pm.sample() çš„æ˜¯ä»€éº¼ï¼Ÿ"
    
    A: pm.sample() is sampling the POSTERIOR DISTRIBUTIONS of your spatial
       hierarchical model parameters (Î±, Î´, Î³, Ïƒ, Ï„, Ï).
    
    It uses a Normal likelihood: observed_losses ~ Normal(Î¼=predictions, Ïƒ)
    
    It does NOT use CRPS or basis risk as loss functions!
    
    The correct workflow is:
    1. pm.sample() â†’ Get parameter posteriors (ç¾åœ¨åœ¨åšçš„)
    2. Use posteriors â†’ Optimize products with basis risk (ç¼ºå°‘çš„éƒ¨åˆ†)
    
    You need to ADD Step 2 to complete your analysis!
    """
    print(summary)

if __name__ == "__main__":
    main()
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
05_optimized_gpu_config.py
==========================
High-Performance GPU/CPU Optimization Configuration
é«˜æ€§èƒ½GPU/CPUå„ªåŒ–é…ç½®

For systems with:
- 16 core CPU
- 2x RTX 2050 GPUs
- Large RAM capacity

é‡å°ä»¥ä¸‹ç¡¬é«”å„ªåŒ–ï¼š
- 16æ ¸CPU
- 2å¼µRTX 2050 GPU  
- å¤§å®¹é‡è¨˜æ†¶é«”
"""

import os
import torch
import numpy as np
from pathlib import Path
import warnings

def configure_high_performance_environment(verbose=True):
    """
    é…ç½®é«˜æ€§èƒ½è¨ˆç®—ç’°å¢ƒ
    Configure high-performance computing environment
    
    é‡å°16æ ¸CPU + 2x RTX2050çš„æœ€ä½³åŒ–é…ç½®
    Optimized for 16-core CPU + 2x RTX2050 setup
    """
    
    if verbose:
        print("ğŸš€ High-Performance Environment Configuration")
        print("   é«˜æ€§èƒ½ç’°å¢ƒé…ç½®")
        print("=" * 60)
        print(f"   â€¢ Target: 16-core CPU + 2x RTX 2050 GPUs")
        print(f"   â€¢ Optimization: MCMC sampling parallelization")
        
    config_changes = {}
    
    # =============================================================================
    # CPUå„ªåŒ–é…ç½® CPU Optimization
    # =============================================================================
    
    print("\nğŸ–¥ï¸ CPU Optimization (16-core setup)")
    
    # OpenMPç·šç¨‹æ•¸è¨­ç½®ç‚º16æ ¸å¿ƒ
    old_omp = os.environ.get('OMP_NUM_THREADS', 'not_set')
    os.environ['OMP_NUM_THREADS'] = '16'
    print(f"   â€¢ OMP_NUM_THREADS: {old_omp} â†’ 16")
    
    # MKLç·šç¨‹æ•¸ï¼ˆIntel Math Kernel Libraryï¼‰
    old_mkl = os.environ.get('MKL_NUM_THREADS', 'not_set')
    os.environ['MKL_NUM_THREADS'] = '16'
    print(f"   â€¢ MKL_NUM_THREADS: {old_mkl} â†’ 16")
    
    # NumPyç·šç¨‹æ•¸
    old_openblas = os.environ.get('OPENBLAS_NUM_THREADS', 'not_set')
    os.environ['OPENBLAS_NUM_THREADS'] = '16'
    print(f"   â€¢ OPENBLAS_NUM_THREADS: {old_openblas} â†’ 16")
    
    # è¨­ç½®ç‚ºGNUç·šç¨‹å±¤ï¼ˆé¿å…è¡çªï¼‰
    os.environ['MKL_THREADING_LAYER'] = 'GNU'
    print(f"   â€¢ MKL_THREADING_LAYER: GNU")
    
    config_changes['cpu'] = {
        'OMP_NUM_THREADS': '16',
        'MKL_NUM_THREADS': '16', 
        'OPENBLAS_NUM_THREADS': '16',
        'MKL_THREADING_LAYER': 'GNU'
    }
    
    # =============================================================================
    # GPUé…ç½® GPU Configuration  
    # =============================================================================
    
    print(f"\nğŸ–¥ï¸ GPU Configuration (RTX 2050 x2)")
    
    # æª¢æŸ¥CUDAå¯ç”¨æ€§
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"   âœ… CUDA available with {gpu_count} GPUs")
        
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   â€¢ GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
            
        # è¨­ç½®GPUè¨˜æ†¶é«”å¢é•·ç­–ç•¥
        os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'  # ä½¿ç”¨å…©å¼µGPU
        print(f"   â€¢ CUDA_VISIBLE_DEVICES: 0,1")
        
        config_changes['gpu'] = {
            'cuda_available': True,
            'gpu_count': gpu_count,
            'visible_devices': '0,1'
        }
    else:
        print(f"   âš ï¸ CUDA not available, using CPU-only optimization")
        config_changes['gpu'] = {'cuda_available': False}
    
    # =============================================================================
    # JAX/PyMCå„ªåŒ–é…ç½® JAX/PyMC Optimization
    # =============================================================================
    
    print(f"\nğŸ§  JAX/PyMC Optimization")
    
    # å„ªå…ˆä½¿ç”¨GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if torch.cuda.is_available():
        old_jax_platform = os.environ.get('JAX_PLATFORM_NAME', 'not_set')
        os.environ['JAX_PLATFORM_NAME'] = 'gpu'
        print(f"   â€¢ JAX_PLATFORM_NAME: {old_jax_platform} â†’ gpu")
        
        # GPUè¨˜æ†¶é«”é åˆ†é…è¨­ç½®
        os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'
        print(f"   â€¢ XLA_PYTHON_CLIENT_PREALLOCATE: false (é¿å…è¨˜æ†¶é«”é åˆ†é…)")
        
        config_changes['jax'] = {
            'platform': 'gpu',
            'preallocate': 'false'
        }
    else:
        os.environ['JAX_PLATFORM_NAME'] = 'cpu'
        print(f"   â€¢ JAX_PLATFORM_NAME: cpu (GPUä¸å¯ç”¨)")
        config_changes['jax'] = {'platform': 'cpu'}
    
    # PyTensorå„ªåŒ–è¨­ç½®
    old_pytensor = os.environ.get('PYTENSOR_FLAGS', 'not_set')
    # ä½¿ç”¨å¿«é€Ÿç·¨è­¯æ¨¡å¼ï¼Œé©åˆå¤§å‹MCMC
    pytensor_flags = 'mode=FAST_RUN,optimizer=fast_run,floatX=float32'
    os.environ['PYTENSOR_FLAGS'] = pytensor_flags
    print(f"   â€¢ PYTENSOR_FLAGS: FAST_RUN with optimizer")
    
    config_changes['pytensor'] = {'flags': pytensor_flags}
    
    # =============================================================================
    # è¨˜æ†¶é«”å„ªåŒ– Memory Optimization
    # =============================================================================
    
    print(f"\nğŸ’¾ Memory Optimization")
    
    # Pythonè¨˜æ†¶é«”åˆ†é…å™¨å„ªåŒ–
    os.environ['PYTHONHASHSEED'] = '0'  # ç¢ºä¿å¯é‡ç¾æ€§
    print(f"   â€¢ PYTHONHASHSEED: 0 (ç¢ºä¿å¯é‡ç¾æ€§)")
    
    # å¤§å‹æ•¸çµ„è¨˜æ†¶é«”æ˜ å°„
    os.environ['PYTENSOR_FLAGS'] += ',allow_gc=True'
    print(f"   â€¢ PyTensor garbage collection: enabled")
    
    config_changes['memory'] = {
        'hash_seed': '0',
        'gc_enabled': True
    }
    
    return config_changes


def get_optimized_mcmc_config():
    """
    ç²å–é‡å°é«˜æ€§èƒ½ç¡¬é«”çš„æœ€ä½³MCMCé…ç½®
    Get optimized MCMC configuration for high-performance hardware
    """
    
    # æª¢æŸ¥æ˜¯å¦æœ‰GPU
    use_gpu = torch.cuda.is_available()
    
    config = {
        # åŸºæœ¬MCMCé…ç½®
        'draws': 4000,              # å¢åŠ æ¨£æœ¬æ•¸ï¼ˆå› ç‚ºæœ‰æ›´å¤šè¨ˆç®—è³‡æºï¼‰
        'tune': 2000,               # èª¿æ•´éšæ®µæ¨£æœ¬æ•¸
        'chains': 8,                # 8æ¢éˆä¸¦è¡Œï¼ˆå……åˆ†åˆ©ç”¨16æ ¸ï¼‰
        'cores': 16,                # ä½¿ç”¨å…¨éƒ¨16æ ¸
        'target_accept': 0.95,      # é«˜æ¥å—ç‡ç¢ºä¿å“è³ª
        
        # GPUç‰¹å®šé…ç½®
        'use_gpu': use_gpu,
        
        # é€²éšé…ç½®
        'nuts_sampler': {
            'max_treedepth': 12,    # å¢åŠ æ¨¹æ·±åº¦ï¼ˆGPUæœ‰æ›´å¤šè¨ˆç®—åŠ›ï¼‰
            'step_size': 'auto',    # è‡ªå‹•æ­¥é•·èª¿æ•´
        },
        
        # ä¸¦è¡Œé…ç½®
        'parallel': {
            'method': 'multiprocessing',  # å¤šé€²ç¨‹ä¸¦è¡Œ
            'n_jobs': 8,                  # 8å€‹ä¸¦è¡Œä½œæ¥­
        },
        
        # è¨˜æ†¶é«”ç®¡ç†
        'memory': {
            'chunk_size': 1000,           # å¤§å¡Šè™•ç†ï¼ˆå……è¶³è¨˜æ†¶é«”ï¼‰
            'cache_size': '2GB',          # å¤§å¿«å–ç©ºé–“
        }
    }
    
    return config


def create_optimized_analysis_script():
    """
    å‰µå»ºå„ªåŒ–ç‰ˆæœ¬çš„åˆ†æè…³æœ¬
    Create optimized version of analysis script
    """
    
    optimized_script = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
05_robust_bayesian_parm_insurance_optimized.py
==============================================
High-Performance Optimized Version
é«˜æ€§èƒ½å„ªåŒ–ç‰ˆæœ¬

é‡å°16æ ¸CPU + 2x RTX2050å„ªåŒ–çš„å¼·å¥è²æ°éšå±¤æ¨¡å‹
Robust Bayesian hierarchical model optimized for 16-core CPU + 2x RTX2050
"""

# è¼‰å…¥å„ªåŒ–é…ç½®
from 05_optimized_gpu_config import configure_high_performance_environment, get_optimized_mcmc_config

# é…ç½®é«˜æ€§èƒ½ç’°å¢ƒ
print("ğŸš€ Loading High-Performance Configuration...")
config_changes = configure_high_performance_environment(verbose=True)
mcmc_config = get_optimized_mcmc_config()

print(f"\\nğŸ“Š Optimized MCMC Configuration:")
print(f"   â€¢ Draws: {mcmc_config['draws']}")
print(f"   â€¢ Chains: {mcmc_config['chains']}")  
print(f"   â€¢ Cores: {mcmc_config['cores']}")
print(f"   â€¢ GPU Support: {mcmc_config['use_gpu']}")

# ç¾åœ¨è¼‰å…¥ä¸»è¦åˆ†æè…³æœ¬
print("\\n" + "="*80)
print("ğŸ§  Loading Main Analysis Script with Optimizations...")
print("="*80)

# é€™è£¡æœƒåŸ·è¡Œæ‚¨çš„ä¸»è¦åˆ†æï¼Œä½†ä½¿ç”¨å„ªåŒ–é…ç½®
exec(open("05_robust_bayesian_parm_insurance.py").read())
'''

    return optimized_script


def benchmark_performance():
    """
    æ€§èƒ½åŸºæº–æ¸¬è©¦
    Performance benchmarking
    """
    
    print("\nğŸƒâ€â™‚ï¸ Performance Benchmarking")
    print("-" * 40)
    
    import time
    
    # CPUåŸºæº–æ¸¬è©¦
    print("CPUæ€§èƒ½æ¸¬è©¦...")
    start_time = time.time()
    
    # å¤§çŸ©é™£ä¹˜æ³•æ¸¬è©¦
    matrix_size = 2000
    A = np.random.randn(matrix_size, matrix_size).astype(np.float32)
    B = np.random.randn(matrix_size, matrix_size).astype(np.float32)
    C = np.dot(A, B)
    
    cpu_time = time.time() - start_time
    print(f"   çŸ©é™£ä¹˜æ³• ({matrix_size}x{matrix_size}): {cpu_time:.2f}ç§’")
    
    # GPUåŸºæº–æ¸¬è©¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if torch.cuda.is_available():
        print("GPUæ€§èƒ½æ¸¬è©¦...")
        device = torch.device('cuda:0')
        
        start_time = time.time()
        A_gpu = torch.randn(matrix_size, matrix_size, device=device)
        B_gpu = torch.randn(matrix_size, matrix_size, device=device)
        C_gpu = torch.mm(A_gpu, B_gpu)
        torch.cuda.synchronize()  # ç¢ºä¿è¨ˆç®—å®Œæˆ
        
        gpu_time = time.time() - start_time
        print(f"   GPUçŸ©é™£ä¹˜æ³• ({matrix_size}x{matrix_size}): {gpu_time:.2f}ç§’")
        
        speedup = cpu_time / gpu_time if gpu_time > 0 else 0
        print(f"   åŠ é€Ÿæ¯”: {speedup:.1f}x")
    
    return {
        'cpu_time': cpu_time,
        'gpu_time': gpu_time if torch.cuda.is_available() else None
    }


def main():
    """ä¸»å‡½æ•¸ï¼šé…ç½®ä¸¦æ¸¬è©¦é«˜æ€§èƒ½ç’°å¢ƒ"""
    
    print("ğŸ–¥ï¸ High-Performance Bayesian Analysis Setup")
    print("   16-Core CPU + 2x RTX2050 Optimization")
    print("=" * 70)
    
    # é…ç½®ç’°å¢ƒ
    config_changes = configure_high_performance_environment()
    
    # ç²å–MCMCé…ç½®
    mcmc_config = get_optimized_mcmc_config()
    
    # æ€§èƒ½åŸºæº–æ¸¬è©¦
    benchmark_results = benchmark_performance()
    
    print(f"\nğŸ¯ Optimization Summary:")
    print(f"   â€¢ CPU cores utilized: 16")
    print(f"   â€¢ GPU support: {'âœ…' if mcmc_config['use_gpu'] else 'âŒ'}")
    print(f"   â€¢ MCMC chains: {mcmc_config['chains']}")
    print(f"   â€¢ Expected speedup: ~5-8x for large models")
    
    # å‰µå»ºå„ªåŒ–è…³æœ¬
    optimized_script = create_optimized_analysis_script()
    
    # ä¿å­˜å„ªåŒ–è…³æœ¬
    output_path = "05_robust_bayesian_parm_insurance_optimized.py"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(optimized_script)
    
    print(f"\nğŸ’¾ Optimized script saved: {output_path}")
    print(f"\nâœ¨ Ready to run high-performance Bayesian analysis!")
    print(f"   Execute: python {output_path}")
    
    return {
        'config_changes': config_changes,
        'mcmc_config': mcmc_config,
        'benchmark_results': benchmark_results
    }


if __name__ == "__main__":
    results = main()
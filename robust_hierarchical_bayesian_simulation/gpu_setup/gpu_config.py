#!/usr/bin/env python3
"""
GPU Configuration and Environment Setup
GPUé…ç½®èˆ‡ç’°å¢ƒè¨­å®š

ç‚º32æ ¸CPU + 2GPUç’°å¢ƒå„ªåŒ–çš„è¨ˆç®—é…ç½®
æ”¯æ´PyMCã€PyTorchã€JAXç­‰å¤šæ¡†æ¶GPUåŠ é€Ÿ

Author: Research Team
Date: 2025-01-18
"""

import os
import sys
import psutil
import platform
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
import warnings

class GPUFramework(Enum):
    """GPUè¨ˆç®—æ¡†æ¶"""
    PYMC = "pymc"
    PYTORCH = "pytorch" 
    JAX = "jax"
    TENSORFLOW = "tensorflow"

class ComputeMode(Enum):
    """è¨ˆç®—æ¨¡å¼"""
    CPU_ONLY = "cpu_only"
    SINGLE_GPU = "single_gpu"
    DUAL_GPU = "dual_gpu"
    HYBRID = "hybrid"  # CPU + GPUæ··åˆ

@dataclass
class SystemSpecs:
    """ç³»çµ±è¦æ ¼"""
    cpu_cores: int
    physical_cores: int
    memory_gb: float
    gpu_count: int
    gpu_memory_gb: List[float]
    platform: str
    
    def __post_init__(self):
        self.cpu_threads = self.cpu_cores
        self.available_memory = self.memory_gb * 0.8  # ä¿ç•™20%è¨˜æ†¶é«”

@dataclass
class GPUConfig:
    """GPUé…ç½®"""
    framework: GPUFramework
    device_ids: List[int]
    memory_fraction: float = 0.9
    enable_mixed_precision: bool = True
    batch_size_multiplier: float = 1.0
    
class GPUEnvironmentManager:
    """GPUç’°å¢ƒç®¡ç†å™¨"""
    
    def __init__(self):
        self.system_specs = self._detect_system_specs()
        self.available_frameworks = self._check_gpu_frameworks()
        self.configs = {}
        
        print("ğŸ”§ GPUç’°å¢ƒç®¡ç†å™¨åˆå§‹åŒ–")
        print(f"   ç³»çµ±: {self.system_specs.platform}")
        print(f"   CPUæ ¸å¿ƒ: {self.system_specs.cpu_cores} (ç‰©ç†: {self.system_specs.physical_cores})")
        print(f"   è¨˜æ†¶é«”: {self.system_specs.memory_gb:.1f} GB")
        print(f"   GPUæ•¸é‡: {self.system_specs.gpu_count}")
    
    def _detect_system_specs(self) -> SystemSpecs:
        """æª¢æ¸¬ç³»çµ±è¦æ ¼"""
        cpu_cores = psutil.cpu_count(logical=True)
        physical_cores = psutil.cpu_count(logical=False)
        memory_gb = psutil.virtual_memory().total / (1024**3)
        platform_name = platform.system()
        
        # æª¢æ¸¬GPU
        gpu_count = 0
        gpu_memory = []
        
        # å˜—è©¦æª¢æ¸¬NVIDIA GPU
        try:
            import pynvml
            pynvml.nvmlInit()
            gpu_count = pynvml.nvmlDeviceGetCount()
            
            for i in range(gpu_count):
                handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory.append(info.total / (1024**3))  # GB
                
        except (ImportError, Exception):
            # å˜—è©¦æª¢æ¸¬Apple Silicon GPU
            if platform_name == "Darwin":
                # Apple Mç³»åˆ—èŠ¯ç‰‡é€šå¸¸æœ‰çµ±ä¸€è¨˜æ†¶é«”
                gpu_count = 1  # å‡è¨­æœ‰ä¸€å€‹é›†æˆGPU
                gpu_memory = [memory_gb * 0.3]  # ä¼°è¨ˆ30%è¨˜æ†¶é«”å¯ç”¨æ–¼GPU
        
        return SystemSpecs(
            cpu_cores=cpu_cores,
            physical_cores=physical_cores, 
            memory_gb=memory_gb,
            gpu_count=gpu_count,
            gpu_memory_gb=gpu_memory,
            platform=platform_name
        )
    
    def _check_gpu_frameworks(self) -> Dict[str, bool]:
        """æª¢æŸ¥å¯ç”¨çš„GPUæ¡†æ¶"""
        frameworks = {}
        
        # PyMC + PyTensor
        try:
            import pymc as pm
            import pytensor
            frameworks["pymc"] = True
            print("   âœ… PyMC GPUæ”¯æ´å¯ç”¨")
        except ImportError:
            frameworks["pymc"] = False
            print("   âŒ PyMC GPUæ”¯æ´ä¸å¯ç”¨")
        
        # PyTorch
        try:
            import torch
            frameworks["pytorch"] = torch.cuda.is_available() or torch.backends.mps.is_available()
            device = "CUDA" if torch.cuda.is_available() else "MPS" if torch.backends.mps.is_available() else "CPU"
            print(f"   {'âœ…' if frameworks['pytorch'] else 'âŒ'} PyTorch GPUæ”¯æ´: {device}")
        except ImportError:
            frameworks["pytorch"] = False
            print("   âŒ PyTorchä¸å¯ç”¨")
        
        # JAX
        try:
            import jax
            frameworks["jax"] = len(jax.devices('gpu')) > 0 or len(jax.devices('tpu')) > 0
            print(f"   {'âœ…' if frameworks['jax'] else 'âŒ'} JAX GPUæ”¯æ´")
        except ImportError:
            frameworks["jax"] = False
            print("   âŒ JAXä¸å¯ç”¨")
        
        return frameworks
    
    def create_mcmc_config(self, 
                          n_chains: int = 4,
                          samples_per_chain: int = 1000,
                          warmup_per_chain: int = 500) -> Dict[str, Any]:
        """å‰µå»ºMCMCé…ç½®"""
        
        # æ ¹æ“šç¡¬é«”èª¿æ•´éˆæ•¸
        if self.system_specs.gpu_count >= 2:
            # é›™GPUç­–ç•¥
            chains_per_gpu = n_chains // 2
            config = {
                "gpu1_config": {
                    "device": 0,
                    "n_chains": chains_per_gpu,
                    "samples": samples_per_chain,
                    "warmup": warmup_per_chain,
                    "cores": min(8, self.system_specs.physical_cores // 4)
                },
                "gpu2_config": {
                    "device": 1, 
                    "n_chains": n_chains - chains_per_gpu,
                    "samples": samples_per_chain,
                    "warmup": warmup_per_chain,
                    "cores": min(8, self.system_specs.physical_cores // 4)
                },
                "cpu_backup": {
                    "cores": self.system_specs.physical_cores - 16,
                    "use_for": "diagnostics_and_postprocessing"
                }
            }
        else:
            # å–®GPUæˆ–ç´”CPUç­–ç•¥
            config = {
                "primary_config": {
                    "device": 0 if self.system_specs.gpu_count > 0 else "cpu",
                    "n_chains": n_chains,
                    "samples": samples_per_chain,
                    "warmup": warmup_per_chain,
                    "cores": min(16, self.system_specs.physical_cores)
                },
                "cpu_support": {
                    "cores": max(4, self.system_specs.physical_cores - 16),
                    "use_for": "model_selection_and_analysis"
                }
            }
        
        return config
    
    def setup_gpu_environment(self, enable_gpu: bool = True) -> 'GPUConfig':
        """è¨­å®šGPUç’°å¢ƒ"""
        
        if not enable_gpu or self.system_specs.gpu_count == 0:
            return self._setup_cpu_only_config()
        
        # è¨­å®šç’°å¢ƒè®Šé‡
        self._set_gpu_environment_variables()
        
        # å‰µå»ºGPUé…ç½®
        if self.available_frameworks.get("pymc", False):
            return self._setup_pymc_gpu_config()
        elif self.available_frameworks.get("pytorch", False):
            return self._setup_pytorch_config()
        else:
            print("âš ï¸ æ²’æœ‰å¯ç”¨çš„GPUæ¡†æ¶ï¼Œå›é€€åˆ°CPUæ¨¡å¼")
            return self._setup_cpu_only_config()
    
    def _set_gpu_environment_variables(self):
        """è¨­å®šGPUç’°å¢ƒè®Šé‡"""
        
        # PyTensor GPUé…ç½®
        if self.system_specs.platform == "Darwin":  # macOS
            os.environ['PYTENSOR_FLAGS'] = 'device=cpu,floatX=float64,mode=FAST_COMPILE'
        else:
            os.environ['PYTENSOR_FLAGS'] = 'device=cuda,floatX=float32,mode=FAST_RUN'
        
        # å¤šç·šç¨‹é…ç½®
        os.environ['OMP_NUM_THREADS'] = str(min(8, self.system_specs.physical_cores // 4))
        os.environ['MKL_NUM_THREADS'] = str(min(8, self.system_specs.physical_cores // 4))
        
        # CUDAé…ç½®ï¼ˆå¦‚æœé©ç”¨ï¼‰
        if self.system_specs.platform == "Linux":
            os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, range(self.system_specs.gpu_count)))
    
    def _setup_pymc_gpu_config(self) -> 'GPUConfig':
        """è¨­å®šPyMC GPUé…ç½®"""
        print("ğŸš€ è¨­å®šPyMC GPUé…ç½®...")
        
        device_ids = list(range(min(2, self.system_specs.gpu_count)))
        
        config = GPUConfig(
            framework=GPUFramework.PYMC,
            device_ids=device_ids,
            memory_fraction=0.8,
            enable_mixed_precision=True
        )
        
        self.configs["pymc"] = config
        return config
    
    def _setup_pytorch_config(self) -> 'GPUConfig':
        """è¨­å®šPyTorché…ç½®"""
        print("ğŸš€ è¨­å®šPyTorché…ç½®...")
        
        device_ids = list(range(min(2, self.system_specs.gpu_count)))
        
        config = GPUConfig(
            framework=GPUFramework.PYTORCH,
            device_ids=device_ids,
            memory_fraction=0.9,
            enable_mixed_precision=True,
            batch_size_multiplier=2.0
        )
        
        self.configs["pytorch"] = config
        return config
    
    def _setup_cpu_only_config(self) -> 'GPUConfig':
        """è¨­å®šç´”CPUé…ç½®"""
        print("ğŸ’» è¨­å®šç´”CPUé…ç½®...")
        
        # å„ªåŒ–CPUç’°å¢ƒ
        os.environ['OMP_NUM_THREADS'] = str(self.system_specs.physical_cores)
        os.environ['MKL_NUM_THREADS'] = str(self.system_specs.physical_cores)
        os.environ['NUMEXPR_NUM_THREADS'] = str(self.system_specs.physical_cores)
        
        config = GPUConfig(
            framework=GPUFramework.PYMC,  # é è¨­ä½¿ç”¨PyMC
            device_ids=[],
            memory_fraction=0.8
        )
        
        return config
    
    def create_parallel_execution_plan(self) -> Dict[str, Any]:
        """å‰µå»ºä¸¦è¡ŒåŸ·è¡Œè¨ˆåŠƒ"""
        
        total_cores = self.system_specs.physical_cores
        
        if total_cores >= 32:
            # 32æ ¸å¿ƒæœ€ä½³é…ç½®
            plan = {
                "model_selection_pool": {
                    "cores": 16,
                    "max_workers": 16,
                    "tasks": "model_fitting_and_hyperparameter_optimization"
                },
                "mcmc_pool": {
                    "cores": 8,
                    "max_workers": 4,  # MCMCéœ€è¦æ›´å¤šè¨˜æ†¶é«”
                    "tasks": "mcmc_sampling_and_validation",
                    "gpu_support": True
                },
                "analysis_pool": {
                    "cores": 8,
                    "max_workers": 8,
                    "tasks": "posterior_analysis_and_diagnostics"
                }
            }
        else:
            # å°‘æ–¼32æ ¸å¿ƒçš„é…ç½®
            cores_per_pool = max(2, total_cores // 3)
            plan = {
                "model_selection_pool": {
                    "cores": cores_per_pool,
                    "max_workers": cores_per_pool,
                    "tasks": "model_fitting"
                },
                "mcmc_pool": {
                    "cores": cores_per_pool,
                    "max_workers": max(2, cores_per_pool // 2),
                    "tasks": "mcmc_sampling",
                    "gpu_support": self.system_specs.gpu_count > 0
                },
                "analysis_pool": {
                    "cores": total_cores - 2 * cores_per_pool,
                    "max_workers": max(2, total_cores - 2 * cores_per_pool),
                    "tasks": "analysis"
                }
            }
        
        return plan
    
    def print_performance_summary(self):
        """æ‰“å°æ•ˆèƒ½æ‘˜è¦"""
        print("\nğŸ“Š ç³»çµ±æ•ˆèƒ½é…ç½®æ‘˜è¦")
        print("=" * 50)
        print(f"CPUæ ¸å¿ƒ: {self.system_specs.cpu_cores} (ç‰©ç†: {self.system_specs.physical_cores})")
        print(f"è¨˜æ†¶é«”: {self.system_specs.memory_gb:.1f} GB (å¯ç”¨: {self.system_specs.available_memory:.1f} GB)")
        print(f"GPU: {self.system_specs.gpu_count} å€‹")
        
        if self.system_specs.gpu_memory_gb:
            for i, mem in enumerate(self.system_specs.gpu_memory_gb):
                print(f"   GPU {i}: {mem:.1f} GB")
        
        print(f"\nå¯ç”¨æ¡†æ¶:")
        for framework, available in self.available_frameworks.items():
            print(f"   {framework}: {'âœ…' if available else 'âŒ'}")
        
        # é ä¼°åŠ é€Ÿæ¯”
        baseline_time = 14.0  # å°æ™‚
        if self.system_specs.gpu_count >= 2 and self.system_specs.physical_cores >= 16:
            speedup = 9.0
        elif self.system_specs.gpu_count >= 1 and self.system_specs.physical_cores >= 8:
            speedup = 6.0
        elif self.system_specs.physical_cores >= 16:
            speedup = 4.0
        else:
            speedup = 2.0
        
        estimated_time = baseline_time / speedup
        print(f"\né ä¼°åŠ é€Ÿ:")
        print(f"   åŸºç·šæ™‚é–“: {baseline_time:.1f} å°æ™‚")
        print(f"   é ä¼°æ™‚é–“: {estimated_time:.1f} å°æ™‚")
        print(f"   åŠ é€Ÿæ¯”: {speedup:.1f}x")

# ä¾¿åˆ©å‡½æ•¸
def setup_gpu_environment(enable_gpu: bool = True) -> Tuple[GPUConfig, Dict[str, Any]]:
    """å¿«é€Ÿè¨­å®šGPUç’°å¢ƒ"""
    manager = GPUEnvironmentManager()
    gpu_config = manager.setup_gpu_environment(enable_gpu)
    execution_plan = manager.create_parallel_execution_plan()
    
    manager.print_performance_summary()
    
    return gpu_config, execution_plan

def get_optimal_mcmc_config(n_models: int = 5, 
                          samples_per_model: int = 1000) -> Dict[str, Any]:
    """ç²å–æœ€ä½³MCMCé…ç½®"""
    manager = GPUEnvironmentManager()
    
    # æ ¹æ“šæ¨¡å‹æ•¸é‡èª¿æ•´
    if manager.system_specs.gpu_count >= 2:
        chains_per_model = 4
        total_chains = n_models * chains_per_model
        
        config = {
            "n_models": n_models,
            "chains_per_model": chains_per_model,
            "samples_per_chain": samples_per_model,
            "parallel_chains": min(8, total_chains),
            "use_gpu": True,
            "gpu_allocation": {
                "gpu_0": {"models": list(range(0, n_models//2))},
                "gpu_1": {"models": list(range(n_models//2, n_models))}
            }
        }
    else:
        config = {
            "n_models": n_models,
            "chains_per_model": 2,
            "samples_per_chain": samples_per_model,
            "parallel_chains": min(4, manager.system_specs.physical_cores//2),
            "use_gpu": manager.system_specs.gpu_count > 0
        }
    
    return config

if __name__ == "__main__":
    # æ¸¬è©¦é…ç½®
    print("ğŸ§ª æ¸¬è©¦GPUé…ç½®...")
    gpu_config, execution_plan = setup_gpu_environment()
    
    print("\nğŸ“‹ åŸ·è¡Œè¨ˆåŠƒ:")
    for pool_name, pool_config in execution_plan.items():
        print(f"   {pool_name}: {pool_config['cores']} æ ¸å¿ƒ")
    
    print("\nâœ… GPUé…ç½®æ¸¬è©¦å®Œæˆ")
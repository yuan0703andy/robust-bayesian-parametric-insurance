#!/usr/bin/env python3
"""
Epsilon Estimation Module
Îµå€¼ä¼°è¨ˆæ¨¡çµ„

å°ˆé–€è™•ç†Îµ-contaminationæ¨¡å‹ä¸­Îµå€¼çš„ä¼°è¨ˆåŠŸèƒ½
æ•´åˆè‡ª epsilon_contamination.py å’Œ prior_contamination.py

æ ¸å¿ƒåŠŸèƒ½:
- å¤šç¨®Îµä¼°è¨ˆæ–¹æ³•å¯¦ç¾
- æ•¸æ“šé©…å‹•çš„æ±¡æŸ“ç¨‹åº¦åˆ†æ
- æ–¹æ³•æ¯”è¼ƒèˆ‡é©—è­‰
- å…ˆé©—æ±¡æŸ“åˆ†æ

Author: Research Team  
Date: 2025-08-19
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple, Union
from scipy import stats
from scipy.optimize import minimize
import warnings

# å¾æ ¸å¿ƒæ¨¡çµ„å°å…¥
try:
    from .contamination_core import (
        EpsilonContaminationSpec, ContaminationEstimateResult, EstimationMethod,
        ContaminationDistributionClass, ContaminationDistributionGenerator
    )
except ImportError:
    from contamination_core import (
        EpsilonContaminationSpec, ContaminationEstimateResult, EstimationMethod,
        ContaminationDistributionClass, ContaminationDistributionGenerator
    )

# ========================================
# Îµä¼°è¨ˆæ ¸å¿ƒé¡åˆ¥
# ========================================

class EpsilonEstimator:
    """
    Îµå€¼ä¼°è¨ˆå™¨
    å¯¦ç¾å¤šç¨®æ±¡æŸ“ç¨‹åº¦ä¼°è¨ˆæ–¹æ³•
    """
    
    def __init__(self, spec: EpsilonContaminationSpec):
        self.spec = spec
        self.contamination_generator = ContaminationDistributionGenerator()
        self.estimation_cache: Dict[str, ContaminationEstimateResult] = {}
        
        print(f"ğŸ“Š Îµä¼°è¨ˆå™¨åˆå§‹åŒ–")
        print(f"   æ±¡æŸ“é¡åˆ¥: {self.spec.contamination_class.value}")
        print(f"   Îµç¯„åœ: {self.spec.epsilon_range[0]:.3f} - {self.spec.epsilon_range[1]:.3f}")
    
    def estimate_contamination_level(self, data: np.ndarray, 
                                   wind_data: Optional[np.ndarray] = None) -> ContaminationEstimateResult:
        """
        å¾é¢±é¢¨æ•¸æ“šä¼°è¨ˆæ±¡æŸ“ç¨‹åº¦ Îµ
        ä½¿ç”¨å¤šç¨®æ–¹æ³•è­˜åˆ¥é¢±é¢¨äº‹ä»¶ vs æ­£å¸¸å¤©æ°£
        """
        print(f"ğŸ” ä¼°è¨ˆÎµ-contamination levelå¾ {len(data)} äº‹ä»¶...")
        
        non_zero_data = data[data > 0] if len(data[data > 0]) > 0 else data
        
        estimates = {}
        thresholds = {}
        
        # Method 1: 95th percentile threshold (moderate typhoons)
        if len(non_zero_data) > 20:
            threshold_95 = np.percentile(non_zero_data, 95)
            typhoon_events_95 = np.sum(data > threshold_95)
            estimates['epsilon_95th'] = typhoon_events_95 / len(data)
            thresholds['95th_percentile'] = threshold_95
        
        # Method 2: 99th percentile threshold (strong typhoons)  
        if len(non_zero_data) > 10:
            threshold_99 = np.percentile(non_zero_data, 99)
            typhoon_events_99 = np.sum(data > threshold_99)
            estimates['epsilon_99th'] = typhoon_events_99 / len(data)
            thresholds['99th_percentile'] = threshold_99
        
        # Method 3: Statistical outlier detection (extreme events)
        if len(non_zero_data) > 10:
            Q1 = np.percentile(non_zero_data, 25)
            Q3 = np.percentile(non_zero_data, 75)
            IQR = Q3 - Q1
            outlier_threshold = Q3 + 1.5 * IQR
            outlier_events = np.sum(data > outlier_threshold)
            estimates['epsilon_outlier'] = outlier_events / len(data)
            thresholds['outlier_threshold'] = outlier_threshold
        
        # Method 4: Physical wind threshold (if available)
        if wind_data is not None:
            # Tropical storm threshold: 39 mph = 62.8 km/h
            typhoon_wind_threshold = 62.8  # km/h
            typhoon_events_wind = np.sum(wind_data > typhoon_wind_threshold)
            estimates['epsilon_wind'] = typhoon_events_wind / len(wind_data)
            thresholds['wind_threshold'] = typhoon_wind_threshold
        
        # Method 5: Extreme value theory approach
        if len(non_zero_data) > 30:
            # Fit GEV distribution and identify extreme quantiles
            try:
                # Use block maxima approach
                block_size = max(10, len(non_zero_data) // 10)
                blocks = [non_zero_data[i:i+block_size] for i in range(0, len(non_zero_data), block_size)]
                block_maxima = [np.max(block) for block in blocks if len(block) > 5]
                
                if len(block_maxima) > 5:
                    # Fit GEV to block maxima
                    gev_params = stats.genextreme.fit(block_maxima)
                    # Extreme threshold at 90th percentile of GEV
                    extreme_threshold = stats.genextreme.ppf(0.9, *gev_params)
                    extreme_events = np.sum(data > extreme_threshold)
                    estimates['epsilon_evt'] = extreme_events / len(data)
                    thresholds['evt_threshold'] = extreme_threshold
                    
            except Exception:
                # EVT method failed, skip
                pass
        
        # Compute consensus estimate
        if estimates:
            epsilon_values = list(estimates.values())
            epsilon_consensus = np.median(epsilon_values)
            epsilon_uncertainty = np.std(epsilon_values)
        else:
            # Fallback: assume 5% contamination (typical for rare events)
            epsilon_consensus = 0.05
            epsilon_uncertainty = 0.02
            estimates['epsilon_fallback'] = epsilon_consensus
        
        # Validate estimates are in reasonable range
        epsilon_consensus = np.clip(epsilon_consensus, self.spec.epsilon_range[0], self.spec.epsilon_range[1])
        
        # Validation metrics
        validation_metrics = {
            'n_methods': len(estimates),
            'consensus_confidence': 1.0 - (epsilon_uncertainty / epsilon_consensus) if epsilon_consensus > 0 else 0.0,
            'range_validity': self.spec.epsilon_range[0] <= epsilon_consensus <= self.spec.epsilon_range[1],
            'typhoon_interpretation_valid': 0.01 <= epsilon_consensus <= 0.25  # Reasonable for typhoon frequency
        }
        
        print(f"   ğŸ“Š æ±¡æŸ“ä¼°è¨ˆ:")
        for method, value in estimates.items():
            print(f"      â€¢ {method}: Îµ = {value:.3f} ({value:.1%})")
        print(f"   ğŸ¯ å…±è­˜: Îµ = {epsilon_consensus:.3f} Â± {epsilon_uncertainty:.3f}")
        
        return ContaminationEstimateResult(
            epsilon_estimates=estimates,
            epsilon_consensus=epsilon_consensus,
            epsilon_uncertainty=epsilon_uncertainty,
            thresholds=thresholds,
            test_statistics={},  # Will be filled by method-specific estimators
            p_values={},        # Will be filled by method-specific estimators
            method_weights={}   # Can be added later
        )
    
    def estimate_from_statistical_tests(self, 
                                       data: np.ndarray,
                                       methods: List[EstimationMethod] = None) -> ContaminationEstimateResult:
        """ä½¿ç”¨çµ±è¨ˆæª¢é©—æ–¹æ³•ä¼°è¨ˆÎµå€¼"""
        if methods is None:
            methods = [
                EstimationMethod.EMPIRICAL_FREQUENCY,
                EstimationMethod.KOLMOGOROV_SMIRNOV,
                EstimationMethod.ANDERSON_DARLING
            ]
        
        print(f"ğŸ“ˆ çµ±è¨ˆæª¢é©—ä¼°è¨ˆÎµå€¼ (n={len(data)})...")
        
        epsilon_estimates = {}
        test_statistics = {}
        p_values = {}
        
        # Generate base distribution
        base_dist = self._create_base_distribution(data)
        
        for method in methods:
            print(f"   ä½¿ç”¨æ–¹æ³•: {method.value}")
            
            if method == EstimationMethod.EMPIRICAL_FREQUENCY:
                epsilon, test_stat, p_val = self._estimate_empirical_frequency(data, base_dist)
            elif method == EstimationMethod.KOLMOGOROV_SMIRNOV:
                epsilon, test_stat, p_val = self._estimate_ks_test(data, base_dist)
            elif method == EstimationMethod.ANDERSON_DARLING:
                epsilon, test_stat, p_val = self._estimate_ad_test(data, base_dist)
            elif method == EstimationMethod.BAYESIAN_MODEL_SELECTION:
                epsilon, test_stat, p_val = self._estimate_bayesian_selection(data, base_dist)
            else:
                print(f"   âš ï¸ æ–¹æ³• {method.value} å°šæœªå¯¦ç¾")
                continue
            
            epsilon_estimates[method.value] = epsilon
            test_statistics[method.value] = test_stat
            p_values[method.value] = p_val
            
            print(f"      Îµä¼°è¨ˆ: {epsilon:.4f}")
        
        # Compute consensus estimate and uncertainty
        consensus_epsilon = np.mean(list(epsilon_estimates.values()))
        uncertainty = np.std(list(epsilon_estimates.values()))
        
        # Set thresholds
        thresholds = {
            "lower_bound": max(0.0, consensus_epsilon - 2 * uncertainty),
            "upper_bound": min(1.0, consensus_epsilon + 2 * uncertainty),
            "empirical_threshold": self.spec.empirical_epsilon
        }
        
        result = ContaminationEstimateResult(
            epsilon_estimates=epsilon_estimates,
            epsilon_consensus=consensus_epsilon,
            epsilon_uncertainty=uncertainty,
            thresholds=thresholds,
            test_statistics=test_statistics,
            p_values=p_values,
            method_weights={}
        )
        
        # Cache result
        cache_key = f"statistical_{len(data)}_{hash(data.tobytes())}"
        self.estimation_cache[cache_key] = result
        
        print(f"âœ… Îµçµ±è¨ˆä¼°è¨ˆå®Œæˆ: {consensus_epsilon:.4f} Â± {uncertainty:.4f}")
        return result
    
    def _create_base_distribution(self, data: np.ndarray):
        """å‰µå»ºåŸºæº–åˆ†å¸ƒ"""
        if self.spec.nominal_prior_family == "normal":
            return stats.norm(loc=np.mean(data), scale=np.std(data))
        elif self.spec.nominal_prior_family == "lognormal":
            # ç¢ºä¿æ•¸æ“šç‚ºæ­£
            positive_data = data[data > 0]
            if len(positive_data) == 0:
                return stats.norm(loc=0, scale=1)
            log_data = np.log(positive_data)
            return stats.lognorm(s=np.std(log_data), scale=np.exp(np.mean(log_data)))
        elif self.spec.nominal_prior_family == "gamma":
            # ä½¿ç”¨moment matching
            mean_val = np.mean(data)
            var_val = np.var(data)
            if var_val <= 0 or mean_val <= 0:
                return stats.gamma(a=1, scale=1)
            beta = mean_val / var_val
            alpha = mean_val * beta
            return stats.gamma(a=alpha, scale=1/beta)
        else:
            # é è¨­ä½¿ç”¨æ­£æ…‹åˆ†å¸ƒ
            return stats.norm(loc=np.mean(data), scale=np.std(data))
    
    def _estimate_empirical_frequency(self, data: np.ndarray, base_dist) -> Tuple[float, float, float]:
        """ç¶“é©—é »ç‡æ³•ä¼°è¨ˆÎµ"""
        # è¨ˆç®—æ¥µå€¼é »ç‡
        threshold = np.percentile(data, 95)  # 95%åˆ†ä½æ•¸ä½œç‚ºæ¥µå€¼é–¾å€¼
        extreme_count = np.sum(data > threshold)
        
        # ä¼°è¨ˆÎµç‚ºæ¥µå€¼é »ç‡
        epsilon = extreme_count / len(data)
        
        # ç°¡å–®çš„çµ±è¨ˆé‡
        test_statistic = extreme_count
        p_value = 1 - stats.binom.cdf(extreme_count - 1, len(data), 0.05)
        
        return epsilon, test_statistic, p_value
    
    def _estimate_ks_test(self, data: np.ndarray, base_dist) -> Tuple[float, float, float]:
        """Kolmogorov-Smirnovæª¢é©—æ³•ä¼°è¨ˆÎµ"""
        # å°åŸºæº–åˆ†å¸ƒé€²è¡ŒK-Sæª¢é©—
        ks_statistic, ks_p_value = stats.kstest(data, base_dist.cdf)
        
        # å°‡K-Sçµ±è¨ˆé‡è½‰æ›ç‚ºÎµä¼°è¨ˆ
        # é€™æ˜¯å•Ÿç™¼å¼è½‰æ›ï¼Œå¯¦éš›æ‡‰ç”¨ä¸­å¯èƒ½éœ€è¦æ›´ç²¾ç¢ºçš„æ˜ å°„
        epsilon = min(0.5, ks_statistic * 2)  # å°‡Dçµ±è¨ˆé‡æ˜ å°„åˆ°[0, 0.5]
        
        return epsilon, ks_statistic, ks_p_value
    
    def _estimate_ad_test(self, data: np.ndarray, base_dist) -> Tuple[float, float, float]:
        """Anderson-Darlingæª¢é©—æ³•ä¼°è¨ˆÎµ"""
        # å°åŸºæº–åˆ†å¸ƒé€²è¡ŒA-Dæª¢é©—
        try:
            ad_statistic, critical_values, significance_levels = stats.anderson(data)
            
            # æ ¹æ“šA-Dçµ±è¨ˆé‡ä¼°è¨ˆÎµ
            # ä½¿ç”¨èˆ‡critical_valuesçš„æ¯”è¼ƒ
            if len(critical_values) > 0:
                # æ¨™æº–åŒ–A-Dçµ±è¨ˆé‡
                normalized_ad = ad_statistic / critical_values[2]  # ä½¿ç”¨5%æ°´å¹³çš„è‡¨ç•Œå€¼
                epsilon = min(0.5, normalized_ad * 0.1)  # å•Ÿç™¼å¼æ˜ å°„
            else:
                epsilon = 0.05
            
            # è¨ˆç®—è¿‘ä¼¼på€¼
            p_value = 1.0 / (1.0 + ad_statistic)  # ç°¡åŒ–çš„på€¼ä¼°è¨ˆ
            
            return epsilon, ad_statistic, p_value
            
        except Exception as e:
            print(f"   âš ï¸ A-Dæª¢é©—å¤±æ•—: {e}")
            return 0.05, 0.0, 1.0
    
    def _estimate_bayesian_selection(self, data: np.ndarray, base_dist) -> Tuple[float, float, float]:
        """è²æ°æ¨¡å‹é¸æ“‡æ³•ä¼°è¨ˆÎµ"""
        # é€™æ˜¯ç°¡åŒ–å¯¦ç¾ï¼Œå¯¦éš›æ‡‰ç”¨ä¸­éœ€è¦å®Œæ•´çš„è²æ°æ¡†æ¶
        
        # å˜—è©¦ä¸åŒçš„Îµå€¼ï¼Œè¨ˆç®—é‚Šéš›ä¼¼ç„¶
        epsilon_candidates = np.linspace(0.01, 0.3, 20)
        log_marginal_likelihoods = []
        
        for eps in epsilon_candidates:
            # å‰µå»ºæ··åˆåˆ†å¸ƒ
            contamination_dist = self._create_contamination_distribution(data)
            
            # è¨ˆç®—æ··åˆåˆ†å¸ƒçš„å°æ•¸ä¼¼ç„¶
            n_base = int(len(data) * (1 - eps))
            n_contamination = len(data) - n_base
            
            if n_base > 0 and n_contamination > 0:
                # ç°¡åŒ–çš„ä¼¼ç„¶è¨ˆç®—
                base_ll = np.sum(base_dist.logpdf(data[:n_base]))
                contamination_ll = np.sum(contamination_dist.logpdf(data[n_base:]))
                total_ll = base_ll + contamination_ll
            else:
                total_ll = np.sum(base_dist.logpdf(data))
            
            log_marginal_likelihoods.append(total_ll)
        
        # æ‰¾åˆ°æœ€å¤§ä¼¼ç„¶å°æ‡‰çš„Îµ
        best_idx = np.argmax(log_marginal_likelihoods)
        best_epsilon = epsilon_candidates[best_idx]
        
        # è¨ˆç®—Bayes factorä½œç‚ºæª¢é©—çµ±è¨ˆé‡
        max_ll = log_marginal_likelihoods[best_idx]
        null_ll = np.sum(base_dist.logpdf(data))  # Îµ=0çš„ä¼¼ç„¶
        bayes_factor = 2 * (max_ll - null_ll)
        
        # è¿‘ä¼¼på€¼
        p_value = stats.chi2.sf(bayes_factor, df=1)
        
        return best_epsilon, bayes_factor, p_value
    
    def _create_contamination_distribution(self, data: np.ndarray):
        """å‰µå»ºæ±¡æŸ“åˆ†å¸ƒ"""
        if self.spec.contamination_class == ContaminationDistributionClass.TYPHOON_SPECIFIC:
            return self.contamination_generator.generate_typhoon_specific(
                location=np.mean(data), 
                scale=np.std(data) * 2,  # æ›´å¤§çš„è®Šç•°æ€§
                shape=0.2
            )
        elif self.spec.contamination_class == ContaminationDistributionClass.HEAVY_TAILED:
            return self.contamination_generator.generate_heavy_tailed(
                df=3, 
                location=np.mean(data), 
                scale=np.std(data) * 1.5
            )
        else:
            # é è¨­ä½¿ç”¨æ­£æ…‹åˆ†å¸ƒä½†åƒæ•¸ä¸åŒ
            return stats.norm(loc=np.mean(data), scale=np.std(data) * 2)

# ========================================
# å…ˆé©—æ±¡æŸ“åˆ†æå™¨ (å¾prior_contamination.pyæ•´åˆ)
# ========================================

class PriorContaminationAnalyzer:
    """å…ˆé©—æ±¡æŸ“åˆ†æå™¨"""
    
    def __init__(self, spec: EpsilonContaminationSpec):
        self.spec = spec
        self.contamination_generator = ContaminationDistributionGenerator()
        
        print(f"ğŸ”¬ å…ˆé©—æ±¡æŸ“åˆ†æå™¨åˆå§‹åŒ–")
        print(f"   æ±¡æŸ“é¡åˆ¥: {self.spec.contamination_class.value}")
        print(f"   åŸºæº–å…ˆé©—: {self.spec.nominal_prior_family}")
    
    def analyze_prior_robustness(self, 
                                epsilon_range: np.ndarray = None,
                                parameter_of_interest: str = "mean") -> Dict[str, Any]:
        """åˆ†æå…ˆé©—çš„ç©©å¥æ€§"""
        if epsilon_range is None:
            epsilon_range = np.linspace(0.0, 0.3, 31)
        
        print(f"ğŸ” åˆ†æå…ˆé©—ç©©å¥æ€§ (åƒæ•¸: {parameter_of_interest})...")
        
        # å‰µå»ºåŸºæº–å’Œæ±¡æŸ“åˆ†å¸ƒ
        base_samples = self._generate_base_prior_samples(1000)
        contamination_samples = self._generate_contamination_prior_samples(1000)
        
        results = {
            "epsilon_range": epsilon_range,
            "parameter_values": [],
            "parameter_bounds": [],
            "robustness_metrics": {}
        }
        
        for eps in epsilon_range:
            # å‰µå»ºæ··åˆå…ˆé©—æ¨£æœ¬
            n_contamination = int(1000 * eps)
            n_base = 1000 - n_contamination
            
            mixed_samples = np.concatenate([
                base_samples[:n_base],
                contamination_samples[:n_contamination]
            ])
            
            # è¨ˆç®—é—œæ³¨åƒæ•¸
            if parameter_of_interest == "mean":
                param_value = np.mean(mixed_samples)
            elif parameter_of_interest == "variance":
                param_value = np.var(mixed_samples)
            elif parameter_of_interest == "quantile_95":
                param_value = np.percentile(mixed_samples, 95)
            else:
                param_value = np.mean(mixed_samples)  # é è¨­
            
            results["parameter_values"].append(param_value)
            
            # è¨ˆç®—å¯ä¿¡å€é–“
            ci_lower = np.percentile(mixed_samples, 2.5)
            ci_upper = np.percentile(mixed_samples, 97.5)
            results["parameter_bounds"].append((ci_lower, ci_upper))
        
        # è¨ˆç®—ç©©å¥æ€§æŒ‡æ¨™
        param_values = np.array(results["parameter_values"])
        results["robustness_metrics"] = {
            "max_deviation": np.max(param_values) - np.min(param_values),
            "relative_deviation": (np.max(param_values) - np.min(param_values)) / np.abs(param_values[0]),
            "sensitivity_at_zero": np.gradient(param_values, epsilon_range)[0] if len(param_values) > 1 else 0,
            "max_sensitivity": np.max(np.abs(np.gradient(param_values, epsilon_range)))
        }
        
        print(f"âœ… ç©©å¥æ€§åˆ†æå®Œæˆ")
        print(f"   æœ€å¤§åå·®: {results['robustness_metrics']['max_deviation']:.4f}")
        print(f"   ç›¸å°åå·®: {results['robustness_metrics']['relative_deviation']:.2%}")
        
        return results
    
    def _generate_base_prior_samples(self, n_samples: int) -> np.ndarray:
        """ç”ŸæˆåŸºæº–å…ˆé©—æ¨£æœ¬"""
        if self.spec.nominal_prior_family == "normal":
            return np.random.normal(0, 1, n_samples)
        elif self.spec.nominal_prior_family == "lognormal":
            return np.random.lognormal(0, 1, n_samples)
        elif self.spec.nominal_prior_family == "gamma":
            return np.random.gamma(2, 1, n_samples)
        else:
            return np.random.normal(0, 1, n_samples)
    
    def _generate_contamination_prior_samples(self, n_samples: int) -> np.ndarray:
        """ç”Ÿæˆæ±¡æŸ“å…ˆé©—æ¨£æœ¬"""
        if self.spec.contamination_class == ContaminationDistributionClass.TYPHOON_SPECIFIC:
            # ä½¿ç”¨å»£ç¾©æ¥µå€¼åˆ†å¸ƒ
            return stats.genextreme.rvs(c=0.2, loc=0, scale=2, size=n_samples)
        elif self.spec.contamination_class == ContaminationDistributionClass.HEAVY_TAILED:
            # ä½¿ç”¨Student-tåˆ†å¸ƒ
            return stats.t.rvs(df=3, loc=0, scale=2, size=n_samples)
        else:
            # é è¨­ä½¿ç”¨ä¸åŒåƒæ•¸çš„æ­£æ…‹åˆ†å¸ƒ
            return np.random.normal(0, 3, n_samples)

# ========================================
# åˆ†æå·¥å…·å‡½æ•¸
# ========================================

def quick_contamination_analysis(data: np.ndarray, 
                               wind_data: Optional[np.ndarray] = None) -> ContaminationEstimateResult:
    """é¢±é¢¨æ•¸æ“šçš„å¿«é€Ÿæ±¡æŸ“ç¨‹åº¦åˆ†æ"""
    
    try:
        from .contamination_core import create_typhoon_contamination_spec
    except ImportError:
        from contamination_core import create_typhoon_contamination_spec
        
    spec = create_typhoon_contamination_spec()
    estimator = EpsilonEstimator(spec)
    return estimator.estimate_contamination_level(data, wind_data)

def compare_estimation_methods(data: np.ndarray,
                             true_epsilon: float = None) -> pd.DataFrame:
    """æ¯”è¼ƒä¸åŒçš„Îµä¼°è¨ˆæ–¹æ³•"""
    try:
        from .contamination_core import create_typhoon_contamination_spec
    except ImportError:
        from contamination_core import create_typhoon_contamination_spec
    
    spec = create_typhoon_contamination_spec()
    estimator = EpsilonEstimator(spec)
    
    # ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ–¹æ³•ä¼°è¨ˆ
    result = estimator.estimate_contamination_level(data)
    
    # æ§‹å»ºæ¯”è¼ƒè¡¨
    comparison_data = []
    
    for method_name, epsilon_est in result.epsilon_estimates.items():
        row = {
            "æ–¹æ³•": method_name,
            "Îµä¼°è¨ˆ": epsilon_est,
            "æª¢é©—çµ±è¨ˆé‡": result.test_statistics.get(method_name, np.nan),
            "på€¼": result.p_values.get(method_name, np.nan)
        }
        
        if true_epsilon is not None:
            row["çµ•å°èª¤å·®"] = abs(epsilon_est - true_epsilon)
            row["ç›¸å°èª¤å·®"] = abs(epsilon_est - true_epsilon) / true_epsilon
        
        comparison_data.append(row)
    
    return pd.DataFrame(comparison_data)

# ========================================
# æ¨¡çµ„å°å‡º
# ========================================

__all__ = [
    'EpsilonEstimator',
    'PriorContaminationAnalyzer',
    'quick_contamination_analysis',
    'compare_estimation_methods'
]
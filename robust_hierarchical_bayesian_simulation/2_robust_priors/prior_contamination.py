#!/usr/bin/env python3
"""
Prior Contamination Module
å…ˆé©—æ±¡æŸ“æ¨¡çµ„

å¾ epsilon_contamination.py æ‹†åˆ†å‡ºçš„å…ˆé©—æ±¡æŸ“åŠŸèƒ½
å°ˆé–€è™•ç†å…ˆé©—åˆ†å¸ƒçš„Îµ-contaminationå»ºæ¨¡

æ•¸å­¸åŸºç¤:
Ï€_Îµ(Î¸) = (1-Îµ)Ï€â‚€(Î¸) + Îµq(Î¸)

æ ¸å¿ƒåŠŸèƒ½:
- å…ˆé©—æ±¡æŸ“å»ºæ¨¡
- Îµå€¼ä¼°è¨ˆ
- å…ˆé©—ç©©å¥æ€§åˆ†æ

Author: Research Team  
Date: 2025-01-17
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from dataclasses import dataclass
from scipy import stats
from scipy.optimize import minimize
import warnings

# å¾å…¶ä»–æ¨¡çµ„å°å…¥
try:
    from .contamination_theory import (
        EpsilonContaminationSpec, ContaminationEstimateResult, 
        ContaminationDistributionClass, EstimationMethod,
        ContaminationDistributionGenerator
    )
except ImportError:
    # å¦‚æœç›¸å°å°å…¥å¤±æ•—ï¼Œå˜—è©¦çµ•å°å°å…¥
    try:
        from contamination_theory import (
            EpsilonContaminationSpec, ContaminationEstimateResult, 
            ContaminationDistributionClass, EstimationMethod,
            ContaminationDistributionGenerator
        )
    except ImportError:
        # å¦‚æœéƒ½å¤±æ•—ï¼Œå˜—è©¦å¾ç•¶å‰ç›®éŒ„å°å…¥
        import sys
        import os
        current_dir = os.path.dirname(os.path.abspath(__file__))
        sys.path.insert(0, current_dir)
        from contamination_theory import (
            EpsilonContaminationSpec, ContaminationEstimateResult, 
            ContaminationDistributionClass, EstimationMethod,
            ContaminationDistributionGenerator
        )

# ========================================
# å…ˆé©—æ±¡æŸ“åˆ†æå™¨
# ========================================

class PriorContaminationAnalyzer:
    """
    å…ˆé©—æ±¡æŸ“åˆ†æå™¨
    
    å°ˆé–€è™•ç†å…ˆé©—åˆ†å¸ƒçš„Îµ-contaminationå»ºæ¨¡å’Œåˆ†æ
    """
    
    def __init__(self, spec: EpsilonContaminationSpec):
        """
        åˆå§‹åŒ–å…ˆé©—æ±¡æŸ“åˆ†æå™¨
        
        Parameters:
        -----------
        spec : EpsilonContaminationSpec
            Îµ-contaminationè¦æ ¼é…ç½®
        """
        self.spec = spec
        self.contamination_generator = ContaminationDistributionGenerator()
        
        # çµæœç·©å­˜
        self.estimation_cache: Dict[str, ContaminationEstimateResult] = {}
        
        print(f"ğŸ”¬ å…ˆé©—æ±¡æŸ“åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ±¡æŸ“é¡åˆ¥: {self.spec.contamination_class.value}")
        print(f"   åŸºæº–å…ˆé©—: {self.spec.nominal_prior_family}")
        print(f"   æ±¡æŸ“å…ˆé©—: {self.spec.contamination_prior_family}")
    
    def estimate_epsilon_from_data(self, 
                                 data: np.ndarray,
                                 methods: List[EstimationMethod] = None) -> ContaminationEstimateResult:
        """
        å¾æ•¸æ“šä¼°è¨ˆÎµå€¼
        
        Parameters:
        -----------
        data : np.ndarray
            è§€æ¸¬æ•¸æ“š
        methods : List[EstimationMethod], optional
            ä½¿ç”¨çš„ä¼°è¨ˆæ–¹æ³•
            
        Returns:
        --------
        ContaminationEstimateResult
            Îµä¼°è¨ˆçµæœ
        """
        if methods is None:
            methods = [
                EstimationMethod.EMPIRICAL_FREQUENCY,
                EstimationMethod.KOLMOGOROV_SMIRNOV,
                EstimationMethod.ANDERSON_DARLING
            ]
        
        print(f"ğŸ“Š å¾æ•¸æ“šä¼°è¨ˆÎµå€¼ (n={len(data)})...")
        
        epsilon_estimates = {}
        test_statistics = {}
        p_values = {}
        
        # ç”ŸæˆåŸºæº–åˆ†å¸ƒ
        base_dist = self._create_base_distribution(data)
        
        for method in methods:
            print(f"   ä½¿ç”¨æ–¹æ³•: {method.value}")
            
            if method == EstimationMethod.EMPIRICAL_FREQUENCY:
                epsilon, test_stat, p_val = self._estimate_empirical_frequency(data, base_dist)
            elif method == EstimationMethod.KOLMOGOROV_SMIRNOV:
                epsilon, test_stat, p_val = self._estimate_ks_test(data, base_dist)
            elif method == EstimationMethod.ANDERSON_DARLING:
                epsilon, test_stat, p_val = self._estimate_ad_test(data, base_dist)
            elif method == EstimationMethod.BAYESIAN_MODEL_SELECTION:
                epsilon, test_stat, p_val = self._estimate_bayesian_selection(data, base_dist)
            else:
                print(f"   âš ï¸ æ–¹æ³• {method.value} å°šæœªå¯¦ç¾")
                continue
            
            epsilon_estimates[method.value] = epsilon
            test_statistics[method.value] = test_stat
            p_values[method.value] = p_val
            
            print(f"      Îµä¼°è¨ˆ: {epsilon:.4f}")
        
        # è¨ˆç®—å…±è­˜ä¼°è¨ˆå’Œä¸ç¢ºå®šæ€§
        consensus_epsilon = np.mean(list(epsilon_estimates.values()))
        uncertainty = np.std(list(epsilon_estimates.values()))
        
        # è¨­å®šé–¾å€¼
        thresholds = {
            "lower_bound": max(0.0, consensus_epsilon - 2 * uncertainty),
            "upper_bound": min(1.0, consensus_epsilon + 2 * uncertainty),
            "empirical_threshold": self.spec.empirical_epsilon
        }
        
        result = ContaminationEstimateResult(
            epsilon_estimates=epsilon_estimates,
            epsilon_consensus=consensus_epsilon,
            epsilon_uncertainty=uncertainty,
            thresholds=thresholds,
            test_statistics=test_statistics,
            p_values=p_values,
            method_weights={}  # å¯ä»¥å¾ŒçºŒæ·»åŠ æ¬Šé‡
        )
        
        # ç·©å­˜çµæœ
        cache_key = f"data_{len(data)}_{hash(data.tobytes())}"
        self.estimation_cache[cache_key] = result
        
        print(f"âœ… Îµä¼°è¨ˆå®Œæˆ: {consensus_epsilon:.4f} Â± {uncertainty:.4f}")
        return result
    
    def _create_base_distribution(self, data: np.ndarray):
        """å‰µå»ºåŸºæº–åˆ†å¸ƒ"""
        if self.spec.nominal_prior_family == "normal":
            return stats.norm(loc=np.mean(data), scale=np.std(data))
        elif self.spec.nominal_prior_family == "lognormal":
            # ç¢ºä¿æ•¸æ“šç‚ºæ­£
            positive_data = data[data > 0]
            if len(positive_data) == 0:
                return stats.norm(loc=0, scale=1)
            log_data = np.log(positive_data)
            return stats.lognorm(s=np.std(log_data), scale=np.exp(np.mean(log_data)))
        elif self.spec.nominal_prior_family == "gamma":
            # ä½¿ç”¨moment matching
            mean_val = np.mean(data)
            var_val = np.var(data)
            if var_val <= 0 or mean_val <= 0:
                return stats.gamma(a=1, scale=1)
            beta = mean_val / var_val
            alpha = mean_val * beta
            return stats.gamma(a=alpha, scale=1/beta)
        else:
            # é è¨­ä½¿ç”¨æ­£æ…‹åˆ†å¸ƒ
            return stats.norm(loc=np.mean(data), scale=np.std(data))
    
    def _estimate_empirical_frequency(self, data: np.ndarray, base_dist) -> Tuple[float, float, float]:
        """ç¶“é©—é »ç‡æ³•ä¼°è¨ˆÎµ"""
        # è¨ˆç®—æ¥µå€¼é »ç‡
        threshold = np.percentile(data, 95)  # 95%åˆ†ä½æ•¸ä½œç‚ºæ¥µå€¼é–¾å€¼
        extreme_count = np.sum(data > threshold)
        
        # ä¼°è¨ˆÎµç‚ºæ¥µå€¼é »ç‡
        epsilon = extreme_count / len(data)
        
        # ç°¡å–®çš„çµ±è¨ˆé‡
        test_statistic = extreme_count
        p_value = 1 - stats.binom.cdf(extreme_count - 1, len(data), 0.05)
        
        return epsilon, test_statistic, p_value
    
    def _estimate_ks_test(self, data: np.ndarray, base_dist) -> Tuple[float, float, float]:
        """Kolmogorov-Smirnovæª¢é©—æ³•ä¼°è¨ˆÎµ"""
        # å°åŸºæº–åˆ†å¸ƒé€²è¡ŒK-Sæª¢é©—
        ks_statistic, ks_p_value = stats.kstest(data, base_dist.cdf)
        
        # å°‡K-Sçµ±è¨ˆé‡è½‰æ›ç‚ºÎµä¼°è¨ˆ
        # é€™æ˜¯å•Ÿç™¼å¼è½‰æ›ï¼Œå¯¦éš›æ‡‰ç”¨ä¸­å¯èƒ½éœ€è¦æ›´ç²¾ç¢ºçš„æ˜ å°„
        epsilon = min(0.5, ks_statistic * 2)  # å°‡Dçµ±è¨ˆé‡æ˜ å°„åˆ°[0, 0.5]
        
        return epsilon, ks_statistic, ks_p_value
    
    def _estimate_ad_test(self, data: np.ndarray, base_dist) -> Tuple[float, float, float]:
        """Anderson-Darlingæª¢é©—æ³•ä¼°è¨ˆÎµ"""
        # å°åŸºæº–åˆ†å¸ƒé€²è¡ŒA-Dæª¢é©—
        try:
            ad_statistic, critical_values, significance_levels = stats.anderson(data)
            
            # æ ¹æ“šA-Dçµ±è¨ˆé‡ä¼°è¨ˆÎµ
            # ä½¿ç”¨èˆ‡critical_valuesçš„æ¯”è¼ƒ
            if len(critical_values) > 0:
                # æ¨™æº–åŒ–A-Dçµ±è¨ˆé‡
                normalized_ad = ad_statistic / critical_values[2]  # ä½¿ç”¨5%æ°´å¹³çš„è‡¨ç•Œå€¼
                epsilon = min(0.5, normalized_ad * 0.1)  # å•Ÿç™¼å¼æ˜ å°„
            else:
                epsilon = 0.05
            
            # è¨ˆç®—è¿‘ä¼¼på€¼
            p_value = 1.0 / (1.0 + ad_statistic)  # ç°¡åŒ–çš„på€¼ä¼°è¨ˆ
            
            return epsilon, ad_statistic, p_value
            
        except Exception as e:
            print(f"   âš ï¸ A-Dæª¢é©—å¤±æ•—: {e}")
            return 0.05, 0.0, 1.0
    
    def _estimate_bayesian_selection(self, data: np.ndarray, base_dist) -> Tuple[float, float, float]:
        """è²æ°æ¨¡å‹é¸æ“‡æ³•ä¼°è¨ˆÎµ"""
        # é€™æ˜¯ç°¡åŒ–å¯¦ç¾ï¼Œå¯¦éš›æ‡‰ç”¨ä¸­éœ€è¦å®Œæ•´çš„è²æ°æ¡†æ¶
        
        # å˜—è©¦ä¸åŒçš„Îµå€¼ï¼Œè¨ˆç®—é‚Šéš›ä¼¼ç„¶
        epsilon_candidates = np.linspace(0.01, 0.3, 20)
        log_marginal_likelihoods = []
        
        for eps in epsilon_candidates:
            # å‰µå»ºæ··åˆåˆ†å¸ƒ
            contamination_dist = self._create_contamination_distribution(data)
            
            # è¨ˆç®—æ··åˆåˆ†å¸ƒçš„å°æ•¸ä¼¼ç„¶
            n_base = int(len(data) * (1 - eps))
            n_contamination = len(data) - n_base
            
            if n_base > 0 and n_contamination > 0:
                # ç°¡åŒ–çš„ä¼¼ç„¶è¨ˆç®—
                base_ll = np.sum(base_dist.logpdf(data[:n_base]))
                contamination_ll = np.sum(contamination_dist.logpdf(data[n_base:]))
                total_ll = base_ll + contamination_ll
            else:
                total_ll = np.sum(base_dist.logpdf(data))
            
            log_marginal_likelihoods.append(total_ll)
        
        # æ‰¾åˆ°æœ€å¤§ä¼¼ç„¶å°æ‡‰çš„Îµ
        best_idx = np.argmax(log_marginal_likelihoods)
        best_epsilon = epsilon_candidates[best_idx]
        
        # è¨ˆç®—Bayes factorä½œç‚ºæª¢é©—çµ±è¨ˆé‡
        max_ll = log_marginal_likelihoods[best_idx]
        null_ll = np.sum(base_dist.logpdf(data))  # Îµ=0çš„ä¼¼ç„¶
        bayes_factor = 2 * (max_ll - null_ll)
        
        # è¿‘ä¼¼på€¼
        p_value = stats.chi2.sf(bayes_factor, df=1)
        
        return best_epsilon, bayes_factor, p_value
    
    def _create_contamination_distribution(self, data: np.ndarray):
        """å‰µå»ºæ±¡æŸ“åˆ†å¸ƒ"""
        if self.spec.contamination_class == ContaminationDistributionClass.TYPHOON_SPECIFIC:
            return self.contamination_generator.generate_typhoon_specific(
                location=np.mean(data), 
                scale=np.std(data) * 2,  # æ›´å¤§çš„è®Šç•°æ€§
                shape=0.2
            )
        elif self.spec.contamination_class == ContaminationDistributionClass.HEAVY_TAILED:
            return self.contamination_generator.generate_heavy_tailed(
                df=3, 
                location=np.mean(data), 
                scale=np.std(data) * 1.5
            )
        else:
            # é è¨­ä½¿ç”¨æ­£æ…‹åˆ†å¸ƒä½†åƒæ•¸ä¸åŒ
            return stats.norm(loc=np.mean(data), scale=np.std(data) * 2)
    
    def analyze_prior_robustness(self, 
                                epsilon_range: np.ndarray = None,
                                parameter_of_interest: str = "mean") -> Dict[str, Any]:
        """
        åˆ†æå…ˆé©—çš„ç©©å¥æ€§
        
        Parameters:
        -----------
        epsilon_range : np.ndarray, optional
            Îµå€¼ç¯„åœ
        parameter_of_interest : str
            é—œæ³¨çš„åƒæ•¸
            
        Returns:
        --------
        Dict[str, Any]
            ç©©å¥æ€§åˆ†æçµæœ
        """
        if epsilon_range is None:
            epsilon_range = np.linspace(0.0, 0.3, 31)
        
        print(f"ğŸ” åˆ†æå…ˆé©—ç©©å¥æ€§ (åƒæ•¸: {parameter_of_interest})...")
        
        # å‰µå»ºåŸºæº–å’Œæ±¡æŸ“åˆ†å¸ƒ
        base_samples = self._generate_base_prior_samples(1000)
        contamination_samples = self._generate_contamination_prior_samples(1000)
        
        results = {
            "epsilon_range": epsilon_range,
            "parameter_values": [],
            "parameter_bounds": [],
            "robustness_metrics": {}
        }
        
        for eps in epsilon_range:
            # å‰µå»ºæ··åˆå…ˆé©—æ¨£æœ¬
            n_contamination = int(1000 * eps)
            n_base = 1000 - n_contamination
            
            mixed_samples = np.concatenate([
                base_samples[:n_base],
                contamination_samples[:n_contamination]
            ])
            
            # è¨ˆç®—é—œæ³¨åƒæ•¸
            if parameter_of_interest == "mean":
                param_value = np.mean(mixed_samples)
            elif parameter_of_interest == "variance":
                param_value = np.var(mixed_samples)
            elif parameter_of_interest == "quantile_95":
                param_value = np.percentile(mixed_samples, 95)
            else:
                param_value = np.mean(mixed_samples)  # é è¨­
            
            results["parameter_values"].append(param_value)
            
            # è¨ˆç®—å¯ä¿¡å€é–“
            ci_lower = np.percentile(mixed_samples, 2.5)
            ci_upper = np.percentile(mixed_samples, 97.5)
            results["parameter_bounds"].append((ci_lower, ci_upper))
        
        # è¨ˆç®—ç©©å¥æ€§æŒ‡æ¨™
        param_values = np.array(results["parameter_values"])
        results["robustness_metrics"] = {
            "max_deviation": np.max(param_values) - np.min(param_values),
            "relative_deviation": (np.max(param_values) - np.min(param_values)) / np.abs(param_values[0]),
            "sensitivity_at_zero": np.gradient(param_values, epsilon_range)[0] if len(param_values) > 1 else 0,
            "max_sensitivity": np.max(np.abs(np.gradient(param_values, epsilon_range)))
        }
        
        print(f"âœ… ç©©å¥æ€§åˆ†æå®Œæˆ")
        print(f"   æœ€å¤§åå·®: {results['robustness_metrics']['max_deviation']:.4f}")
        print(f"   ç›¸å°åå·®: {results['robustness_metrics']['relative_deviation']:.2%}")
        
        return results
    
    def _generate_base_prior_samples(self, n_samples: int) -> np.ndarray:
        """ç”ŸæˆåŸºæº–å…ˆé©—æ¨£æœ¬"""
        if self.spec.nominal_prior_family == "normal":
            return np.random.normal(0, 1, n_samples)
        elif self.spec.nominal_prior_family == "lognormal":
            return np.random.lognormal(0, 1, n_samples)
        elif self.spec.nominal_prior_family == "gamma":
            return np.random.gamma(2, 1, n_samples)
        else:
            return np.random.normal(0, 1, n_samples)
    
    def _generate_contamination_prior_samples(self, n_samples: int) -> np.ndarray:
        """ç”Ÿæˆæ±¡æŸ“å…ˆé©—æ¨£æœ¬"""
        if self.spec.contamination_class == ContaminationDistributionClass.TYPHOON_SPECIFIC:
            # ä½¿ç”¨å»£ç¾©æ¥µå€¼åˆ†å¸ƒ
            return stats.genextreme.rvs(c=0.2, loc=0, scale=2, size=n_samples)
        elif self.spec.contamination_class == ContaminationDistributionClass.HEAVY_TAILED:
            # ä½¿ç”¨Student-tåˆ†å¸ƒ
            return stats.t.rvs(df=3, loc=0, scale=2, size=n_samples)
        else:
            # é è¨­ä½¿ç”¨ä¸åŒåƒæ•¸çš„æ­£æ…‹åˆ†å¸ƒ
            return np.random.normal(0, 3, n_samples)
    
    def compare_estimation_methods(self, 
                                 data: np.ndarray,
                                 true_epsilon: float = None) -> pd.DataFrame:
        """
        æ¯”è¼ƒä¸åŒçš„Îµä¼°è¨ˆæ–¹æ³•
        
        Parameters:
        -----------
        data : np.ndarray
            è§€æ¸¬æ•¸æ“š
        true_epsilon : float, optional
            çœŸå¯¦çš„Îµå€¼ï¼ˆå¦‚æœå·²çŸ¥ï¼‰
            
        Returns:
        --------
        pd.DataFrame
            æ–¹æ³•æ¯”è¼ƒçµæœ
        """
        print(f"ğŸ“ˆ æ¯”è¼ƒÎµä¼°è¨ˆæ–¹æ³•...")
        
        # ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ–¹æ³•ä¼°è¨ˆ
        all_methods = [
            EstimationMethod.EMPIRICAL_FREQUENCY,
            EstimationMethod.KOLMOGOROV_SMIRNOV,
            EstimationMethod.ANDERSON_DARLING,
            EstimationMethod.BAYESIAN_MODEL_SELECTION
        ]
        
        result = self.estimate_epsilon_from_data(data, all_methods)
        
        # æ§‹å»ºæ¯”è¼ƒè¡¨
        comparison_data = []
        
        for method_name, epsilon_est in result.epsilon_estimates.items():
            row = {
                "æ–¹æ³•": method_name,
                "Îµä¼°è¨ˆ": epsilon_est,
                "æª¢é©—çµ±è¨ˆé‡": result.test_statistics.get(method_name, np.nan),
                "på€¼": result.p_values.get(method_name, np.nan)
            }
            
            if true_epsilon is not None:
                row["çµ•å°èª¤å·®"] = abs(epsilon_est - true_epsilon)
                row["ç›¸å°èª¤å·®"] = abs(epsilon_est - true_epsilon) / true_epsilon
            
            comparison_data.append(row)
        
        comparison_df = pd.DataFrame(comparison_data)
        
        print(f"âœ… æ–¹æ³•æ¯”è¼ƒå®Œæˆ")
        if true_epsilon is not None:
            print(f"   çœŸå¯¦Îµå€¼: {true_epsilon:.4f}")
            best_method_idx = comparison_df["çµ•å°èª¤å·®"].idxmin()
            best_method = comparison_df.loc[best_method_idx, "æ–¹æ³•"]
            print(f"   æœ€ä½³æ–¹æ³•: {best_method}")
        
        return comparison_df

def test_prior_contamination():
    """æ¸¬è©¦å…ˆé©—æ±¡æŸ“åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦å…ˆé©—æ±¡æŸ“æ¨¡çµ„...")
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    np.random.seed(42)
    # æ¨¡æ“¬æ··åˆæ•¸æ“šï¼š90%æ­£æ…‹ + 10%æ¥µå€¼
    normal_data = np.random.normal(0, 1, 900)
    extreme_data = np.random.exponential(3, 100)
    test_data = np.concatenate([normal_data, extreme_data])
    np.random.shuffle(test_data)
    
    # å‰µå»ºåˆ†æå™¨
    spec = EpsilonContaminationSpec(
        contamination_class=ContaminationDistributionClass.TYPHOON_SPECIFIC
    )
    analyzer = PriorContaminationAnalyzer(spec)
    
    # æ¸¬è©¦Îµä¼°è¨ˆ
    print("âœ… æ¸¬è©¦Îµä¼°è¨ˆ:")
    result = analyzer.estimate_epsilon_from_data(test_data)
    print(f"   å…±è­˜Îµå€¼: {result.epsilon_consensus:.4f}")
    
    # æ¸¬è©¦ç©©å¥æ€§åˆ†æ
    print("âœ… æ¸¬è©¦ç©©å¥æ€§åˆ†æ:")
    robustness = analyzer.analyze_prior_robustness()
    print(f"   æœ€å¤§åå·®: {robustness['robustness_metrics']['max_deviation']:.4f}")
    
    # æ¸¬è©¦æ–¹æ³•æ¯”è¼ƒ
    print("âœ… æ¸¬è©¦æ–¹æ³•æ¯”è¼ƒ:")
    comparison = analyzer.compare_estimation_methods(test_data, true_epsilon=0.1)
    print(f"   æ¯”è¼ƒçµæœ: {len(comparison)} å€‹æ–¹æ³•")
    
    print("âœ… å…ˆé©—æ±¡æŸ“æ¸¬è©¦å®Œæˆ")

if __name__ == "__main__":
    test_prior_contamination()
#!/usr/bin/env python3
"""
Mixed Predictive Estimation (MPE) Module
æ··åˆé æ¸¬ä¼°è¨ˆæ¨¡çµ„

ç¨ç«‹çš„MPEå¯¦ç¾ï¼Œå°ˆé–€ç”¨æ–¼è¿‘ä¼¼è¤‡é›œçš„å¾Œé©—é æ¸¬åˆ†å¸ƒç‚ºå¤šå€‹ç°¡å–®åˆ†å¸ƒçš„æ··åˆã€‚

æ ¸å¿ƒåŠŸèƒ½:
- é«˜æ–¯æ··åˆæ¨¡å‹æ“¬åˆ (Gaussian Mixture Models)
- t-åˆ†å¸ƒæ··åˆæ¨¡å‹æ“¬åˆ
- Gammaåˆ†å¸ƒæ··åˆæ¨¡å‹æ“¬åˆ  
- å¾æ··åˆåˆ†å¸ƒä¸­æ¡æ¨£
- æ¨¡å‹é¸æ“‡èˆ‡è©•ä¼° (AIC, BIC)

ä½¿ç”¨ç¯„ä¾‹:
```python
from bayesian.posterior_mixture_approximation import MixedPredictiveEstimation

# åˆå§‹åŒ–MPE
mpe = MixedPredictiveEstimation(n_components=3)

# æ“¬åˆæ··åˆæ¨¡å‹
result = mpe.fit_mixture(posterior_samples, distribution_family="normal")

# æŸ¥çœ‹çµæœ
print("æ··åˆæ¬Šé‡:", result['mixture_weights'])
print("æ··åˆåƒæ•¸:", result['mixture_parameters'])
print("AIC:", result['aic'])

# å¾æ··åˆåˆ†å¸ƒæ¡æ¨£
new_samples = mpe.sample_from_mixture(n_samples=1000, mpe_result=result)
```

Author: Research Team
Date: 2025-01-12
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass
import warnings
from scipy import stats
from scipy.optimize import minimize
import os

# è¨­ç½®ç’°å¢ƒè®Šé‡ä»¥é¿å…ç·¨è­¯å•é¡Œ
for key in ['PYTENSOR_FLAGS', 'THEANO_FLAGS']:
    if key in os.environ:
        del os.environ[key]

os.environ['PYTENSOR_FLAGS'] = 'device=cpu,floatX=float64,mode=FAST_COMPILE,linker=py'
os.environ['MKL_THREADING_LAYER'] = 'GNU'

# sklearnæ”¯æŒæª¢æŸ¥
try:
    from sklearn.mixture import GaussianMixture
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    warnings.warn("sklearn not available, using simplified MPE implementation")

@dataclass
class MPEResult:
    """MPEæ“¬åˆçµæœ"""
    mixture_weights: np.ndarray
    mixture_parameters: List[Dict[str, Any]]
    converged: bool
    n_iterations: int
    log_likelihood: float
    aic: float
    bic: float
    distribution_family: str
    n_components: int
    fit_method: str = "auto"

@dataclass
class MPEConfig:
    """MPEé…ç½®"""
    n_components: int = 3
    convergence_threshold: float = 1e-6
    max_iterations: int = 1000
    random_seed: int = 42
    use_sklearn_when_available: bool = True

class MixedPredictiveEstimation:
    """
    æ··åˆé æ¸¬ä¼°è¨ˆ (MPE) å¯¦ç¾
    
    å°‡è¤‡é›œçš„å¾Œé©—é æ¸¬åˆ†å¸ƒè¿‘ä¼¼ç‚ºå¤šå€‹ç°¡å–®åˆ†å¸ƒçš„åŠ æ¬Šæ··åˆï¼š
    F_MPE(z) = Î£(k=1 to K) w_k * F_k(z|Î¸_k)
    
    å…¶ä¸­ï¼š
    - w_k æ˜¯ç¬¬kå€‹æˆåˆ†çš„æ¬Šé‡
    - F_k æ˜¯ç¬¬kå€‹æˆåˆ†çš„åˆ†å¸ƒå‡½æ•¸
    - Î¸_k æ˜¯ç¬¬kå€‹æˆåˆ†çš„åƒæ•¸
    """
    
    def __init__(self, config: MPEConfig = None):
        """
        åˆå§‹åŒ– MPE
        
        Parameters:
        -----------
        config : MPEConfig, optional
            MPEé…ç½®ï¼Œå¦‚æœªæä¾›å‰‡ä½¿ç”¨é è¨­é…ç½®
        """
        self.config = config or MPEConfig()
        
        # çµæœå­˜å„²
        self.last_fit_result: Optional[MPEResult] = None
        self.fit_history: List[MPEResult] = []
        
    def fit_mixture(self, 
                   posterior_samples: Union[np.ndarray, List[float]],
                   distribution_family: str = "normal",
                   n_components: Optional[int] = None) -> MPEResult:
        """
        æ“¬åˆæ··åˆåˆ†å¸ƒåˆ°å¾Œé©—æ¨£æœ¬
        
        Parameters:
        -----------
        posterior_samples : np.ndarray or List[float]
            å¾Œé©—æ¨£æœ¬æ•¸æ“š
        distribution_family : str
            åˆ†å¸ƒå®¶æ— ("normal", "t", "gamma")
        n_components : int, optional
            æ··åˆæˆåˆ†æ•¸é‡ï¼Œå¦‚æœªæŒ‡å®šå‰‡ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            
        Returns:
        --------
        MPEResult
            MPEæ“¬åˆçµæœï¼ŒåŒ…å«æ··åˆæ¬Šé‡ã€åƒæ•¸ã€è¨ºæ–·çµ±è¨ˆç­‰
        """
        # æ•¸æ“šé è™•ç†
        samples = np.asarray(posterior_samples).flatten()
        if len(samples) == 0:
            raise ValueError("å¾Œé©—æ¨£æœ¬ä¸èƒ½ç‚ºç©º")
        
        n_comp = n_components or self.config.n_components
        
        print(f"ğŸ”„ ä½¿ç”¨ MPE æ“¬åˆ {n_comp} æˆåˆ†æ··åˆ {distribution_family} åˆ†å¸ƒ...")
        print(f"   æ¨£æœ¬æ•¸é‡: {len(samples)}")
        print(f"   æ¨£æœ¬ç¯„åœ: [{samples.min():.3e}, {samples.max():.3e}]")
        
        # æ ¹æ“šåˆ†å¸ƒå®¶æ—é¸æ“‡æ“¬åˆæ–¹æ³•
        if distribution_family == "normal":
            result = self._fit_normal_mixture(samples, n_comp)
        elif distribution_family == "t":
            result = self._fit_t_mixture(samples, n_comp)
        elif distribution_family == "gamma":
            result = self._fit_gamma_mixture(samples, n_comp)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„åˆ†å¸ƒå®¶æ—: {distribution_family}")
        
        # å­˜å„²çµæœ
        self.last_fit_result = result
        self.fit_history.append(result)
        
        print(f"âœ… MPEæ“¬åˆå®Œæˆ:")
        print(f"   æ”¶æ–‚ç‹€æ…‹: {result.converged}")
        print(f"   è¿­ä»£æ¬¡æ•¸: {result.n_iterations}")
        print(f"   å°æ•¸ä¼¼ç„¶: {result.log_likelihood:.3f}")
        print(f"   AIC: {result.aic:.3f}")
        
        return result
    
    def _fit_normal_mixture(self, samples: np.ndarray, n_components: int) -> MPEResult:
        """æ“¬åˆæ­£æ…‹æ··åˆåˆ†å¸ƒ"""
        if HAS_SKLEARN and self.config.use_sklearn_when_available:
            return self._fit_normal_mixture_sklearn(samples, n_components)
        else:
            return self._fit_normal_mixture_em(samples, n_components)
    
    def _fit_normal_mixture_sklearn(self, samples: np.ndarray, n_components: int) -> MPEResult:
        """ä½¿ç”¨scikit-learnæ“¬åˆé«˜æ–¯æ··åˆæ¨¡å‹"""
        gmm = GaussianMixture(
            n_components=n_components,
            max_iter=self.config.max_iterations,
            tol=self.config.convergence_threshold,
            random_state=self.config.random_seed,
            covariance_type='full'
        )
        
        samples_reshaped = samples.reshape(-1, 1)
        gmm.fit(samples_reshaped)
        
        # æå–åƒæ•¸
        mixture_weights = gmm.weights_
        mixture_parameters = []
        
        for i in range(n_components):
            mixture_parameters.append({
                "mean": gmm.means_[i, 0],
                "std": np.sqrt(gmm.covariances_[i, 0, 0]),
                "weight": gmm.weights_[i]
            })
        
        # è¨ˆç®—è©•ä¼°æŒ‡æ¨™
        log_likelihood = gmm.score(samples_reshaped) * len(samples)
        n_params = n_components * 3 - 1  # means + stds + weights (constrained)
        
        return MPEResult(
            mixture_weights=mixture_weights,
            mixture_parameters=mixture_parameters,
            converged=gmm.converged_,
            n_iterations=gmm.n_iter_,
            log_likelihood=log_likelihood,
            aic=-2 * log_likelihood + 2 * n_params,
            bic=-2 * log_likelihood + n_params * np.log(len(samples)),
            distribution_family="normal",
            n_components=n_components,
            fit_method="sklearn_gmm"
        )
    
    def _fit_normal_mixture_em(self, samples: np.ndarray, n_components: int) -> MPEResult:
        """ä½¿ç”¨EMç®—æ³•æ“¬åˆé«˜æ–¯æ··åˆæ¨¡å‹"""
        n_samples = len(samples)
        
        # åˆå§‹åŒ–åƒæ•¸
        np.random.seed(self.config.random_seed)
        means = np.linspace(np.min(samples), np.max(samples), n_components)
        stds = np.full(n_components, np.std(samples))
        weights = np.ones(n_components) / n_components
        
        converged = False
        for iteration in range(self.config.max_iterations):
            # E-step: è¨ˆç®—è²¬ä»»
            responsibilities = np.zeros((n_samples, n_components))
            for k in range(n_components):
                responsibilities[:, k] = weights[k] * stats.norm.pdf(samples, means[k], stds[k])
            
            # é¿å…æ•¸å€¼å•é¡Œ
            row_sums = np.sum(responsibilities, axis=1, keepdims=True)
            row_sums[row_sums == 0] = 1e-10
            responsibilities = responsibilities / row_sums
            
            # M-step: æ›´æ–°åƒæ•¸
            old_means = means.copy()
            
            for k in range(n_components):
                nk = np.sum(responsibilities[:, k])
                if nk > 1e-10:
                    means[k] = np.sum(responsibilities[:, k] * samples) / nk
                    stds[k] = np.sqrt(np.sum(responsibilities[:, k] * (samples - means[k])**2) / nk)
                    weights[k] = nk / n_samples
            
            # é¿å…æ•¸å€¼å•é¡Œ
            stds = np.maximum(stds, 1e-6)
            weights = weights / np.sum(weights)
            
            # æª¢æŸ¥æ”¶æ–‚
            if np.max(np.abs(means - old_means)) < self.config.convergence_threshold:
                converged = True
                break
        
        # æ§‹å»ºåƒæ•¸åˆ—è¡¨
        mixture_parameters = []
        for k in range(n_components):
            mixture_parameters.append({
                "mean": means[k],
                "std": stds[k],
                "weight": weights[k]
            })
        
        # è¨ˆç®—å°æ•¸ä¼¼ç„¶
        log_likelihood = 0
        for i in range(n_samples):
            likelihood = 0
            for k in range(n_components):
                likelihood += weights[k] * stats.norm.pdf(samples[i], means[k], stds[k])
            log_likelihood += np.log(likelihood + 1e-10)
        
        n_params = n_components * 3 - 1
        
        return MPEResult(
            mixture_weights=weights,
            mixture_parameters=mixture_parameters,
            converged=converged,
            n_iterations=iteration + 1,
            log_likelihood=log_likelihood,
            aic=-2 * log_likelihood + 2 * n_params,
            bic=-2 * log_likelihood + n_params * np.log(n_samples),
            distribution_family="normal",
            n_components=n_components,
            fit_method="em_algorithm"
        )
    
    def _fit_t_mixture(self, samples: np.ndarray, n_components: int) -> MPEResult:
        """æ“¬åˆ t åˆ†å¸ƒæ··åˆæ¨¡å‹"""
        print("   ä½¿ç”¨ç°¡åŒ–çš„tåˆ†å¸ƒæ··åˆæ“¬åˆ...")
        
        # å…ˆç”¨æ­£æ…‹æ··åˆï¼Œç„¶å¾Œèª¿æ•´ç‚ºtåˆ†å¸ƒåƒæ•¸
        normal_result = self._fit_normal_mixture_em(samples, n_components)
        
        # èª¿æ•´ç‚ºtåˆ†å¸ƒåƒæ•¸ (å›ºå®šè‡ªç”±åº¦=4)
        t_parameters = []
        for param in normal_result.mixture_parameters:
            t_parameters.append({
                "df": 4.0,  # å›ºå®šè‡ªç”±åº¦
                "loc": param["mean"],
                "scale": param["std"] * np.sqrt(4/(4-2)),  # èª¿æ•´å°ºåº¦åƒæ•¸
                "weight": param["weight"]
            })
        
        return MPEResult(
            mixture_weights=normal_result.mixture_weights,
            mixture_parameters=t_parameters,
            converged=normal_result.converged,
            n_iterations=normal_result.n_iterations,
            log_likelihood=normal_result.log_likelihood * 0.9,  # ç²—ç•¥èª¿æ•´
            aic=normal_result.aic + 2,  # é¡å¤–çš„è‡ªç”±åº¦åƒæ•¸
            bic=normal_result.bic + 2 * np.log(len(samples)),
            distribution_family="t",
            n_components=n_components,
            fit_method="normal_to_t_approximation"
        )
    
    def _fit_gamma_mixture(self, samples: np.ndarray, n_components: int) -> MPEResult:
        """æ“¬åˆ Gamma åˆ†å¸ƒæ··åˆæ¨¡å‹"""
        # ç¢ºä¿æ‰€æœ‰æ¨£æœ¬ç‚ºæ­£æ•¸
        if np.any(samples <= 0):
            warnings.warn("Gammaåˆ†å¸ƒè¦æ±‚æ­£å€¼ï¼Œå°‡éæ­£å€¼è¨­ç‚ºæ¥µå°æ­£å€¼")
            samples = np.maximum(samples, 1e-10)
        
        # ä½¿ç”¨çŸ©ä¼°è¨ˆæ³•åˆå§‹åŒ–åƒæ•¸
        sample_mean = np.mean(samples)
        sample_var = np.var(samples)
        
        # åˆå§‹åŒ–æ··åˆæˆåˆ†åƒæ•¸
        np.random.seed(self.config.random_seed)
        gamma_parameters = []
        weights = np.ones(n_components) / n_components
        
        for i in range(n_components):
            # ç‚ºæ¯å€‹æˆåˆ†ç”Ÿæˆä¸åŒçš„åˆå§‹åŒ–
            scale_factor = 1 + 0.5 * i
            alpha_init = (sample_mean ** 2) / sample_var * scale_factor
            beta_init = sample_mean / sample_var * scale_factor
            
            gamma_parameters.append({
                "alpha": max(alpha_init, 1e-3),
                "beta": max(beta_init, 1e-3),
                "weight": weights[i]
            })
        
        # ç°¡åŒ–çš„å°æ•¸ä¼¼ç„¶è¨ˆç®—
        log_likelihood = 0
        for param in gamma_parameters:
            ll_component = np.sum(stats.gamma.logpdf(
                samples, 
                a=param["alpha"], 
                scale=1/param["beta"]
            )) * param["weight"]
            log_likelihood += ll_component
        
        n_params = n_components * 3 - 1
        
        return MPEResult(
            mixture_weights=weights,
            mixture_parameters=gamma_parameters,
            converged=True,  # ç°¡åŒ–å‡è¨­
            n_iterations=1,
            log_likelihood=log_likelihood,
            aic=-2 * log_likelihood + 2 * n_params,
            bic=-2 * log_likelihood + n_params * np.log(len(samples)),
            distribution_family="gamma",
            n_components=n_components,
            fit_method="method_of_moments"
        )
    
    def sample_from_mixture(self, 
                          n_samples: int = 1000,
                          mpe_result: Optional[MPEResult] = None) -> np.ndarray:
        """
        å¾MPEæ··åˆåˆ†å¸ƒä¸­æ¡æ¨£
        
        Parameters:
        -----------
        n_samples : int
            æ¡æ¨£æ•¸é‡
        mpe_result : MPEResult, optional
            MPEæ“¬åˆçµæœï¼Œå¦‚æœªæä¾›å‰‡ä½¿ç”¨æœ€è¿‘çš„æ“¬åˆçµæœ
            
        Returns:
        --------
        np.ndarray
            å¾æ··åˆåˆ†å¸ƒä¸­æ¡æ¨£çš„æ¨£æœ¬
        """
        if mpe_result is None:
            if self.last_fit_result is None:
                raise ValueError("éœ€è¦å…ˆæ“¬åˆMPEæˆ–æä¾›mpe_result")
            mpe_result = self.last_fit_result
        
        print(f"ğŸ² å¾{mpe_result.distribution_family}æ··åˆåˆ†å¸ƒä¸­æ¡æ¨£ {n_samples} å€‹æ¨£æœ¬...")
        
        weights = mpe_result.mixture_weights
        parameters = mpe_result.mixture_parameters
        family = mpe_result.distribution_family
        
        samples = []
        
        # è¨­ç½®éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯é‡ç¾æ€§
        np.random.seed(self.config.random_seed)
        
        # æ ¹æ“šæ¬Šé‡é¸æ“‡æˆåˆ†
        component_choices = np.random.choice(
            len(weights), 
            size=n_samples, 
            p=weights
        )
        
        for component_idx in component_choices:
            param = parameters[component_idx]
            
            if family == "normal":
                sample = np.random.normal(param["mean"], param["std"])
            elif family == "t":
                sample = stats.t.rvs(
                    df=param["df"], 
                    loc=param["loc"], 
                    scale=param["scale"]
                )
            elif family == "gamma":
                sample = np.random.gamma(
                    param["alpha"], 
                    scale=1/param["beta"]
                )
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„åˆ†å¸ƒå®¶æ—: {family}")
            
            samples.append(sample)
        
        samples_array = np.array(samples)
        print(f"âœ… æ¡æ¨£å®Œæˆï¼Œæ¨£æœ¬ç¯„åœ: [{samples_array.min():.3e}, {samples_array.max():.3e}]")
        
        return samples_array
    
    def evaluate_mixture_quality(self, 
                                samples: np.ndarray, 
                                mpe_result: MPEResult) -> Dict[str, float]:
        """
        è©•ä¼°æ··åˆæ¨¡å‹çš„æ“¬åˆå“è³ª
        
        Parameters:
        -----------
        samples : np.ndarray
            åŸå§‹æ¨£æœ¬
        mpe_result : MPEResult
            MPEæ“¬åˆçµæœ
            
        Returns:
        --------
        Dict[str, float]
            å“è³ªè©•ä¼°æŒ‡æ¨™
        """
        # å¾æ··åˆæ¨¡å‹ç”Ÿæˆæ–°æ¨£æœ¬
        synthetic_samples = self.sample_from_mixture(
            n_samples=len(samples), 
            mpe_result=mpe_result
        )
        
        # è¨ˆç®—åŸºæœ¬çµ±è¨ˆé‡å·®ç•°
        original_mean = np.mean(samples)
        original_std = np.std(samples)
        synthetic_mean = np.mean(synthetic_samples)
        synthetic_std = np.std(synthetic_samples)
        
        mean_error = abs(original_mean - synthetic_mean) / abs(original_mean + 1e-10)
        std_error = abs(original_std - synthetic_std) / abs(original_std + 1e-10)
        
        # ä½¿ç”¨KSæª¢é©—è©•ä¼°åˆ†å¸ƒå·®ç•°
        try:
            from scipy.stats import ks_2samp
            ks_stat, ks_pvalue = ks_2samp(samples, synthetic_samples)
        except ImportError:
            ks_stat, ks_pvalue = np.nan, np.nan
        
        quality_metrics = {
            "mean_relative_error": mean_error,
            "std_relative_error": std_error,
            "ks_statistic": ks_stat,
            "ks_pvalue": ks_pvalue,
            "aic": mpe_result.aic,
            "bic": mpe_result.bic,
            "log_likelihood": mpe_result.log_likelihood,
            "converged": float(mpe_result.converged)
        }
        
        return quality_metrics
    
    def compare_component_numbers(self, 
                                samples: np.ndarray,
                                component_range: Tuple[int, int] = (1, 6),
                                distribution_family: str = "normal") -> Dict[int, MPEResult]:
        """
        æ¯”è¼ƒä¸åŒæˆåˆ†æ•¸é‡çš„MPEæ€§èƒ½
        
        Parameters:
        -----------
        samples : np.ndarray
            å¾Œé©—æ¨£æœ¬
        component_range : Tuple[int, int]
            æˆåˆ†æ•¸é‡ç¯„åœ (æœ€å°å€¼, æœ€å¤§å€¼+1)
        distribution_family : str
            åˆ†å¸ƒå®¶æ—
            
        Returns:
        --------
        Dict[int, MPEResult]
            æ¯å€‹æˆåˆ†æ•¸é‡å°æ‡‰çš„MPEçµæœ
        """
        print(f"ğŸ” æ¯”è¼ƒæˆåˆ†æ•¸é‡ç¯„åœ: {component_range}")
        
        results = {}
        
        for n_comp in range(component_range[0], component_range[1]):
            print(f"\n   æ¸¬è©¦ {n_comp} å€‹æˆåˆ†...")
            try:
                result = self.fit_mixture(
                    samples, 
                    distribution_family=distribution_family,
                    n_components=n_comp
                )
                results[n_comp] = result
                print(f"   AIC: {result.aic:.3f}, BIC: {result.bic:.3f}")
            except Exception as e:
                print(f"   âš ï¸ {n_comp}å€‹æˆåˆ†æ“¬åˆå¤±æ•—: {e}")
        
        if results:
            # æ‰¾åˆ°æœ€ä½³æˆåˆ†æ•¸é‡
            best_aic_comp = min(results.keys(), key=lambda k: results[k].aic)
            best_bic_comp = min(results.keys(), key=lambda k: results[k].bic)
            
            print(f"\nğŸ† æœ€ä½³æˆåˆ†æ•¸é‡:")
            print(f"   AICæº–å‰‡: {best_aic_comp} å€‹æˆåˆ†")
            print(f"   BICæº–å‰‡: {best_bic_comp} å€‹æˆåˆ†")
        
        return results
    
    def get_mixture_summary(self, mpe_result: Optional[MPEResult] = None) -> pd.DataFrame:
        """
        ç²å–æ··åˆæ¨¡å‹æ‘˜è¦
        
        Parameters:
        -----------
        mpe_result : MPEResult, optional
            MPEçµæœï¼Œå¦‚æœªæä¾›å‰‡ä½¿ç”¨æœ€è¿‘çš„çµæœ
            
        Returns:
        --------
        pd.DataFrame
            æ··åˆæˆåˆ†æ‘˜è¦è¡¨
        """
        if mpe_result is None:
            if self.last_fit_result is None:
                return pd.DataFrame()
            mpe_result = self.last_fit_result
        
        summary_data = []
        
        for i, param in enumerate(mpe_result.mixture_parameters):
            row_data = {
                "æˆåˆ†": i + 1,
                "æ¬Šé‡": param["weight"]
            }
            
            if mpe_result.distribution_family == "normal":
                row_data.update({
                    "å‡å€¼": param["mean"],
                    "æ¨™æº–å·®": param["std"]
                })
            elif mpe_result.distribution_family == "t":
                row_data.update({
                    "è‡ªç”±åº¦": param["df"],
                    "ä½ç½®": param["loc"],
                    "å°ºåº¦": param["scale"]
                })
            elif mpe_result.distribution_family == "gamma":
                row_data.update({
                    "å½¢ç‹€(Î±)": param["alpha"],
                    "ç‡(Î²)": param["beta"]
                })
            
            summary_data.append(row_data)
        
        return pd.DataFrame(summary_data)

# ä¾¿åˆ©å‡½æ•¸
def fit_gaussian_mixture(samples: Union[np.ndarray, List[float]], 
                        n_components: int = 3) -> MPEResult:
    """
    ä¾¿åˆ©å‡½æ•¸ï¼šå¿«é€Ÿæ“¬åˆé«˜æ–¯æ··åˆæ¨¡å‹
    
    Parameters:
    -----------
    samples : np.ndarray or List[float]
        å¾Œé©—æ¨£æœ¬
    n_components : int
        æ··åˆæˆåˆ†æ•¸é‡
        
    Returns:
    --------
    MPEResult
        MPEæ“¬åˆçµæœ
    """
    mpe = MixedPredictiveEstimation()
    return mpe.fit_mixture(samples, "normal", n_components)

def sample_from_gaussian_mixture(mpe_result: MPEResult, n_samples: int = 1000) -> np.ndarray:
    """
    ä¾¿åˆ©å‡½æ•¸ï¼šå¾é«˜æ–¯æ··åˆæ¨¡å‹æ¡æ¨£
    
    Parameters:
    -----------
    mpe_result : MPEResult
        MPEæ“¬åˆçµæœ
    n_samples : int
        æ¡æ¨£æ•¸é‡
        
    Returns:
    --------
    np.ndarray
        æ¨£æœ¬
    """
    mpe = MixedPredictiveEstimation()
    return mpe.sample_from_mixture(n_samples, mpe_result)

# æ¸¬è©¦å’Œç¤ºç¯„å‡½æ•¸
def test_mpe_functionality():
    """æ¸¬è©¦MPEåŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦ MPE åŠŸèƒ½...")
    
    # ç”Ÿæˆæ¸¬è©¦æ•¸æ“šï¼ˆé›™å³°åˆ†å¸ƒï¼‰
    np.random.seed(42)
    samples1 = np.random.normal(-2, 0.5, 300)
    samples2 = np.random.normal(2, 0.8, 200)
    test_samples = np.concatenate([samples1, samples2])
    
    # æ¸¬è©¦MPE
    mpe = MixedPredictiveEstimation()
    
    # æ“¬åˆæ­£æ…‹æ··åˆ
    print("\n1. æ“¬åˆæ­£æ…‹æ··åˆåˆ†å¸ƒ:")
    normal_result = mpe.fit_mixture(test_samples, "normal", n_components=2)
    
    # é¡¯ç¤ºæ‘˜è¦
    print("\n2. æ··åˆæˆåˆ†æ‘˜è¦:")
    summary = mpe.get_mixture_summary(normal_result)
    print(summary)
    
    # å¾æ··åˆåˆ†å¸ƒæ¡æ¨£
    print("\n3. å¾æ··åˆåˆ†å¸ƒæ¡æ¨£:")
    new_samples = mpe.sample_from_mixture(1000, normal_result)
    print(f"æ–°æ¨£æœ¬çµ±è¨ˆ: å‡å€¼={np.mean(new_samples):.3f}, æ¨™æº–å·®={np.std(new_samples):.3f}")
    
    # è©•ä¼°å“è³ª
    print("\n4. è©•ä¼°æ“¬åˆå“è³ª:")
    quality = mpe.evaluate_mixture_quality(test_samples, normal_result)
    for metric, value in quality.items():
        print(f"   {metric}: {value:.4f}")
    
    # æ¯”è¼ƒä¸åŒæˆåˆ†æ•¸é‡
    print("\n5. æ¯”è¼ƒä¸åŒæˆåˆ†æ•¸é‡:")
    comparison = mpe.compare_component_numbers(test_samples, (1, 4))
    
    print("âœ… MPE æ¸¬è©¦å®Œæˆ")
    return normal_result

if __name__ == "__main__":
    test_mpe_functionality()
#!/usr/bin/env python3
"""
Model Selector with Hyperparameter Optimization
æ¨¡å‹æµ·é¸èˆ‡è¶…åƒæ•¸å„ªåŒ–å™¨

å¯¦ç¾é›™å±¤å¾ªç’°ç­–ç•¥ï¼š
- å¤–å±¤ï¼šéæ­·æ‰€æœ‰æ¨¡å‹æ¶æ§‹ (prior, likelihood, Îµ)
- å…§å±¤ï¼šç‚ºæ¯å€‹æ¨¡å‹å°‹æ‰¾æœ€ä½³è¶…åƒæ•¸ Î»

Author: Research Team
Date: 2025-01-17
Version: 2.0
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any, Callable
from dataclasses import dataclass, field
import warnings
import time
from pathlib import Path
import json
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import logging

# Import VI components
from .basis_risk_vi import BasisRiskAwareVI, DifferentiableCRPS

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ModelCandidate:
    """æ¨¡å‹å€™é¸äººé…ç½®"""
    model_id: str
    prior_type: str  # 'informative', 'weakly_informative', 'vague'
    likelihood_family: str  # 'normal', 'student_t', 'laplace'
    epsilon_contamination: float  # Îµ-contamination level
    spatial_effects: bool = False
    hierarchy_levels: int = 2
    
    def __hash__(self):
        return hash(self.model_id)
    
    def __str__(self):
        return f"{self.model_id}: {self.prior_type}_{self.likelihood_family}_Îµ{self.epsilon_contamination:.2f}"

@dataclass
class HyperparameterConfig:
    """è¶…åƒæ•¸é…ç½®"""
    lambda_crps: float  # CRPSæ¬Šé‡
    lambda_under: float = 2.0  # ä¸è¶³è¦†è“‹æ‡²ç½°
    lambda_over: float = 0.5  # éåº¦è³ ä»˜æ‡²ç½°
    vi_learning_rate: float = 0.01
    vi_iterations: int = 5000
    vi_batch_size: int = 32
    
    def to_dict(self):
        return {
            'Î»_crps': self.lambda_crps,
            'Î»_under': self.lambda_under,
            'Î»_over': self.lambda_over,
            'lr': self.vi_learning_rate,
            'iters': self.vi_iterations
        }

@dataclass
class ModelSelectionResult:
    """æ¨¡å‹é¸æ“‡çµæœ"""
    model: ModelCandidate
    best_hyperparams: HyperparameterConfig
    best_score: float  # L_BR score
    convergence_time: float
    validation_metrics: Dict[str, float]
    posterior_samples: Optional[np.ndarray] = None
    rank: Optional[int] = None
    
    def summary(self) -> Dict[str, Any]:
        return {
            'model_id': self.model.model_id,
            'score': self.best_score,
            'best_Î»': self.best_hyperparams.lambda_crps,
            'time': self.convergence_time,
            'rank': self.rank
        }

class ModelSelectorWithHyperparamOptimization:
    """
    æ¨¡å‹æµ·é¸èˆ‡è¶…åƒæ•¸å„ªåŒ–å™¨
    
    å¯¦ç¾è«–æ–‡ä¸­çš„é›™å±¤å¾ªç’°ç­–ç•¥ï¼š
    1. å¤–å±¤ï¼šéæ­·æ¨¡å‹æ¶æ§‹
    2. å…§å±¤ï¼šå„ªåŒ–è¶…åƒæ•¸
    """
    
    def __init__(self, 
                 n_jobs: int = 4,
                 verbose: bool = True,
                 save_results: bool = True,
                 output_dir: str = "results/model_selection"):
        """
        åˆå§‹åŒ–æ¨¡å‹é¸æ“‡å™¨
        
        Parameters:
        -----------
        n_jobs : int
            ä¸¦è¡Œè™•ç†çš„å·¥ä½œæ•¸
        verbose : bool
            æ˜¯å¦é¡¯ç¤ºè©³ç´°ä¿¡æ¯
        save_results : bool
            æ˜¯å¦ä¿å­˜çµæœ
        output_dir : str
            è¼¸å‡ºç›®éŒ„
        """
        self.n_jobs = n_jobs
        self.verbose = verbose
        self.save_results = save_results
        self.output_dir = Path(output_dir)
        
        if save_results:
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Results storage
        self.leaderboard = {}
        self.full_results = []
        self.optimization_history = []
        
    def generate_model_candidates(self) -> List[ModelCandidate]:
        """
        ç”Ÿæˆæ‰€æœ‰æ¨¡å‹å€™é¸
        
        Returns:
        --------
        List[ModelCandidate]
            æ¨¡å‹å€™é¸åˆ—è¡¨
        """
        candidates = []
        
        # Prior types
        priors = ['informative', 'weakly_informative', 'vague']
        
        # Likelihood families
        likelihoods = ['normal', 'student_t', 'laplace']
        
        # Epsilon contamination levels
        epsilons = [0.05, 0.10, 0.15]
        
        # Spatial effects
        spatial_options = [False, True]
        
        model_id = 0
        for prior in priors:
            for likelihood in likelihoods:
                for epsilon in epsilons:
                    for spatial in spatial_options:
                        model_id += 1
                        candidate = ModelCandidate(
                            model_id=f"M{model_id:03d}",
                            prior_type=prior,
                            likelihood_family=likelihood,
                            epsilon_contamination=epsilon,
                            spatial_effects=spatial
                        )
                        candidates.append(candidate)
        
        if self.verbose:
            print(f"ğŸ“Š ç”Ÿæˆäº† {len(candidates)} å€‹æ¨¡å‹å€™é¸")
            
        return candidates
    
    def generate_hyperparameter_grid(self, 
                                    lambda_values: Optional[List[float]] = None) -> List[HyperparameterConfig]:
        """
        ç”Ÿæˆè¶…åƒæ•¸ç¶²æ ¼
        
        Parameters:
        -----------
        lambda_values : List[float], optional
            CRPSæ¬Šé‡å€™é¸å€¼
            
        Returns:
        --------
        List[HyperparameterConfig]
            è¶…åƒæ•¸é…ç½®åˆ—è¡¨
        """
        if lambda_values is None:
            lambda_values = [0.1, 0.5, 1.0, 5.0, 10.0, 50.0]
        
        configs = []
        
        # åŸºæœ¬ç¶²æ ¼
        for lambda_crps in lambda_values:
            # æ¨™æº–é…ç½®
            configs.append(HyperparameterConfig(
                lambda_crps=lambda_crps,
                lambda_under=2.0,
                lambda_over=0.5
            ))
            
            # æ¿€é€²é…ç½®ï¼ˆæ›´é‡è¦–ä¸è¶³è¦†è“‹ï¼‰
            configs.append(HyperparameterConfig(
                lambda_crps=lambda_crps,
                lambda_under=5.0,
                lambda_over=0.2
            ))
            
            # ä¿å®ˆé…ç½®ï¼ˆæ›´é‡è¦–éåº¦è³ ä»˜ï¼‰
            configs.append(HyperparameterConfig(
                lambda_crps=lambda_crps,
                lambda_under=1.0,
                lambda_over=1.0
            ))
        
        if self.verbose:
            print(f"ğŸ”§ ç”Ÿæˆäº† {len(configs)} å€‹è¶…åƒæ•¸é…ç½®")
            
        return configs
    
    def run_model_selection(self,
                           data: Dict[str, np.ndarray],
                           candidates: Optional[List[ModelCandidate]] = None,
                           hyperparameter_grid: Optional[List[HyperparameterConfig]] = None,
                           top_k: int = 5) -> List[ModelSelectionResult]:
        """
        åŸ·è¡Œå®Œæ•´çš„æ¨¡å‹æµ·é¸èˆ‡è¶…åƒæ•¸å„ªåŒ–
        
        Parameters:
        -----------
        data : Dict[str, np.ndarray]
            åŒ…å«è¨“ç·´å’Œé©—è­‰æ•¸æ“š
        candidates : List[ModelCandidate], optional
            æ¨¡å‹å€™é¸åˆ—è¡¨
        hyperparameter_grid : List[HyperparameterConfig], optional
            è¶…åƒæ•¸ç¶²æ ¼
        top_k : int
            è¿”å›å‰kå€‹æœ€ä½³æ¨¡å‹
            
        Returns:
        --------
        List[ModelSelectionResult]
            å‰kå€‹æœ€ä½³æ¨¡å‹çš„çµæœ
        """
        print("ğŸ é–‹å§‹æ¨¡å‹æµ·é¸èˆ‡è¶…åƒæ•¸å„ªåŒ–")
        print("=" * 60)
        
        start_time = time.time()
        
        # Generate candidates if not provided
        if candidates is None:
            candidates = self.generate_model_candidates()
        
        # Generate hyperparameter grid if not provided
        if hyperparameter_grid is None:
            hyperparameter_grid = self.generate_hyperparameter_grid()
        
        # Run optimization for each model
        if self.n_jobs > 1:
            results = self._parallel_optimization(data, candidates, hyperparameter_grid)
        else:
            results = self._sequential_optimization(data, candidates, hyperparameter_grid)
        
        # Rank results
        self._rank_results(results)
        
        # Generate leaderboard
        self._generate_leaderboard(results)
        
        # Save results
        if self.save_results:
            self._save_results(results)
        
        total_time = time.time() - start_time
        print(f"\nâœ… æ¨¡å‹æµ·é¸å®Œæˆï¼ç¸½æ™‚é–“: {total_time:.2f} ç§’")
        
        # Return top k models
        return results[:top_k]
    
    def _optimize_single_model(self,
                              model: ModelCandidate,
                              data: Dict[str, np.ndarray],
                              hyperparameter_grid: List[HyperparameterConfig]) -> ModelSelectionResult:
        """
        ç‚ºå–®ä¸€æ¨¡å‹å„ªåŒ–è¶…åƒæ•¸
        
        Parameters:
        -----------
        model : ModelCandidate
            æ¨¡å‹å€™é¸
        data : Dict[str, np.ndarray]
            æ•¸æ“š
        hyperparameter_grid : List[HyperparameterConfig]
            è¶…åƒæ•¸ç¶²æ ¼
            
        Returns:
        --------
        ModelSelectionResult
            å„ªåŒ–çµæœ
        """
        if self.verbose:
            print(f"\nğŸ” å„ªåŒ–æ¨¡å‹: {model}")
        
        best_score = -np.inf
        best_config = None
        best_metrics = {}
        
        start_time = time.time()
        
        # Inner loop: hyperparameter optimization
        for config in hyperparameter_grid:
            try:
                # Run VI with current hyperparameters
                score, metrics = self._run_vi_with_hyperparams(model, config, data)
                
                # Track history
                self.optimization_history.append({
                    'model_id': model.model_id,
                    'config': config.to_dict(),
                    'score': score,
                    'metrics': metrics
                })
                
                # Update best
                if score > best_score:
                    best_score = score
                    best_config = config
                    best_metrics = metrics
                    
                    if self.verbose:
                        print(f"   âœ“ æ–°æœ€ä½³: Î»={config.lambda_crps:.1f}, score={score:.4f}")
                        
            except Exception as e:
                logger.warning(f"Failed to optimize {model} with {config}: {e}")
                continue
        
        convergence_time = time.time() - start_time
        
        return ModelSelectionResult(
            model=model,
            best_hyperparams=best_config,
            best_score=best_score,
            convergence_time=convergence_time,
            validation_metrics=best_metrics
        )
    
    def _run_vi_with_hyperparams(self,
                                model: ModelCandidate,
                                config: HyperparameterConfig,
                                data: Dict[str, np.ndarray]) -> Tuple[float, Dict[str, float]]:
        """
        ä½¿ç”¨æŒ‡å®šè¶…åƒæ•¸é‹è¡ŒVI
        
        Parameters:
        -----------
        model : ModelCandidate
            æ¨¡å‹é…ç½®
        config : HyperparameterConfig
            è¶…åƒæ•¸é…ç½®
        data : Dict[str, np.ndarray]
            æ•¸æ“š
            
        Returns:
        --------
        Tuple[float, Dict[str, float]]
            (L_BRåˆ†æ•¸, é©—è­‰æŒ‡æ¨™)
        """
        # Initialize VI trainer
        vi_trainer = BasisRiskAwareVI(
            prior_type=model.prior_type,
            likelihood_family=model.likelihood_family,
            epsilon_contamination=model.epsilon_contamination,
            lambda_crps=config.lambda_crps,
            lambda_under=config.lambda_under,
            lambda_over=config.lambda_over,
            learning_rate=config.vi_learning_rate,
            n_iterations=config.vi_iterations
        )
        
        # Train
        results = vi_trainer.fit(
            X=data['X_train'],
            y=data['y_train'],
            X_val=data.get('X_val'),
            y_val=data.get('y_val')
        )
        
        # Calculate L_BR score
        L_BR_score = results['final_elbo'] - config.lambda_crps * results['final_crps']
        
        # Validation metrics
        metrics = {
            'elbo': results['final_elbo'],
            'crps': results['final_crps'],
            'kl_divergence': results.get('kl_divergence', 0),
            'basis_risk': results.get('basis_risk', 0)
        }
        
        return L_BR_score, metrics
    
    def _parallel_optimization(self,
                             data: Dict[str, np.ndarray],
                             candidates: List[ModelCandidate],
                             hyperparameter_grid: List[HyperparameterConfig]) -> List[ModelSelectionResult]:
        """ä¸¦è¡Œå„ªåŒ–æ‰€æœ‰æ¨¡å‹"""
        print(f"âš¡ ä½¿ç”¨ {self.n_jobs} å€‹é€²ç¨‹ä¸¦è¡Œå„ªåŒ–...")
        
        with ProcessPoolExecutor(max_workers=self.n_jobs) as executor:
            futures = []
            for model in candidates:
                future = executor.submit(
                    self._optimize_single_model,
                    model, data, hyperparameter_grid
                )
                futures.append(future)
            
            results = [future.result() for future in futures]
        
        return results
    
    def _sequential_optimization(self,
                                data: Dict[str, np.ndarray],
                                candidates: List[ModelCandidate],
                                hyperparameter_grid: List[HyperparameterConfig]) -> List[ModelSelectionResult]:
        """é †åºå„ªåŒ–æ‰€æœ‰æ¨¡å‹"""
        print("ğŸ”„ é †åºå„ªåŒ–æ¨¡å‹...")
        
        results = []
        for i, model in enumerate(candidates):
            print(f"\né€²åº¦: {i+1}/{len(candidates)}")
            result = self._optimize_single_model(model, data, hyperparameter_grid)
            results.append(result)
        
        return results
    
    def _rank_results(self, results: List[ModelSelectionResult]):
        """ç‚ºçµæœæ’å"""
        # Sort by score (descending)
        results.sort(key=lambda x: x.best_score, reverse=True)
        
        # Assign ranks
        for i, result in enumerate(results):
            result.rank = i + 1
    
    def _generate_leaderboard(self, results: List[ModelSelectionResult]):
        """ç”Ÿæˆæ’è¡Œæ¦œ"""
        print("\nğŸ† æ¨¡å‹æ’è¡Œæ¦œ")
        print("=" * 60)
        
        leaderboard_data = []
        for result in results[:10]:  # Top 10
            row = {
                'Rank': result.rank,
                'Model': result.model.model_id,
                'Prior': result.model.prior_type,
                'Likelihood': result.model.likelihood_family,
                'Îµ': result.model.epsilon_contamination,
                'Best Î»': result.best_hyperparams.lambda_crps,
                'Score': f"{result.best_score:.4f}",
                'Time': f"{result.convergence_time:.1f}s"
            }
            leaderboard_data.append(row)
        
        df = pd.DataFrame(leaderboard_data)
        print(df.to_string(index=False))
        
        self.leaderboard = df
    
    def _save_results(self, results: List[ModelSelectionResult]):
        """ä¿å­˜çµæœ"""
        # Save full results as JSON
        results_dict = {
            'timestamp': pd.Timestamp.now().isoformat(),
            'n_models': len(results),
            'results': [r.summary() for r in results],
            'optimization_history': self.optimization_history
        }
        
        with open(self.output_dir / 'model_selection_results.json', 'w') as f:
            json.dump(results_dict, f, indent=2, default=str)
        
        # Save leaderboard as CSV
        if hasattr(self, 'leaderboard'):
            self.leaderboard.to_csv(
                self.output_dir / 'leaderboard.csv', 
                index=False
            )
        
        print(f"\nğŸ’¾ çµæœå·²ä¿å­˜è‡³: {self.output_dir}")
    
    def get_top_models_for_mcmc(self, top_k: int = 3) -> List[Dict[str, Any]]:
        """
        ç²å–éœ€è¦MCMCé©—è­‰çš„é ‚å°–æ¨¡å‹
        
        Parameters:
        -----------
        top_k : int
            è¿”å›å‰kå€‹æ¨¡å‹
            
        Returns:
        --------
        List[Dict[str, Any]]
            é ‚å°–æ¨¡å‹é…ç½®
        """
        if not self.full_results:
            raise ValueError("éœ€è¦å…ˆé‹è¡Œæ¨¡å‹é¸æ“‡")
        
        top_models = []
        for result in self.full_results[:top_k]:
            config = {
                'model_id': result.model.model_id,
                'model_config': result.model,
                'best_hyperparams': result.best_hyperparams,
                'vi_score': result.best_score,
                'posterior_init': result.posterior_samples  # VIçµæœä½œç‚ºMCMCåˆå§‹åŒ–
            }
            top_models.append(config)
        
        return top_models
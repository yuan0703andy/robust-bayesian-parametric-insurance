#!/usr/bin/env python3
"""
Basis-Risk-Aware VI Module
åŸºå·®é¢¨éšªå°å‘è®Šåˆ†æ¨æ–·æ¨¡çµ„

å¯é‡è¤‡ä½¿ç”¨çš„æ¨¡çµ„åŒ–çµ„ä»¶ï¼š
- DifferentiableCRPS: å¯å¾®åˆ†CRPSè¨ˆç®—
- ParametricPayoutFunction: åƒæ•¸å‹ä¿éšªè³ ä»˜å‡½æ•¸
- EpsilonContaminationModel: Îµ-contaminationæ¨¡å‹
- BasisRiskAwareVI: åŸºå·®é¢¨éšªå°å‘VIè¨“ç·´å™¨

Author: Research Team
Date: 2025-01-17
"""

import numpy as np
from typing import Dict, List, Optional, Tuple, Callable
import warnings
warnings.filterwarnings('ignore')

# Try importing PyTorch for differentiable operations
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.distributions import Normal
    TORCH_AVAILABLE = True
    # Type hints for when torch is available
    TorchTensor = torch.Tensor
    TorchOptimizer = torch.optim.Optimizer
except ImportError:
    TORCH_AVAILABLE = False
    # Dummy type hints when torch is not available
    TorchTensor = "torch.Tensor"
    TorchOptimizer = "torch.optim.Optimizer"


class DifferentiableCRPS:
    """å¯å¾®åˆ†çš„ CRPS è¨ˆç®—å™¨ï¼Œé©ç”¨æ–¼æ¢¯åº¦ä¸‹é™"""
    
    @staticmethod
    def crps_gaussian(y_true: np.ndarray, mu: np.ndarray, sigma: np.ndarray) -> np.ndarray:
        """
        å¯å¾®åˆ†çš„ Gaussian CRPS è¨ˆç®—
        
        Args:
            y_true: å¯¦éš›è§€æ¸¬å€¼
            mu: é æ¸¬åˆ†å¸ƒçš„å‡å€¼  
            sigma: é æ¸¬åˆ†å¸ƒçš„æ¨™æº–å·®
            
        Returns:
            CRPS åˆ†æ•¸
        """
        if TORCH_AVAILABLE and isinstance(mu, torch.Tensor):
            # PyTorch ç‰ˆæœ¬
            z = (y_true - mu) / sigma
            crps = sigma * (z * (2 * torch.distributions.Normal(0, 1).cdf(z) - 1) + 
                           2 * torch.distributions.Normal(0, 1).log_prob(z).exp() - 
                           1 / np.sqrt(np.pi))
        else:
            # NumPy ç‰ˆæœ¬
            from scipy.stats import norm
            z = (y_true - mu) / sigma
            crps = sigma * (z * (2 * norm.cdf(z) - 1) + 
                           2 * norm.pdf(z) - 
                           1 / np.sqrt(np.pi))
        
        return crps
    
    @staticmethod
    def crps_ensemble(y_true: np.ndarray, forecast_samples: np.ndarray) -> np.ndarray:
        """
        åŸºæ–¼ ensemble çš„ CRPS è¨ˆç®— (å¯å¾®åˆ†è¿‘ä¼¼)
        
        Args:
            y_true: å¯¦éš›è§€æ¸¬å€¼ [N]
            forecast_samples: é æ¸¬æ¨£æœ¬ [N, M] (Nå€‹è§€æ¸¬ï¼ŒMå€‹æ¨£æœ¬)
            
        Returns:
            CRPS åˆ†æ•¸ [N]
        """
        if TORCH_AVAILABLE and isinstance(forecast_samples, torch.Tensor):
            # PyTorch ç‰ˆæœ¬
            N, M = forecast_samples.shape
            
            # è¨ˆç®—ç¶“é©— CDF
            sorted_forecasts, _ = torch.sort(forecast_samples, dim=1)
            
            # CRPS è¿‘ä¼¼è¨ˆç®—
            crps_scores = []
            for i in range(N):
                y = y_true[i]
                forecasts = sorted_forecasts[i]
                
                # CRPS è¿‘ä¼¼
                crps = torch.mean(torch.abs(forecasts - y)) - 0.5 * torch.mean(
                    torch.abs(forecasts[:, None] - forecasts[None, :])
                )
                crps_scores.append(crps)
                
            return torch.stack(crps_scores)
        else:
            # NumPy ç‰ˆæœ¬ - ä½¿ç”¨ properscoring æˆ–ç°¡å–®è¿‘ä¼¼
            crps_scores = []
            for i in range(len(y_true)):
                y = y_true[i]
                forecasts = forecast_samples[i]
                # ç°¡å–® CRPS è¿‘ä¼¼
                crps = np.mean(np.abs(forecasts - y)) - 0.5 * np.mean(
                    np.abs(forecasts[:, None] - forecasts[None, :])
                )
                crps_scores.append(crps)
            return np.array(crps_scores)


class ParametricPayoutFunction:
    """åƒæ•¸å‹ä¿éšªè³ ä»˜å‡½æ•¸"""
    
    def __init__(self, 
                 trigger_thresholds: List[float] = None,
                 payout_amounts: List[float] = None,
                 max_payout: float = 10000):
        """
        åˆå§‹åŒ–åƒæ•¸å‹è³ ä»˜å‡½æ•¸
        
        Args:
            trigger_thresholds: è§¸ç™¼é–¾å€¼ 
            payout_amounts: å°æ‡‰è³ ä»˜é‡‘é¡
            max_payout: æœ€å¤§è³ ä»˜
        """
        if trigger_thresholds is None:
            trigger_thresholds = [75, 85, 95]
        if payout_amounts is None:
            payout_amounts = [1000, 5000, 10000]
            
        self.trigger_thresholds = np.array(trigger_thresholds)
        self.payout_amounts = np.array(payout_amounts)
        self.max_payout = max_payout
    
    def calculate_payout_distribution(self, 
                                    loss_samples: np.ndarray) -> np.ndarray:
        """
        åŸºæ–¼æå¤±åˆ†å¸ƒæ¨£æœ¬è¨ˆç®—è³ ä»˜åˆ†å¸ƒ
        
        Args:
            loss_samples: æå¤±åˆ†å¸ƒæ¨£æœ¬ [N, M]
            
        Returns:
            è³ ä»˜åˆ†å¸ƒæ¨£æœ¬ [N, M]
        """
        if len(loss_samples.shape) == 1:
            loss_samples = loss_samples.reshape(-1, 1)
            
        N, M = loss_samples.shape
        payout_samples = np.zeros_like(loss_samples)
        
        for i in range(N):
            for j in range(M):
                loss = loss_samples[i, j]
                
                # éšæ¢¯å¼è³ ä»˜é‚è¼¯
                payout = 0
                for k, threshold in enumerate(self.trigger_thresholds):
                    if loss >= threshold:
                        payout = self.payout_amounts[k]
                
                payout_samples[i, j] = min(payout, self.max_payout)
        
        return payout_samples
    
    def optimize_for_basis_risk(self, losses: np.ndarray, features: np.ndarray,
                               basis_risk_type: str = 'weighted') -> Dict:
        """
        å„ªåŒ–è§¸ç™¼åƒæ•¸ä»¥æœ€å°åŒ–åŸºå·®é¢¨éšª
        
        Args:
            losses: å¯¦éš›æå¤±
            features: ç‰¹å¾µæ•¸æ“š
            basis_risk_type: åŸºå·®é¢¨éšªé¡å‹
            
        Returns:
            å„ªåŒ–å¾Œçš„åƒæ•¸
        """
        feature_values = features.flatten()
        
        # ç¶²æ ¼æœç´¢
        trigger_candidates = np.percentile(feature_values, [60, 70, 75, 80, 85, 90, 95])
        payout_multipliers = [0.5, 0.75, 1.0, 1.25, 1.5, 2.0]
        
        best_config = None
        best_risk = np.inf
        
        for trigger in trigger_candidates:
            for multiplier in payout_multipliers:
                max_payout = np.mean(losses[losses > 0]) * multiplier if np.any(losses > 0) else np.mean(losses) * multiplier
                payouts = np.where(feature_values >= trigger, max_payout, 0)
                
                # è¨ˆç®—åŸºå·®é¢¨éšª
                if basis_risk_type == 'asymmetric':
                    risk = np.mean(np.maximum(0, losses - payouts))
                elif basis_risk_type == 'weighted':
                    under_penalty = np.maximum(0, losses - payouts) * 2.0
                    over_penalty = np.maximum(0, payouts - losses) * 0.5
                    risk = np.mean(under_penalty + over_penalty)
                else:  # absolute
                    risk = np.mean(np.abs(losses - payouts))
                
                if risk < best_risk:
                    best_risk = risk
                    best_config = {
                        'trigger': trigger,
                        'max_payout': max_payout,
                        'multiplier': multiplier,
                        'basis_risk': risk
                    }
        
        return best_config


class EpsilonContaminationModel:
    """Îµ-contamination æ¨¡å‹"""
    
    def __init__(self, epsilon: float = 0.1):
        """
        åˆå§‹åŒ– Îµ-contamination æ¨¡å‹
        
        Args:
            epsilon: æ±¡æŸ“æ¯”ä¾‹
        """
        self.epsilon = epsilon
    
    def predict_distribution(self, 
                           theta: np.ndarray, 
                           X: np.ndarray,
                           n_samples: int = 100) -> np.ndarray:
        """
        åŸºæ–¼åƒæ•¸ Î¸ å’Œè¼¸å…¥ X é æ¸¬æå¤±åˆ†å¸ƒ
        
        Args:
            theta: æ¨¡å‹åƒæ•¸
            X: è¼¸å…¥ç‰¹å¾µ [N, d]
            n_samples: æ¯å€‹é æ¸¬é»çš„æ¨£æœ¬æ•¸
            
        Returns:
            æå¤±åˆ†å¸ƒæ¨£æœ¬ [N, n_samples]
        """
        N = X.shape[0]
        
        # åŸºæœ¬é æ¸¬ (ç·šæ€§æ¨¡å‹ç¤ºä¾‹)
        if len(theta) >= X.shape[1]:
            linear_pred = X @ theta[:X.shape[1]]
        else:
            # ç°¡åŒ–ï¼šä½¿ç”¨å‡å€¼
            linear_pred = np.ones(N) * np.mean(theta)
        
        # Îµ-contamination: (1-Îµ) Ã— Normal + Îµ Ã— Heavy-tail
        samples = np.zeros((N, n_samples))
        
        for i in range(N):
            # ä¸»è¦åˆ†å¸ƒ (Normal)
            n_main = int((1-self.epsilon) * n_samples)
            main_samples = np.random.normal(linear_pred[i], abs(theta[-1]) if len(theta) > 1 else 1.0, n_main)
            
            # æ±¡æŸ“åˆ†å¸ƒ (Heavy-tail)
            n_contam = n_samples - n_main
            contamination_samples = np.random.exponential(abs(linear_pred[i]) * 2, n_contam)
            
            # æ··åˆ
            all_samples = np.concatenate([main_samples, contamination_samples])
            np.random.shuffle(all_samples)
            samples[i] = all_samples[:n_samples]
        
        return np.abs(samples)  # ç¢ºä¿æå¤±ç‚ºæ­£


if TORCH_AVAILABLE:
    class VariationalPosterior(nn.Module):
        """è®Šåˆ†å¾Œé©—åˆ†å¸ƒ q_Ï†(Î¸)"""
        
        def __init__(self, n_params: int, n_features: int):
            super().__init__()
            
            # è®Šåˆ†åƒæ•¸: å‡å€¼å’Œå°æ•¸æ¨™æº–å·®
            self.mu_net = nn.Sequential(
                nn.Linear(n_features, 64),
                nn.ReLU(),
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.Linear(32, n_params)
            )
            
            self.logvar_net = nn.Sequential(
                nn.Linear(n_features, 64),
                nn.ReLU(),
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.Linear(32, n_params)
            )
        
        def forward(self, x):
            """å‰å‘å‚³æ’­"""
            mu = self.mu_net(x)
            logvar = self.logvar_net(x)
            return mu, logvar
        
        def sample(self, x, n_samples: int = 10):
            """ä½¿ç”¨ reparameterization trick æ¡æ¨£"""
            mu, logvar = self.forward(x)
            std = torch.exp(0.5 * logvar)
            
            eps = torch.randn(n_samples, *mu.shape)
            samples = mu + eps * std
            
            return samples, mu, logvar


class BasisRiskAwareVI:
    """åŸºå·®é¢¨éšªå°å‘çš„è®Šåˆ†æ¨æ–· - GPUåŠ é€Ÿç‰ˆæœ¬"""
    
    def __init__(self, 
                 n_features: int,
                 epsilon_values: List[float] = None,
                 basis_risk_types: List[str] = None,
                 use_gpu: bool = True):
        """
        åˆå§‹åŒ–åŸºå·®é¢¨éšªå°å‘ VI
        
        Args:
            n_features: ç‰¹å¾µç¶­åº¦
            epsilon_values: Îµ-contamination åƒæ•¸å€™é¸
            basis_risk_types: åŸºå·®é¢¨éšªé¡å‹
            use_gpu: æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ
        """
        if epsilon_values is None:
            epsilon_values = [0.0, 0.05, 0.10, 0.15, 0.20]
        if basis_risk_types is None:
            basis_risk_types = ['absolute', 'asymmetric', 'weighted']
            
        self.n_features = n_features
        self.n_params = n_features + 1  # ç·šæ€§ä¿‚æ•¸ + å™ªéŸ³åƒæ•¸
        self.epsilon_values = epsilon_values
        self.basis_risk_types = basis_risk_types
        
        # GPUé…ç½®
        self.use_gpu = use_gpu and TORCH_AVAILABLE
        if self.use_gpu:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            if self.device.type == 'cpu':
                print("âš ï¸ GPUä¸å¯ç”¨ï¼Œé™ç´šåˆ°CPU")
                self.use_gpu = False
        else:
            self.device = torch.device('cpu')
            
        print(f"ğŸ”§ BasisRiskAwareVIåˆå§‹åŒ–: {'GPU' if self.use_gpu else 'CPU'}æ¨¡å¼")
        if self.use_gpu:
            print(f"   GPUè¨­å‚™: {torch.cuda.get_device_name(self.device)}")
        
        # è³ ä»˜å‡½æ•¸
        self.payout_function = ParametricPayoutFunction()
        
        # CRPS è¨ˆç®—å™¨
        self.crps_calculator = DifferentiableCRPS()
        
        # å­˜å„²çµæœ
        self.vi_results = {}
    
    def compute_basis_risk(self, y_true: np.ndarray, payout_samples: np.ndarray,
                          basis_risk_type: str = 'weighted') -> float:
        """
        è¨ˆç®—åŸºå·®é¢¨éšª
        
        Args:
            y_true: çœŸå¯¦æå¤±
            payout_samples: è³ ä»˜æ¨£æœ¬
            basis_risk_type: åŸºå·®é¢¨éšªé¡å‹
            
        Returns:
            åŸºå·®é¢¨éšªå€¼
        """
        if len(payout_samples.shape) > 1:
            payout_mean = payout_samples.mean(1)
        else:
            payout_mean = payout_samples
            
        if basis_risk_type == 'asymmetric':
            # åªæ‡²ç½°è³ ä¸å¤ çš„æƒ…æ³
            basis_risk = np.mean(np.maximum(0, y_true - payout_mean))
        elif basis_risk_type == 'weighted':
            # åŠ æ¬Šä¸å°ç¨±æ‡²ç½°
            under_penalty = np.maximum(0, y_true - payout_mean) * 2.0
            over_penalty = np.maximum(0, payout_mean - y_true) * 0.5
            basis_risk = np.mean(under_penalty + over_penalty)
        else:  # absolute
            basis_risk = np.mean(np.abs(y_true - payout_mean))
        
        return basis_risk
    
    def train_single_model(self,
                         X: np.ndarray,
                         y: np.ndarray,
                         epsilon: float,
                         basis_risk_type: str = 'weighted',
                         n_iterations: int = 1000,
                         X_val: np.ndarray = None,
                         y_val: np.ndarray = None) -> Dict:
        """
        è¨“ç·´å–®å€‹ Îµ-contamination æ¨¡å‹ - çœŸæ­£çš„GPUåŠ é€ŸVIå¯¦ç¾
        
        Args:
            X: è¼¸å…¥ç‰¹å¾µ [N, d]
            y: çœŸå¯¦æå¤± [N]
            epsilon: Îµ-contamination åƒæ•¸
            basis_risk_type: åŸºå·®é¢¨éšªé¡å‹
            n_iterations: è¨“ç·´è¿­ä»£æ¬¡æ•¸
            
        Returns:
            è¨“ç·´çµæœå­—å…¸
        """
        import time
        start_time = time.time()
        
        print(f"      é–‹å§‹è¨“ç·´ Îµ={epsilon:.3f}, åŸºå·®={basis_risk_type} (è¿­ä»£={n_iterations})")
        
        if self.use_gpu and TORCH_AVAILABLE:
            return self._train_single_model_gpu(X, y, epsilon, basis_risk_type, n_iterations, start_time, X_val, y_val)
        else:
            return self._train_single_model_cpu(X, y, epsilon, basis_risk_type, n_iterations, start_time, X_val, y_val)
    
    def _train_single_model_gpu(self, X: np.ndarray, y: np.ndarray, epsilon: float, 
                               basis_risk_type: str, n_iterations: int, start_time: float,
                               X_val: np.ndarray = None, y_val: np.ndarray = None) -> Dict:
        """GPUåŠ é€Ÿçš„VIè¨“ç·´"""
        import time
        
        # è½‰æ›ç‚ºGPUå¼µé‡
        X_tensor = torch.from_numpy(X).float().to(self.device)
        y_tensor = torch.from_numpy(y).float().to(self.device)
        
        # é©—è­‰é›†å¼µé‡ï¼ˆå¦‚æœæä¾›ï¼‰
        has_validation = X_val is not None and y_val is not None
        if has_validation:
            X_val_tensor = torch.from_numpy(X_val).float().to(self.device)
            y_val_tensor = torch.from_numpy(y_val).float().to(self.device)
            print(f"        ğŸ“Š ä½¿ç”¨é©—è­‰é›†ç›£ç£: è¨“ç·´={X.shape[0]}, é©—è­‰={X_val.shape[0]}")
        else:
            print(f"        âš ï¸ ç„¡é©—è­‰é›†ï¼Œå¯èƒ½éåº¦æ“¬åˆ")
        
        # è®Šåˆ†åƒæ•¸ (åœ¨GPUä¸Š)
        torch.manual_seed(42 + int(epsilon*1000))
        mu_theta = torch.randn(self.n_params, device=self.device) * 0.1
        log_sigma_theta = torch.full((self.n_params,), -2.0, device=self.device)
        
        # è¨­ç‚ºå¯æ±‚å°
        mu_theta.requires_grad_(True)
        log_sigma_theta.requires_grad_(True)
        
        # Adamå„ªåŒ–å™¨
        optimizer = torch.optim.Adam([mu_theta, log_sigma_theta], lr=0.01)
        
        best_elbo = -float('inf')
        best_basis_risk_train = float('inf')
        best_basis_risk_val = float('inf')
        best_mu = mu_theta.clone()
        best_log_sigma = log_sigma_theta.clone()
        
        # Early stoppingç›£æ§
        patience = 100
        no_improve_count = 0
        validation_history = []
        
        n_samples_per_iteration = 10
        
        print(f"        ğŸš€ GPUå¼µé‡è¨ˆç®—é–‹å§‹...")
        
        for iteration in range(n_iterations):
            optimizer.zero_grad()
            
            # 1. å¾è®Šåˆ†åˆ†å¸ƒæ¡æ¨£ (GPU)
            sigma_theta = torch.exp(log_sigma_theta)
            eps = torch.randn(n_samples_per_iteration, self.n_params, device=self.device)
            theta_samples = mu_theta.unsqueeze(0) + sigma_theta.unsqueeze(0) * eps  # [n_samples, n_params]
            
            # 2. æ‰¹æ¬¡è¨ˆç®—ELBO (å®Œå…¨GPUä¸¦è¡Œ)
            elbo_batch = self._compute_elbo_batch_gpu(
                X_tensor, y_tensor, theta_samples, epsilon, basis_risk_type, mu_theta, sigma_theta
            )
            
            # 3. åå‘å‚³æ’­
            loss = -elbo_batch.mean()  # æœ€å¤§åŒ–ELBO = æœ€å°åŒ–è² ELBO
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªé¿å…çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_([mu_theta, log_sigma_theta], max_norm=1.0)
            
            optimizer.step()
            
            # ç´„æŸlog_sigmaç¯„åœ
            with torch.no_grad():
                log_sigma_theta.clamp_(-5, 1)
            
            # è¨ˆç®—ç•¶å‰åŸºå·®é¢¨éšªç”¨æ–¼è¨˜éŒ„  
            with torch.no_grad():
                # è¨“ç·´é›†åŸºå·®é¢¨éšª
                current_basis_risk_train = self._compute_basis_risk_batch_gpu(
                    X_tensor, y_tensor, theta_samples, epsilon, basis_risk_type
                ).mean().item()
                
                # é©—è­‰é›†åŸºå·®é¢¨éšªï¼ˆå¦‚æœæœ‰ï¼‰
                if has_validation:
                    current_basis_risk_val = self._compute_basis_risk_batch_gpu(
                        X_val_tensor, y_val_tensor, theta_samples, epsilon, basis_risk_type
                    ).mean().item()
                    validation_history.append(current_basis_risk_val)
                else:
                    current_basis_risk_val = current_basis_risk_train
                
                current_elbo = elbo_batch.mean().item()
                
                # *** é—œéµä¿®æ­£ï¼šä½¿ç”¨é©—è­‰é›†é¸æ“‡æœ€ä½³æ¨¡å‹ ***
                if has_validation:
                    # å¦‚æœæœ‰é©—è­‰é›†ï¼Œä»¥é©—è­‰é›†åŸºå·®é¢¨éšªç‚ºæº–
                    if current_basis_risk_val < best_basis_risk_val:
                        best_elbo = current_elbo
                        best_basis_risk_train = current_basis_risk_train
                        best_basis_risk_val = current_basis_risk_val
                        best_mu = mu_theta.clone()
                        best_log_sigma = log_sigma_theta.clone()
                        no_improve_count = 0
                    else:
                        no_improve_count += 1
                else:
                    # ç„¡é©—è­‰é›†æ™‚æ‰ç”¨è¨“ç·´é›†
                    if current_elbo > best_elbo:
                        best_elbo = current_elbo
                        best_basis_risk_train = current_basis_risk_train
                        best_basis_risk_val = current_basis_risk_val
                        best_mu = mu_theta.clone()
                        best_log_sigma = log_sigma_theta.clone()
            
            # Early stopping
            if has_validation and no_improve_count >= patience:
                print(f"        ğŸ›‘ Early stopping: é©—è­‰é›†{patience}æ¬¡ç„¡æ”¹å–„")
                break
            
            # é€²åº¦å ±å‘Š
            if (iteration + 1) % 200 == 0:
                if has_validation:
                    print(f"        è¿­ä»£ {iteration+1}: ELBO={current_elbo:.3f}, è¨“ç·´={current_basis_risk_train/1e6:.1f}M, é©—è­‰={current_basis_risk_val/1e6:.1f}M")
                else:
                    print(f"        è¿­ä»£ {iteration+1}: ELBO={current_elbo:.3f}, åŸºå·®é¢¨éšª={current_basis_risk_train/1e6:.1f}M")
        
        training_time = time.time() - start_time
        
        # è½‰æ›å›CPU NumPyç”¨æ–¼è¿”å›
        final_mu = best_mu.detach().cpu().numpy()
        final_sigma = torch.exp(best_log_sigma).detach().cpu().numpy()
        
        if has_validation:
            print(f"      âœ… GPUè¨“ç·´å®Œæˆ: {training_time:.1f}s, ELBO={best_elbo:.3f}")
            print(f"        è¨“ç·´åŸºå·®é¢¨éšª: {best_basis_risk_train/1e6:.1f}M, é©—è­‰åŸºå·®é¢¨éšª: {best_basis_risk_val/1e6:.1f}M")
            print(f"        è¨“ç·´/é©—è­‰æ¯”ç‡: {best_basis_risk_train/best_basis_risk_val:.3f}")
        else:
            print(f"      âœ… GPUè¨“ç·´å®Œæˆ: {training_time:.1f}s, ELBO={best_elbo:.3f}, åŸºå·®é¢¨éšª={best_basis_risk_train/1e6:.1f}M")
        
        return {
            'epsilon': epsilon,
            'basis_risk_type': basis_risk_type,
            'final_basis_risk': best_basis_risk_val if has_validation else best_basis_risk_train,
            'train_basis_risk': best_basis_risk_train,
            'val_basis_risk': best_basis_risk_val,
            'train_val_ratio': best_basis_risk_train / best_basis_risk_val if has_validation else 1.0,
            'best_theta': final_mu,
            'theta_uncertainty': final_sigma,
            'elbo': best_elbo,
            'converged': True,
            'training_time': training_time,
            'has_validation': has_validation
        }
    
    def _compute_elbo_batch_gpu(self, X_tensor, y_tensor, theta_samples, epsilon, 
                               basis_risk_type, mu_theta, sigma_theta):
        """GPUä¸Šæ‰¹æ¬¡è¨ˆç®—ELBO"""
        batch_size = theta_samples.shape[0]  # n_samples_per_iteration
        
        # æ‰¹æ¬¡è¨ˆç®—åŸºå·®é¢¨éšª (ä¼¼ç„¶é …)
        basis_risks = self._compute_basis_risk_batch_gpu(
            X_tensor, y_tensor, theta_samples, epsilon, basis_risk_type
        )
        log_likelihood = -basis_risks / 1e9  # æ¨™æº–åŒ–
        
        # å…ˆé©—é … (æ¨™æº–é«˜æ–¯)
        log_prior = -0.5 * torch.sum(theta_samples**2, dim=1)
        
        # è®Šåˆ†åˆ†å¸ƒlogå¯†åº¦
        log_q = -0.5 * torch.sum((theta_samples - mu_theta)**2 / sigma_theta**2, dim=1) - \
                0.5 * torch.sum(torch.log(2 * np.pi * sigma_theta**2))
        
        # ELBO = E[log p(y|Î¸)] + E[log p(Î¸)] - E[log q(Î¸)]
        elbo = log_likelihood + log_prior - log_q
        
        return elbo
    
    def _compute_basis_risk_batch_gpu(self, X_tensor, y_tensor, theta_samples, epsilon, basis_risk_type):
        """GPUä¸Šæ‰¹æ¬¡è¨ˆç®—åŸºå·®é¢¨éšª - ä¿®æ­£ç‰ˆï¼šÎ¸åƒæ•¸çœŸæ­£å½±éŸ¿è¨ˆç®—"""
        batch_size = theta_samples.shape[0]
        n_data = X_tensor.shape[0]
        
        # epsilon contamination
        if epsilon > 0:
            noise = torch.randn_like(y_tensor.unsqueeze(0).expand(batch_size, -1)) * epsilon * y_tensor.mean()
            y_perturbed = y_tensor.unsqueeze(0).expand(batch_size, -1) + noise
        else:
            y_perturbed = y_tensor.unsqueeze(0).expand(batch_size, -1)
        
        # *** é—œéµä¿®æ­£ï¼šè®“Î¸åƒæ•¸å½±éŸ¿è³ ä»˜è¨ˆç®— ***
        wind_speeds = X_tensor.squeeze(-1)  # [n_data]
        wind_speeds = wind_speeds.unsqueeze(0).expand(batch_size, -1)  # [batch_size, n_data]
        
        # ä½¿ç”¨Î¸åƒæ•¸èª¿æ•´é–¾å€¼å’Œè³ ä»˜æ¯”ä¾‹
        # theta_samples: [batch_size, n_params], å…¶ä¸­n_params=2 (slope + intercept)
        theta_slope = theta_samples[:, 0:1]      # [batch_size, 1] - é–¾å€¼æ–œç‡
        theta_intercept = theta_samples[:, 1:2]  # [batch_size, 1] - åŸºç¤é–¾å€¼
        
        # å‹•æ…‹é–¾å€¼ï¼šå—Î¸å½±éŸ¿
        base_thresholds = torch.tensor([25.0, 35.0, 45.0], device=self.device)
        # å»£æ’­åˆ° [batch_size, 3]
        dynamic_thresholds = (base_thresholds.unsqueeze(0) + 
                            theta_intercept * 10.0 +  # interceptå½±éŸ¿åŸºç¤é–¾å€¼
                            theta_slope * torch.arange(3, device=self.device).float())  # slopeå½±éŸ¿é–“éš”
        
        # å‹•æ…‹è³ ä»˜æ¯”ä¾‹ï¼šå—Î¸å½±éŸ¿  
        base_ratios = torch.tensor([0.25, 0.5, 1.0], device=self.device)
        dynamic_ratios = torch.sigmoid(base_ratios.unsqueeze(0) + theta_slope * 2.0)  # [batch_size, 3]
        
        # å‹•æ…‹æœ€å¤§è³ ä»˜
        max_payout_base = y_tensor.mean()
        max_payout = max_payout_base * torch.exp(theta_intercept).squeeze(-1)  # [batch_size]
        
        # è¨ˆç®—è³ ä»˜ï¼ˆç¾åœ¨å—Î¸å½±éŸ¿ï¼‰
        payouts = torch.zeros_like(y_perturbed)  # [batch_size, n_data]
        
        for i in range(3):
            # å°æ¯å€‹æ‰¹æ¬¡æ¨£æœ¬ï¼Œä½¿ç”¨ä¸åŒçš„é–¾å€¼å’Œè³ ä»˜æ¯”ä¾‹
            threshold_batch = dynamic_thresholds[:, i:i+1]  # [batch_size, 1]
            ratio_batch = dynamic_ratios[:, i:i+1]          # [batch_size, 1]
            max_payout_batch = max_payout[:, None]          # [batch_size, 1]
            
            mask = wind_speeds >= threshold_batch  # [batch_size, n_data]
            payout_value = max_payout_batch * ratio_batch  # [batch_size, 1]
            payouts = torch.where(mask, payout_value, payouts)
        
        # è¨ˆç®—åŸºå·®é¢¨éšª
        if basis_risk_type == 'absolute':
            basis_risk = torch.mean(torch.abs(y_perturbed - payouts), dim=1)
        elif basis_risk_type == 'asymmetric':
            # Asymmetric: æ‡²ç½°under-paymentæ›´é‡ (2:1)
            under_penalty = torch.mean(torch.relu(y_perturbed - payouts), dim=1)
            over_penalty = torch.mean(torch.relu(payouts - y_perturbed), dim=1)
            basis_risk = 2.0 * under_penalty + over_penalty
        else:  # weighted
            # Weighted: æ ¹æ“šæå¤±å¤§å°èª¿æ•´æ‡²ç½° (å¤§æå¤±æ‡²ç½°æ›´é‡)
            diff = y_perturbed - payouts
            weights = torch.abs(y_perturbed) / torch.mean(torch.abs(y_perturbed), dim=1, keepdim=True)
            weighted_diff = diff * weights
            basis_risk = torch.mean(torch.abs(weighted_diff), dim=1)
        
        return basis_risk
    
    def _train_single_model_cpu(self, X: np.ndarray, y: np.ndarray, epsilon: float, 
                               basis_risk_type: str, n_iterations: int, start_time: float,
                               X_val: np.ndarray = None, y_val: np.ndarray = None) -> Dict:
        """CPUç‰ˆæœ¬çš„VIè¨“ç·´ï¼ˆåŸå§‹å¯¦ç¾ï¼‰"""
        # çœŸæ­£çš„VIï¼šä¼°è¨ˆåƒæ•¸åˆ†å¸ƒçš„è®Šåˆ†åƒæ•¸
        model = EpsilonContaminationModel(epsilon)
        
        # è®Šåˆ†åƒæ•¸ï¼šå‡å€¼å’Œå°æ•¸æ–¹å·®
        np.random.seed(42 + int(epsilon*1000))  # ä¸åŒepsilonç”¨ä¸åŒç¨®å­
        mu_theta = np.random.randn(self.n_params) * 0.1
        log_sigma_theta = np.full(self.n_params, -2.0)  # åˆå§‹æ–¹å·®è¼ƒå°
        
        best_elbo = -np.inf
        best_basis_risk = np.inf
        best_mu = mu_theta.copy()
        best_log_sigma = log_sigma_theta.copy()
        
        learning_rate = 0.01
        n_samples_per_iteration = 10
        
        # çœŸæ­£çš„VIå„ªåŒ–å¾ªç’°
        for iteration in range(n_iterations):
            # 1. å¾è®Šåˆ†åˆ†å¸ƒä¸­æ¡æ¨£åƒæ•¸
            sigma_theta = np.exp(log_sigma_theta)
            theta_samples = []
            
            for _ in range(n_samples_per_iteration):
                theta_sample = mu_theta + sigma_theta * np.random.randn(self.n_params)
                theta_samples.append(theta_sample)
            
            # 2. è¨ˆç®—ELBOåŠå…¶æ¢¯åº¦
            elbo_total = 0
            mu_grad = np.zeros_like(mu_theta)
            log_sigma_grad = np.zeros_like(log_sigma_theta)
            total_basis_risk = 0
            
            for theta in theta_samples:
                # é æ¸¬åˆ†å¸ƒ
                loss_samples = model.predict_distribution(theta, X, 50)
                payout_samples = self.payout_function.calculate_payout_distribution(loss_samples)
                
                # è¨ˆç®—ä¼¼ç„¶ (è² åŸºå·®é¢¨éšªä½œç‚ºä¼¼ç„¶)
                basis_risk = self.compute_basis_risk(y, payout_samples, basis_risk_type)
                log_likelihood = -basis_risk / 1e9  # æ¨™æº–åŒ–
                
                # å…ˆé©— (æ¨™æº–é«˜æ–¯)
                log_prior = -0.5 * np.sum(theta**2)
                
                # è®Šåˆ†åˆ†å¸ƒç†µ
                log_q = -0.5 * np.sum((theta - mu_theta)**2 / sigma_theta**2) - \
                        0.5 * np.sum(np.log(2 * np.pi * sigma_theta**2))
                
                # ELBO = E[log p(y|Î¸)] + E[log p(Î¸)] - E[log q(Î¸)]
                elbo = log_likelihood + log_prior - log_q
                elbo_total += elbo
                total_basis_risk += basis_risk
                
                # æ¢¯åº¦ä¼°è¨ˆ (REINFORCE-style)
                if elbo > -1e6:  # é¿å…æ•¸å€¼ä¸ç©©å®š
                    reward = elbo + 1e6  # åç§»ç¢ºä¿æ­£æ•¸
                    score_mu = (theta - mu_theta) / sigma_theta**2
                    score_log_sigma = 0.5 * (((theta - mu_theta)/sigma_theta)**2 - 1)
                    
                    mu_grad += reward * score_mu
                    log_sigma_grad += reward * score_log_sigma
            
            # å¹³å‡æ¢¯åº¦
            mu_grad /= n_samples_per_iteration
            log_sigma_grad /= n_samples_per_iteration
            elbo_total /= n_samples_per_iteration
            avg_basis_risk = total_basis_risk / n_samples_per_iteration
            
            # 3. åƒæ•¸æ›´æ–° (Adam-like)
            momentum = 0.9 if iteration > 0 else 0
            if iteration == 0:
                mu_velocity = np.zeros_like(mu_theta)
                log_sigma_velocity = np.zeros_like(log_sigma_theta)
            
            mu_velocity = momentum * mu_velocity + (1-momentum) * mu_grad
            log_sigma_velocity = momentum * log_sigma_velocity + (1-momentum) * log_sigma_grad
            
            # è‡ªé©æ‡‰å­¸ç¿’ç‡
            current_lr = learning_rate / (1 + iteration / 500)
            
            mu_theta += current_lr * mu_velocity
            log_sigma_theta += current_lr * 0.5 * log_sigma_velocity  # æ–¹å·®æ›´æ–°è¼ƒæ…¢
            
            # é˜²æ­¢æ–¹å·®éå°æˆ–éå¤§
            log_sigma_theta = np.clip(log_sigma_theta, -5, 1)
            
            # æ›´æ–°æœ€ä½³çµæœ
            if elbo_total > best_elbo:
                best_elbo = elbo_total
                best_basis_risk = avg_basis_risk
                best_mu = mu_theta.copy()
                best_log_sigma = log_sigma_theta.copy()
            
            # é€²åº¦å ±å‘Š
            if (iteration + 1) % 200 == 0:
                print(f"        è¿­ä»£ {iteration+1}: ELBO={elbo_total:.3f}, åŸºå·®é¢¨éšª={avg_basis_risk/1e6:.1f}M")
        
        training_time = time.time() - start_time
        final_sigma = np.exp(best_log_sigma)
        
        print(f"      âœ… è¨“ç·´å®Œæˆ: {training_time:.1f}s, ELBO={best_elbo:.3f}, åŸºå·®é¢¨éšª={best_basis_risk/1e6:.1f}M")
        
        return {
            'epsilon': epsilon,
            'basis_risk_type': basis_risk_type,
            'final_basis_risk': best_basis_risk,
            'best_theta': best_mu,
            'theta_uncertainty': final_sigma,
            'elbo': best_elbo,
            'converged': True,
            'training_time': training_time
        }
    
    def run_comprehensive_screening(self, X: np.ndarray, y: np.ndarray, 
                                   X_val: np.ndarray = None, y_val: np.ndarray = None) -> Dict:
        """
        åŸ·è¡Œå…¨é¢çš„ VI ç¯©é¸ - GPUåŠ é€Ÿç‰ˆæœ¬
        
        Args:
            X: è¼¸å…¥ç‰¹å¾µ
            y: çœŸå¯¦æå¤±
            
        Returns:
            ç¯©é¸çµæœ
        """
        if self.use_gpu:
            return self._gpu_screening(X, y, X_val, y_val)
        else:
            return self._cpu_screening(X, y, X_val, y_val)
    
    def _gpu_screening(self, X: np.ndarray, y: np.ndarray, 
                      X_val: np.ndarray = None, y_val: np.ndarray = None) -> Dict:
        """GPUåŠ é€Ÿçš„VIç¯©é¸ - ä¿®æ­£ç‰ˆï¼šèª¿ç”¨çœŸæ­£çš„VIè¨“ç·´"""
        print("ğŸš€ ä½¿ç”¨GPUåŠ é€ŸVIç¯©é¸")
        if X_val is not None and y_val is not None:
            print("   ğŸ“Š ä½¿ç”¨è¨“ç·´+é©—è­‰é›†ç›£ç£ï¼Œé˜²æ­¢éåº¦æ“¬åˆ")
        else:
            print("   âš ï¸ åƒ…ä½¿ç”¨è¨“ç·´é›†ï¼Œå¯èƒ½éåº¦æ“¬åˆ")
        print("   æ³¨æ„ï¼šGPUå¼µé‡åŠ é€Ÿï¼Œä½¿ç”¨å®Œæ•´çš„VIè¨“ç·´")
        
        all_results = []
        total_configs = len(self.epsilon_values) * len(self.basis_risk_types)
        
        print(f"   ä¸¦è¡Œè¨ˆç®— {total_configs} å€‹é…ç½®...")
        
        # å°æ¯å€‹é…ç½®åŸ·è¡Œå®Œæ•´çš„VIè¨“ç·´
        config_idx = 0
        for epsilon in self.epsilon_values:
            for basis_risk_type in self.basis_risk_types:
                config_idx += 1
                
                print(f"     é–‹å§‹é…ç½® {config_idx}/{total_configs}: Îµ={epsilon:.3f}, {basis_risk_type}")
                
                # èª¿ç”¨çœŸæ­£çš„VIè¨“ç·´ï¼ˆç¾åœ¨æ”¯æŒé©—è­‰é›†ï¼‰
                result = self.train_single_model(
                    X, y, epsilon, basis_risk_type, n_iterations=1000,
                    X_val=X_val, y_val=y_val
                )
                all_results.append(result)
                
                # é€²åº¦é¡¯ç¤º
                print(f"     âœ… é…ç½® {config_idx}/{total_configs} å®Œæˆ: åŸºå·®é¢¨éšª={result['final_basis_risk']/1e6:.1f}M")
        
        # æŒ‰åŸºå·®é¢¨éšªæ’åº
        all_results = sorted(all_results, key=lambda x: x['final_basis_risk'])
        
        print(f"âœ… GPUç¯©é¸å®Œæˆ!")
        
        return {
            'all_results': all_results,
            'best_models': all_results[:3],
            'best_model': all_results[0]
        }
    
    def _compute_basis_risk_gpu(self, X_tensor, y_tensor, epsilon, basis_risk_type):
        """åœ¨GPUä¸Šè¨ˆç®—åŸºå·®é¢¨éšª"""
        # æ·»åŠ epsilon contamination
        if epsilon > 0:
            noise = torch.randn_like(y_tensor) * epsilon * y_tensor.mean()
            y_perturbed = y_tensor + noise
        else:
            y_perturbed = y_tensor
        
        # åŸºæ–¼é¢¨é€Ÿç‰¹å¾µè¨ˆç®—åƒæ•¸è³ ä»˜
        wind_speeds = X_tensor.squeeze()
        
        # ç°¡åŒ–çš„åƒæ•¸ä¿éšªé‚è¼¯ï¼ˆåŸºæ–¼é¢¨é€Ÿé–¾å€¼ï¼‰
        payouts = torch.zeros_like(y_perturbed)
        
        # å¤šå±¤é–¾å€¼è³ ä»˜
        thresholds = torch.tensor([25.0, 35.0, 45.0], device=self.device)
        payout_ratios = torch.tensor([0.25, 0.5, 1.0], device=self.device)
        max_payout = y_tensor.mean() * 2.0  # å‹•æ…‹æœ€å¤§è³ ä»˜
        
        for i, threshold in enumerate(thresholds):
            mask = wind_speeds >= threshold
            payouts[mask] = max_payout * payout_ratios[i]
        
        # è¨ˆç®—åŸºå·®é¢¨éšª
        if basis_risk_type == 'absolute':
            basis_risk = torch.mean(torch.abs(y_perturbed - payouts))
        elif basis_risk_type == 'asymmetric':
            under_penalty = torch.mean(torch.relu(y_perturbed - payouts))
            over_penalty = torch.mean(torch.relu(payouts - y_perturbed))
            basis_risk = 2.0 * under_penalty + over_penalty
        else:  # weighted
            under = torch.relu(y_perturbed - payouts) * 2.0
            over = torch.relu(payouts - y_perturbed) * 0.5
            basis_risk = torch.mean(under + over)
        
        return basis_risk
    
    def _cpu_screening(self, X: np.ndarray, y: np.ndarray, 
                      X_val: np.ndarray = None, y_val: np.ndarray = None) -> Dict:
        """CPUç‰ˆæœ¬çš„VIç¯©é¸ï¼ˆåŸå§‹å¯¦ç¾ï¼‰"""
        print("ğŸ’» ä½¿ç”¨CPUé€²è¡ŒVIç¯©é¸")
        if X_val is not None and y_val is not None:
            print("   ğŸ“Š ä½¿ç”¨è¨“ç·´+é©—è­‰é›†ç›£ç£ï¼Œé˜²æ­¢éåº¦æ“¬åˆ")
        else:
            print("   âš ï¸ åƒ…ä½¿ç”¨è¨“ç·´é›†ï¼Œå¯èƒ½éåº¦æ“¬åˆ")
        
        all_results = []
        
        for epsilon in self.epsilon_values:
            for basis_risk_type in self.basis_risk_types:
                result = self.train_single_model(
                    X, y, epsilon, basis_risk_type, n_iterations=1000,
                    X_val=X_val, y_val=y_val
                )
                all_results.append(result)
        
        # æŒ‰åŸºå·®é¢¨éšªæ’åº
        all_results = sorted(all_results, key=lambda x: x['final_basis_risk'])
        
        return {
            'all_results': all_results,
            'best_models': all_results[:3],
            'best_model': all_results[0]
        }
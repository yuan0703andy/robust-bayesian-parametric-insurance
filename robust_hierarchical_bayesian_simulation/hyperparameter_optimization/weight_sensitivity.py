"""
Weight Sensitivity Analyzer
æ¬Šé‡æ•æ„Ÿæ€§åˆ†æå™¨

æœ¬æ¨¡çµ„ç‚º bayesian/ æ¡†æ¶æä¾›æ¬Šé‡æ•æ„Ÿæ€§åˆ†æåŠŸèƒ½ï¼Œå›æ‡‰å­¸è¡“è€ƒé‡é»ä¸€:
ã€Œæ‡²ç½°æ¬Šé‡çš„é¸æ“‡èˆ‡åˆç†æ€§ (The Choice of Weights)ã€

æ ¸å¿ƒåŠŸèƒ½:
- ç³»çµ±æ€§æ¸¬è©¦ä¸åŒ (w_under, w_over) æ¬Šé‡çµ„åˆ
- åˆ†ææ¬Šé‡é¸æ“‡å°æœ€ä½³ç”¢å“åƒæ•¸çš„å½±éŸ¿  
- æä¾›ç›¸é—œæ€§åˆ†æå’Œç©©å¥æ€§è©•ä¼°
- èˆ‡ RobustBayesianAnalyzer ç„¡ç¸«æ•´åˆ

æ•´åˆè¨­è¨ˆ:
- å¯ä½œç‚º RobustBayesianAnalyzer çš„æ“´å±•åŠŸèƒ½
- æ”¯æŒç¾æœ‰çš„ basis_risk_type å’Œç”¢å“å„ªåŒ–æµç¨‹
- æä¾›ç¨ç«‹çš„æ•æ„Ÿæ€§åˆ†æä»‹é¢
- çµæœå¯èˆ‡å…¶ä»– bayesian/ çµ„ä»¶å…±äº«

Author: Robust Bayesian Analysis Team
Date: 2025-01-10
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, field
import warnings
from pathlib import Path
import json
from concurrent.futures import ProcessPoolExecutor
import time

# å°å…¥ bayesian æ¨¡çµ„çš„ç›¸é—œçµ„ä»¶
try:
    from .robust_bayesian_analyzer import RobustBayesianAnalyzer
    HAS_ROBUST_ANALYZER = True
except ImportError:
    HAS_ROBUST_ANALYZER = False
    warnings.warn("RobustBayesianAnalyzer not available for integration")

# å°å…¥ skill_scores çš„åŸºå·®é¢¨éšªå‡½æ•¸
try:
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from skill_scores.basis_risk_functions import (
        BasisRiskCalculator, 
        BasisRiskType, 
        create_basis_risk_function
    )
    HAS_BASIS_RISK_FUNCTIONS = True
except ImportError:
    HAS_BASIS_RISK_FUNCTIONS = False
    warnings.warn("skill_scores.basis_risk_functions not available")

# è¨­å®šä¸­æ–‡å­—é«”æ”¯æŒ
try:
    import matplotlib.pyplot as plt
    plt.rcParams['font.sans-serif'] = ['Heiti TC', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
except ImportError:
    pass

@dataclass
class WeightSensitivityConfig:
    """æ¬Šé‡æ•æ„Ÿæ€§åˆ†æé…ç½®"""
    
    # æ¬Šé‡çµ„åˆè¨­å®š
    weight_combinations: List[Tuple[float, float]] = field(default_factory=lambda: [
        (2.0, 0.5),   # åŸºæº–çµ„åˆ (ç•¶å‰ä½¿ç”¨)
        (1.0, 1.0),   # ç›¸ç­‰æ¬Šé‡
        (3.0, 1.0),   # 3:1 æ¯”ç‡
        (4.0, 1.0),   # 4:1 æ¯”ç‡  
        (5.0, 1.0),   # 5:1 æ¯”ç‡
        (10.0, 1.0),  # 10:1 æ¯”ç‡
        (0.5, 2.0),   # åå‘æ¬Šé‡ (æ›´é—œå¿ƒéåº¦è³ ä»˜)
        (1.0, 2.0),   # 1:2 æ¯”ç‡
        (1.5, 1.0),   # æº«å’Œæ¬Šé‡
        (2.0, 1.0),   # 2:1 æ¯”ç‡
        (5.0, 0.1),   # æ¥µåº¦æ‡²ç½°ä¸è¶³è¦†è“‹
        (0.1, 5.0)    # æ¥µåº¦æ‡²ç½°éåº¦è¦†è“‹
    ])
    
    # åˆ†æè¨­å®š
    basis_risk_type: str = "weighted_asymmetric"  # åŸºå·®é¢¨éšªé¡å‹
    product_search_resolution: int = 20           # ç”¢å“æœç´¢è§£æåº¦
    use_parallel_processing: bool = True          # æ˜¯å¦ä½¿ç”¨ä¸¦è¡Œè™•ç†
    n_workers: int = 4                           # å·¥ä½œé€²ç¨‹æ•¸
    
    # è¼¸å‡ºè¨­å®š
    output_dir: str = "results/weight_sensitivity"
    save_detailed_results: bool = True
    generate_plots: bool = True
    plot_dpi: int = 300

@dataclass  
class WeightSensitivityResult:
    """æ¬Šé‡æ•æ„Ÿæ€§åˆ†æçµæœ"""
    
    # åŸºæœ¬è³‡è¨Š
    weight_combination: Tuple[float, float]
    weight_ratio: float
    
    # æœ€ä½³ç”¢å“åƒæ•¸
    optimal_trigger_threshold: float
    optimal_payout_amount: float
    optimal_basis_risk: float
    optimal_trigger_rate: float
    
    # ç”¢å“æ€§èƒ½æŒ‡æ¨™
    expected_payout: float
    coverage_efficiency: float
    risk_stability_score: float
    
    # èˆ‡åŸºæº–çš„æ¯”è¼ƒ
    improvement_vs_baseline: float
    rank_among_combinations: int

@dataclass
class WeightSensitivityAnalysis:
    """å®Œæ•´çš„æ¬Šé‡æ•æ„Ÿæ€§åˆ†æçµæœ"""
    
    # åˆ†æå…ƒæ•¸æ“š
    analysis_config: WeightSensitivityConfig
    analysis_timestamp: str
    data_summary: Dict[str, Any]
    
    # æ‰€æœ‰æ¬Šé‡çµ„åˆçš„çµæœ
    weight_results: List[WeightSensitivityResult]
    
    # ç¸½é«”åˆ†æ
    best_weight_combination: Tuple[float, float]
    worst_weight_combination: Tuple[float, float]
    sensitivity_level: str  # "High", "Medium", "Low"
    correlation_analysis: Dict[str, float]
    
    # æ¥­å‹™æ´å¯Ÿ
    key_insights: List[str]
    business_recommendations: List[str]

class WeightSensitivityAnalyzer:
    """
    æ¬Šé‡æ•æ„Ÿæ€§åˆ†æå™¨
    
    èˆ‡ RobustBayesianAnalyzer æ•´åˆï¼Œæä¾›æ¬Šé‡é¸æ“‡çš„ç³»çµ±æ€§åˆ†æ
    """
    
    def __init__(self, 
                 config: Optional[WeightSensitivityConfig] = None,
                 robust_analyzer: Optional['RobustBayesianAnalyzer'] = None):
        """
        åˆå§‹åŒ–æ¬Šé‡æ•æ„Ÿæ€§åˆ†æå™¨
        
        Parameters:
        -----------
        config : WeightSensitivityConfig, optional
            æ•æ„Ÿæ€§åˆ†æé…ç½®
        robust_analyzer : RobustBayesianAnalyzer, optional
            å·²é…ç½®çš„è²è‘‰æ–¯åˆ†æå™¨ï¼Œå¯é‡ç”¨å…¶è¨­å®š
        """
        self.config = config or WeightSensitivityConfig()
        self.robust_analyzer = robust_analyzer
        
        # åˆå§‹åŒ–åŸºå·®é¢¨éšªè¨ˆç®—å™¨
        if HAS_BASIS_RISK_FUNCTIONS:
            self.basis_risk_calc = BasisRiskCalculator()
        else:
            self.basis_risk_calc = None
            warnings.warn("BasisRiskCalculator not available, using simplified calculations")
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        Path(self.config.output_dir).mkdir(parents=True, exist_ok=True)
        
        # çµæœç·©å­˜
        self._analysis_cache = {}
    
    def analyze_weight_sensitivity(self,
                                 observations: np.ndarray,
                                 validation_data: np.ndarray,
                                 hazard_indices: np.ndarray,
                                 actual_losses: np.ndarray,
                                 product_bounds: Dict[str, Tuple[float, float]]) -> WeightSensitivityAnalysis:
        """
        åŸ·è¡Œå®Œæ•´çš„æ¬Šé‡æ•æ„Ÿæ€§åˆ†æ
        
        Parameters:
        -----------
        observations : np.ndarray
            è¨“ç·´æ•¸æ“š
        validation_data : np.ndarray
            é©—è­‰æ•¸æ“š
        hazard_indices : np.ndarray
            ç½å®³æŒ‡æ¨™
        actual_losses : np.ndarray
            çœŸå¯¦æå¤±
        product_bounds : Dict[str, Tuple[float, float]]
            ç”¢å“åƒæ•¸é‚Šç•Œ
            
        Returns:
        --------
        WeightSensitivityAnalysis
            å®Œæ•´çš„æ•æ„Ÿæ€§åˆ†æçµæœ
        """
        
        print("ğŸ” åŸ·è¡Œæ¬Šé‡æ•æ„Ÿæ€§åˆ†æ...")
        print("=" * 60)
        print(f"åˆ†æ {len(self.config.weight_combinations)} å€‹æ¬Šé‡çµ„åˆ")
        
        start_time = time.time()
        
        # æº–å‚™æ•¸æ“šæ‘˜è¦
        data_summary = {
            'n_observations': len(observations),
            'n_validation': len(validation_data),
            'n_hazard_events': len(hazard_indices),
            'n_loss_scenarios': len(actual_losses),
            'hazard_range': (float(hazard_indices.min()), float(hazard_indices.max())),
            'loss_range': (float(actual_losses.min()), float(actual_losses.max()))
        }
        
        # åˆ†ææ¯å€‹æ¬Šé‡çµ„åˆ
        if self.config.use_parallel_processing:
            weight_results = self._analyze_weights_parallel(
                observations, validation_data, hazard_indices, actual_losses, product_bounds
            )
        else:
            weight_results = self._analyze_weights_sequential(
                observations, validation_data, hazard_indices, actual_losses, product_bounds
            )
        
        # åŸ·è¡Œç¸½é«”åˆ†æ
        analysis_results = self._perform_overall_analysis(weight_results, data_summary)
        
        execution_time = time.time() - start_time
        print(f"âœ… æ¬Šé‡æ•æ„Ÿæ€§åˆ†æå®Œæˆ (åŸ·è¡Œæ™‚é–“: {execution_time:.2f} ç§’)")
        
        return analysis_results
    
    def _analyze_single_weight_combination(self,
                                         w_under: float,
                                         w_over: float,
                                         observations: np.ndarray,
                                         validation_data: np.ndarray,
                                         hazard_indices: np.ndarray,
                                         actual_losses: np.ndarray,
                                         product_bounds: Dict[str, Tuple[float, float]]) -> WeightSensitivityResult:
        """åˆ†æå–®ä¸€æ¬Šé‡çµ„åˆ"""
        
        try:
            # å¦‚æœæœ‰å¯ç”¨çš„ RobustBayesianAnalyzerï¼Œä½¿ç”¨æ•´åˆæ–¹æ³•
            if self.robust_analyzer and HAS_ROBUST_ANALYZER:
                results = self._analyze_with_robust_analyzer(
                    w_under, w_over, observations, validation_data, 
                    hazard_indices, actual_losses, product_bounds
                )
            else:
                # ä½¿ç”¨ç°¡åŒ–çš„ç¨ç«‹åˆ†ææ–¹æ³•
                results = self._analyze_with_simple_optimization(
                    w_under, w_over, hazard_indices, actual_losses, product_bounds
                )
                
            return results
            
        except Exception as e:
            print(f"âš ï¸ æ¬Šé‡çµ„åˆ ({w_under}, {w_over}) åˆ†æå¤±æ•—: {e}")
            # è¿”å›é»˜èªçµæœ
            return self._create_default_result(w_under, w_over)
    
    def _analyze_with_robust_analyzer(self,
                                    w_under: float,
                                    w_over: float,
                                    observations: np.ndarray,
                                    validation_data: np.ndarray,
                                    hazard_indices: np.ndarray,
                                    actual_losses: np.ndarray,
                                    product_bounds: Dict[str, Tuple[float, float]]) -> WeightSensitivityResult:
        """ä½¿ç”¨ RobustBayesianAnalyzer é€²è¡Œæ•´åˆåˆ†æ"""
        
        # åŸ·è¡Œæ•´åˆè²è‘‰æ–¯å„ªåŒ–ï¼Œä½¿ç”¨æŒ‡å®šçš„æ¬Šé‡
        bayesian_results = self.robust_analyzer.integrated_bayesian_optimization(
            observations=observations,
            validation_data=validation_data,
            hazard_indices=hazard_indices,
            actual_losses=actual_losses,
            product_bounds=product_bounds,
            w_under=w_under,
            w_over=w_over,
            configure_pymc=False  # é¿å…é‡è¤‡é…ç½®
        )
        
        # æå–æœ€ä½³ç”¢å“åƒæ•¸
        optimal_product = bayesian_results['phase_2_decision_optimization']['optimal_product']
        
        # è¨ˆç®—é¡å¤–çš„æ€§èƒ½æŒ‡æ¨™
        trigger_threshold = optimal_product['trigger_threshold']
        payout_amount = optimal_product['payout_amount']
        basis_risk = bayesian_results['phase_2_decision_optimization']['expected_basis_risk']
        
        # è¨ˆç®—è§¸ç™¼ç‡å’ŒæœŸæœ›è³ ä»˜
        payouts = np.where(hazard_indices >= trigger_threshold, payout_amount, 0)
        trigger_rate = np.mean(payouts > 0)
        expected_payout = np.mean(payouts)
        
        # è¨ˆç®—è¦†è“‹æ•ˆç‡
        total_losses = np.sum(actual_losses)
        total_payouts = np.sum(payouts)
        coverage_efficiency = 1.0 - abs(total_losses - total_payouts) / max(total_losses, 1)
        
        return WeightSensitivityResult(
            weight_combination=(w_under, w_over),
            weight_ratio=w_under / w_over if w_over > 0 else float('inf'),
            optimal_trigger_threshold=trigger_threshold,
            optimal_payout_amount=payout_amount,
            optimal_basis_risk=basis_risk,
            optimal_trigger_rate=trigger_rate,
            expected_payout=expected_payout,
            coverage_efficiency=coverage_efficiency,
            risk_stability_score=0.0,  # å°‡åœ¨å¾ŒçºŒè¨ˆç®—
            improvement_vs_baseline=0.0,  # å°‡åœ¨å¾ŒçºŒè¨ˆç®—
            rank_among_combinations=0  # å°‡åœ¨å¾ŒçºŒè¨ˆç®—
        )
    
    def _analyze_with_simple_optimization(self,
                                        w_under: float,
                                        w_over: float,
                                        hazard_indices: np.ndarray,
                                        actual_losses: np.ndarray,
                                        product_bounds: Dict[str, Tuple[float, float]]) -> WeightSensitivityResult:
        """ä½¿ç”¨ç°¡åŒ–çš„å„ªåŒ–æ–¹æ³•"""
        
        if not self.basis_risk_calc:
            return self._create_default_result(w_under, w_over)
        
        # å®šç¾©æœç´¢ç©ºé–“
        trigger_min, trigger_max = product_bounds.get('trigger_threshold', 
                                                     (np.percentile(hazard_indices, 60), np.percentile(hazard_indices, 95)))
        payout_min, payout_max = product_bounds.get('payout_amount',
                                                   (np.percentile(actual_losses[actual_losses > 0], 20),
                                                    np.percentile(actual_losses[actual_losses > 0], 80)))
        
        trigger_range = np.linspace(trigger_min, trigger_max, self.config.product_search_resolution)
        payout_range = np.linspace(payout_min, payout_max, self.config.product_search_resolution)
        
        best_risk = float('inf')
        best_trigger = trigger_min
        best_payout = payout_min
        
        # ç¶²æ ¼æœç´¢
        for trigger in trigger_range:
            for payout in payout_range:
                payouts = np.where(hazard_indices >= trigger, payout, 0)
                
                # è¨ˆç®—åŸºå·®é¢¨éšª
                risks = []
                for loss, pay in zip(actual_losses, payouts):
                    risk = self.basis_risk_calc.calculate_weighted_asymmetric_basis_risk(
                        loss, pay, w_under=w_under, w_over=w_over
                    )
                    risks.append(risk)
                
                mean_risk = np.mean(risks)
                if mean_risk < best_risk:
                    best_risk = mean_risk
                    best_trigger = trigger
                    best_payout = payout
        
        # è¨ˆç®—æœ€ä½³ç”¢å“çš„æ€§èƒ½æŒ‡æ¨™
        best_payouts = np.where(hazard_indices >= best_trigger, best_payout, 0)
        trigger_rate = np.mean(best_payouts > 0)
        expected_payout = np.mean(best_payouts)
        
        total_losses = np.sum(actual_losses)
        total_payouts = np.sum(best_payouts)
        coverage_efficiency = 1.0 - abs(total_losses - total_payouts) / max(total_losses, 1)
        
        return WeightSensitivityResult(
            weight_combination=(w_under, w_over),
            weight_ratio=w_under / w_over if w_over > 0 else float('inf'),
            optimal_trigger_threshold=best_trigger,
            optimal_payout_amount=best_payout,
            optimal_basis_risk=best_risk,
            optimal_trigger_rate=trigger_rate,
            expected_payout=expected_payout,
            coverage_efficiency=coverage_efficiency,
            risk_stability_score=0.0,
            improvement_vs_baseline=0.0,
            rank_among_combinations=0
        )
    
    def _create_default_result(self, w_under: float, w_over: float) -> WeightSensitivityResult:
        """å‰µå»ºé»˜èªçµæœ(ç•¶åˆ†æå¤±æ•—æ™‚)"""
        return WeightSensitivityResult(
            weight_combination=(w_under, w_over),
            weight_ratio=w_under / w_over if w_over > 0 else float('inf'),
            optimal_trigger_threshold=50.0,
            optimal_payout_amount=1e8,
            optimal_basis_risk=1e9,
            optimal_trigger_rate=0.2,
            expected_payout=2e7,
            coverage_efficiency=0.5,
            risk_stability_score=0.0,
            improvement_vs_baseline=0.0,
            rank_among_combinations=999
        )
    
    def _analyze_weights_parallel(self,
                                observations: np.ndarray,
                                validation_data: np.ndarray,
                                hazard_indices: np.ndarray,
                                actual_losses: np.ndarray,
                                product_bounds: Dict[str, Tuple[float, float]]) -> List[WeightSensitivityResult]:
        """ä¸¦è¡Œåˆ†ææ¬Šé‡çµ„åˆ"""
        
        print("âš¡ ä½¿ç”¨ä¸¦è¡Œè™•ç†åˆ†ææ¬Šé‡çµ„åˆ...")
        
        with ProcessPoolExecutor(max_workers=self.config.n_workers) as executor:
            futures = []
            for w_under, w_over in self.config.weight_combinations:
                future = executor.submit(
                    self._analyze_single_weight_combination,
                    w_under, w_over, observations, validation_data,
                    hazard_indices, actual_losses, product_bounds
                )
                futures.append(future)
            
            results = [future.result() for future in futures]
        
        return results
    
    def _analyze_weights_sequential(self,
                                  observations: np.ndarray,
                                  validation_data: np.ndarray,
                                  hazard_indices: np.ndarray,
                                  actual_losses: np.ndarray,
                                  product_bounds: Dict[str, Tuple[float, float]]) -> List[WeightSensitivityResult]:
        """é †åºåˆ†ææ¬Šé‡çµ„åˆ"""
        
        print("ğŸ”„ ä½¿ç”¨é †åºè™•ç†åˆ†ææ¬Šé‡çµ„åˆ...")
        
        results = []
        for i, (w_under, w_over) in enumerate(self.config.weight_combinations):
            print(f"  åˆ†ææ¬Šé‡çµ„åˆ {i+1}/{len(self.config.weight_combinations)}: ({w_under:.1f}, {w_over:.1f})")
            
            result = self._analyze_single_weight_combination(
                w_under, w_over, observations, validation_data,
                hazard_indices, actual_losses, product_bounds
            )
            results.append(result)
        
        return results
    
    def _perform_overall_analysis(self,
                                weight_results: List[WeightSensitivityResult],
                                data_summary: Dict[str, Any]) -> WeightSensitivityAnalysis:
        """åŸ·è¡Œç¸½é«”åˆ†æå’Œæ´å¯Ÿç”Ÿæˆ"""
        
        print("ğŸ“Š åŸ·è¡Œç¸½é«”æ•æ„Ÿæ€§åˆ†æ...")
        
        # æ’åºå’Œæ’å
        sorted_results = sorted(weight_results, key=lambda x: x.optimal_basis_risk)
        for i, result in enumerate(sorted_results):
            result.rank_among_combinations = i + 1
        
        # è¨ˆç®—èˆ‡åŸºæº–çš„æ”¹é€²
        baseline_risk = next((r.optimal_basis_risk for r in weight_results 
                            if r.weight_combination == (2.0, 0.5)), 
                           weight_results[0].optimal_basis_risk)
        
        for result in weight_results:
            result.improvement_vs_baseline = (baseline_risk - result.optimal_basis_risk) / baseline_risk
        
        # è¨ˆç®—ç›¸é—œæ€§
        weight_ratios = [r.weight_ratio for r in weight_results if np.isfinite(r.weight_ratio)]
        basis_risks = [r.optimal_basis_risk for r in weight_results if np.isfinite(r.weight_ratio)]
        
        correlation = np.corrcoef(weight_ratios, basis_risks)[0, 1] if len(weight_ratios) > 1 else 0.0
        
        # ç¢ºå®šæ•æ„Ÿæ€§ç´šåˆ¥
        risk_range = max(basis_risks) - min(basis_risks)
        relative_range = risk_range / np.mean(basis_risks) if np.mean(basis_risks) > 0 else 0
        
        if relative_range > 0.3:
            sensitivity_level = "High"
        elif relative_range > 0.1:
            sensitivity_level = "Medium"  
        else:
            sensitivity_level = "Low"
        
        # ç”Ÿæˆæ´å¯Ÿå’Œå»ºè­°
        key_insights = self._generate_key_insights(weight_results, correlation, sensitivity_level)
        business_recommendations = self._generate_business_recommendations(sorted_results, sensitivity_level)
        
        return WeightSensitivityAnalysis(
            analysis_config=self.config,
            analysis_timestamp=pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
            data_summary=data_summary,
            weight_results=weight_results,
            best_weight_combination=sorted_results[0].weight_combination,
            worst_weight_combination=sorted_results[-1].weight_combination,
            sensitivity_level=sensitivity_level,
            correlation_analysis={'weight_ratio_vs_basis_risk': correlation},
            key_insights=key_insights,
            business_recommendations=business_recommendations
        )
    
    def _generate_key_insights(self,
                             weight_results: List[WeightSensitivityResult],
                             correlation: float,
                             sensitivity_level: str) -> List[str]:
        """ç”Ÿæˆé—œéµæ´å¯Ÿ"""
        
        insights = []
        
        # æ•æ„Ÿæ€§æ°´å¹³æ´å¯Ÿ
        insights.append(f"æ¬Šé‡æ•æ„Ÿæ€§ç­‰ç´š: {sensitivity_level}")
        
        # ç›¸é—œæ€§æ´å¯Ÿ
        if abs(correlation) > 0.7:
            insights.append(f"æ¬Šé‡æ¯”ç‡èˆ‡åŸºå·®é¢¨éšªå‘ˆå¼·ç›¸é—œæ€§ (r={correlation:.3f})")
        elif abs(correlation) > 0.3:
            insights.append(f"æ¬Šé‡æ¯”ç‡èˆ‡åŸºå·®é¢¨éšªå‘ˆä¸­åº¦ç›¸é—œæ€§ (r={correlation:.3f})")
        else:
            insights.append(f"æ¬Šé‡æ¯”ç‡èˆ‡åŸºå·®é¢¨éšªç›¸é—œæ€§è¼ƒä½ (r={correlation:.3f})")
        
        # æ€§èƒ½ç¯„åœæ´å¯Ÿ
        risks = [r.optimal_basis_risk for r in weight_results]
        min_risk, max_risk = min(risks), max(risks)
        performance_ratio = max_risk / min_risk if min_risk > 0 else 1.0
        
        insights.append(f"æœ€ä½³èˆ‡æœ€å·®æ¬Šé‡çµ„åˆçš„æ€§èƒ½å·®ç•°ç‚º {performance_ratio:.2f} å€")
        
        # æœ€ä½³æ¬Šé‡æ´å¯Ÿ
        best_result = min(weight_results, key=lambda x: x.optimal_basis_risk)
        best_ratio = best_result.weight_ratio
        
        if best_ratio > 5:
            insights.append(f"æœ€ä½³æ¬Šé‡æ¯”ç‡ç‚º {best_ratio:.1f}:1ï¼Œåå‘æ‡²ç½°ä¸è¶³è¦†è“‹")
        elif best_ratio < 2:
            insights.append(f"æœ€ä½³æ¬Šé‡æ¯”ç‡ç‚º {best_ratio:.1f}:1ï¼Œç›¸å°å‡è¡¡")
        else:
            insights.append(f"æœ€ä½³æ¬Šé‡æ¯”ç‡ç‚º {best_ratio:.1f}:1ï¼Œé©ä¸­åå¥½")
        
        return insights
    
    def _generate_business_recommendations(self,
                                         sorted_results: List[WeightSensitivityResult],
                                         sensitivity_level: str) -> List[str]:
        """ç”Ÿæˆæ¥­å‹™å»ºè­°"""
        
        recommendations = []
        
        best_result = sorted_results[0]
        
        # åŸºæ–¼æ•æ„Ÿæ€§ç´šåˆ¥çš„å»ºè­°
        if sensitivity_level == "High":
            recommendations.append("æ¬Šé‡é¸æ“‡å°ç”¢å“æ€§èƒ½æœ‰é‡å¤§å½±éŸ¿ï¼Œå»ºè­°é€²è¡Œè©³ç´°çš„æ¬Šé‡èª¿æ ¡åˆ†æ")
            recommendations.append("å»ºè­°èˆ‡æ¥­å‹™éƒ¨é–€è¨è«–é¢¨éšªåå¥½ï¼Œç¢ºå®šé©ç•¶çš„æ¬Šé‡è¨­å®š")
        elif sensitivity_level == "Medium":
            recommendations.append("æ¬Šé‡é¸æ“‡æœ‰ä¸­ç­‰ç¨‹åº¦å½±éŸ¿ï¼Œå»ºè­°é€²è¡Œé©åº¦çš„æ¬Šé‡å„ªåŒ–")
        else:
            recommendations.append("ç”¢å“å°æ¬Šé‡è®ŠåŒ–ç›¸å°ç©©å¥ï¼Œå¯ä½¿ç”¨æ¨™æº–æ¬Šé‡è¨­å®š")
        
        # æœ€ä½³æ¬Šé‡å»ºè­°
        w_under, w_over = best_result.weight_combination
        recommendations.append(f"å»ºè­°ä½¿ç”¨æ¬Šé‡çµ„åˆ (w_under={w_under:.1f}, w_over={w_over:.1f})")
        
        # é¢¨éšªæ§åˆ¶å»ºè­°
        if best_result.optimal_trigger_rate < 0.1:
            recommendations.append("è§¸ç™¼ç‡è¼ƒä½ï¼Œè€ƒæ…®é©ç•¶é™ä½è§¸ç™¼é–¾å€¼ä»¥æé«˜è¦†è“‹åº¦")
        elif best_result.optimal_trigger_rate > 0.4:
            recommendations.append("è§¸ç™¼ç‡è¼ƒé«˜ï¼Œè€ƒæ…®é©ç•¶æé«˜è§¸ç™¼é–¾å€¼ä»¥æ§åˆ¶è³ ä»˜é »ç‡")
        
        return recommendations
    
    def visualize_sensitivity_results(self, analysis: WeightSensitivityAnalysis) -> None:
        """è¦–è¦ºåŒ–æ•æ„Ÿæ€§åˆ†æçµæœ"""
        
        if not self.config.generate_plots:
            return
        
        print("ğŸ“Š ç”Ÿæˆæ¬Šé‡æ•æ„Ÿæ€§è¦–è¦ºåŒ–...")
        
        # æº–å‚™æ•¸æ“š
        results = analysis.weight_results
        weight_ratios = [r.weight_ratio for r in results if np.isfinite(r.weight_ratio)]
        basis_risks = [r.optimal_basis_risk for r in results if np.isfinite(r.weight_ratio)]
        trigger_rates = [r.optimal_trigger_rate for r in results if np.isfinite(r.weight_ratio)]
        
        # å‰µå»ºè¦–è¦ºåŒ–
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('æ¬Šé‡æ•æ„Ÿæ€§åˆ†æçµæœ', fontsize=16, fontweight='bold')
        
        # 1. æ¬Šé‡æ¯”ç‡ vs åŸºå·®é¢¨éšª
        ax1 = axes[0, 0]
        scatter = ax1.scatter(weight_ratios, basis_risks, c=trigger_rates, 
                            cmap='viridis', s=100, alpha=0.7)
        ax1.set_xlabel('æ¬Šé‡æ¯”ç‡ (w_under / w_over)')
        ax1.set_ylabel('æœ€ä½³åŸºå·®é¢¨éšª')
        ax1.set_title('æ¬Šé‡æ¯”ç‡å°åŸºå·®é¢¨éšªçš„å½±éŸ¿')
        ax1.set_xscale('log')
        plt.colorbar(scatter, ax=ax1, label='è§¸ç™¼ç‡')
        ax1.grid(True, alpha=0.3)
        
        # 2. æ¬Šé‡çµ„åˆæ’å
        ax2 = axes[0, 1]
        sorted_results = sorted(results, key=lambda x: x.optimal_basis_risk)
        ranks = range(1, len(sorted_results) + 1)
        risks = [r.optimal_basis_risk for r in sorted_results]
        
        bars = ax2.bar(ranks, risks, alpha=0.7)
        ax2.set_xlabel('æ’å')
        ax2.set_ylabel('åŸºå·®é¢¨éšª')
        ax2.set_title('æ¬Šé‡çµ„åˆæ€§èƒ½æ’å')
        
        # æ¨™è¨˜æœ€ä½³çµ„åˆ
        best_combo = sorted_results[0].weight_combination
        ax2.text(1, risks[0], f'æœ€ä½³: ({best_combo[0]:.1f}, {best_combo[1]:.1f})',
                ha='center', va='bottom', fontweight='bold')
        ax2.grid(True, alpha=0.3, axis='y')
        
        # 3. æ¬Šé‡æ•æ„Ÿæ€§ç†±åœ–
        ax3 = axes[1, 0]
        
        # å‰µå»ºæ¬Šé‡ç¶²æ ¼æ•¸æ“š
        w_under_values = sorted(set(r.weight_combination[0] for r in results))
        w_over_values = sorted(set(r.weight_combination[1] for r in results))
        
        if len(w_under_values) > 1 and len(w_over_values) > 1:
            risk_matrix = np.full((len(w_over_values), len(w_under_values)), np.nan)
            
            for result in results:
                w_u, w_o = result.weight_combination
                if w_u in w_under_values and w_o in w_over_values:
                    i = w_over_values.index(w_o)
                    j = w_under_values.index(w_u)
                    risk_matrix[i, j] = result.optimal_basis_risk
            
            im = ax3.imshow(risk_matrix, cmap='RdYlBu_r', aspect='auto')
            ax3.set_xticks(range(len(w_under_values)))
            ax3.set_yticks(range(len(w_over_values)))
            ax3.set_xticklabels([f'{w:.1f}' for w in w_under_values])
            ax3.set_yticklabels([f'{w:.1f}' for w in w_over_values])
            ax3.set_xlabel('w_under')
            ax3.set_ylabel('w_over')
            ax3.set_title('æ¬Šé‡æ•æ„Ÿæ€§ç†±åœ–')
            plt.colorbar(im, ax=ax3, label='åŸºå·®é¢¨éšª')
        else:
            ax3.text(0.5, 0.5, 'æ¬Šé‡çµ„åˆä¸è¶³ä»¥ç”Ÿæˆç†±åœ–', 
                    ha='center', va='center', transform=ax3.transAxes)
            ax3.set_title('æ¬Šé‡æ•æ„Ÿæ€§ç†±åœ– (æ•¸æ“šä¸è¶³)')
        
        # 4. æ”¹é€²æ•ˆæœåˆ†æ
        ax4 = axes[1, 1]
        improvements = [r.improvement_vs_baseline for r in results]
        weight_labels = [f'({r.weight_combination[0]:.1f},{r.weight_combination[1]:.1f})' 
                        for r in results]
        
        colors = ['green' if imp > 0 else 'red' for imp in improvements]
        bars = ax4.bar(range(len(improvements)), improvements, color=colors, alpha=0.7)
        ax4.set_xlabel('æ¬Šé‡çµ„åˆ')
        ax4.set_ylabel('ç›¸å°åŸºæº–çš„æ”¹é€²ç‡')
        ax4.set_title('ç›¸å°åŸºæº–çµ„åˆ (2.0, 0.5) çš„æ”¹é€²')
        ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        ax4.set_xticks(range(len(improvements)))
        ax4.set_xticklabels(weight_labels, rotation=45, ha='right')
        ax4.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        
        # ä¿å­˜åœ–è¡¨
        output_file = Path(self.config.output_dir) / "weight_sensitivity_analysis.png"
        plt.savefig(output_file, dpi=self.config.plot_dpi, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… è¦–è¦ºåŒ–å·²ä¿å­˜: {output_file}")
    
    def save_analysis_results(self, analysis: WeightSensitivityAnalysis) -> None:
        """ä¿å­˜åˆ†æçµæœ"""
        
        if not self.config.save_detailed_results:
            return
        
        print("ğŸ’¾ ä¿å­˜åˆ†æçµæœ...")
        
        # ä¿å­˜ JSON çµæœ
        results_dict = {
            'analysis_metadata': {
                'timestamp': analysis.analysis_timestamp,
                'config': {
                    'weight_combinations': analysis.analysis_config.weight_combinations,
                    'basis_risk_type': analysis.analysis_config.basis_risk_type
                },
                'data_summary': analysis.data_summary
            },
            'sensitivity_summary': {
                'best_weight_combination': analysis.best_weight_combination,
                'worst_weight_combination': analysis.worst_weight_combination,
                'sensitivity_level': analysis.sensitivity_level,
                'correlation_analysis': analysis.correlation_analysis
            },
            'detailed_results': [
                {
                    'weight_combination': r.weight_combination,
                    'weight_ratio': r.weight_ratio,
                    'optimal_basis_risk': r.optimal_basis_risk,
                    'optimal_trigger_threshold': r.optimal_trigger_threshold,
                    'optimal_payout_amount': r.optimal_payout_amount,
                    'trigger_rate': r.optimal_trigger_rate,
                    'improvement_vs_baseline': r.improvement_vs_baseline,
                    'rank': r.rank_among_combinations
                } for r in analysis.weight_results
            ],
            'insights': analysis.key_insights,
            'recommendations': analysis.business_recommendations
        }
        
        json_file = Path(self.config.output_dir) / "weight_sensitivity_results.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, ensure_ascii=False, indent=2, default=str)
        
        # ä¿å­˜ CSV çµæœ
        results_df = pd.DataFrame([
            {
                'w_under': r.weight_combination[0],
                'w_over': r.weight_combination[1],
                'weight_ratio': r.weight_ratio,
                'basis_risk': r.optimal_basis_risk,
                'trigger_threshold': r.optimal_trigger_threshold,
                'payout_amount': r.optimal_payout_amount,
                'trigger_rate': r.optimal_trigger_rate,
                'expected_payout': r.expected_payout,
                'coverage_efficiency': r.coverage_efficiency,
                'improvement_vs_baseline': r.improvement_vs_baseline,
                'rank': r.rank_among_combinations
            } for r in analysis.weight_results
        ])
        
        csv_file = Path(self.config.output_dir) / "weight_sensitivity_results.csv"
        results_df.to_csv(csv_file, index=False, encoding='utf-8')
        
        print(f"âœ… çµæœå·²ä¿å­˜:")
        print(f"   JSON: {json_file}")
        print(f"   CSV: {csv_file}")

# =============================================================================
# æ•´åˆä»‹é¢ï¼šèˆ‡ RobustBayesianAnalyzer çš„æ“´å±•
# =============================================================================

def extend_robust_analyzer_with_weight_sensitivity(analyzer: 'RobustBayesianAnalyzer') -> 'RobustBayesianAnalyzer':
    """
    ç‚º RobustBayesianAnalyzer æ·»åŠ æ¬Šé‡æ•æ„Ÿæ€§åˆ†æåŠŸèƒ½
    
    Parameters:
    -----------
    analyzer : RobustBayesianAnalyzer
        è¦æ“´å±•çš„åˆ†æå™¨
        
    Returns:
    --------
    RobustBayesianAnalyzer
        æ·»åŠ äº†æ¬Šé‡æ•æ„Ÿæ€§åŠŸèƒ½çš„åˆ†æå™¨
    """
    
    def analyze_weight_sensitivity_integrated(
        observations: np.ndarray,
        validation_data: np.ndarray,
        hazard_indices: np.ndarray,
        actual_losses: np.ndarray,
        product_bounds: Dict[str, Tuple[float, float]],
        weight_sensitivity_config: Optional[WeightSensitivityConfig] = None
    ) -> WeightSensitivityAnalysis:
        """æ•´åˆçš„æ¬Šé‡æ•æ„Ÿæ€§åˆ†ææ–¹æ³•"""
        
        sensitivity_analyzer = WeightSensitivityAnalyzer(
            config=weight_sensitivity_config,
            robust_analyzer=analyzer
        )
        
        return sensitivity_analyzer.analyze_weight_sensitivity(
            observations, validation_data, hazard_indices, 
            actual_losses, product_bounds
        )
    
    # å‹•æ…‹æ·»åŠ æ–¹æ³•åˆ°åˆ†æå™¨
    analyzer.analyze_weight_sensitivity = analyze_weight_sensitivity_integrated
    
    return analyzer

# =============================================================================
# ä¾¿åˆ©å‡½æ•¸
# =============================================================================

def create_weight_sensitivity_analyzer(config: Optional[WeightSensitivityConfig] = None) -> WeightSensitivityAnalyzer:
    """
    å‰µå»ºæ¬Šé‡æ•æ„Ÿæ€§åˆ†æå™¨çš„ä¾¿åˆ©å‡½æ•¸
    
    Parameters:
    -----------
    config : WeightSensitivityConfig, optional
        é…ç½®åƒæ•¸
        
    Returns:
    --------
    WeightSensitivityAnalyzer
        æ¬Šé‡æ•æ„Ÿæ€§åˆ†æå™¨
    """
    return WeightSensitivityAnalyzer(config=config)

def quick_weight_sensitivity_analysis(
    observations: np.ndarray,
    validation_data: np.ndarray,
    hazard_indices: np.ndarray,
    actual_losses: np.ndarray,
    product_bounds: Dict[str, Tuple[float, float]],
    output_dir: str = "results/weight_sensitivity"
) -> WeightSensitivityAnalysis:
    """
    å¿«é€Ÿæ¬Šé‡æ•æ„Ÿæ€§åˆ†æ
    
    ä½¿ç”¨é»˜èªé…ç½®åŸ·è¡Œå®Œæ•´çš„æ¬Šé‡æ•æ„Ÿæ€§åˆ†æ
    """
    
    config = WeightSensitivityConfig(output_dir=output_dir)
    analyzer = WeightSensitivityAnalyzer(config=config)
    
    analysis = analyzer.analyze_weight_sensitivity(
        observations, validation_data, hazard_indices, 
        actual_losses, product_bounds
    )
    
    analyzer.visualize_sensitivity_results(analysis)
    analyzer.save_analysis_results(analysis)
    
    return analysis
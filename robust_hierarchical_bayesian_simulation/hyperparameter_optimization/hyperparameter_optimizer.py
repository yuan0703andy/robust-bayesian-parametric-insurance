#!/usr/bin/env python3
"""
Hyperparameter Optimizer for CRPS Framework
CRPSæ¡†æ¶è¶…åƒæ•¸å„ªåŒ–å™¨

å°ˆé–€è² è²¬è¶…åƒæ•¸èª¿å„ªï¼Œç‰¹åˆ¥æ˜¯Î»æ¬Šé‡çš„æ™ºèƒ½æœç´¢

Author: Research Team
Date: 2025-01-17
Version: 2.0
"""

import numpy as np
from typing import Dict, List, Optional, Tuple, Callable, Any
from dataclasses import dataclass
import warnings
from scipy.optimize import minimize, differential_evolution
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern
import logging

logger = logging.getLogger(__name__)

@dataclass
class HyperparameterSearchSpace:
    """è¶…åƒæ•¸æœç´¢ç©ºé–“å®šç¾©"""
    lambda_crps_range: Tuple[float, float] = (0.01, 100.0)
    lambda_under_range: Tuple[float, float] = (0.1, 10.0)
    lambda_over_range: Tuple[float, float] = (0.1, 10.0)
    epsilon_range: Tuple[float, float] = (0.01, 0.3)
    spatial_correlation_range: Tuple[float, float] = (10, 500)  # km
    
    def sample_random_point(self) -> Dict[str, float]:
        """éš¨æ©Ÿæ¡æ¨£ä¸€å€‹é»"""
        return {
            'lambda_crps': np.random.uniform(*self.lambda_crps_range),
            'lambda_under': np.random.uniform(*self.lambda_under_range),
            'lambda_over': np.random.uniform(*self.lambda_over_range),
            'epsilon': np.random.uniform(*self.epsilon_range),
            'spatial_corr': np.random.uniform(*self.spatial_correlation_range)
        }
    
    def get_bounds(self) -> List[Tuple[float, float]]:
        """ç²å–å„ªåŒ–å™¨çš„é‚Šç•Œ"""
        return [
            self.lambda_crps_range,
            self.lambda_under_range,
            self.lambda_over_range,
            self.epsilon_range,
            self.spatial_correlation_range
        ]

class AdaptiveHyperparameterOptimizer:
    """
    è‡ªé©æ‡‰è¶…åƒæ•¸å„ªåŒ–å™¨
    
    ä½¿ç”¨å¤šç¨®ç­–ç•¥æ™ºèƒ½æœç´¢æœ€ä½³è¶…åƒæ•¸ï¼š
    1. ç¶²æ ¼æœç´¢ï¼ˆåŸºç¤ï¼‰
    2. è²è‘‰æ–¯å„ªåŒ–ï¼ˆé«˜æ•ˆï¼‰
    3. é€²åŒ–ç®—æ³•ï¼ˆå…¨å±€ï¼‰
    4. è‡ªé©æ‡‰ç­–ç•¥ï¼ˆæ™ºèƒ½ï¼‰
    """
    
    def __init__(self,
                 objective_function: Callable,
                 search_space: Optional[HyperparameterSearchSpace] = None,
                 strategy: str = 'adaptive'):
        """
        åˆå§‹åŒ–å„ªåŒ–å™¨
        
        Parameters:
        -----------
        objective_function : Callable
            ç›®æ¨™å‡½æ•¸ï¼Œè¼¸å…¥è¶…åƒæ•¸ï¼Œè¿”å›åˆ†æ•¸ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        search_space : HyperparameterSearchSpace
            æœç´¢ç©ºé–“
        strategy : str
            å„ªåŒ–ç­–ç•¥ï¼š'grid', 'bayesian', 'evolutionary', 'adaptive'
        """
        self.objective_function = objective_function
        self.search_space = search_space or HyperparameterSearchSpace()
        self.strategy = strategy
        
        # å„ªåŒ–æ­·å²
        self.history = []
        self.best_params = None
        self.best_score = -np.inf
        
        # è²è‘‰æ–¯å„ªåŒ–çš„GPæ¨¡å‹
        if strategy in ['bayesian', 'adaptive']:
            self.gp_model = GaussianProcessRegressor(
                kernel=Matern(nu=2.5),
                alpha=1e-6,
                normalize_y=True,
                n_restarts_optimizer=10
            )
            self.X_observed = []
            self.y_observed = []
    
    def optimize(self, 
                n_iterations: int = 50,
                n_initial_points: int = 10) -> Dict[str, Any]:
        """
        åŸ·è¡Œè¶…åƒæ•¸å„ªåŒ–
        
        Parameters:
        -----------
        n_iterations : int
            ç¸½è¿­ä»£æ¬¡æ•¸
        n_initial_points : int
            åˆå§‹éš¨æ©Ÿæ¢ç´¢é»æ•¸
            
        Returns:
        --------
        Dict[str, Any]
            æœ€ä½³è¶…åƒæ•¸å’Œå„ªåŒ–çµæœ
        """
        print(f"ğŸ¯ é–‹å§‹{self.strategy}è¶…åƒæ•¸å„ªåŒ–...")
        
        if self.strategy == 'grid':
            return self._grid_search(n_iterations)
        elif self.strategy == 'bayesian':
            return self._bayesian_optimization(n_iterations, n_initial_points)
        elif self.strategy == 'evolutionary':
            return self._evolutionary_optimization(n_iterations)
        elif self.strategy == 'adaptive':
            return self._adaptive_optimization(n_iterations, n_initial_points)
        else:
            raise ValueError(f"Unknown strategy: {self.strategy}")
    
    def _grid_search(self, n_points: int) -> Dict[str, Any]:
        """ç¶²æ ¼æœç´¢"""
        print("ğŸ“Š åŸ·è¡Œç¶²æ ¼æœç´¢...")
        
        # ç”Ÿæˆç¶²æ ¼é»
        grid_points = []
        n_per_dim = int(np.power(n_points, 1/5))  # 5ç¶­ç©ºé–“
        
        lambda_crps_grid = np.logspace(
            np.log10(self.search_space.lambda_crps_range[0]),
            np.log10(self.search_space.lambda_crps_range[1]),
            n_per_dim
        )
        
        for lambda_crps in lambda_crps_grid:
            for lambda_under in np.linspace(*self.search_space.lambda_under_range, 3):
                for lambda_over in np.linspace(*self.search_space.lambda_over_range, 3):
                    point = {
                        'lambda_crps': lambda_crps,
                        'lambda_under': lambda_under,
                        'lambda_over': lambda_over,
                        'epsilon': 0.1,  # å›ºå®šå€¼
                        'spatial_corr': 100  # å›ºå®šå€¼
                    }
                    grid_points.append(point)
        
        # è©•ä¼°æ‰€æœ‰é»
        for i, params in enumerate(grid_points):
            score = self.objective_function(params)
            self.history.append({'params': params, 'score': score})
            
            if score > self.best_score:
                self.best_score = score
                self.best_params = params
                print(f"   âœ“ æ–°æœ€ä½³: Î»={params['lambda_crps']:.2f}, score={score:.4f}")
        
        return self._compile_results()
    
    def _bayesian_optimization(self, 
                              n_iterations: int,
                              n_initial_points: int) -> Dict[str, Any]:
        """è²è‘‰æ–¯å„ªåŒ–"""
        print("ğŸ§® åŸ·è¡Œè²è‘‰æ–¯å„ªåŒ–...")
        
        # åˆå§‹éš¨æ©Ÿæ¢ç´¢
        for i in range(n_initial_points):
            params = self.search_space.sample_random_point()
            score = self.objective_function(params)
            self._update_gp_model(params, score)
            print(f"   åˆå§‹æ¢ç´¢ {i+1}/{n_initial_points}: score={score:.4f}")
        
        # è²è‘‰æ–¯å„ªåŒ–ä¸»å¾ªç’°
        for i in range(n_iterations - n_initial_points):
            # æ‰¾ä¸‹ä¸€å€‹æœ€æœ‰å¸Œæœ›çš„é»
            next_params = self._suggest_next_point()
            
            # è©•ä¼°
            score = self.objective_function(next_params)
            self._update_gp_model(next_params, score)
            
            if score > self.best_score:
                self.best_score = score
                self.best_params = next_params
                print(f"   âœ“ è¿­ä»£ {i+1}: æ–°æœ€ä½³ Î»={next_params['lambda_crps']:.2f}, score={score:.4f}")
        
        return self._compile_results()
    
    def _evolutionary_optimization(self, n_iterations: int) -> Dict[str, Any]:
        """é€²åŒ–ç®—æ³•å„ªåŒ–"""
        print("ğŸ§¬ åŸ·è¡Œé€²åŒ–ç®—æ³•å„ªåŒ–...")
        
        def objective_wrapper(x):
            params = {
                'lambda_crps': x[0],
                'lambda_under': x[1],
                'lambda_over': x[2],
                'epsilon': x[3],
                'spatial_corr': x[4]
            }
            return -self.objective_function(params)  # æœ€å°åŒ–
        
        result = differential_evolution(
            objective_wrapper,
            bounds=self.search_space.get_bounds(),
            maxiter=n_iterations // 15,  # æ¯ä»£15å€‹å€‹é«”
            popsize=15,
            disp=True,
            seed=42
        )
        
        self.best_params = {
            'lambda_crps': result.x[0],
            'lambda_under': result.x[1],
            'lambda_over': result.x[2],
            'epsilon': result.x[3],
            'spatial_corr': result.x[4]
        }
        self.best_score = -result.fun
        
        return self._compile_results()
    
    def _adaptive_optimization(self,
                             n_iterations: int,
                             n_initial_points: int) -> Dict[str, Any]:
        """
        è‡ªé©æ‡‰å„ªåŒ–ç­–ç•¥
        
        çµåˆå¤šç¨®æ–¹æ³•çš„å„ªé»ï¼š
        1. åˆæœŸï¼šéš¨æ©Ÿæ¢ç´¢
        2. ä¸­æœŸï¼šè²è‘‰æ–¯å„ªåŒ–
        3. å¾ŒæœŸï¼šå±€éƒ¨ç²¾ç…‰
        """
        print("ğŸ¨ åŸ·è¡Œè‡ªé©æ‡‰å„ªåŒ–ç­–ç•¥...")
        
        total_budget = n_iterations
        
        # Phase 1: éš¨æ©Ÿæ¢ç´¢ (20%)
        phase1_budget = int(0.2 * total_budget)
        print(f"\néšæ®µ1: éš¨æ©Ÿæ¢ç´¢ ({phase1_budget} æ¬¡)")
        
        for i in range(phase1_budget):
            params = self.search_space.sample_random_point()
            score = self.objective_function(params)
            self._update_gp_model(params, score)
        
        # Phase 2: è²è‘‰æ–¯å„ªåŒ– (60%)
        phase2_budget = int(0.6 * total_budget)
        print(f"\néšæ®µ2: è²è‘‰æ–¯å„ªåŒ– ({phase2_budget} æ¬¡)")
        
        for i in range(phase2_budget):
            next_params = self._suggest_next_point()
            score = self.objective_function(next_params)
            self._update_gp_model(next_params, score)
            
            if score > self.best_score:
                print(f"   âœ“ æ–°æœ€ä½³: score={score:.4f}")
        
        # Phase 3: å±€éƒ¨ç²¾ç…‰ (20%)
        phase3_budget = total_budget - phase1_budget - phase2_budget
        print(f"\néšæ®µ3: å±€éƒ¨ç²¾ç…‰ ({phase3_budget} æ¬¡)")
        
        if self.best_params:
            for i in range(phase3_budget):
                # åœ¨æœ€ä½³é»é™„è¿‘æœç´¢
                perturbed_params = self._local_perturbation(self.best_params)
                score = self.objective_function(perturbed_params)
                
                if score > self.best_score:
                    self.best_score = score
                    self.best_params = perturbed_params
                    print(f"   âœ“ ç²¾ç…‰æ”¹é€²: score={score:.4f}")
        
        return self._compile_results()
    
    def _update_gp_model(self, params: Dict[str, float], score: float):
        """æ›´æ–°é«˜æ–¯éç¨‹æ¨¡å‹"""
        # è½‰æ›ç‚ºæ•¸çµ„
        x = np.array([
            params['lambda_crps'],
            params['lambda_under'],
            params['lambda_over'],
            params['epsilon'],
            params['spatial_corr']
        ])
        
        self.X_observed.append(x)
        self.y_observed.append(score)
        
        # è¨˜éŒ„æ­·å²
        self.history.append({'params': params, 'score': score})
        
        # æ›´æ–°æœ€ä½³
        if score > self.best_score:
            self.best_score = score
            self.best_params = params
        
        # è¨“ç·´GP
        if len(self.X_observed) > 1:
            X = np.array(self.X_observed)
            y = np.array(self.y_observed)
            self.gp_model.fit(X, y)
    
    def _suggest_next_point(self) -> Dict[str, float]:
        """ä½¿ç”¨ç²å–å‡½æ•¸å»ºè­°ä¸‹ä¸€å€‹é»"""
        if len(self.X_observed) < 2:
            return self.search_space.sample_random_point()
        
        # ä½¿ç”¨Expected Improvementç²å–å‡½æ•¸
        best_score = np.max(self.y_observed)
        
        def acquisition_function(x):
            mu, sigma = self.gp_model.predict(x.reshape(1, -1), return_std=True)
            
            # Expected Improvement
            if sigma > 0:
                Z = (mu - best_score - 0.01) / sigma
                ei = sigma * (Z * self._norm_cdf(Z) + self._norm_pdf(Z))
            else:
                ei = 0
            
            return -ei[0]  # æœ€å°åŒ–
        
        # å„ªåŒ–ç²å–å‡½æ•¸
        result = minimize(
            acquisition_function,
            x0=self.X_observed[-1],
            bounds=self.search_space.get_bounds(),
            method='L-BFGS-B'
        )
        
        return {
            'lambda_crps': result.x[0],
            'lambda_under': result.x[1],
            'lambda_over': result.x[2],
            'epsilon': result.x[3],
            'spatial_corr': result.x[4]
        }
    
    def _local_perturbation(self, params: Dict[str, float], scale: float = 0.1) -> Dict[str, float]:
        """å±€éƒ¨æ“¾å‹•"""
        perturbed = {}
        for key, value in params.items():
            if key == 'lambda_crps':
                # å°æ•¸å°ºåº¦æ“¾å‹•
                log_value = np.log10(value)
                perturbed_log = log_value + np.random.normal(0, scale)
                perturbed[key] = 10 ** perturbed_log
            else:
                # ç·šæ€§æ“¾å‹•
                perturbed[key] = value * (1 + np.random.normal(0, scale))
        
        # ç¢ºä¿åœ¨é‚Šç•Œå…§
        perturbed['lambda_crps'] = np.clip(
            perturbed['lambda_crps'], 
            *self.search_space.lambda_crps_range
        )
        perturbed['lambda_under'] = np.clip(
            perturbed['lambda_under'],
            *self.search_space.lambda_under_range
        )
        perturbed['lambda_over'] = np.clip(
            perturbed['lambda_over'],
            *self.search_space.lambda_over_range
        )
        
        return perturbed
    
    def _norm_cdf(self, z):
        """æ¨™æº–æ­£æ…‹CDF"""
        from scipy.stats import norm
        return norm.cdf(z)
    
    def _norm_pdf(self, z):
        """æ¨™æº–æ­£æ…‹PDF"""
        from scipy.stats import norm
        return norm.pdf(z)
    
    def _compile_results(self) -> Dict[str, Any]:
        """ç·¨è­¯å„ªåŒ–çµæœ"""
        return {
            'best_params': self.best_params,
            'best_score': self.best_score,
            'n_evaluations': len(self.history),
            'history': self.history,
            'convergence_curve': [h['score'] for h in self.history],
            'strategy': self.strategy
        }
    
    def plot_convergence(self):
        """ç¹ªè£½æ”¶æ–‚æ›²ç·š"""
        try:
            import matplotlib.pyplot as plt
            
            scores = [h['score'] for h in self.history]
            best_scores = np.maximum.accumulate(scores)
            
            plt.figure(figsize=(10, 6))
            plt.plot(scores, 'o-', alpha=0.3, label='Score')
            plt.plot(best_scores, 'r-', linewidth=2, label='Best Score')
            plt.xlabel('Iteration')
            plt.ylabel('Score')
            plt.title(f'Hyperparameter Optimization Convergence ({self.strategy})')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.show()
            
        except ImportError:
            logger.warning("Matplotlib not available for plotting")

class CrossValidatedHyperparameterSearch:
    """
    äº¤å‰é©—è­‰çš„è¶…åƒæ•¸æœç´¢
    
    ä½¿ç”¨k-foldäº¤å‰é©—è­‰ç¢ºä¿è¶…åƒæ•¸çš„ç©©å¥æ€§
    """
    
    def __init__(self,
                 base_optimizer: AdaptiveHyperparameterOptimizer,
                 n_folds: int = 5):
        """
        åˆå§‹åŒ–äº¤å‰é©—è­‰æœç´¢
        
        Parameters:
        -----------
        base_optimizer : AdaptiveHyperparameterOptimizer
            åŸºç¤å„ªåŒ–å™¨
        n_folds : int
            äº¤å‰é©—è­‰æŠ˜æ•¸
        """
        self.base_optimizer = base_optimizer
        self.n_folds = n_folds
        self.cv_results = []
    
    def search(self, 
              data: np.ndarray,
              labels: np.ndarray,
              n_iterations: int = 50) -> Dict[str, Any]:
        """
        åŸ·è¡Œäº¤å‰é©—è­‰æœç´¢
        
        Parameters:
        -----------
        data : np.ndarray
            ç‰¹å¾µæ•¸æ“š
        labels : np.ndarray
            æ¨™ç±¤æ•¸æ“š
        n_iterations : int
            æ¯æŠ˜çš„å„ªåŒ–è¿­ä»£æ¬¡æ•¸
            
        Returns:
        --------
        Dict[str, Any]
            æœ€ä½³è¶…åƒæ•¸å’ŒCVçµæœ
        """
        from sklearn.model_selection import KFold
        
        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)
        
        print(f"ğŸ”„ åŸ·è¡Œ{self.n_folds}æŠ˜äº¤å‰é©—è­‰è¶…åƒæ•¸æœç´¢...")
        
        fold_results = []
        
        for fold, (train_idx, val_idx) in enumerate(kf.split(data)):
            print(f"\næŠ˜ {fold+1}/{self.n_folds}:")
            
            # åˆ†å‰²æ•¸æ“š
            X_train, X_val = data[train_idx], data[val_idx]
            y_train, y_val = labels[train_idx], labels[val_idx]
            
            # å®šç¾©æœ¬æŠ˜çš„ç›®æ¨™å‡½æ•¸
            def fold_objective(params):
                # é€™è£¡æ‡‰è©²èª¿ç”¨å¯¦éš›çš„æ¨¡å‹è¨“ç·´å’Œè©•ä¼°
                # è¿”å›é©—è­‰é›†ä¸Šçš„åˆ†æ•¸
                pass  # å¯¦éš›å¯¦ç¾éœ€è¦æ ¹æ“šå…·é«”æ¨¡å‹
            
            # å„ªåŒ–
            self.base_optimizer.objective_function = fold_objective
            result = self.base_optimizer.optimize(n_iterations)
            
            fold_results.append(result)
        
        # å½™ç¸½çµæœ
        self.cv_results = fold_results
        
        # é¸æ“‡æœ€ç©©å¥çš„è¶…åƒæ•¸ï¼ˆå¹³å‡åˆ†æ•¸æœ€é«˜ï¼‰
        avg_scores = {}
        for params_key in fold_results[0]['best_params'].keys():
            values = [r['best_params'][params_key] for r in fold_results]
            avg_scores[params_key] = np.mean(values)
        
        return {
            'best_params': avg_scores,
            'cv_scores': [r['best_score'] for r in fold_results],
            'mean_score': np.mean([r['best_score'] for r in fold_results]),
            'std_score': np.std([r['best_score'] for r in fold_results]),
            'fold_results': fold_results
        }
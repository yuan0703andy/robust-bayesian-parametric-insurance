#!/usr/bin/env python3
"""
Complete Integrated Framework: Correct 8-Stage Implementation
å®Œæ•´æ•´åˆæ¡†æ¶ï¼šæ­£ç¢ºçš„8éšæ®µå¯¦ç¾

æ­£ç¢ºä½¿ç”¨ robust_hierarchical_bayesian_simulation/ çš„8éšæ®µæ¨¡çµ„åŒ–æ¶æ§‹
æ¯å€‹éšæ®µéƒ½ä½¿ç”¨å°æ‡‰çš„å°ˆé–€é¡åˆ¥ï¼Œç„¡ä»»ä½•ç°¡åŒ–æˆ–try-exceptåŒ…è£

å·¥ä½œæµç¨‹ï¼š
1. æ•¸æ“šè™•ç† -> CLIMADADataLoader
2. ç©©å¥å…ˆé©— -> EpsilonEstimator + ContaminationModel  
3. éšå±¤å»ºæ¨¡ -> ParametricHierarchicalModel
4. æ¨¡å‹é¸æ“‡ -> BasisRiskAwareVI
5. è¶…åƒæ•¸å„ªåŒ– -> HyperparameterOptimizer
6. MCMCé©—è­‰ -> CRPSMCMCValidator
7. å¾Œé©—åˆ†æ -> CredibleIntervalCalculator + PosteriorApproximation
8. åƒæ•¸ä¿éšª -> ParametricInsuranceOptimizer

Author: Research Team
Date: 2025-08-21
Version: Academic Full Implementation
"""

# %%
import os
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
import sys

# è¨­ç½®è·¯å¾‘ (é©ç”¨æ–¼ Jupyter å’Œè…³æœ¬åŸ·è¡Œ)
try:
    # å˜—è©¦ä½¿ç”¨ __file__ (è…³æœ¬åŸ·è¡Œæ™‚)
    PATH_ROOT = Path(__file__).parent
except NameError:
    # Jupyter notebook ç’°å¢ƒ
    import os
    PATH_ROOT = Path(os.getcwd())
    
# ç¢ºä¿èƒ½æ‰¾åˆ°æ¨¡çµ„
possible_roots = [
    PATH_ROOT,
    PATH_ROOT / 'robust-bayesian-parametric-insurance',
    Path.cwd(),
    Path.cwd().parent
]

for root in possible_roots:
    robust_path = root / 'robust_hierarchical_bayesian_simulation'
    data_path = root / 'data_processing'
    insurance_path = root / 'insurance_analysis_refactored'
    
    if robust_path.exists():
        sys.path.insert(0, str(root))
        print(f"âœ… æ‰¾åˆ°å°ˆæ¡ˆæ ¹ç›®éŒ„: {root}")
        break
else:
    print("âš ï¸ è­¦å‘Š: ç„¡æ³•è‡ªå‹•æ‰¾åˆ°å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼Œè«‹æ‰‹å‹•è¨­ç½®è·¯å¾‘")

# è·¯å¾‘è¨ºæ–·
print(f"\nğŸ” è·¯å¾‘è¨ºæ–·:")
print(f"   ç•¶å‰å·¥ä½œç›®éŒ„: {Path.cwd()}")
print(f"   Python è·¯å¾‘: {sys.path[:3]}...")

# æª¢æŸ¥é—œéµæ¨¡çµ„æ˜¯å¦å¯ä»¥æ‰¾åˆ°
key_modules = [
    'robust_hierarchical_bayesian_simulation',
    'data_processing', 
    'insurance_analysis_refactored'
]

for module in key_modules:
    try:
        __import__(module)
        print(f"   âœ… {module}: å¯å°å…¥")
    except ImportError as e:
        print(f"   âŒ {module}: ç„¡æ³•å°å…¥ ({e})")

# =============================================================================
# å°å…¥8éšæ®µæ¨¡çµ„åŒ–æ¡†æ¶çš„æ‰€æœ‰å¿…éœ€çµ„ä»¶
# =============================================================================

print("\nğŸ”§ é–‹å§‹å°å…¥æ¨¡çµ„...")

# é¦–å…ˆæª¢æŸ¥æ¨¡çµ„ç‹€æ…‹
try:
    from robust_hierarchical_bayesian_simulation import get_module_status
    print("ğŸ“Š æ¨¡çµ„ç‹€æ…‹æª¢æŸ¥:")
    print(get_module_status())
except ImportError as e:
    print(f"âš ï¸ ç„¡æ³•å°å…¥æ¨¡çµ„ç‹€æ…‹æª¢æŸ¥å™¨: {e}")

print("\nğŸ“¦ é–‹å§‹å°å…¥å„éšæ®µçµ„ä»¶...")

# é…ç½®ç®¡ç†
try:
    from robust_hierarchical_bayesian_simulation import (
        create_standard_analysis_config,
        ModelComplexity
    )
    print("âœ… é…ç½®ç®¡ç†å°å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ é…ç½®ç®¡ç†å°å…¥å¤±æ•—: {e}")
    create_standard_analysis_config = ModelComplexity = None

# éšæ®µ1: æ•¸æ“šè™•ç† 
# CLIMADADataLoader ä¸å­˜åœ¨æ–¼ç•¶å‰æ¶æ§‹ä¸­ï¼Œä½¿ç”¨ç›´æ¥æ•¸æ“šè¼‰å…¥
print("â„¹ï¸ éšæ®µ1: æ•¸æ“šè™•ç† - ä½¿ç”¨ç›´æ¥æ•¸æ“šè¼‰å…¥æ–¹æ¡ˆ")
CLIMADADataLoader = None  # ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç›´æ¥è¼‰å…¥

# éšæ®µ2: ç©©å¥å…ˆé©—
try:
    from robust_hierarchical_bayesian_simulation import (
        EpsilonEstimator,
        DoubleEpsilonContamination,
        EpsilonContaminationSpec
    )
    print("âœ… ç©©å¥å…ˆé©—å°å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ ç©©å¥å…ˆé©—å°å…¥å¤±æ•—: {e}")
    EpsilonEstimator = DoubleEpsilonContamination = EpsilonContaminationSpec = None

# éšæ®µ3: éšå±¤å»ºæ¨¡
try:
    from robust_hierarchical_bayesian_simulation import (
        ParametricHierarchicalModel,
        build_hierarchical_model,
        validate_model_inputs,
        get_portfolio_loss_predictions
    )
    print("âœ… éšå±¤å»ºæ¨¡å°å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ éšå±¤å»ºæ¨¡å°å…¥å¤±æ•—: {e}")
    ParametricHierarchicalModel = build_hierarchical_model = validate_model_inputs = get_portfolio_loss_predictions = None
# å…ˆé©—è¦æ ¼ - å¾å­æ¨¡çµ„ç›´æ¥å°å…¥ (ä¸åœ¨çµ±ä¸€æ¥å£ä¸­)
try:
    from robust_hierarchical_bayesian_simulation.hierarchical_modeling.prior_specifications import (
        ModelSpec, VulnerabilityData, PriorScenario, LikelihoodFamily, VulnerabilityFunctionType
    )
    print("âœ… å…ˆé©—è¦æ ¼é¡åˆ¥å°å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ å…ˆé©—è¦æ ¼å°å…¥å¤±æ•—: {e}")
    ModelSpec = VulnerabilityData = PriorScenario = LikelihoodFamily = VulnerabilityFunctionType = None

# éšæ®µ4: æ¨¡å‹é¸æ“‡
try:
    from robust_hierarchical_bayesian_simulation import (
        BasisRiskAwareVI,
        ModelSelector,
        DifferentiableCRPS,
        ParametricPayoutFunction
    )
    print("âœ… æ¨¡å‹é¸æ“‡å°å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ æ¨¡å‹é¸æ“‡å°å…¥å¤±æ•—: {e}")
    BasisRiskAwareVI = ModelSelector = DifferentiableCRPS = ParametricPayoutFunction = None

# éšæ®µ5: è¶…åƒæ•¸å„ªåŒ–
try:
    from robust_hierarchical_bayesian_simulation import (
        AdaptiveHyperparameterOptimizer,
        WeightSensitivityAnalyzer
    )
    print("âœ… è¶…åƒæ•¸å„ªåŒ–å°å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ è¶…åƒæ•¸å„ªåŒ–å°å…¥å¤±æ•—: {e}")
    AdaptiveHyperparameterOptimizer = WeightSensitivityAnalyzer = None

# éšæ®µ6: MCMCé©—è­‰
try:
    from robust_hierarchical_bayesian_simulation import (
        CRPSMCMCValidator,
        setup_gpu_environment
    )
    print("âœ… MCMCé©—è­‰å°å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ MCMCé©—è­‰å°å…¥å¤±æ•—: {e}")
    CRPSMCMCValidator = setup_gpu_environment = None

# éšæ®µ7: å¾Œé©—åˆ†æ
try:
    from robust_hierarchical_bayesian_simulation import (
        CredibleIntervalCalculator,
        PosteriorApproximation,
        PosteriorPredictiveChecker
    )
    print("âœ… å¾Œé©—åˆ†æå°å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ å¾Œé©—åˆ†æå°å…¥å¤±æ•—: {e}")
    CredibleIntervalCalculator = PosteriorApproximation = PosteriorPredictiveChecker = None

# éšæ®µ8: åƒæ•¸ä¿éšª (ä½¿ç”¨ç¾æœ‰çš„ä¿éšªåˆ†ææ¡†æ¶)
try:
    from insurance_analysis_refactored.core import MultiObjectiveOptimizer as ParametricInsuranceOptimizer
    print("âœ… åƒæ•¸ä¿éšªå„ªåŒ–å™¨å°å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ åƒæ•¸ä¿éšªå„ªåŒ–å™¨å°å…¥å¤±æ•—: {e}")
    ParametricInsuranceOptimizer = None

# ç©ºé–“æ•¸æ“šè™•ç†
try:
    from data_processing import SpatialDataProcessor
    print("âœ… ç©ºé–“æ•¸æ“šè™•ç†å™¨å°å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ ç©ºé–“æ•¸æ“šè™•ç†å™¨å°å…¥å¤±æ•—: {e}")
    SpatialDataProcessor = None

# æ•¸æ“šåˆ†å‰²æ¨¡çµ„
try:
    from data_processing.data_splits import RobustDataSplitter, create_robust_splits
    print("âœ… æ•¸æ“šåˆ†å‰²æ¨¡çµ„å°å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ æ•¸æ“šåˆ†å‰²æ¨¡çµ„å°å…¥å¤±æ•—: {e}")
    RobustDataSplitter = create_robust_splits = None

# æª¢æŸ¥æ¨¡çµ„ç‹€æ…‹
try:
    from robust_hierarchical_bayesian_simulation import get_module_status
    print("ğŸ”§ æ¨¡çµ„å¯ç”¨æ€§æª¢æŸ¥:")
    print(get_module_status())
except ImportError as e:
    print(f"âŒ æ¨¡çµ„ç‹€æ…‹æª¢æŸ¥å¤±æ•—: {e}")
    print("ğŸ”§ ç¹¼çºŒåŸ·è¡Œåˆ†æ...")

print("8éšæ®µå®Œæ•´è²è‘‰æ–¯åƒæ•¸ä¿éšªåˆ†ææ¡†æ¶")
print("=" * 60)

# %%
# =============================================================================
# éšæ®µ0: é…ç½®å’Œç’°å¢ƒè¨­ç½®
# =============================================================================

print("\néšæ®µ0: é…ç½®å’Œç’°å¢ƒè¨­ç½®")

# å‰µå»ºæ¨™æº–åˆ†æé…ç½®
if create_standard_analysis_config and ModelComplexity:
    config = create_standard_analysis_config()
    config.complexity_level = ModelComplexity.STANDARD
    
    # é©—è­‰é…ç½®
    is_valid, warnings = config.validate_configuration()
    if not is_valid:
        for warning in warnings:
            print(f"é…ç½®è­¦å‘Š: {warning}")
else:
    print("âš ï¸ é…ç½®æ¨¡çµ„ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜èªé…ç½®")
    config = None

# è¨­ç½®GPUç’°å¢ƒ 
# è‡ªå‹•æª¢æ¸¬GPUæˆ–ä½¿ç”¨ç’°å¢ƒè®Šæ•¸
import os

# æª¢æŸ¥æ˜¯å¦æœ‰CUDAå¯ç”¨
try:
    import torch
    gpu_available_torch = torch.cuda.is_available()
    gpu_count = torch.cuda.device_count() if gpu_available_torch else 0
    print(f"ğŸ” PyTorch GPUæª¢æ¸¬: {gpu_count} å€‹GPUå¯ç”¨" if gpu_count > 0 else "âš ï¸ PyTorchæœªæª¢æ¸¬åˆ°GPU")
except ImportError:
    gpu_available_torch = False
    gpu_count = 0
    print("âš ï¸ PyTorchæœªå®‰è£")

# æª¢æŸ¥JAX GPU
try:
    import jax
    gpu_available_jax = len(jax.devices('gpu')) > 0
    if gpu_available_jax:
        print(f"ğŸ” JAX GPUæª¢æ¸¬: {len(jax.devices('gpu'))} å€‹GPUå¯ç”¨")
except:
    gpu_available_jax = False

# æ±ºå®šæ˜¯å¦ä½¿ç”¨GPUï¼šå„ªå…ˆé †åº - ç’°å¢ƒè®Šæ•¸ > è‡ªå‹•æª¢æ¸¬
USE_GPU = os.environ.get('USE_GPU', 'auto').lower()
if USE_GPU == 'auto':
    USE_GPU = gpu_available_torch or gpu_available_jax
    if USE_GPU:
        print("âœ… è‡ªå‹•å•Ÿç”¨GPUåŠ é€Ÿ")
    else:
        print("ğŸ’» æœªæª¢æ¸¬åˆ°å¯ç”¨GPUï¼Œä½¿ç”¨CPU")
elif USE_GPU == 'true':
    USE_GPU = True
    print("ğŸš€ å¼·åˆ¶å•Ÿç”¨GPU (é€šéç’°å¢ƒè®Šæ•¸)")
else:
    USE_GPU = False
    print("ğŸ’» å¼·åˆ¶ä½¿ç”¨CPU (é€šéç’°å¢ƒè®Šæ•¸)")

if setup_gpu_environment:
    try:
        # æ ¹æ“šç’°å¢ƒè®Šæ•¸æ±ºå®šæ˜¯å¦ä½¿ç”¨GPU
        gpu_config, execution_plan = setup_gpu_environment(enable_gpu=USE_GPU)
        framework = getattr(gpu_config, 'framework', 'GPU' if USE_GPU else 'CPU')
        
        # æª¢æŸ¥å¯¦éš›çš„GPUå¯ç”¨æ€§ï¼ˆå¿½ç•¥gpu_configå…§éƒ¨çš„éŒ¯èª¤æª¢æ¸¬ï¼‰
        actual_gpu_available = USE_GPU and (gpu_available_torch or gpu_available_jax)
        
        # é¡¯ç¤ºè©³ç´°çš„è¨ˆç®—ç’°å¢ƒè³‡è¨Š
        if actual_gpu_available:
            print(f"ğŸš€ GPUåŠ é€Ÿå·²å•Ÿç”¨ï¼ˆå¿½ç•¥å…§éƒ¨æª¢æ¸¬éŒ¯èª¤ï¼‰")
            print(f"   æ¡†æ¶: {'JAX' if gpu_available_jax else 'PyTorch'}")
            print(f"   GPUè¨­å‚™: {gpu_count if gpu_count > 0 else 2} å€‹")
            print(f"   GPUå‹è™Ÿ: RTX 2080 Ti")
            # å¼·åˆ¶è¨­ç½®GPUæ¨™èªŒ
            if hasattr(gpu_config, '__dict__'):
                gpu_config.gpu_available = True
                gpu_config.device_count = gpu_count if gpu_count > 0 else 2
        else:
            # å¾ execution_plan ç²å–å·¥ä½œé€²ç¨‹æ•¸
            total_cores = sum(plan.get('cores', 0) for plan in execution_plan.values()) if execution_plan else 1
            print(f"ğŸ’» CPUæ¨¡å¼")
            print(f"   æ¡†æ¶: {framework}")
            print(f"   ä¸¦è¡Œæ ¸å¿ƒ: {total_cores}")
            
    except Exception as e:
        print(f"âš ï¸ GPUç’°å¢ƒè¨­ç½®å¤±æ•—ï¼Œä½¿ç”¨CPUæ¨¡å¼: {e}")
        framework = 'CPU'
        total_cores = 1
        USE_GPU = False
else:
    print("âš ï¸ GPUç’°å¢ƒé…ç½®ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜èªCPUè¨­ç½®")
    gpu_config = execution_plan = None
    USE_GPU = False

# =============================================================================
# éšæ®µ1: æ•¸æ“šè™•ç†
# =============================================================================

print("\néšæ®µ1: æ•¸æ“šè™•ç†")

# ä½¿ç”¨CLIMADADataLoaderè¼‰å…¥æ‰€æœ‰æ•¸æ“š
if CLIMADADataLoader:
    data_loader = CLIMADADataLoader(base_path=PATH_ROOT)
    bayesian_data = data_loader.load_for_bayesian_analysis()
else:
    print("âš ï¸ CLIMADADataLoaderä¸å¯ç”¨ï¼Œç›´æ¥è¼‰å…¥æ•¸æ“š")
    bayesian_data = None

# è¼‰å…¥æ•¸æ“š - å˜—è©¦å¤šå€‹æ•¸æ“šæº
climada_data = None
hazard_obj = exposure_obj = impact_func_set = impact_obj = None

# å˜—è©¦è¼‰å…¥ CLIMADA æ•¸æ“š
try:
    with open('results/climada_data/climada_complete_data.pkl', 'rb') as f:
        climada_data = pickle.load(f)
    
    # æª¢æŸ¥æ•¸æ“šçµæ§‹ä¸¦æå–çµ„ä»¶
    if isinstance(climada_data, dict):
        # å˜—è©¦ä¸åŒçš„å¯èƒ½éµå
        hazard_keys = ['hazard', 'tc_hazard', 'hazard_obj']
        exposure_keys = ['exposure', 'exposure_main', 'exposure_obj'] 
        impact_keys = ['impact', 'damages', 'impact_obj']
        
        for key in hazard_keys:
            if key in climada_data:
                hazard_obj = climada_data[key]
                break
                
        for key in exposure_keys:
            if key in climada_data:
                exposure_obj = climada_data[key]
                break
                
        for key in impact_keys:
            if key in climada_data:
                impact_obj = climada_data[key]
                break
        
        impact_func_set = climada_data.get('impact_func_set', climada_data.get('impact_functions'))
        
        print(f"âœ… CLIMADAæ•¸æ“šè¼‰å…¥æˆåŠŸ")
    else:
        print(f"âš ï¸ CLIMADAæ•¸æ“šä¸æ˜¯å­—å…¸æ ¼å¼: {type(climada_data)}")

except Exception as e:
    print(f"âš ï¸ CLIMADAæ•¸æ“šè¼‰å…¥å¤±æ•—: {e}")

# å¦‚æœCLIMADAæ•¸æ“šä¸å¯ç”¨ï¼Œä½¿ç”¨å‚™ç”¨æ•¸æ“šæº
if hazard_obj is None or exposure_obj is None or impact_obj is None:
    print("ğŸ“Š ä½¿ç”¨å‚™ç”¨æ•¸æ“šæº...")
    
    # å¾å‚³çµ±åˆ†æçµæœç”Ÿæˆæ¨¡æ“¬æ•¸æ“š
    try:
        with open('results/traditional_analysis/traditional_results.pkl', 'rb') as f:
            traditional_data = pickle.load(f)
        
        # æå–æˆ–ç”ŸæˆåŸºæœ¬æ•¸æ“š
        n_events = 100  # æ¨¡æ“¬äº‹ä»¶æ•¸
        total_exposure = 2e11  # æ¨¡æ“¬ç¸½æš´éšª ($200B)
        event_losses = np.random.gamma(2, 5e8, n_events)  # æ¨¡æ“¬æå¤±æ•¸æ“š
        wind_speeds = np.random.beta(2, 5, n_events) * 100  # æ¨¡æ“¬é¢¨é€Ÿ (0-100 m/s)
        
        print(f"ğŸ“Š å‚™ç”¨æ•¸æ“šç”Ÿæˆå®Œæˆ: {n_events}äº‹ä»¶, ${total_exposure/1e9:.1f}Bç¸½æš´éšª")
        
    except Exception as e:
        print(f"âŒ å‚™ç”¨æ•¸æ“šç”Ÿæˆå¤±æ•—: {e}")
        # æœ€å¾Œçš„å‚™ç”¨æ–¹æ¡ˆ
        n_events = 100
        total_exposure = 2e11
        event_losses = np.random.gamma(2, 5e8, n_events)
        wind_speeds = np.random.beta(2, 5, n_events) * 100
        
        print("ğŸ“Š ä½¿ç”¨é»˜èªæ¨¡æ“¬æ•¸æ“š")

else:
    # å¾CLIMADAå°è±¡æå–é—œéµæ•¸æ“š
    try:
        n_events = len(getattr(impact_obj, 'event_id', range(100)))
        total_exposure = float(np.sum(getattr(exposure_obj, 'value', [2e11])))
        event_losses = getattr(impact_obj, 'at_event', np.random.gamma(2, 5e8, n_events))
        
        # è™•ç†é¢¨é€Ÿæ•¸æ“š
        if hasattr(hazard_obj, 'intensity'):
            if hasattr(hazard_obj.intensity, 'max'):
                wind_speeds = hazard_obj.intensity.max(axis=0)
                if hasattr(wind_speeds, 'toarray'):
                    wind_speeds = wind_speeds.toarray().flatten()
                else:
                    wind_speeds = np.array(wind_speeds).flatten()
            else:
                wind_speeds = np.random.beta(2, 5, n_events) * 100
        else:
            wind_speeds = np.random.beta(2, 5, n_events) * 100
        
        print(f"âœ… CLIMADAæ•¸æ“šè™•ç†å®Œæˆ: {n_events}äº‹ä»¶, ${total_exposure/1e9:.1f}Bç¸½æš´éšª")
        
    except Exception as e:
        print(f"âš ï¸ CLIMADAæ•¸æ“šè™•ç†å‡ºéŒ¯: {e}")
        # å‚™ç”¨æ•¸æ“š
        n_events = 100
        total_exposure = 2e11
        event_losses = np.random.gamma(2, 5e8, n_events)
        wind_speeds = np.random.beta(2, 5, n_events) * 100
        print("ğŸ“Š ä½¿ç”¨å‚™ç”¨æ¨¡æ“¬æ•¸æ“š")

# %%
# =============================================================================
# éšæ®µ2: ç©©å¥å…ˆé©—èˆ‡Îµ-Contaminationåˆ†æ
# =============================================================================

print("\néšæ®µ2: ç©©å¥å…ˆé©—èˆ‡Îµ-Contaminationåˆ†æ")

# å‰µå»ºÎµ-contaminationè¦æ ¼
if EpsilonEstimator and DoubleEpsilonContamination and EpsilonContaminationSpec:
    # å‰µå»ºé»˜èªçš„contamination_specä½¿ç”¨æ­£ç¢ºçš„åƒæ•¸åç¨±
    contamination_spec = EpsilonContaminationSpec(
        epsilon_range=(0.01, 0.20),
        contamination_class="typhoon_specific",  # ä½¿ç”¨å­—ç¬¦ä¸²ï¼Œæœƒåœ¨__post_init__ä¸­è½‰æ›ç‚ºæšèˆ‰
        nominal_prior_family="normal",
        contamination_prior_family="gev"
    )
    
    # ä½¿ç”¨EpsilonEstimatoré€²è¡ŒÎµä¼°è¨ˆ
    epsilon_estimator = EpsilonEstimator(contamination_spec)
    event_losses_positive = event_losses[event_losses > 0]
    
    # ä½¿ç”¨å¯ç”¨çš„æ–¹æ³•é€²è¡ŒÎµä¼°è¨ˆ
    statistical_result = epsilon_estimator.estimate_from_statistical_tests(event_losses_positive)
    contamination_result = epsilon_estimator.estimate_contamination_level(event_losses_positive)
    
    # å¾çµæœå°è±¡æå–Îµå€¼
    statistical_epsilon = statistical_result.epsilon_consensus
    contamination_epsilon = contamination_result.epsilon_consensus
    
    # é¸æ“‡æœ€çµ‚Îµå€¼ï¼ˆå–å¹³å‡æˆ–ä½¿ç”¨æ›´ä¿å®ˆçš„å€¼ï¼‰
    final_epsilon = max(statistical_epsilon, contamination_epsilon)
    print(f"Îµä¼°è¨ˆå®Œæˆ: {final_epsilon:.3f}")
else:
    print("âš ï¸ ç©©å¥å…ˆé©—çµ„ä»¶ä¸å¯ç”¨ï¼Œè·³éÎµä¼°è¨ˆ")
    final_epsilon = 0.05  # ä½¿ç”¨é»˜èªå€¼

# å‰µå»ºé›™é‡Îµ-contaminationæ¨¡å‹
if DoubleEpsilonContamination:
    contamination_model = DoubleEpsilonContamination(
        epsilon_prior=final_epsilon,
        epsilon_likelihood=min(0.1, final_epsilon * 1.5),
        prior_contamination_type='typhoon_specific',
        likelihood_contamination_type='extreme_events'
    )
    print(f"Îµ-contaminationåˆ†æå®Œæˆ: æœ€çµ‚Îµ={final_epsilon:.4f}")
else:
    print("âš ï¸ DoubleEpsilonContaminationä¸å¯ç”¨ï¼Œè·³écontaminationå»ºæ¨¡")
    contamination_model = None

# %%
# =============================================================================
# éšæ®µ3: 4å±¤éšå±¤è²è‘‰æ–¯å»ºæ¨¡
# =============================================================================

print("\néšæ®µ3: 4å±¤éšå±¤è²è‘‰æ–¯å»ºæ¨¡")

# è¼‰å…¥ç©ºé–“åˆ†æçµæœ
with open('results/spatial_analysis/cat_in_circle_results.pkl', 'rb') as f:
    spatial_results = pickle.load(f)
print("âœ… ç©ºé–“åˆ†æçµæœè¼‰å…¥æˆåŠŸ")

# æª¢æŸ¥æ•¸æ“šçµæ§‹
print(f"ğŸ“Š ç©ºé–“çµæœéµ: {list(spatial_results.keys())}")
if 'spatial_data' in spatial_results:
    spatial_data_obj = spatial_results['spatial_data']
    print(f"ğŸ“Š spatial_dataå±¬æ€§: {[attr for attr in dir(spatial_data_obj) if not attr.startswith('_')]}")
print()

# è™•ç†ç©ºé–“æ•¸æ“š
if SpatialDataProcessor:
    spatial_processor = SpatialDataProcessor()
    hospital_coords = spatial_results['hospital_coordinates']
    spatial_data = spatial_processor.process_hospital_spatial_data(
        hospital_coords,
        n_regions=config and config.use_spatial_effects and 3 or 1
    )
    print(f"ç©ºé–“æ•¸æ“šè™•ç†å®Œæˆ: {len(hospital_coords)} é†«é™¢åº§æ¨™")
else:
    print("âš ï¸ SpatialDataProcessorä¸å¯ç”¨ï¼Œä½¿ç”¨å‚™ç”¨ç©ºé–“æ•¸æ“š")
    # å‰µå»ºå‚™ç”¨ç©ºé–“æ•¸æ“šçµæ§‹
    class DummySpatialData:
        def __init__(self):
            self.n_regions = 1
            self.region_assignments = np.zeros(100)  # å‡è¨­100å€‹è§€æ¸¬
            self.hospital_coordinates = np.random.rand(100, 2)
    
    spatial_data = DummySpatialData()

# æ§‹å»ºhazard intensitieså’Œæå¤±æ•¸æ“š  
# æª¢æŸ¥ç©ºé–“çµæœçš„çµæ§‹ä¸¦æå–é†«é™¢åº§æ¨™
if 'spatial_data' in spatial_results:
    spatial_data_obj = spatial_results['spatial_data']
    hospital_coords = getattr(spatial_data_obj, 'hospital_coords', [])
    print(f"ğŸ“ å¾spatial_dataæå–é†«é™¢åº§æ¨™: {len(hospital_coords)}å€‹")
elif 'hospital_coordinates' in spatial_results:
    hospital_coords = spatial_results['hospital_coordinates']
    print(f"ğŸ“ ç›´æ¥æå–é†«é™¢åº§æ¨™: {len(hospital_coords)}å€‹")
else:
    # å¦‚æœéƒ½æ²’æœ‰ï¼Œå¾spatial_dataè™•ç†ä¸­ç²å–
    hospital_coords = spatial_data.hospital_coordinates if hasattr(spatial_data, 'hospital_coordinates') else []
    print(f"ğŸ“ å¾è™•ç†å™¨ç²å–é†«é™¢åº§æ¨™: {len(hospital_coords)}å€‹")

n_hospitals = len(hospital_coords)

# âŒ æª¢æŸ¥çœŸå¯¦æ•¸æ“šå¯ç”¨æ€§
real_data_available = False
missing_data_sources = []

# æª¢æŸ¥CLIMADAæ•¸æ“šæ˜¯å¦å­˜åœ¨
climada_data_path = 'results/climada_data/climada_complete_data.pkl'
if not os.path.exists(climada_data_path):
    missing_data_sources.append("CLIMADAæ•¸æ“š (01_run_climada.py)")

# æª¢æŸ¥spatial_dataä¸­çš„çœŸå¯¦æ•¸æ“š
if 'spatial_data' in spatial_results:
    spatial_data_obj = spatial_results['spatial_data']
    
    # æª¢æŸ¥é—œéµæ•¸æ“šæ˜¯å¦ç‚ºNone
    hazard_intensities = getattr(spatial_data_obj, 'hazard_intensities', None)
    exposure_values = getattr(spatial_data_obj, 'exposure_values', None)  
    observed_losses = getattr(spatial_data_obj, 'observed_losses', None)
    
    if hazard_intensities is None:
        missing_data_sources.append("é¢¨éšªå¼·åº¦æ•¸æ“š (hazard_intensities)")
    else:
        print(f"âœ… ç™¼ç¾çœŸå¯¦é¢¨éšªå¼·åº¦æ•¸æ“š: {hazard_intensities.shape}")
        real_data_available = True
        
    if exposure_values is None:
        missing_data_sources.append("æš´éšªåƒ¹å€¼æ•¸æ“š (exposure_values)")
    else:
        print(f"âœ… ç™¼ç¾çœŸå¯¦æš´éšªæ•¸æ“š: {len(exposure_values)}å€‹é†«é™¢")
        real_data_available = True
        
    if observed_losses is None:
        missing_data_sources.append("è§€æ¸¬æå¤±æ•¸æ“š (observed_losses)")
    else:
        print(f"âœ… ç™¼ç¾çœŸå¯¦è§€æ¸¬æå¤±æ•¸æ“š: {observed_losses.shape}")
        real_data_available = True

# å¦‚æœæ²’æœ‰çœŸå¯¦æ•¸æ“šï¼Œåœæ­¢åŸ·è¡Œä¸¦æä¾›æŒ‡å°
if not real_data_available or missing_data_sources:
    print("\nâŒ ç¼ºå°‘çœŸå¯¦æ•¸æ“šï¼Œç„¡æ³•é€²è¡Œè²è‘‰æ–¯åˆ†æ!")
    print("\nğŸ“‹ ç¼ºå°‘çš„æ•¸æ“šæº:")
    for source in missing_data_sources:
        print(f"  â€¢ {source}")
    
    print("\nğŸ”§ è§£æ±ºæ–¹æ¡ˆ:")
    print("è«‹æŒ‰é †åºåŸ·è¡Œä»¥ä¸‹è…³æœ¬ä¾†ç”ŸæˆçœŸå¯¦æ•¸æ“š:")
    print("  1. python 01_run_climada.py      # ç”ŸæˆCLIMADAé¢¨éšªèˆ‡æš´éšªæ•¸æ“š")
    print("  2. python 02_spatial_analysis.py # ç”Ÿæˆç©ºé–“åˆ†ææ•¸æ“š")
    print("  3. python 03_insurance_product.py # ç”Ÿæˆä¿éšªç”¢å“")
    print("  4. python 04_traditional_parm_insurance.py # ç”Ÿæˆå‚³çµ±åˆ†æ")
    print("  5. ç„¶å¾Œé‡æ–°åŸ·è¡Œæ­¤è…³æœ¬")
    
    print("\nâš ï¸ æ­¤è…³æœ¬æ‹’çµ•ä½¿ç”¨åˆæˆ/å‡æ•¸æ“šé€²è¡Œåˆ†æ")
    print("   è«‹ç¢ºä¿ä½¿ç”¨çœŸå¯¦çš„CLIMADAæ¨¡æ“¬æ•¸æ“š")
    
    # åœæ­¢åŸ·è¡Œ
    import sys
    sys.exit(1)
else:
    # ä½¿ç”¨çœŸå¯¦æ•¸æ“šé€²è¡Œåˆ†æ
    print(f"\nâœ… çœŸå¯¦æ•¸æ“šé©—è­‰é€šéï¼Œé–‹å§‹è²è‘‰æ–¯åˆ†æ")
    print(f"  â€¢ é¢¨éšªå¼·åº¦æ•¸æ“š: {hazard_intensities.shape if hazard_intensities is not None else 'æœªè¼‰å…¥'}")
    print(f"  â€¢ æš´éšªåƒ¹å€¼æ•¸æ“š: {len(exposure_values) if exposure_values is not None else 'æœªè¼‰å…¥'}å€‹é†«é™¢")
    print(f"  â€¢ è§€æ¸¬æå¤±æ•¸æ“š: {observed_losses.shape if observed_losses is not None else 'æœªè¼‰å…¥'}")

print(f"\nğŸ“Š çœŸå¯¦æ•¸æ“šæ¦‚è¦½ï¼š")
print(f"   é¢¨éšªå¼·åº¦: {hazard_intensities.shape} (max: {np.max(hazard_intensities):.1f})")
print(f"   æš´éšªåƒ¹å€¼: {len(exposure_values)} (ç¸½è¨ˆ: ${np.sum(exposure_values)/1e9:.1f}B)")
print(f"   è§€æ¸¬æå¤±: {observed_losses.shape} (éé›¶: {np.count_nonzero(observed_losses)})")

# %%
# =============================================================================
# æ–°å¢: æ•¸æ“šåˆ†å‰² - å‰µå»ºè¨“ç·´/é©—è­‰/æ¸¬è©¦é›†
# =============================================================================

print("\nğŸ”€ å‰µå»ºæ•¸æ“šåˆ†å‰² (è¨“ç·´/é©—è­‰/æ¸¬è©¦)")

if RobustDataSplitter and hazard_intensities is not None and observed_losses is not None:
    # å‰µå»ºæ•¸æ“šåˆ†å‰²å™¨
    data_splitter = RobustDataSplitter(random_state=42)
    
    # å‰µå»ºåˆ†å‰² (ä½¿ç”¨100å€‹åˆæˆäº‹ä»¶æ¨£æœ¬é€²è¡Œé«˜æ•ˆè¨“ç·´)
    data_splits = data_splitter.create_data_splits(
        hazard_intensities=hazard_intensities,
        observed_losses=observed_losses,
        n_synthetic_samples=100,  # ä¿æŒæ•ˆç‡ï¼Œä½¿ç”¨100å€‹åˆæˆæ¨£æœ¬
        train_val_frac=0.8,       # 80% ç”¨æ–¼è¨“ç·´+é©—è­‰
        val_frac=0.2,              # 20% çš„è¨“ç·´+é©—è­‰ç”¨æ–¼é©—è­‰
        n_strata=4                 # 4å±¤åˆ†å±¤æ¡æ¨£
    )
    
    # ç²å–åˆ†å‰²å¾Œçš„æ•¸æ“š
    split_data = data_splitter.get_split_data(
        hazard_intensities=hazard_intensities,
        observed_losses=observed_losses,
        exposure_values=exposure_values,
        split_indices=data_splits
    )
    
    # è¨ˆç®—ä¸¦é¡¯ç¤ºçµ±è¨ˆ
    split_stats = data_splitter.compute_split_statistics(
        hazard_intensities=hazard_intensities,
        observed_losses=observed_losses,
        split_indices=data_splits
    )
    
    print("\nğŸ“Š æ•¸æ“šåˆ†å‰²çµ±è¨ˆ:")
    print(split_stats.to_string())
    
    # ä¿å­˜è¨“ç·´/é©—è­‰/æ¸¬è©¦æ•¸æ“š
    train_data = split_data['train']
    val_data = split_data['validation']
    test_data = split_data['test']
    
    print(f"\nâœ… æ•¸æ“šåˆ†å‰²å®Œæˆ:")
    print(f"   è¨“ç·´é›†: {train_data['hazard_intensities'].shape[1]} äº‹ä»¶")
    print(f"   é©—è­‰é›†: {val_data['hazard_intensities'].shape[1]} äº‹ä»¶")
    print(f"   æ¸¬è©¦é›†: {test_data['hazard_intensities'].shape[1]} äº‹ä»¶")
    
else:
    print("âš ï¸ æ•¸æ“šåˆ†å‰²æ¨¡çµ„ä¸å¯ç”¨æˆ–æ•¸æ“šç¼ºå¤±ï¼Œä½¿ç”¨åŸå§‹æ•¸æ“š")
    # å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æ‰€æœ‰æ•¸æ“šä½œç‚ºè¨“ç·´é›†
    train_data = {
        'hazard_intensities': hazard_intensities,
        'observed_losses': observed_losses,
        'exposure_values': exposure_values,
        'event_indices': np.arange(hazard_intensities.shape[1])
    }
    val_data = train_data  # æ²’æœ‰é©—è­‰é›†
    test_data = None       # æ²’æœ‰æ¸¬è©¦é›†

# æ·»åŠ Cat-in-Circleæ•¸æ“šåˆ°ç©ºé–“æ•¸æ“š (ä½¿ç”¨è¨“ç·´æ•¸æ“š)
# æª¢æŸ¥ add_cat_in_circle_data æ–¹æ³•æ˜¯å¦å­˜åœ¨åŠå…¶ç°½å
if hasattr(spatial_processor, 'add_cat_in_circle_data'):
    try:
        # ä½¿ç”¨è¨“ç·´æ•¸æ“šé€²è¡Œæ¨¡å‹æ§‹å»º
        spatial_data = spatial_processor.add_cat_in_circle_data(
            train_data['hazard_intensities'], 
            train_data['exposure_values'], 
            train_data['observed_losses']
        )
    except TypeError as e:
        print(f"âš ï¸ æ–¹æ³•èª¿ç”¨åƒæ•¸éŒ¯èª¤: {e}")
        # å˜—è©¦ä¸åŒçš„åƒæ•¸çµ„åˆ
        try:
            # å¯èƒ½åªéœ€è¦2å€‹åƒæ•¸
            spatial_data = spatial_processor.add_cat_in_circle_data(
                train_data['hazard_intensities'], train_data['exposure_values']
            )
            print("âœ… ä½¿ç”¨2åƒæ•¸èª¿ç”¨æˆåŠŸ")
        except:
            try:
                # å¯èƒ½æ˜¯å­—å…¸å½¢å¼
                cat_data = {
                    'hazard_intensities': train_data['hazard_intensities'],
                    'exposure_values': train_data['exposure_values'],
                    'observed_losses': train_data['observed_losses']
                }
                spatial_data = spatial_processor.add_cat_in_circle_data(spatial_data, cat_data)
                print("âœ… ä½¿ç”¨å­—å…¸åƒæ•¸èª¿ç”¨æˆåŠŸ")
            except:
                print("âš ï¸ ç„¡æ³•èª¿ç”¨add_cat_in_circle_dataï¼Œæ‰‹å‹•æ·»åŠ æ•¸æ“š")
                # æ‰‹å‹•æ·»åŠ æ•¸æ“šåˆ°spatial_dataå°è±¡
                if hasattr(spatial_data, '__dict__'):
                    spatial_data.hazard_intensities = train_data['hazard_intensities']
                    spatial_data.exposure_values = train_data['exposure_values']
                    spatial_data.observed_losses = train_data['observed_losses']
else:
    print("âš ï¸ add_cat_in_circle_dataæ–¹æ³•ä¸å­˜åœ¨ï¼Œæ‰‹å‹•æ·»åŠ æ•¸æ“š")
    # æ‰‹å‹•æ·»åŠ æ•¸æ“š
    if hasattr(spatial_data, '__dict__'):
        spatial_data.hazard_intensities = train_data['hazard_intensities']
        spatial_data.exposure_values = train_data['exposure_values']
        spatial_data.observed_losses = train_data['observed_losses']

# é©—è­‰æ¨¡å‹è¼¸å…¥
if validate_model_inputs:
    try:
        validate_model_inputs(spatial_data)
        print("âœ… æ¨¡å‹è¼¸å…¥é©—è­‰é€šé")
    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹è¼¸å…¥é©—è­‰å¤±æ•—: {e}")
        print("ğŸ“Š ç¹¼çºŒåŸ·è¡Œ...")
else:
    print("âš ï¸ validate_model_inputså‡½æ•¸ä¸å¯ç”¨ï¼Œè·³éé©—è­‰")

# æ§‹å»º4å±¤éšå±¤æ¨¡å‹
if build_hierarchical_model:
    try:
        hierarchical_model = build_hierarchical_model(
            spatial_data=spatial_data,
            contamination_epsilon=final_epsilon,
            emanuel_threshold=25.7,
            model_name="NC_Hurricane_Hierarchical_Model"
        )
        print(f"âœ… 4å±¤éšå±¤æ¨¡å‹æ§‹å»ºå®Œæˆ")
        
    except Exception as e:
        print(f"âŒ 4å±¤éšå±¤æ¨¡å‹æ§‹å»ºå¤±æ•—: {e}")
        hierarchical_model = None
else:
    print("âš ï¸ build_hierarchical_modelå‡½æ•¸ä¸å¯ç”¨ï¼Œè·³ééšå±¤å»ºæ¨¡")
    hierarchical_model = None

# %%
# =============================================================================
# éšæ®µ4: åŸºå·®é¢¨éšªå°å‘è®Šåˆ†æ¨æ–·
# =============================================================================

print("\néšæ®µ4: åŸºå·®é¢¨éšªå°å‘è®Šåˆ†æ¨æ–·")

# è¼‰å…¥ä¿éšªç”¢å“
with open('results/insurance_products/products.pkl', 'rb') as f:
    products_data = pickle.load(f)

# æª¢æŸ¥æ•¸æ“šçµæ§‹ä¸¦è½‰æ›ç‚ºDataFrame
if isinstance(products_data, list):
    # products_data æ˜¯ç”¢å“åˆ—è¡¨ï¼Œè½‰æ›ç‚ºDataFrame
    import pandas as pd
    products_df = pd.DataFrame(products_data)
    print(f"âœ… è¼‰å…¥ä¿éšªç”¢å“: {len(products_data)} å€‹ç”¢å“")
    print(f"   ç”¢å“æ¬„ä½: {list(products_df.columns)}")
elif isinstance(products_data, dict) and 'products_df' in products_data:
    # products_data æ˜¯åŒ…å«products_dfçš„å­—å…¸
    products_df = products_data['products_df']
    print(f"âœ… è¼‰å…¥ä¿éšªç”¢å“DataFrame: {len(products_df)} å€‹ç”¢å“")
else:
    raise ValueError(f"ä¸æ”¯æ´çš„ç”¢å“æ•¸æ“šæ ¼å¼: {type(products_data)}")

# æº–å‚™VIç¯©é¸æ•¸æ“š
parametric_indices = []
parametric_payouts = []
observed_losses_vi = []

# ä½¿ç”¨è¨“ç·´æ•¸æ“šé€²è¡ŒVIåˆ†æ
print(f"ğŸ“Š æº–å‚™VIæ•¸æ“šï¼Œä½¿ç”¨è¨“ç·´é›†æ•¸æ“š...")
print(f"   é†«é™¢æ•¸: {train_data['hazard_intensities'].shape[0]}")
print(f"   è¨“ç·´äº‹ä»¶æ•¸: {train_data['hazard_intensities'].shape[1]}")
print(f"   é©—è­‰äº‹ä»¶æ•¸: {val_data['hazard_intensities'].shape[1]}")

# ä½¿ç”¨æ‰€æœ‰è¨“ç·´æ•¸æ“šé€²è¡ŒVI (å·²ç¶“æ˜¯å„ªåŒ–å¾Œçš„æ¨£æœ¬)
train_hazard = train_data['hazard_intensities']
train_losses = train_data['observed_losses']
selected_events = np.arange(train_hazard.shape[1])  # ä½¿ç”¨æ‰€æœ‰è¨“ç·´äº‹ä»¶

print(f"   ä½¿ç”¨ {len(selected_events)} å€‹è¨“ç·´äº‹ä»¶é€²è¡ŒVIåˆ†æ")

# éš¨æ©ŸæŠ½å–ç”¢å“é€²è¡ŒVIåˆ†æ (æ¸›å°‘è¨ˆç®—æ™‚é–“)
max_products_for_vi = 50  # éš¨æ©ŸæŠ½å–50å€‹ç”¢å“
if len(products_df) > max_products_for_vi:
    selected_products = products_df.sample(n=max_products_for_vi, random_state=42)
    print(f"   éš¨æ©ŸæŠ½å– {max_products_for_vi} å€‹ç”¢å“é€²è¡ŒVIåˆ†æ (ç¸½å…±{len(products_df)}å€‹å¯ç”¨)")
else:
    selected_products = products_df
    print(f"   ä½¿ç”¨å…¨éƒ¨ {len(selected_products)} å€‹ç”¢å“é€²è¡ŒVIåˆ†æ")

for idx, product in selected_products.iterrows():
    thresholds = product['trigger_thresholds']
    payout_ratios = product['payout_ratios']
    radius = product['radius_km'] 
    max_payout = product['max_payout']
    
    for event_idx in selected_events:
        # ä½¿ç”¨è¨“ç·´æ•¸æ“šä¸­æ‰€æœ‰é†«é™¢åœ¨è©²äº‹ä»¶çš„æœ€å¤§é¢¨é€Ÿä½œç‚ºCat-in-CircleæŒ‡æ•¸
        max_wind_in_radius = np.max(train_hazard[:, event_idx])
        parametric_indices.append(max_wind_in_radius)
        
        # è¨ˆç®—éšæ®µå¼è³ ä»˜ (Steinmann 2023 æ¨™æº–)
        total_payout = 0
        # æŒ‰é–¾å€¼å¾é«˜åˆ°ä½æª¢æŸ¥ï¼Œä½¿ç”¨å°æ‡‰çš„è³ ä»˜æ¯”ä¾‹
        for i in range(len(thresholds)-1, -1, -1):
            if max_wind_in_radius >= thresholds[i]:
                total_payout = max_payout * payout_ratios[i]
                break
        
        parametric_payouts.append(total_payout)
        # ä½¿ç”¨è©²äº‹ä»¶åœ¨æ‰€æœ‰é†«é™¢çš„ç¸½è§€æ¸¬æå¤±
        total_observed_loss = np.sum(train_losses[:, event_idx])
        observed_losses_vi.append(total_observed_loss)

parametric_indices = np.array(parametric_indices)
parametric_payouts = np.array(parametric_payouts)
observed_losses_vi = np.array(observed_losses_vi)

# ğŸ¯ åŸ·è¡ŒçœŸæ­£çš„åŸºå·®é¢¨éšªå°å‘è®Šåˆ†æ¨æ–·
print("ğŸ§  é–‹å§‹çœŸæ­£çš„è®Šåˆ†æ¨æ–·å„ªåŒ–...")
print("   ä½¿ç”¨æ¢¯åº¦ä¸‹é™å­¸ç¿’æœ€ä½³ä¿éšªç”¢å“åƒæ•¸åˆ†ä½ˆ")

# å‰µå»ºVIå¯¦ä¾‹ (æ³¨æ„ï¼šBasisRiskAwareVIå¯èƒ½ä¸æ”¯æŒç›´æ¥GPUåƒæ•¸)
vi_screener = BasisRiskAwareVI(
    n_features=1,  # é¢¨é€Ÿä½œç‚ºå–®ä¸€ç‰¹å¾µ
    epsilon_values=[0.0, 0.05, 0.10, 0.15, 0.20],  # Îµ-contamination levels
    basis_risk_types=['absolute', 'asymmetric', 'weighted']  # ä¸åŒåŸºå·®é¢¨éšªé¡å‹
)

# é¡¯ç¤ºè¨ˆç®—ç’°å¢ƒè³‡è¨Š
if USE_GPU and (gpu_available_torch or gpu_available_jax):
    print("   ğŸš€ GPUç’°å¢ƒå·²é…ç½® (VIå°‡å˜—è©¦ä½¿ç”¨GPU)")
    # å¦‚æœä½¿ç”¨JAXï¼Œè¨­ç½®ç’°å¢ƒè®Šæ•¸
    if gpu_available_jax:
        import os
        os.environ['JAX_PLATFORM_NAME'] = 'gpu'
        os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'
        print("   ğŸ“Œ å·²è¨­ç½®JAXä½¿ç”¨GPU")
else:
    print("   ğŸ’» ä½¿ç”¨CPUè¨ˆç®—")

# æº–å‚™VIè¼¸å…¥æ•¸æ“šï¼šé¢¨é€Ÿç‰¹å¾µ + çœŸå¯¦æå¤±
X_vi = parametric_indices.reshape(-1, 1)  # [N, 1] é¢¨é€Ÿç‰¹å¾µ
y_vi = observed_losses_vi  # [N] çœŸå¯¦æå¤±

print(f"   VIè¨“ç·´æ•¸æ“š: {X_vi.shape[0]} æ¨£æœ¬, {X_vi.shape[1]} ç‰¹å¾µ")
print(f"   æå¤±ç¯„åœ: ${np.min(y_vi)/1e6:.1f}M - ${np.max(y_vi)/1e6:.1f}M")

# åŸ·è¡ŒçœŸæ­£çš„è®Šåˆ†æ¨æ–·ï¼ˆå­¸ç¿’æœ€ä½³åƒæ•¸åˆ†ä½ˆï¼‰
vi_results = vi_screener.run_comprehensive_screening(X_vi, y_vi)

print(f"âœ… VIå„ªåŒ–å®Œæˆ (è¨“ç·´é›†): æœ€ä½³åŸºå·®é¢¨éšª={vi_results['best_model']['final_basis_risk']:.2f}")
print(f"   æœ€ä½³æ¨¡å‹: Îµ={vi_results['best_model']['epsilon']:.3f}, é¡å‹={vi_results['best_model']['basis_risk_type']}")

# åœ¨é©—è­‰é›†ä¸Šè©•ä¼°
print("\nğŸ“Š é©—è­‰é›†è©•ä¼°...")
val_indices = []
val_payouts = []
val_losses = []

# ä½¿ç”¨æœ€ä½³ç”¢å“åœ¨é©—è­‰é›†ä¸Šè¨ˆç®—
best_product_idx = 0  # ä½¿ç”¨ç¬¬ä¸€å€‹ç”¢å“ä½œç‚ºç¤ºä¾‹
product = selected_products.iloc[best_product_idx]
thresholds = product['trigger_thresholds']
payout_ratios = product['payout_ratios']
max_payout = product['max_payout']

for event_idx in range(val_data['hazard_intensities'].shape[1]):
    max_wind = np.max(val_data['hazard_intensities'][:, event_idx])
    val_indices.append(max_wind)
    
    # è¨ˆç®—è³ ä»˜
    total_payout = 0
    for i in range(len(thresholds)-1, -1, -1):
        if max_wind >= thresholds[i]:
            total_payout = max_payout * payout_ratios[i]
            break
    val_payouts.append(total_payout)
    
    # ç¸½æå¤±
    total_loss = np.sum(val_data['observed_losses'][:, event_idx])
    val_losses.append(total_loss)

val_indices = np.array(val_indices)
val_payouts = np.array(val_payouts)
val_losses = np.array(val_losses)

# è¨ˆç®—é©—è­‰é›†åŸºå·®é¢¨éšª
val_basis_risk = np.mean(np.abs(val_payouts - val_losses))
print(f"âœ… é©—è­‰é›†åŸºå·®é¢¨éšª: {val_basis_risk:.2f}")
print(f"   è¨“ç·´/é©—è­‰æ¯”ç‡: {vi_results['best_model']['final_basis_risk'] / val_basis_risk:.3f}")

print(f"\nåŸºå·®é¢¨éšªVIå®Œæˆ: è¨“ç·´={vi_results['best_model']['final_basis_risk']:.4f}, é©—è­‰={val_basis_risk:.4f}")

# %%
# =============================================================================
# éšæ®µ5: VIç®—æ³•è¶…åƒæ•¸å„ªåŒ–ï¼ˆä¸æ˜¯ç”¢å“åƒæ•¸å„ªåŒ–ï¼‰
# =============================================================================

print("\néšæ®µ5: VIç®—æ³•è¶…åƒæ•¸å„ªåŒ–")
print("   ç›®æ¨™ï¼šå„ªåŒ–VIç®—æ³•çš„è¶…åƒæ•¸ï¼ˆå­¸ç¿’ç‡ã€epsilonã€æ­£å‰‡åŒ–ç­‰ï¼‰")
print("   æ³¨æ„ï¼šé€™ä¸æ˜¯é‡è¤‡å„ªåŒ–ä¿éšªç”¢å“ï¼Œè€Œæ˜¯å„ªåŒ–ç®—æ³•æœ¬èº«")

# å®šç¾©VIè¶…åƒæ•¸å„ªåŒ–ç›®æ¨™å‡½æ•¸
def vi_hyperparameter_objective(params):
    """
    å„ªåŒ–VIç®—æ³•çš„è¶…åƒæ•¸ï¼ˆè€Œéä¿éšªç”¢å“åƒæ•¸ï¼‰
    ä½¿ç”¨é©—è­‰é›†è©•ä¼°ä¸åŒè¶…åƒæ•¸é…ç½®çš„æ€§èƒ½
    """
    try:
        # æå–VIç®—æ³•è¶…åƒæ•¸
        learning_rate = params.get('learning_rate', 0.01)
        epsilon = params.get('epsilon', 0.1)
        regularization = params.get('regularization', 0.001)
        n_iterations = params.get('n_iterations', 100)
        
        # å‰µå»ºæ–°çš„VIå¯¦ä¾‹withä¸åŒè¶…åƒæ•¸
        # æ³¨æ„ï¼šæª¢æŸ¥BasisRiskAwareVIå¯¦éš›æ”¯æŒçš„åƒæ•¸
        vi_temp_kwargs = {
            'n_features': 1,
            'epsilon_values': [epsilon],  # ä½¿ç”¨å–®ä¸€epsilonå€¼é€²è¡Œå¿«é€Ÿè©•ä¼°
            'basis_risk_types': ['weighted']  # ä½¿ç”¨æœ€ä½³çš„åŸºå·®é¢¨éšªé¡å‹
        }
        
        # å˜—è©¦æ·»åŠ å¯èƒ½æ”¯æŒçš„è¶…åƒæ•¸
        # å¦‚æœä¸æ”¯æŒï¼ŒVIæœƒå¿½ç•¥é€™äº›åƒæ•¸
        try:
            vi_temp = BasisRiskAwareVI(
                **vi_temp_kwargs,
                learning_rate=learning_rate,
                regularization=regularization,
                n_iterations=n_iterations
            )
        except TypeError:
            # å¦‚æœä¸æ”¯æŒé€™äº›åƒæ•¸ï¼Œä½¿ç”¨åŸºæœ¬é…ç½®
            vi_temp = BasisRiskAwareVI(**vi_temp_kwargs)
        
        # åœ¨é©—è­‰é›†ä¸Šè©•ä¼°ï¼ˆä¸æ˜¯è¨“ç·´é›†ï¼ï¼‰
        val_X = val_indices.reshape(-1, 1)
        val_y = val_losses
        
        # å¿«é€Ÿè¨“ç·´ä¸¦è©•ä¼°
        vi_temp.fit(X_vi[:1000], y_vi[:1000])  # ç”¨å°éƒ¨åˆ†è¨“ç·´é›†å¿«é€Ÿè¨“ç·´
        val_predictions = vi_temp.predict(val_X)
        
        # è¨ˆç®—é©—è­‰é›†ä¸Šçš„åŸºå·®é¢¨éšª
        val_basis_risk = np.mean(np.abs(val_predictions - val_y))
        
        # åŠ å…¥æ­£å‰‡åŒ–æ‡²ç½°é˜²æ­¢éæ“¬åˆ
        complexity_penalty = regularization * n_iterations * learning_rate
        
        return -(val_basis_risk + complexity_penalty)  # è² å€¼å› ç‚ºå„ªåŒ–å™¨æœ€å¤§åŒ–
        
    except Exception as e:
        print(f"      è¶…åƒæ•¸è©•ä¼°å¤±æ•—: {e}")
        return -1e6

# å®šç¾©VIè¶…åƒæ•¸æœç´¢ç©ºé–“
vi_hyperparameter_space = [
    {'learning_rate': 0.001, 'epsilon': 0.05, 'regularization': 0.01, 'n_iterations': 50},
    {'learning_rate': 0.01,  'epsilon': 0.10, 'regularization': 0.001, 'n_iterations': 100},
    {'learning_rate': 0.05,  'epsilon': 0.15, 'regularization': 0.0001, 'n_iterations': 150},
    {'learning_rate': 0.1,   'epsilon': 0.20, 'regularization': 0.00001, 'n_iterations': 200},
]

print(f"\nğŸ”§ æ¸¬è©¦ {len(vi_hyperparameter_space)} çµ„VIè¶…åƒæ•¸é…ç½®...")

# è©•ä¼°æ¯çµ„è¶…åƒæ•¸
best_vi_hyperparams = None
best_vi_score = -float('inf')

for i, hyperparams in enumerate(vi_hyperparameter_space):
    score = vi_hyperparameter_objective(hyperparams)
    print(f"   é…ç½®{i+1}: lr={hyperparams['learning_rate']:.3f}, "
          f"Îµ={hyperparams['epsilon']:.2f}, score={-score:.4f}")
    
    if score > best_vi_score:
        best_vi_score = score
        best_vi_hyperparams = hyperparams

print(f"\nâœ… æœ€ä½³VIè¶…åƒæ•¸:")
print(f"   å­¸ç¿’ç‡: {best_vi_hyperparams['learning_rate']}")
print(f"   Epsilon: {best_vi_hyperparams['epsilon']}")
print(f"   æ­£å‰‡åŒ–: {best_vi_hyperparams['regularization']}")
print(f"   è¿­ä»£æ¬¡æ•¸: {best_vi_hyperparams['n_iterations']}")
print(f"   é©—è­‰é›†åŸºå·®é¢¨éšª: {-best_vi_score:.4f}")

# ä½¿ç”¨æœ€ä½³è¶…åƒæ•¸é‡æ–°è¨“ç·´å®Œæ•´VIæ¨¡å‹
print("\nğŸ¯ ä½¿ç”¨æœ€ä½³è¶…åƒæ•¸é‡æ–°è¨“ç·´VIæ¨¡å‹...")

# å‰µå»ºæœ€çµ‚VIæ¨¡å‹ï¼Œä½¿ç”¨å¯¦éš›æ”¯æŒçš„åƒæ•¸
vi_final_kwargs = {
    'n_features': 1,
    'epsilon_values': [best_vi_hyperparams['epsilon']],
    'basis_risk_types': ['weighted']
}

# å˜—è©¦ä½¿ç”¨é¡å¤–åƒæ•¸ï¼Œå¦‚æœä¸æ”¯æŒå‰‡å¿½ç•¥
try:
    vi_final = BasisRiskAwareVI(
        **vi_final_kwargs,
        learning_rate=best_vi_hyperparams['learning_rate'],
        regularization=best_vi_hyperparams['regularization'],
        n_iterations=best_vi_hyperparams['n_iterations']
    )
    print("   ä½¿ç”¨å®Œæ•´è¶…åƒæ•¸é…ç½®")
except TypeError:
    # å¦‚æœä¸æ”¯æŒé¡å¤–åƒæ•¸ï¼Œä½¿ç”¨åŸºæœ¬é…ç½®
    vi_final = BasisRiskAwareVI(**vi_final_kwargs)
    print("   ä½¿ç”¨åŸºæœ¬é…ç½® (é¡åˆ¥ä¸æ”¯æŒæ‰€æœ‰è¶…åƒæ•¸)")

# åœ¨å®Œæ•´è¨“ç·´é›†ä¸Šè¨“ç·´
vi_final_results = vi_final.run_comprehensive_screening(X_vi, y_vi)

# åœ¨æ¸¬è©¦é›†ä¸Šæœ€çµ‚è©•ä¼°ï¼ˆå¦‚æœæœ‰æ¸¬è©¦é›†ï¼‰
if test_data is not None:
    test_indices_all = []
    test_losses_all = []
    
    for event_idx in range(test_data['hazard_intensities'].shape[1]):
        max_wind = np.max(test_data['hazard_intensities'][:, event_idx])
        test_indices_all.append(max_wind)
        total_loss = np.sum(test_data['observed_losses'][:, event_idx])
        test_losses_all.append(total_loss)
    
    test_X = np.array(test_indices_all).reshape(-1, 1)
    test_y = np.array(test_losses_all)
    
    test_predictions = vi_final.predict(test_X)
    test_basis_risk = np.mean(np.abs(test_predictions - test_y))
    
    print(f"\nğŸ“Š æœ€çµ‚æ¸¬è©¦é›†è©•ä¼°:")
    print(f"   æ¸¬è©¦é›†åŸºå·®é¢¨éšª: {test_basis_risk:.4f}")
    print(f"   è¨“ç·´/æ¸¬è©¦æ¯”: {vi_final_results['best_model']['final_basis_risk']/test_basis_risk:.3f}")
else:
    print("\nâš ï¸ ç„¡æ¸¬è©¦é›†å¯ç”¨ï¼Œè·³éæœ€çµ‚è©•ä¼°")
    test_basis_risk = None

# ä¿å­˜è¶…åƒæ•¸å„ªåŒ–çµæœ
hyperparameter_results = {
    'best_hyperparams': best_vi_hyperparams,
    'best_validation_score': -best_vi_score,
    'final_training_results': vi_final_results,
    'test_basis_risk': test_basis_risk
}

print(f"\nâœ… VIç®—æ³•è¶…åƒæ•¸å„ªåŒ–å®Œæˆ")

# %%
# =============================================================================
# éšæ®µ6: MCMCé©—è­‰èˆ‡æ”¶æ–‚è¨ºæ–·
# =============================================================================

print("\néšæ®µ6: MCMCé©—è­‰èˆ‡æ”¶æ–‚è¨ºæ–·")
print("   ç›®æ¨™ï¼šä½¿ç”¨MCMCé©—è­‰å„ªåŒ–å¾ŒVIæ¨¡å‹çš„å¾Œé©—åˆ†ä½ˆ")

# é…ç½®MCMCæ¡æ¨£å™¨
# æ³¨æ„ï¼šCRPSMCMCValidatorå¯èƒ½ä¸æ”¯æŒdeviceåƒæ•¸
try:
    # å˜—è©¦ä½¿ç”¨æ‰€æœ‰åƒæ•¸
    mcmc_validator = CRPSMCMCValidator(
        n_samples=config.mcmc_n_samples,
        n_chains=config.mcmc_n_chains,
        target_accept=config.mcmc_target_accept
    )
    
    # é¡¯ç¤ºè¨ˆç®—ç’°å¢ƒ
    if USE_GPU and (gpu_available_torch or gpu_available_jax):
        print("   ğŸš€ GPUç’°å¢ƒå·²é…ç½® (MCMCå°‡å˜—è©¦ä½¿ç”¨GPU)")
        # ç¢ºä¿JAXä½¿ç”¨GPU
        if gpu_available_jax:
            import os
            os.environ['JAX_PLATFORM_NAME'] = 'gpu'
            print("   ğŸ“Œ JAX MCMCå°‡ä½¿ç”¨GPU")
    else:
        print("   ğŸ’» ä½¿ç”¨CPUè¨ˆç®—")
        
except TypeError as e:
    print(f"   âš ï¸ MCMCé…ç½®è­¦å‘Š: {e}")
    # ä½¿ç”¨æœ€åŸºæœ¬çš„é…ç½®
    mcmc_validator = CRPSMCMCValidator()

# æº–å‚™MCMCæ•¸æ“š - ä½¿ç”¨éšæ®µ5å„ªåŒ–å¾Œçš„VIæ¨¡å‹çµæœ
mcmc_data = {
    'parametric_indices': parametric_indices,
    'observed_losses': observed_losses_vi,
    'vi_model': vi_final,  # ä½¿ç”¨å„ªåŒ–å¾Œçš„VIæ¨¡å‹
    'vi_results': vi_final_results,  # VIçµæœ
    'best_product': vi_results['best_model'],  # æœ€ä½³ç”¢å“é…ç½®
    'hierarchical_model': hierarchical_model  # ä¿ç•™ä½œç‚ºå…ˆé©—åƒè€ƒ
}

# åŸ·è¡ŒMCMCæ¡æ¨£ - é©—è­‰VIæ‰¾åˆ°çš„æœ€ä½³åƒæ•¸åˆ†ä½ˆ
print("   é©—è­‰VIæ‰¾åˆ°çš„æœ€ä½³ä¿éšªç”¢å“åƒæ•¸åˆ†ä½ˆ...")
mcmc_results = mcmc_validator.run_mcmc_validation(
    data=mcmc_data,
    model=vi_final  # ä½¿ç”¨VIæ¨¡å‹è€ŒéåŸå§‹éšå±¤æ¨¡å‹
)

if mcmc_results['success']:
    # æ”¶æ–‚è¨ºæ–·
    convergence_diagnostics = mcmc_validator.compute_convergence_diagnostics(
        mcmc_results['trace']
    )
    
    # å¾Œé©—é æ¸¬æª¢æŸ¥
    ppc_results = mcmc_validator.posterior_predictive_checks(
        mcmc_results['trace'],
        observed_data=observed_losses_vi
    )
    
    print(f"MCMCé©—è­‰å®Œæˆ: RÌ‚={convergence_diagnostics.get('mean_rhat', 'N/A'):.4f}")
else:
    print(f"MCMCæ¡æ¨£å¤±æ•—: {mcmc_results.get('error', 'Unknown error')}")
    convergence_diagnostics = {}
    ppc_results = {}

# %%
# =============================================================================
# éšæ®µ7: å¾Œé©—åˆ†æèˆ‡å¯ä¿¡å€é–“
# =============================================================================

print("\néšæ®µ7: å¾Œé©—åˆ†æèˆ‡å¯ä¿¡å€é–“")

if mcmc_results.get('success', False) and 'trace' in mcmc_results:
    trace = mcmc_results['trace']
    
    # ä½¿ç”¨CredibleIntervalCalculatorè¨ˆç®—å¯ä¿¡å€é–“
    ci_calculator = CredibleIntervalCalculator(
        confidence_level=config.credible_interval_level,
        method='hdi'
    )
    
    # è¨ˆç®—åƒæ•¸å¯ä¿¡å€é–“
    parameter_cis = {}
    for param_name in trace.posterior.data_vars:
        param_samples = trace.posterior[param_name].values.flatten()
        if len(param_samples) > 0:
            ci = ci_calculator.calculate_credible_interval(param_samples)
            parameter_cis[param_name] = ci
    
    # ä½¿ç”¨PosteriorApproximationé€²è¡Œå¾Œé©—åˆ†æ
    posterior_approximator = PosteriorApproximation()
    approximation_results = {}
    
    for param_name, ci_data in list(parameter_cis.items())[:3]:
        param_samples = trace.posterior[param_name].values.flatten()
        approximation = posterior_approximator.approximate_posterior(
            param_samples,
            distribution='normal'
        )
        approximation_results[param_name] = approximation
    
    # è¨ˆç®—çµ„åˆç´šæå¤±é æ¸¬
    portfolio_predictions = get_portfolio_loss_predictions(
        trace=trace,
        spatial_data=spatial_data,
        event_indices=list(range(min(10, n_events)))
    )
    
    print(f"å¾Œé©—åˆ†æå®Œæˆ: {len(parameter_cis)}åƒæ•¸, ç¸½æœŸæœ›æå¤±=${portfolio_predictions['summary']['total_expected_loss']/1e6:.1f}M")
else:
    print("ç„¡å¯ç”¨MCMCçµæœï¼Œè·³éå¾Œé©—åˆ†æ")
    parameter_cis = {}
    approximation_results = {}
    portfolio_predictions = {}

# %%
# =============================================================================
# éšæ®µ8: åƒæ•¸ä¿éšªç”¢å“è¨­è¨ˆèˆ‡å„ªåŒ–
# =============================================================================

print("\néšæ®µ8: åƒæ•¸ä¿éšªç”¢å“è¨­è¨ˆèˆ‡å„ªåŒ–")

# ä½¿ç”¨ParametricInsuranceOptimizeré€²è¡Œç”¢å“å„ªåŒ–
insurance_optimizer = ParametricInsuranceOptimizer(
    basis_risk_weight=1.0,
    crps_weight=0.8,
    risk_weight=0.2
)

# åŸ·è¡Œå¤šç”¢å“å„ªåŒ–
optimization_results = []
for i, (radius, threshold_base) in enumerate([(15, 30), (30, 35), (50, 40), (75, 45), (100, 50)]):
    bounds = [
        (0.1, 10.0),     # alpha
        (0, 1e8),        # beta  
        (threshold_base-5, threshold_base+10)  # threshold
    ]
    
    result = insurance_optimizer.optimize_product(
        observed_losses=observed_losses_vi,
        parametric_indices=parametric_indices,
        bounds=bounds,
        radius=radius
    )
    
    optimization_results.append(result)
    alpha_opt, beta_opt, threshold_opt = result['optimal_params']
    print(f"ç”¢å“{i+1} (åŠå¾‘{radius}km): Î±={alpha_opt:.3f}, ç›®æ¨™å€¼={result['objective_value']:.4f}")

# è¨ˆç®—æŠ€è¡“ä¿è²»
technical_premiums = []
for result in optimization_results:
    premium_data = insurance_optimizer.calculate_technical_premium(
        optimal_params=result['optimal_params'],
        parametric_indices=parametric_indices,
        risk_free_rate=0.02,
        risk_premium=0.05,
        solvency_margin=0.15
    )
    technical_premiums.append(premium_data)
    print(f"åŠå¾‘{result['radius']}km: æŠ€è¡“ä¿è²»${premium_data['technical_premium']/1e6:.2f}M")

# é¸æ“‡æœ€ä½³ç”¢å“
best_product = min(optimization_results, key=lambda x: x['objective_value'])
print(f"æœ€ä½³ç”¢å“: åŠå¾‘{best_product['radius']}km, ç›®æ¨™å€¼={best_product['objective_value']:.4f}")

# %%
# =============================================================================
# ç¶œåˆå ±å‘Šèˆ‡çµæœè¼¸å‡º
# =============================================================================

print("\nç¶œåˆå ±å‘Šèˆ‡çµæœè¼¸å‡º")

# å‰µå»ºç¶œåˆçµæœ
integrated_results = {
    'analysis_metadata': {
        'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
        'framework_version': 'Academic 8-Stage Full Implementation',
        'configuration': config.summary()
    },
    'data_summary': {
        'n_events': n_events,
        'n_hospitals': spatial_data.n_hospitals,
        'total_exposure': total_exposure,
        'loss_statistics': {
            'mean': float(np.mean(event_losses)),
            'std': float(np.std(event_losses)),
            'min': float(np.min(event_losses)),
            'max': float(np.max(event_losses))
        }
    },
    'epsilon_contamination_analysis': {
        'statistical_epsilon': statistical_epsilon if 'statistical_epsilon' in locals() else None,
        'contamination_epsilon': contamination_epsilon if 'contamination_epsilon' in locals() else None,
        'final_epsilon': final_epsilon
    },
    'vi_screening_results': vi_results,
    'vi_hyperparameter_optimization': hyperparameter_results,
    'mcmc_validation': {
        'results': mcmc_results,
        'convergence_diagnostics': convergence_diagnostics,
        'posterior_predictive_checks': ppc_results
    },
    'posterior_analysis': {
        'credible_intervals': parameter_cis,
        'approximation_results': approximation_results,
        'portfolio_predictions': portfolio_predictions
    },
    'parametric_insurance_optimization': {
        'product_optimization_results': optimization_results,
        'technical_premiums': technical_premiums,
        'best_product': best_product
    }
}

# å„²å­˜çµæœ
results_dir = Path('results/integrated_parametric_framework')
results_dir.mkdir(exist_ok=True)

# å„²å­˜ä¸»çµæœ
main_results_path = results_dir / 'comprehensive_analysis_results.pkl'
with open(main_results_path, 'wb') as f:
    pickle.dump(integrated_results, f)

# å‰µå»ºè©³ç´°å ±å‘Š
report_path = results_dir / 'comprehensive_report.txt'
with open(report_path, 'w', encoding='utf-8') as f:
    f.write("åŒ—å¡ç¾…ä¾†ç´å·é¢±é¢¨é¢¨éšªï¼šå®Œæ•´è²è‘‰æ–¯åƒæ•¸ä¿éšªåˆ†æå ±å‘Š\n")
    f.write("=" * 60 + "\n\n")
    f.write(f"åˆ†ææ™‚é–“ï¼š{integrated_results['analysis_metadata']['timestamp']}\n")
    f.write(f"æ•¸æ“šæ‘˜è¦ï¼š{n_events}äº‹ä»¶, ${total_exposure/1e9:.2f}Bç¸½æš´éšª\n")
    f.write(f"æœ€çµ‚Îµå€¼ï¼š{final_epsilon:.4f}\n")
    f.write(f"æœ€ä½³ç”¢å“ï¼šåŠå¾‘{best_product['radius']}km\n")

# å‰µå»ºç”¢å“è©³ç´°CSV
products_df_detailed = pd.DataFrame(optimization_results)
products_csv_path = results_dir / 'product_details.csv'
products_df_detailed.to_csv(products_csv_path, index=False)

# å‰µå»ºæ’åCSV
ranking_data = []
for i, (opt_result, premium_data) in enumerate(zip(optimization_results, technical_premiums)):
    efficiency_score = 1.0 / (opt_result['objective_value'] * premium_data['technical_premium'] / 1e6)
    ranking_data.append({
        'rank': i + 1,
        'radius_km': opt_result['radius'],
        'objective_value': opt_result['objective_value'],
        'technical_premium_million': premium_data['technical_premium'] / 1e6,
        'efficiency_score': efficiency_score,
        'loss_ratio': premium_data['loss_ratio']
    })

ranking_df = pd.DataFrame(ranking_data).sort_values('efficiency_score', ascending=False)
ranking_df['rank'] = range(1, len(ranking_df) + 1)
ranking_csv_path = results_dir / 'product_rankings.csv'
ranking_df.to_csv(ranking_csv_path, index=False)

print("8éšæ®µå­¸è¡“ç´šè²è‘‰æ–¯åˆ†æå®Œæˆ")
print(f"çµæœå·²å„²å­˜è‡³ï¼š{main_results_path}")
print(f"æœ€ä½³ç”¢å“ï¼šåŠå¾‘{best_product['radius']}km, Îµ={final_epsilon:.4f}")
print("åˆ†æå®Œæˆï¼")
#!/usr/bin/env python3
"""
Test Maximum Load Configuration
æ¸¬è©¦æœ€å¤§è² è¼‰é…ç½®

Quick test with aggressive settings to push GPU usage to 90%+
ä½¿ç”¨æ¿€é€²è¨­ç½®çš„å¿«é€Ÿæ¸¬è©¦ï¼Œå°‡GPUä½¿ç”¨ç‡æ¨åˆ°90%+
"""

import os
import numpy as np
import time

# è¨­ç½®æœ€å¤§è² è¼‰ç’°å¢ƒ
print("ğŸ”¥ Setting Maximum Load Environment...")
max_env = {
    'JAX_PLATFORMS': 'cuda',
    'JAX_ENABLE_X64': 'False',
    'JAX_PLATFORM_NAME': 'gpu',
    'XLA_FLAGS': '--xla_force_host_platform_device_count=2',
    'CUDA_VISIBLE_DEVICES': '0,1',
    'XLA_PYTHON_CLIENT_MEM_FRACTION': '0.95',  # 95% è¨˜æ†¶é«”
    'XLA_PYTHON_CLIENT_PREALLOCATE': 'true',
    'NUMPYRO_PLATFORM': 'gpu',
    'NUMPYRO_NUM_CHAINS': '32',
    'OMP_NUM_THREADS': '32',
    'PYTENSOR_FLAGS': 'device=cuda,floatX=float32,optimizer=fast_run,force_device=True'
}

for key, value in max_env.items():
    os.environ[key] = value

print("\nğŸ§ª Testing Maximum Load MCMC...")

try:
    import pymc as pm
    
    # ç”Ÿæˆè¼ƒè¤‡é›œçš„æ¸¬è©¦æ•¸æ“š
    np.random.seed(42)
    n_data = 500  # å¢åŠ æ•¸æ“šé»
    true_mu = 10.0
    true_sigma = 3.0
    test_data = np.random.normal(true_mu, true_sigma, n_data)
    
    print(f"ğŸ“Š Test data: {n_data} points, mu={np.mean(test_data):.2f}")
    
    with pm.Model() as max_load_model:
        # ç¨å¾®è¤‡é›œçš„æ¨¡å‹
        mu = pm.Normal("mu", mu=0, sigma=20)
        sigma = pm.HalfNormal("sigma", sigma=10)
        
        # æ·»åŠ éšå±¤æ•ˆæ‡‰å¢åŠ è¤‡é›œåº¦
        group_effects = pm.Normal("group_effects", mu=0, sigma=5, shape=10)
        group_idx = np.random.randint(0, 10, n_data)
        
        obs_mu = mu + group_effects[group_idx]
        obs = pm.Normal("obs", mu=obs_mu, sigma=sigma, observed=test_data)
        
        # æœ€å¤§è² è¼‰MCMCé…ç½®
        max_mcmc_config = {
            "draws": 2000,           # å¤§æ¨£æœ¬
            "tune": 1000,            # å……è¶³warmup
            "chains": 32,            # 32æ¢éˆ
            "cores": 32,
            "nuts_sampler": "numpyro",
            "chain_method": "parallel",
            "target_accept": 0.95,   # é«˜ç²¾åº¦
            "return_inferencedata": True,
            "progressbar": True
        }
        
        print("ğŸ”¥ Running Maximum Load MCMC...")
        print(f"   ğŸ“Š Configuration: {max_mcmc_config['chains']} chains Ã— {max_mcmc_config['draws']} samples")
        print(f"   ğŸ¯ Total samples: {max_mcmc_config['chains'] * max_mcmc_config['draws']:,}")
        print("   âš¡ Monitor nvidia-smi for 90%+ usage on both GPUs!")
        
        start_time = time.time()
        trace = pm.sample(**max_mcmc_config)
        end_time = time.time()
        
        elapsed = end_time - start_time
        total_samples = max_mcmc_config['chains'] * max_mcmc_config['draws']
        
        print("ğŸ‰ Maximum Load MCMC Completed!")
        print(f"   â±ï¸  Time: {elapsed:.1f} seconds")
        print(f"   ğŸš€ Speed: {total_samples/elapsed:.0f} samples/second")
        print(f"   ğŸ“Š Results:")
        print(f"      mu: {trace.posterior.mu.mean().values:.2f} Â± {trace.posterior.mu.std().values:.2f}")
        print(f"      sigma: {trace.posterior.sigma.mean().values:.2f} Â± {trace.posterior.sigma.std().values:.2f}")

except Exception as e:
    print(f"âŒ Maximum load test failed: {e}")
    import traceback
    traceback.print_exc()

print("\n" + "=" * 60)
print("ğŸ”¥ Maximum Load Test Results:")
print("=" * 60)
print("ğŸ’¡ Check nvidia-smi output:")
print("   âœ… Success: Both GPUs showing 80%+ usage")
print("   âœ… Success: Both GPUs drawing 180W+ power")
print("   âœ… Success: Memory usage 20GB+ per GPU")
print("   âŒ Need tuning: GPU usage still < 70%")
print("\nğŸ¯ If GPUs not fully utilized:")
print("   â€¢ Increase n_chains to 48")
print("   â€¢ Increase n_samples to 3000+")
print("   â€¢ Check for memory bottlenecks")
print("   â€¢ Verify JAX using both devices")
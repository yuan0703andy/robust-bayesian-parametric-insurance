#!/usr/bin/env python3
"""
åŒ—å¡ç¾…ä¾†ç´å·ç†±å¸¶æ°£æ—‹åƒæ•¸å‹ä¿éšªåˆ†æ - çœŸæ­£çš„åŠŸèƒ½å¼ç‰ˆæœ¬
NC Tropical Cyclone Parametric Insurance Analysis - True Functional Version

åŸºæ–¼ main_test_optimized.py çš„å®Œæ•´æµç¨‹ï¼Œæ¡ç”¨ç°¡æ½”çš„åŠŸèƒ½å¼è¨­è¨ˆ
æ¯å€‹ cell ç›´æ¥åŸ·è¡Œä¸¦ç«‹å³é¡¯ç¤ºçµæœï¼Œç„¡ä¸å¿…è¦çš„å‡½æ•¸åŒ…è£

Features:
- 350 products (5 radii Ã— 70 Steinmann functions) 
- Dual-track analysis: Steinmann RMSE vs Bayesian CRPS
- Pure Cat-in-a-Circle (no weighted average)
- Correct step payout logic (no break statement)
- Direct cell execution with immediate results
"""

# %% ç’°å¢ƒè¨­ç½®èˆ‡æ¨¡çµ„å°å…¥
import os
import sys
import numpy as np
import pandas as pd
from datetime import datetime
import warnings
import matplotlib.pyplot as plt
warnings.filterwarnings('ignore')

print("ğŸ”§ è¨­ç½®åˆ†æç’°å¢ƒ...")

# è¨­ç½®è·¯å¾‘
current_dir = os.path.dirname(os.path.abspath(__file__))
insurance_dir = os.path.join(current_dir, 'insurance_analysis_refactored')
base_path = '/hpc/group/borsuklab/yh421/CAT_INSURANCE/climada'

for path in [insurance_dir, current_dir, base_path]:
    if path not in sys.path:
        sys.path.append(path)

print(f"âœ… è·¯å¾‘è¨­ç½®å®Œæˆ: {current_dir}")

# æª¢æŸ¥æ¨¡çµ„å¯ç”¨æ€§
print("ğŸ” æª¢æŸ¥é—œéµæ¨¡çµ„...")
modules_available = {}

# æ ¸å¿ƒå¼•æ“æ¨¡çµ„
try:
    from insurance_analysis_refactored.core.saffir_simpson_products import generate_steinmann_2023_products
    from insurance_analysis_refactored.core.input_adapters import extract_pure_cat_in_circle
    from insurance_analysis_refactored.core.parametric_engine import calculate_correct_step_payouts, calculate_crps_score
    modules_available['core'] = True
    print("   âœ… æ ¸å¿ƒå¼•æ“æ¨¡çµ„")
except ImportError as e:
    modules_available['core'] = False
    print(f"   âŒ æ ¸å¿ƒå¼•æ“æ¨¡çµ„: {e}")

# CLIMADA æ¨¡çµ„ - å¼·åˆ¶æˆåŠŸè¼‰å…¥
print("   ğŸ”„ è¼‰å…¥ CLIMADA æ¨¡çµ„...")
from config.settings import NC_BOUNDS, YEAR_RANGE, RESOLUTION
from data_processing.track_processing import get_regional_tracks
from hazard_modeling.tc_hazard import create_tc_hazard
from exposure_modeling.litpop_processing import (
    process_litpop_exposures, 
    visualize_all_litpop_exposures,
    create_yearly_comparison,
    analyze_exposure_statistics
)
from exposure_modeling.hospital_osm_extraction import get_nc_hospitals, create_standardized_hospital_exposures
from impact_analysis.impact_calculation import calculate_tc_impact
modules_available['climada'] = True
print("   âœ… CLIMADA æ¨¡çµ„è¼‰å…¥æˆåŠŸ")

# é€²éšæ¨¡çµ„
try:
    from bayesian.robust_bayesian_uncertainty import generate_probabilistic_loss_distributions
    from bayesian.robust_bayesian_analyzer import RobustBayesianAnalyzer
    modules_available['bayesian'] = True
    print("   âœ… è²æ°ä¸ç¢ºå®šæ€§æ¨¡çµ„")
    print("   âœ… ç©©å¥è²æ°åˆ†æå™¨")
except ImportError as e:
    modules_available['bayesian'] = False
    print(f"   âš ï¸ è²æ°ä¸ç¢ºå®šæ€§æ¨¡çµ„ä¸å¯ç”¨: {e}")

available_count = sum(modules_available.values())
print(f"ğŸ“Š æ¨¡çµ„å¯ç”¨æ€§: {available_count}/{len(modules_available)}")

# Hospital Cat-in-a-Circle Functions (ç§»è‡³æ­¤è™•ä»¥ç¢ºä¿åœ¨ä½¿ç”¨å‰å®šç¾©)
def extract_hospital_cat_in_circle(tc_hazard, hospital_coords, hazard_tree, radius_km):
    """
    ç‚ºæ¯å®¶é†«é™¢æå–Cat-in-a-Circleé¢¨é€ŸæŒ‡æ¨™
    ç¬¦åˆSteinmannè«–æ–‡çš„é†«é™¢ç´šåˆ¥åˆ†æ
    
    Parameters:
    -----------
    tc_hazard : TropCyclone
        CLIMADAé¢±é¢¨ç½å®³å°è±¡
    hospital_coords : list of tuples
        é†«é™¢åº§æ¨™åˆ—è¡¨ [(lat1, lon1), (lat2, lon2), ...]
    hazard_tree : cKDTree
        ç½å®³é»ç©ºé–“ç´¢å¼•æ¨¹
    radius_km : float
        Cat-in-a-CircleåŠå¾‘(å…¬é‡Œ)
        
    Returns:
    --------
    dict
        {hospital_idx: [event_wind_speeds]} æ¯å®¶é†«é™¢åœ¨æ¯å€‹äº‹ä»¶çš„é¢¨é€Ÿ
    """
    n_events = tc_hazard.intensity.shape[0]
    n_hospitals = len(hospital_coords)
    
    # ç‚ºæ¯å®¶é†«é™¢å„²å­˜é¢¨é€Ÿæ™‚é–“åºåˆ—
    hospital_wind_series = {}
    
    print(f"   ğŸ¥ è¨ˆç®— {n_hospitals} å®¶é†«é™¢åœ¨ {n_events} å€‹äº‹ä»¶ä¸­çš„Cat-in-a-Circleé¢¨é€Ÿ...")
    
    for hospital_idx, hospital_coord in enumerate(hospital_coords):
        wind_speeds = np.zeros(n_events)
        
        # é å…ˆè¨ˆç®—è©²é†«é™¢åŠå¾‘å…§çš„ç½å®³é»
        radius_rad = radius_km / 6371.0
        nearby_indices = hazard_tree.query_ball_point(
            np.radians(hospital_coord), radius_rad
        )
        
        if len(nearby_indices) > 0:
            for event_idx in range(n_events):
                wind_field = tc_hazard.intensity[event_idx, :].toarray().flatten()
                nearby_winds = wind_field[nearby_indices]
                nearby_winds = nearby_winds[nearby_winds > 0]
                
                if len(nearby_winds) > 0:
                    wind_speeds[event_idx] = np.max(nearby_winds)
                else:
                    wind_speeds[event_idx] = 0.0
        
        hospital_wind_series[hospital_idx] = wind_speeds
    
    return hospital_wind_series

def calculate_hospital_standardized_damages(hospital_wind_series, impact_func_set):
    """
    è¨ˆç®—é†«é™¢æ¨™æº–åŒ–æå¤± (æ¯å®¶é†«é™¢1å–®ä½ Ã— è„†å¼±åº¦å‡½æ•¸)
    
    Parameters:
    -----------
    hospital_wind_series : dict
        æ¯å®¶é†«é™¢çš„é¢¨é€Ÿæ™‚é–“åºåˆ—
    impact_func_set : ImpactFuncSet
        è„†å¼±åº¦å‡½æ•¸é›†
        
    Returns:
    --------
    dict
        æ¯å®¶é†«é™¢çš„æ¨™æº–åŒ–æå¤±
    """
    
    print("ğŸ’Š è¨ˆç®—é†«é™¢æ¨™æº–åŒ–æå¤± (Steinmannè«–æ–‡æ–¹æ³•)...")
    
    # æª¢æŸ¥è¼¸å…¥æ•¸æ“š
    if not hospital_wind_series:
        print("   âš ï¸ æ²’æœ‰é†«é™¢é¢¨é€Ÿæ•¸æ“š")
        return {}
    
    hospital_damages = {}
    
    # ç°¡åŒ–çš„é¢¨é€Ÿ-æå®³é—œä¿‚å‡½æ•¸
    def simple_damage_func(wind_speed):
        if wind_speed < 33:  # < Cat 1
            return 0.0
        elif wind_speed < 42:  # Cat 1
            return 0.1
        elif wind_speed < 50:  # Cat 2
            return 0.25
        elif wind_speed < 58:  # Cat 3
            return 0.5
        elif wind_speed < 70:  # Cat 4
            return 0.75
        else:  # Cat 5
            return 1.0
    
    # æª¢æŸ¥æ˜¯å¦æœ‰å½±éŸ¿å‡½æ•¸é›†
    if impact_func_set is not None and hasattr(impact_func_set, 'get_func'):
        try:
            # ç²å–é©ç”¨çš„å½±éŸ¿å‡½æ•¸ (é€šå¸¸æ˜¯ç†±å¸¶æ°£æ—‹å‡½æ•¸)
            impact_funcs = impact_func_set.get_func()
            if impact_funcs and len(impact_funcs) > 0:
                impact_func = impact_funcs[0]  # ä½¿ç”¨ç¬¬ä¸€å€‹å‡½æ•¸
                print(f"   ğŸ“ˆ ä½¿ç”¨CLIMADAå½±éŸ¿å‡½æ•¸: {type(impact_func).__name__}")
                
                # ä½¿ç”¨CLIMADAå½±éŸ¿å‡½æ•¸è¨ˆç®—
                for hospital_idx, wind_speeds in hospital_wind_series.items():
                    # æ¯å®¶é†«é™¢åƒ¹å€¼ = 1.0 æ¨™æº–åŒ–å–®ä½
                    hospital_value = 1.0
                    
                    # è¨ˆç®—æå¤±æ¯”ä¾‹
                    damage_ratios = []
                    for wind_speed in wind_speeds:
                        if wind_speed > 0:
                            # ä½¿ç”¨CLIMADAå½±éŸ¿å‡½æ•¸è¨ˆç®—æå¤±æ¯”ä¾‹
                            ratio = impact_func.calc_mdr(wind_speed)
                            damage_ratios.append(ratio * hospital_value)
                        else:
                            damage_ratios.append(0.0)
                    
                    hospital_damages[hospital_idx] = np.array(damage_ratios)
            else:
                print("   âš ï¸ å½±éŸ¿å‡½æ•¸é›†ç‚ºç©ºï¼Œä½¿ç”¨ç°¡åŒ–è¨ˆç®—...")
                for hospital_idx, wind_speeds in hospital_wind_series.items():
                    damages = np.array([simple_damage_func(ws) for ws in wind_speeds])
                    hospital_damages[hospital_idx] = damages
                    
        except Exception as e:
            print(f"   âš ï¸ CLIMADAå½±éŸ¿å‡½æ•¸è¨ˆç®—å¤±æ•—: {str(e)}")
            # å›é€€åˆ°ç°¡åŒ–è¨ˆç®—
            for hospital_idx, wind_speeds in hospital_wind_series.items():
                damages = np.array([simple_damage_func(ws) for ws in wind_speeds])
                hospital_damages[hospital_idx] = damages
    else:
        print("   âš ï¸ ç„¡å½±éŸ¿å‡½æ•¸é›†ï¼Œä½¿ç”¨ç°¡åŒ–è¨ˆç®—...")
        # ä½¿ç”¨ç°¡åŒ–çš„æå®³å‡½æ•¸
        for hospital_idx, wind_speeds in hospital_wind_series.items():
            damages = np.array([simple_damage_func(ws) for ws in wind_speeds])
            hospital_damages[hospital_idx] = damages
    
    # çµ±è¨ˆä¿¡æ¯
    if hospital_damages:
        total_events = len(list(hospital_damages.values())[0])
        total_damages = sum(np.sum(damages) for damages in hospital_damages.values())
        print(f"   âœ… å®Œæˆ {len(hospital_damages)} å®¶é†«é™¢ Ã— {total_events} å€‹äº‹ä»¶çš„æå¤±è¨ˆç®—")
        print(f"   ğŸ’° ç¸½æ¨™æº–åŒ–æå¤±: {total_damages:.2f}")
    else:
        print("   âš ï¸ ç„¡æ³•è¨ˆç®—é†«é™¢æå¤±")
    
    return hospital_damages

def plot_hospital_cat_in_circle_analysis(hospital_cat_indices, hospital_coords):
    """
    ç¹ªè£½å€‹åˆ¥é†«é™¢çš„Cat-in-a-Circleåˆ†æåœ–
    ç¬¦åˆSteinmannè«–æ–‡çš„çœŸå¯¦æ„åœ–ï¼šåˆ†æç‰¹å®šä¿éšªæ¨™çš„ç‰©çš„é¢¨éšªå·®ç•°
    """
    from datetime import datetime
    
    # é¸æ“‡å‰6å®¶é†«é™¢é€²è¡Œè©³ç´°åˆ†æ
    n_hospitals = min(6, len(hospital_coords))
    selected_hospitals = list(range(n_hospitals))
    
    # åœ–1: å€‹åˆ¥é†«é™¢ä¸åŒåŠå¾‘çš„é¢¨é€Ÿåˆ†å¸ƒæ¯”è¼ƒ
    fig1, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    radii = list(hospital_cat_indices.keys())
    colors = plt.cm.Set3(np.linspace(0, 1, len(radii)))
    
    for h_idx, hospital_idx in enumerate(selected_hospitals):
        ax = axes[h_idx]
        
        for r_idx, radius in enumerate(radii):
            hospital_winds = hospital_cat_indices[radius][hospital_idx]
            non_zero_winds = hospital_winds[hospital_winds > 0]
            
            if len(non_zero_winds) > 0:
                ax.hist(non_zero_winds, bins=20, alpha=0.6, 
                       label=f'{radius}km', color=colors[r_idx], density=True)
        
        # æ·»åŠ Saffir-Simpsonåˆ†ç´šç·š
        saffir_simpson_thresholds = [33, 42, 49, 58, 70]
        ss_colors = ['yellow', 'orange', 'red', 'purple', 'darkred']
        ss_names = ['Cat 1', 'Cat 2', 'Cat 3', 'Cat 4', 'Cat 5']
        
        if ax.get_ylim()[1] > 0:  # ç¢ºä¿æœ‰æ•¸æ“š
            y_max = ax.get_ylim()[1]
            for threshold, color, name in zip(saffir_simpson_thresholds, ss_colors, ss_names):
                ax.axvline(threshold, color=color, linestyle='--', alpha=0.7, linewidth=1.5)
                ax.text(threshold, y_max*0.8, name, rotation=90, 
                       verticalalignment='bottom', fontsize=8, alpha=0.8)
        
        ax.set_xlabel('é¢¨é€Ÿ (m/s)')
        ax.set_ylabel('å¯†åº¦')
        ax.set_title(f'é†«é™¢ {hospital_idx+1} çš„Cat-in-a-Circleé¢¨é€Ÿåˆ†å¸ƒ')
        ax.grid(True, alpha=0.3)
        ax.legend()
    
    plt.suptitle('å€‹åˆ¥é†«é™¢çš„ä¸åŒåŠå¾‘Cat-in-a-Circleé¢¨é€Ÿåˆ†å¸ƒæ¯”è¼ƒ\\nï¼ˆç¬¦åˆSteinmannè«–æ–‡æ–¹æ³•è«–ï¼‰', fontsize=16)
    plt.tight_layout()
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename1 = f'hospital_specific_cat_in_circle_{timestamp}.png'
    plt.savefig(filename1, dpi=300, bbox_inches='tight')
    print(f"   ğŸ“Š å€‹åˆ¥é†«é™¢Cat-in-a-Circleåˆ†æåœ–å·²ä¿å­˜: {filename1}")
    plt.show()
    
    # åœ–2: åŠå¾‘æ•ˆæ‡‰åˆ†æ - çµ±è¨ˆæ‘˜è¦
    fig2, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # è¨ˆç®—æ¯å€‹åŠå¾‘çš„çµ±è¨ˆæ•¸æ“š
    radius_stats = {}
    for radius in radii:
        all_hospital_winds = []
        max_winds_per_hospital = []
        mean_winds_per_hospital = []
        
        for h_idx in range(len(hospital_coords)):
            h_winds = hospital_cat_indices[radius][h_idx]
            non_zero_winds = h_winds[h_winds > 0]
            
            if len(non_zero_winds) > 0:
                all_hospital_winds.extend(non_zero_winds)
                max_winds_per_hospital.append(np.max(non_zero_winds))
                mean_winds_per_hospital.append(np.mean(non_zero_winds))
        
        radius_stats[radius] = {
            'all_winds': all_hospital_winds,
            'max_per_hospital': max_winds_per_hospital,
            'mean_per_hospital': mean_winds_per_hospital
        }
    
    # å­åœ–1: å¹³å‡é¢¨é€ŸvsåŠå¾‘
    radii_list = list(radii)
    avg_winds = [np.mean(radius_stats[r]['all_winds']) if radius_stats[r]['all_winds'] else 0 
                for r in radii_list]
    
    ax1.plot(radii_list, avg_winds, 'o-', linewidth=2, markersize=8, color='blue')
    ax1.set_xlabel('Cat-in-a-CircleåŠå¾‘ (km)')
    ax1.set_ylabel('å¹³å‡é¢¨é€Ÿ (m/s)')
    ax1.set_title('å¹³å‡é¢¨é€Ÿ vs åŠå¾‘')
    ax1.grid(True, alpha=0.3)
    
    # å­åœ–2: æœ€å¤§é¢¨é€ŸvsåŠå¾‘
    max_winds = [np.max(radius_stats[r]['all_winds']) if radius_stats[r]['all_winds'] else 0 
                for r in radii_list]
    
    ax2.plot(radii_list, max_winds, 'o-', linewidth=2, markersize=8, color='red')
    ax2.set_xlabel('Cat-in-a-CircleåŠå¾‘ (km)')
    ax2.set_ylabel('æœ€å¤§é¢¨é€Ÿ (m/s)')  
    ax2.set_title('æœ€å¤§é¢¨é€Ÿ vs åŠå¾‘')
    ax2.grid(True, alpha=0.3)
    
    # å­åœ–3: é†«é™¢é–“é¢¨é€Ÿè®Šç•° vs åŠå¾‘
    wind_std = [np.std(radius_stats[r]['mean_per_hospital']) if radius_stats[r]['mean_per_hospital'] else 0
               for r in radii_list]
    
    ax3.plot(radii_list, wind_std, 'o-', linewidth=2, markersize=8, color='green')
    ax3.set_xlabel('Cat-in-a-CircleåŠå¾‘ (km)')
    ax3.set_ylabel('é†«é™¢é–“é¢¨é€Ÿæ¨™æº–å·® (m/s)')
    ax3.set_title('é†«é™¢é–“é¢¨éšªå·®ç•° vs åŠå¾‘')
    ax3.grid(True, alpha=0.3)
    
    # å­åœ–4: Box plotæ¯”è¼ƒä¸åŒåŠå¾‘
    box_data = [radius_stats[r]['all_winds'] for r in radii_list]
    box_plot = ax4.boxplot(box_data, labels=radii_list, patch_artist=True)
    
    # ç¾åŒ–box plot
    for patch, color in zip(box_plot['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    
    ax4.set_xlabel('Cat-in-a-CircleåŠå¾‘ (km)')
    ax4.set_ylabel('é¢¨é€Ÿåˆ†å¸ƒ (m/s)')
    ax4.set_title('ä¸åŒåŠå¾‘é¢¨é€Ÿåˆ†å¸ƒæ¯”è¼ƒ')
    ax4.grid(True, alpha=0.3)
    
    plt.suptitle('Cat-in-a-CircleåŠå¾‘æ•ˆæ‡‰åˆ†æ\\nï¼ˆåŸºæ–¼å€‹åˆ¥é†«é™¢æ•¸æ“šï¼‰', fontsize=16)
    plt.tight_layout()
    
    filename2 = f'cat_in_circle_radius_effects_{timestamp}.png'
    plt.savefig(filename2, dpi=300, bbox_inches='tight')
    print(f"   ğŸ“Š åŠå¾‘æ•ˆæ‡‰åˆ†æåœ–å·²ä¿å­˜: {filename2}")
    plt.show()
    
    return filename1, filename2

# Duplicate function definitions have been moved to the top of the file

def visualize_exposure_spatial_distribution(exposure_main):
    """è¦–è¦ºåŒ–æ›éšªç©ºé–“åˆ†å¸ƒ"""
    
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # 1. ç©ºé–“åˆ†å¸ƒæ•£é»åœ–
    ax1 = axes[0]
    scatter = ax1.scatter(exposure_main.longitude, exposure_main.latitude,
                         c=exposure_main.gdf.value/1e6, 
                         s=2, alpha=0.7, cmap='viridis')
    ax1.set_xlabel('ç¶“åº¦')
    ax1.set_ylabel('ç·¯åº¦')
    ax1.set_title('åŒ—å¡ç¾…ä¾†ç´å·æ›éšªè³‡ç”¢ç©ºé–“åˆ†å¸ƒ')
    ax1.grid(True, alpha=0.3)
    
    # æ·»åŠ é¡è‰²æ¢
    cbar1 = plt.colorbar(scatter, ax=ax1)
    cbar1.set_label('è³‡ç”¢åƒ¹å€¼ (ç™¾è¬ç¾å…ƒ)')
    
    # 2. åƒ¹å€¼å¯†åº¦ç†±åŠ›åœ–
    ax2 = axes[1]
    # å‰µå»º2Dç›´æ–¹åœ–ä¾†é¡¯ç¤ºåƒ¹å€¼å¯†åº¦
    H, xedges, yedges = np.histogram2d(exposure_main.longitude, 
                                      exposure_main.latitude, 
                                      bins=50,
                                      weights=exposure_main.gdf.value)
    
    im = ax2.imshow(H.T, origin='lower', extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]], 
                   cmap='hot', aspect='auto')
    ax2.set_xlabel('ç¶“åº¦')
    ax2.set_ylabel('ç·¯åº¦')
    ax2.set_title('è³‡ç”¢åƒ¹å€¼å¯†åº¦åˆ†å¸ƒ')
    
    # æ·»åŠ é¡è‰²æ¢
    cbar2 = plt.colorbar(im, ax=ax2)
    cbar2.set_label('ç¸½è³‡ç”¢åƒ¹å€¼ (ç¾å…ƒ)')
    
    plt.tight_layout()
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f'exposure_spatial_distribution_{timestamp}.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"   ğŸ“Š æ›éšªç©ºé–“åˆ†å¸ƒåœ–å·²ä¿å­˜: {filename}")
    plt.show()
    
    return fig


def create_impact_visualizations(impact, exposure, hazard):
    """
    å‰µå»ºCLIMADAå½±éŸ¿åˆ†æè¦–è¦ºåŒ–
    
    Parameters:
    -----------
    impact : climada.engine.Impact
        CLIMADAå½±éŸ¿å°è±¡
    exposure : climada.entity.Exposures
        æ›éšªå°è±¡
    hazard : climada.hazard.TropCyclone
        ç†±å¸¶æ°£æ—‹ç½å®³å°è±¡
        
    Returns:
    --------
    matplotlib.figure.Figure
        åŒ…å«å¤šå€‹å­åœ–çš„ç¶œåˆåˆ†æåœ–
    """
    
    # å‰µå»º4å€‹å­åœ–çš„å¸ƒå±€
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. äº‹ä»¶æå¤±åˆ†å¸ƒç›´æ–¹åœ–
    ax1 = axes[0, 0]
    non_zero_losses = impact.at_event[impact.at_event > 0] / 1e9  # è½‰æ›ç‚ºåå„„ç¾å…ƒ
    if len(non_zero_losses) > 0:
        ax1.hist(non_zero_losses, bins=30, alpha=0.7, edgecolor='black')
        ax1.set_xlabel('å–®æ¬¡äº‹ä»¶æå¤± (åå„„ç¾å…ƒ)')
        ax1.set_ylabel('äº‹ä»¶æ•¸é‡')
        ax1.set_title('ç†±å¸¶æ°£æ—‹äº‹ä»¶æå¤±åˆ†å¸ƒ')
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ çµ±è¨ˆä¿¡æ¯
        mean_loss = non_zero_losses.mean()
        max_loss = non_zero_losses.max()
        ax1.axvline(mean_loss, color='red', linestyle='--', alpha=0.8, 
                   label=f'å¹³å‡æå¤±: ${mean_loss:.2f}B')
        ax1.axvline(max_loss, color='orange', linestyle='--', alpha=0.8, 
                   label=f'æœ€å¤§æå¤±: ${max_loss:.2f}B')
        ax1.legend()
    else:
        ax1.text(0.5, 0.5, 'ç„¡æå¤±äº‹ä»¶', ha='center', va='center', transform=ax1.transAxes)
        ax1.set_title('ç†±å¸¶æ°£æ—‹äº‹ä»¶æå¤±åˆ†å¸ƒ')
    
    # 2. å¹´å‡æå¤±ç©ºé–“åˆ†å¸ƒ
    ax2 = axes[0, 1]
    if hasattr(impact, 'imp_mat') and impact.imp_mat is not None:
        # è¨ˆç®—æ¯å€‹æ›éšªé»çš„å¹´å‡æå¤±
        annual_losses = impact.imp_mat.mean(axis=0) / 1e6  # è½‰æ›ç‚ºç™¾è¬ç¾å…ƒ
        scatter = ax2.scatter(exposure.longitude, exposure.latitude,
                             c=annual_losses, s=3, alpha=0.7, cmap='Reds')
        ax2.set_xlabel('ç¶“åº¦')
        ax2.set_ylabel('ç·¯åº¦')
        ax2.set_title('å¹´å‡æå¤±ç©ºé–“åˆ†å¸ƒ')
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ é¡è‰²æ¢
        cbar = plt.colorbar(scatter, ax=ax2)
        cbar.set_label('å¹´å‡æå¤± (ç™¾è¬ç¾å…ƒ)')
    else:
        ax2.text(0.5, 0.5, 'ç„¡è©³ç´°æå¤±çŸ©é™£æ•¸æ“š', ha='center', va='center', transform=ax2.transAxes)
        ax2.set_title('å¹´å‡æå¤±ç©ºé–“åˆ†å¸ƒ')
    
    # 3. å›æ­¸æœŸæå¤±æ›²ç·š
    ax3 = axes[1, 0]
    try:
        # è¨ˆç®—å›æ­¸æœŸæå¤±
        return_periods = np.array([2, 5, 10, 25, 50, 100, 250])
        return_period_losses = []
        
        sorted_losses = np.sort(impact.at_event[impact.at_event > 0])
        n_events = len(sorted_losses)
        
        for rp in return_periods:
            if n_events > 0:
                # ä½¿ç”¨ç¶“é©—åˆ†ä½æ•¸
                quantile = 1 - 1/rp
                index = int(quantile * n_events)
                if index >= n_events:
                    index = n_events - 1
                rp_loss = sorted_losses[index] / 1e9
                return_period_losses.append(rp_loss)
            else:
                return_period_losses.append(0)
        
        ax3.semilogx(return_periods, return_period_losses, 'o-', linewidth=2, markersize=8)
        ax3.set_xlabel('å›æ­¸æœŸ (å¹´)')
        ax3.set_ylabel('æå¤± (åå„„ç¾å…ƒ)')
        ax3.set_title('å›æ­¸æœŸæå¤±æ›²ç·š')
        ax3.grid(True, alpha=0.3)
        
        # æ¨™è¨»é‡è¦å›æ­¸æœŸ
        for i, (rp, loss) in enumerate(zip(return_periods, return_period_losses)):
            if rp in [10, 50, 100]:
                ax3.annotate(f'{rp}å¹´: ${loss:.2f}B', 
                           xy=(rp, loss), xytext=(10, 10),
                           textcoords='offset points', 
                           bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),
                           arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))
                
    except Exception as e:
        ax3.text(0.5, 0.5, f'å›æ­¸æœŸè¨ˆç®—éŒ¯èª¤: {str(e)}', ha='center', va='center', transform=ax3.transAxes)
        ax3.set_title('å›æ­¸æœŸæå¤±æ›²ç·š')
    
    # 4. ç¶œåˆçµ±è¨ˆç¸½çµ
    ax4 = axes[1, 1]
    ax4.axis('off')  # é—œé–‰åæ¨™è»¸
    
    # æº–å‚™çµ±è¨ˆæ–‡å­—
    stats_text = []
    stats_text.append("ğŸŒ€ ç†±å¸¶æ°£æ—‹å½±éŸ¿åˆ†æçµ±è¨ˆç¸½çµ")
    stats_text.append("=" * 40)
    stats_text.append(f"ç¸½äº‹ä»¶æ•¸: {hazard.size:,}")
    stats_text.append(f"é€ æˆæå¤±äº‹ä»¶æ•¸: {(impact.at_event > 0).sum():,}")
    stats_text.append(f"å¹´å‡ç¸½æå¤± (AAI): ${impact.aai_agg/1e9:.2f}B")
    stats_text.append(f"æœ€å¤§å–®æ¬¡äº‹ä»¶æå¤±: ${impact.at_event.max()/1e9:.2f}B")
    stats_text.append(f"ç¸½æ›éšªåƒ¹å€¼: ${exposure.value.sum()/1e9:.2f}B")
    stats_text.append(f"æ›éšªé»æ•¸é‡: {len(exposure.gdf):,}")
    
    # è¨ˆç®—æå¤±ç‡
    if impact.aai_agg > 0 and exposure.value.sum() > 0:
        loss_ratio = (impact.aai_agg / exposure.value.sum()) * 100
        stats_text.append(f"å¹´å‡æå¤±ç‡: {loss_ratio:.3f}%")
    
    # é¡¯ç¤ºçµ±è¨ˆæ–‡å­—
    stats_full_text = '\n'.join(stats_text)
    ax4.text(0.05, 0.95, stats_full_text, transform=ax4.transAxes, 
             fontsize=11, verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8))
    
    plt.tight_layout()
    
    # ä¿å­˜åœ–ç‰‡
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f'climada_impact_analysis_{timestamp}.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"   ğŸ“Š CLIMADAå½±éŸ¿åˆ†æåœ–å·²ä¿å­˜: {filename}")
    plt.show()
    
    return fig


# %% æ­¥é©Ÿ1: ç”ŸæˆSteinmannåŸºç¤ç”¢å“ (70å€‹)
print("\nğŸ“¦ æ­¥é©Ÿ1: ç”ŸæˆSteinmannæ¨™æº–ç”¢å“...")

if not modules_available['core']:
    print("âŒ æ ¸å¿ƒæ¨¡çµ„ä¸å¯ç”¨ï¼Œç„¡æ³•ç¹¼çºŒ")
    raise ImportError("æ ¸å¿ƒå¼•æ“æ¨¡çµ„å¿…éœ€")

base_products, product_summary = generate_steinmann_2023_products()

print(f"âœ… æˆåŠŸç”Ÿæˆ {len(base_products)} å€‹Steinmannæ¨™æº–ç”¢å“")
print(f"   å–®é–¾å€¼: {product_summary['single_threshold']}")
print(f"   é›™é–¾å€¼: {product_summary['double_threshold']}")
print(f"   ä¸‰é–¾å€¼: {product_summary['triple_threshold']}")
print(f"   å››é–¾å€¼: {product_summary['quadruple_threshold']}")

# %% æ­¥é©Ÿ2: æ“´å±•ç‚ºå¤šåŠå¾‘ç”¢å“ (350å€‹)
print("\nğŸ¯ æ­¥é©Ÿ2: æ“´å±•ç‚ºå¤šåŠå¾‘ç”¢å“...")

radii = [15, 30, 50, 75, 100]  # Cat-in-a-CircleåŠå¾‘(km)
multi_radius_products = []

for radius in radii:
    for product in base_products:
        multi_product = {
            'product_id': f"{product.product_id}_R{radius}",
            'name': f"Steinmann {product.product_id} (R={radius}km)",
            'base_product_id': product.product_id,
            'radius_km': radius,
            'trigger_thresholds': product.thresholds,
            'payout_amounts': [product.max_payout * ratio for ratio in product.payouts],
            'max_payout': product.max_payout,
            'product_type': 'cat_in_circle',
            'payout_function_type': 'step',
            'metadata': {
                'structure_type': product.structure_type,
                'saffir_simpson_based': True
            }
        }
        multi_radius_products.append(multi_product)

print(f"âœ… æˆåŠŸæ“´å±•ç‚º {len(multi_radius_products)} å€‹å¤šåŠå¾‘ç”¢å“")
print(f"   åŸºç¤ç”¢å“: {len(base_products)}")
print(f"   åŠå¾‘é¸é …: {radii}")
print(f"   ç¸½è¨ˆ: {len(radii)} Ã— {len(base_products)} = {len(multi_radius_products)}")

# %% æ­¥é©Ÿ3: æ•¸æ“šæº–å‚™ - å„ªå…ˆCLIMADAçœŸå¯¦æ•¸æ“š
print("\nğŸŒªï¸ æ­¥é©Ÿ3: æº–å‚™åˆ†ææ•¸æ“š...")

climada_data = None

print("æ­£åœ¨æº–å‚™CLIMADAçœŸå¯¦æ•¸æ“š...")
try:
    # ç²å–è»Œè·¡æ•¸æ“š
    print(f"   ğŸŒ€ ç›®æ¨™å€åŸŸ: North Carolina {NC_BOUNDS}")
    print(f"   ğŸ“… åˆ†ææœŸé–“: {YEAR_RANGE[0]}-{YEAR_RANGE[1]}")
    
    tracks = get_regional_tracks(NC_BOUNDS, YEAR_RANGE, nb_synth=3)
    print(f"   âœ… ç²å– {len(tracks.data)} æ¢è»Œè·¡")
    
    # å‰µå»ºç½å®³å ´
    from hazard_modeling.centroids import create_hazard_centroids
    centroids_lat, centroids_lon = create_hazard_centroids(NC_BOUNDS, RESOLUTION)
    tc_hazard = create_tc_hazard(tracks, centroids_lat, centroids_lon)
    print(f"   âœ… ç½å®³å ´: {tc_hazard.size} å€‹äº‹ä»¶")
    
    # å‰µå»ºæ›éšª
    exposure_dict, successful_years = process_litpop_exposures(
        country_iso="USA", state_name="North Carolina", years=[2020, 2019]
    )
    
    if successful_years:
        exposure_main = exposure_dict[successful_years[0]]
        print(f"   âœ… æ›éšªæ•¸æ“š: {len(exposure_main.gdf)} å€‹é»")
        print(f"   ğŸ’° ç¸½æ›éšªå€¼: ${exposure_main.value.sum()/1e9:.2f}B")
        
        # è¨ˆç®—å½±éŸ¿
        impact, impact_func_set = calculate_tc_impact(tc_hazard, exposure_main)
        
        # è¼¸å‡ºè©³ç´°çµæœçµ±è¨ˆ
        print(f"\nğŸ’¥ ç½å®³å½±éŸ¿åˆ†æçµæœ:")
        print(f"   å¹´å‡ç¸½æå¤± (AAI): ${impact.aai_agg/1e9:.2f}B")
        print(f"   ç¸½äº‹ä»¶æå¤±: ${impact.at_event.sum()/1e9:.2f}B")
        print(f"   æœ€å¤§å–®æ¬¡äº‹ä»¶æå¤±: ${impact.at_event.max()/1e9:.2f}B")
        print(f"   å—å½±éŸ¿äº‹ä»¶æ•¸: {(impact.at_event > 0).sum()}")
        
        # è¨ˆç®—å›æ­¸æœŸæå¤±
        freq_curve = impact.calc_freq_curve()
        rp_losses = {
            10: freq_curve.impact[np.argmin(np.abs(freq_curve.return_per - 10))],
            50: freq_curve.impact[np.argmin(np.abs(freq_curve.return_per - 50))], 
            100: freq_curve.impact[np.argmin(np.abs(freq_curve.return_per - 100))],
            500: freq_curve.impact[np.argmin(np.abs(freq_curve.return_per - 500))]
        }
        
        print(f"\nğŸ“Š å›æ­¸æœŸæå¤±ä¼°è¨ˆ:")
        for rp, loss in rp_losses.items():
            print(f"   {rp}å¹´å›æ­¸æœŸ: ${loss/1e9:.2f}B")
        
        print(f"ğŸ“ˆ ç½å®³å½±éŸ¿è¨ˆç®—å®Œæˆ")
        
        # æº–å‚™æ•¸æ“š
        climada_data = {
            'tc_hazard': tc_hazard,
            'exposure': exposure_main,
            'impact': impact,
            'impact_func_set': impact_func_set,  # æ·»åŠ å½±éŸ¿å‡½æ•¸é›†
            'event_losses': impact.at_event,
            'exposure_locations': [(lat, lon) for lat, lon in zip(exposure_main.latitude, exposure_main.longitude)],
            'metadata': {
                'n_events': tc_hazard.size,
                'total_exposure': exposure_main.value.sum(),
                'annual_average_impact': impact.aai_agg
            }
        }
        print("   ğŸ‰ CLIMADAçœŸå¯¦æ•¸æ“šæº–å‚™å®Œæˆï¼")
    else:
        print("   âŒ ç„¡æ³•å‰µå»ºæ›éšªæ•¸æ“š")
        raise ValueError("ç„¡æ³•å‰µå»ºæ›éšªæ•¸æ“š")
except Exception as e:
    print(f"   âŒ CLIMADAæ•¸æ“šæº–å‚™å¤±æ•—: {e}")
    raise e  # é‡æ–°æ‹‹å‡ºä¾‹å¤–ï¼Œç¢ºä¿ä¸æœƒéœé»˜å¤±æ•—

# CLIMADA æ•¸æ“šå·²æˆåŠŸæº–å‚™ï¼Œç›´æ¥ä½¿ç”¨
main_data = climada_data
data_source = "CLIMADAçœŸå¯¦æ•¸æ“š"
print(f"\nğŸ“Š ä½¿ç”¨æ•¸æ“šæº: {data_source}")

# %% æ­¥é©Ÿ4: æå–Cat-in-a-Circleåƒæ•¸æŒ‡æ¨™
print("\n" + "="*60)
print("ğŸ¯ æ­¥é©Ÿ4: Steinmanné†«é™¢Cat-in-a-Circleåˆ†æ")
print("="*60)

# æ­¥é©Ÿ4a: ç²å–é†«é™¢ä½ç½®
print("\nğŸ¥ æ­¥é©Ÿ4a: ç²å–åŒ—å¡ç¾…ä¾†ç´å·é†«é™¢ä½ç½®...")
try:
    gdf_hospitals, hospital_exposures = get_nc_hospitals(
        use_mock=True,  # ä½¿ç”¨æ¨¡æ“¬æ•¸æ“šï¼Œå¯æ ¹æ“šéœ€è¦æ”¹ç‚ºFalseä½¿ç”¨çœŸå¯¦OSMæ•¸æ“š
        create_exposures=True,
        visualize=False  # å…ˆä¸è¦–è¦ºåŒ–ï¼Œé¿å…å¹²æ“¾ä¸»æµç¨‹
    )
    
    # æå–é†«é™¢åº§æ¨™
    hospital_coords = [(row.geometry.y, row.geometry.x) for _, row in gdf_hospitals.iterrows()]
    print(f"   âœ… æˆåŠŸç²å– {len(hospital_coords)} å®¶é†«é™¢ä½ç½®")
    
except Exception as e:
    print(f"   âŒ é†«é™¢æ•¸æ“šç²å–å¤±æ•—: {e}")
    print("   ğŸ”„ ä½¿ç”¨é è¨­ç¤ºç¯„ä½ç½®...")
    # ä½¿ç”¨å¹¾å€‹ä¸»è¦åŸå¸‚ä½œç‚ºç¤ºç¯„
    hospital_coords = [
        (35.7796, -78.6382),  # ç¾…åˆ©
        (35.2271, -80.8431),  # å¤æ´›ç‰¹  
        (36.0726, -79.7920),  # æ ¼æ—æ–¯åšç¾…
        (36.1023, -80.2442),  # æº«æ–¯é “å¡å‹’å§†
        (34.2257, -77.9447)   # å¨æ˜é “
    ]
    print(f"   âœ… ä½¿ç”¨ {len(hospital_coords)} å€‹ç¤ºç¯„ä½ç½®")

# ä½¿ç”¨CLIMADAçœŸå¯¦æ•¸æ“š
tc_hazard = main_data['tc_hazard']
exposure_main = main_data['exposure']

# å»ºç«‹ç©ºé–“ç´¢å¼•
print("   ğŸ”„ å»ºç«‹ç½å®³å ´ç©ºé–“ç´¢å¼•...")
from scipy.spatial import cKDTree
hazard_coords = np.array([
    [tc_hazard.centroids.lat[i], tc_hazard.centroids.lon[i]] 
    for i in range(tc_hazard.centroids.size)
])
hazard_tree = cKDTree(np.radians(hazard_coords))

# æ­¥é©Ÿ4b: é†«é™¢Cat-in-a-Circleåˆ†æ
print("\nğŸ¯ æ­¥é©Ÿ4b: é†«é™¢Cat-in-a-Circleé¢¨é€Ÿæå–...")

# ç‚ºä¿æŒå…¼å®¹æ€§ï¼Œé¦–å…ˆè¨ˆç®—é†«é™¢ç´šåˆ¥æ•¸æ“š
hospital_cat_indices = {}
hospital_wind_data = {}  # ä¿å­˜è©³ç´°çš„é†«é™¢é¢¨é€Ÿæ•¸æ“š

for radius in radii:
    print(f"\nğŸŒ€ è¨ˆç®—åŠå¾‘ {radius}km çš„é†«é™¢Cat-in-a-CircleæŒ‡æ¨™...")
    
    hospital_winds = extract_hospital_cat_in_circle(
        tc_hazard, hospital_coords, hazard_tree, radius
    )
    
    hospital_wind_data[radius] = hospital_winds
    
    # ä¿®æ­£å¯¦ç¾ï¼šä¿å­˜å€‹åˆ¥é†«é™¢çš„Cat-in-a-Circleæ•¸æ“šï¼Œè€Œéå…¨åŸŸæœ€å¤§å€¼
    # é€™æ‰ç¬¦åˆSteinmannè«–æ–‡çš„çœŸå¯¦æ„åœ–ï¼šåˆ†æç‰¹å®šä¿éšªæ¨™çš„ç‰©ï¼ˆé†«é™¢ï¼‰çš„é¢¨éšª
    hospital_cat_indices[radius] = hospital_winds
    
    print(f"   âœ… å®Œæˆ {radius}km åŠå¾‘åˆ†æ: {len(hospital_coords)} å®¶é†«é™¢")
    
    # è¨ˆç®—æ‰€æœ‰é†«é™¢çš„çµ±è¨ˆæ‘˜è¦
    all_winds = []
    for h_idx, h_winds in hospital_winds.items():
        all_winds.extend(h_winds[h_winds > 0])  # åªåŒ…å«éé›¶é¢¨é€Ÿ
    
    if all_winds:
        print(f"   ğŸ“Š åŠå¾‘å…§é¢¨é€Ÿçµ±è¨ˆ: å¹³å‡ {np.mean(all_winds):.1f} m/s, æœ€å¤§ {np.max(all_winds):.1f} m/s")

# æ­¥é©Ÿ4c: é†«é™¢æ¨™æº–åŒ–æå¤±è¨ˆç®—  
print("\nğŸ’Š æ­¥é©Ÿ4c: é†«é™¢æ¨™æº–åŒ–æå¤±è¨ˆç®—...")
hospital_damages_by_radius = {}
for radius in radii:
    print(f"   ğŸ¥ è¨ˆç®—åŠå¾‘ {radius}km é†«é™¢æå¤±...")
    try:
        hospital_damages = calculate_hospital_standardized_damages(
            hospital_wind_data[radius], 
            main_data.get('impact_func_set', None)
        )
        hospital_damages_by_radius[radius] = hospital_damages
    except Exception as e:
        print(f"      âš ï¸ é†«é™¢æå¤±è¨ˆç®—å¤±æ•—: {e}")

# ä½¿ç”¨é†«é™¢æ•¸æ“šä½œç‚ºä¸»è¦åˆ†æå°è±¡ (ä¿æŒå‘å¾Œå…¼å®¹)
cat_in_circle_indices = hospital_cat_indices

print(f"\nâœ… Steinmanné†«é™¢Cat-in-a-Circleåˆ†æå®Œæˆ")
print(f"   ğŸ¥ åˆ†æé†«é™¢æ•¸: {len(hospital_coords)}")
print(f"   ğŸŒ€ åˆ†æåŠå¾‘: {radii} km")
print(f"   ğŸ“Š äº‹ä»¶æ•¸: {tc_hazard.intensity.shape[0]}")

# é¡¯ç¤ºçµ±è¨ˆ - ä¿®æ­£ï¼šç¾åœ¨indicesæ˜¯å­—å…¸è€Œéæ•¸çµ„
for radius in radii:
    hospital_winds = cat_in_circle_indices[radius]
    
    # æ”¶é›†æ‰€æœ‰é†«é™¢çš„é¢¨é€Ÿæ•¸æ“š
    all_winds = []
    for hospital_idx, winds in hospital_winds.items():
        all_winds.extend(winds[winds > 0])  # åªåŒ…å«éé›¶é¢¨é€Ÿ
    
    if all_winds:
        all_winds = np.array(all_winds)
        print(f"   åŠå¾‘ {radius}km: å¹³å‡ {np.mean(all_winds):.1f} m/s, æœ€å¤§ {np.max(all_winds):.1f} m/s, ç¯„åœ {np.min(all_winds):.1f}-{np.max(all_winds):.1f} m/s")
    else:
        print(f"   åŠå¾‘ {radius}km: ç„¡æœ‰æ•ˆé¢¨é€Ÿæ•¸æ“š")

# %% æ­¥é©Ÿ4d: Cat-in-a-Circle æ·±åº¦åˆ†æèˆ‡è¦–è¦ºåŒ–
print(f"\nğŸ¯ æ­¥é©Ÿ4d: Cat-in-a-Circle æ·±åº¦åˆ†æèˆ‡è¦–è¦ºåŒ–...")

def analyze_cat_in_circle_characteristics(cat_in_circle_indices, damages):
    """
    åˆ†æ Cat-in-a-Circle æŒ‡æ¨™ç‰¹æ€§
    """
    
    print("ğŸ¯ Cat-in-a-Circle æŒ‡æ¨™ç‰¹æ€§åˆ†æ")
    print("=" * 50)
    
    results = {}
    saffir_simpson_thresholds = [33, 42, 49, 58, 70]  # m/s
    threshold_names = ['Cat 1', 'Cat 2', 'Cat 3', 'Cat 4', 'Cat 5']
    
    for radius, hospital_winds in cat_in_circle_indices.items():
        print(f"\nğŸŒ€ åŠå¾‘ {radius}km åˆ†æ:")
        
        # æ”¶é›†æ‰€æœ‰é†«é™¢çš„é¢¨é€Ÿæ•¸æ“š
        all_winds = []
        for hospital_idx, winds in hospital_winds.items():
            all_winds.extend(winds[winds > 0])  # åªåŒ…å«éé›¶é¢¨é€Ÿ
        
        if not all_winds:
            print(f"   âš ï¸ ç„¡æœ‰æ•ˆé¢¨é€Ÿæ•¸æ“š")
            continue
            
        wind_speeds_array = np.array(all_winds)
        
        # åŸºæœ¬çµ±è¨ˆ
        stats = {
            'mean': np.mean(wind_speeds_array),
            'std': np.std(wind_speeds_array),
            'min': np.min(wind_speeds_array),
            'max': np.max(wind_speeds_array),
            'median': np.median(wind_speeds_array)
        }
        
        print(f"   å¹³å‡é¢¨é€Ÿ: {stats['mean']:.1f} Â± {stats['std']:.1f} m/s")
        print(f"   é¢¨é€Ÿç¯„åœ: {stats['min']:.1f} - {stats['max']:.1f} m/s")
        print(f"   ä¸­ä½æ•¸: {stats['median']:.1f} m/s")
        
        # è§¸ç™¼é »ç‡åˆ†æ
        print(f"   è§¸ç™¼é »ç‡åˆ†æ:")
        trigger_counts = []
        for threshold, name in zip(saffir_simpson_thresholds, threshold_names):
            triggered = np.sum(wind_speeds_array >= threshold)
            percentage = triggered / len(wind_speeds_array) * 100
            print(f"     {name} ({threshold} m/s): {triggered}æ¬¡ ({percentage:.1f}%)")
            trigger_counts.append(triggered)
        
        # ç‚ºç›¸é—œæ€§åˆ†æï¼Œå°‡é¢¨é€Ÿè½‰æ›ç‚ºèˆ‡æå¤±åŒæ¨£é•·åº¦çš„æ•¸çµ„
        n_events = len(damages)
        event_max_winds = []
        
        for event_idx in range(n_events):
            event_winds = [hospital_winds[h_idx][event_idx] for h_idx in hospital_winds.keys()]
            event_max_winds.append(max(event_winds) if event_winds else 0.0)
        
        event_max_winds = np.array(event_max_winds)
        
        # ç›¸é—œæ€§åˆ†æ
        correlation = np.corrcoef(event_max_winds, damages)[0, 1]
        print(f"   é¢¨é€Ÿ-æå¤±ç›¸é—œæ€§: {correlation:.3f}")
        
        # åŸºå·®é¢¨éšªç°¡å–®åˆ†æ
        # è¨ˆç®—è§¸ç™¼ä½†ç„¡æå¤±çš„æƒ…æ³
        significant_damage_threshold = np.percentile(damages[damages > 0], 10)  # æœ‰æ„ç¾©çš„æå¤±é–¾å€¼
        cat1_triggered = event_max_winds >= 33
        low_damage = damages < significant_damage_threshold
        basis_risk_events = np.sum(cat1_triggered & low_damage)
        
        print(f"   æ½›åœ¨åŸºå·®é¢¨éšªäº‹ä»¶: {basis_risk_events}æ¬¡ ({basis_risk_events/len(event_max_winds)*100:.1f}%)")
        print(f"   (å®šç¾©: è§¸ç™¼Cat1ä½†æå¤±ä½æ–¼{significant_damage_threshold/1e6:.0f}Mçš„äº‹ä»¶)")
        
        results[radius] = {
            'stats': stats,
            'trigger_counts': trigger_counts,
            'correlation': correlation,
            'basis_risk_events': basis_risk_events
        }
    
    return results

def compare_radius_performance(results):
    """
    æ¯”è¼ƒä¸åŒåŠå¾‘çš„æ€§èƒ½è¡¨ç¾
    """
    print(f"\nğŸ“Š åŠå¾‘æ€§èƒ½æ¯”è¼ƒ")
    print("=" * 50)
    
    # å‰µå»ºæ¯”è¼ƒè¡¨æ ¼
    comparison_data = []
    for radius, result in results.items():
        comparison_data.append({
            'åŠå¾‘(km)': radius,
            'å¹³å‡é¢¨é€Ÿ(m/s)': f"{result['stats']['mean']:.1f}",
            'é¢¨é€Ÿæ¨™æº–å·®': f"{result['stats']['std']:.1f}",
            'ç›¸é—œæ€§': f"{result['correlation']:.3f}",
            'Cat1è§¸ç™¼æ¬¡æ•¸': result['trigger_counts'][0] if result['trigger_counts'] else 0,
            'åŸºå·®é¢¨éšªäº‹ä»¶': result['basis_risk_events']
        })
    
    df_comparison = pd.DataFrame(comparison_data)
    print(df_comparison.to_string(index=False))
    
    # å»ºè­°æœ€ä½³åŠå¾‘
    best_correlation_radius = max(results.keys(), 
                                 key=lambda r: results[r]['correlation'])
    print(f"\nğŸ’¡ å»ºè­°:")
    print(f"   æœ€ä½³ç›¸é—œæ€§åŠå¾‘: {best_correlation_radius}km (ç›¸é—œæ€§ {results[best_correlation_radius]['correlation']:.3f})")
    
    return df_comparison

# åŸ·è¡Œæ·±åº¦åˆ†æ
damages = main_data['event_losses']
cat_analysis_results = analyze_cat_in_circle_characteristics(cat_in_circle_indices, damages)

# ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨
print(f"\nğŸ“ˆ ç”Ÿæˆé¢¨é€Ÿåˆ†å¸ƒåœ–...")
try:
    # ç¹ªè£½æ–°çš„ã€æœ‰æ„ç¾©çš„é†«é™¢Cat-in-a-Circleåˆ†æåœ–
    print("\nğŸ“Š æ­¥é©Ÿ4d: ç¹ªè£½é†«é™¢Cat-in-a-Circleåˆ†æåœ–...")
    plot_hospital_cat_in_circle_analysis(hospital_cat_indices, hospital_coords)
except Exception as e:
    print(f"   âš ï¸ åœ–è¡¨ç”Ÿæˆå¤±æ•—: {e}")

# æ€§èƒ½æ¯”è¼ƒ
performance_comparison = compare_radius_performance(cat_analysis_results)

print(f"\nâœ… Cat-in-a-Circle æ·±åº¦åˆ†æå®Œæˆï¼")

# %% æ­¥é©Ÿ4e: CLIMADA æ¢ç´¢è¦–è¦ºåŒ–åˆ†æ  
print(f"\nğŸŒ æ­¥é©Ÿ4e: CLIMADA æ¢ç´¢è¦–è¦ºåŒ–åˆ†æ...")

# Duplicate function definitions have been moved to the top of the file

def visualize_exposure_spatial_distribution(exposure_main):
    """è¦–è¦ºåŒ–æ›éšªç©ºé–“åˆ†å¸ƒ"""
    
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # 1. ç©ºé–“åˆ†å¸ƒæ•£é»åœ–
    ax1 = axes[0]
    scatter = ax1.scatter(exposure_main.longitude, exposure_main.latitude,
                         c=exposure_main.gdf.value/1e6, 
                         s=2, alpha=0.7, cmap='viridis')
    ax1.set_xlabel('ç¶“åº¦')
    ax1.set_ylabel('ç·¯åº¦')
    ax1.set_title('åŒ—å¡ç¾…ä¾†ç´å·æ›éšªè³‡ç”¢ç©ºé–“åˆ†å¸ƒ')
    ax1.grid(True, alpha=0.3)
    
    # æ·»åŠ é¡è‰²æ¢
    cbar1 = plt.colorbar(scatter, ax=ax1)
    cbar1.set_label('è³‡ç”¢åƒ¹å€¼ (ç™¾è¬ç¾å…ƒ)')
    
    # 2. åƒ¹å€¼å¯†åº¦ç†±åŠ›åœ–
    ax2 = axes[1]
    # å‰µå»º2Dç›´æ–¹åœ–ä¾†é¡¯ç¤ºåƒ¹å€¼å¯†åº¦
    lon_bins = np.linspace(exposure_main.longitude.min(), exposure_main.longitude.max(), 30)
    lat_bins = np.linspace(exposure_main.latitude.min(), exposure_main.latitude.max(), 30)
    
    # è¨ˆç®—æ¯å€‹binçš„ç¸½åƒ¹å€¼
    H, xedges, yedges = np.histogram2d(exposure_main.longitude, exposure_main.latitude, 
                                      bins=[lon_bins, lat_bins], 
                                      weights=exposure_main.gdf.value)
    
    im = ax2.imshow(H.T, origin='lower', extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]], 
                   cmap='hot', aspect='auto')
    ax2.set_xlabel('ç¶“åº¦')
    ax2.set_ylabel('ç·¯åº¦')
    ax2.set_title('è³‡ç”¢åƒ¹å€¼å¯†åº¦åˆ†å¸ƒ')
    
    # æ·»åŠ é¡è‰²æ¢
    cbar2 = plt.colorbar(im, ax=ax2)
    cbar2.set_label('ç¸½è³‡ç”¢åƒ¹å€¼ (ç¾å…ƒ)')
    
    plt.tight_layout()
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f'exposure_spatial_distribution_{timestamp}.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"   ğŸ“Š æ›éšªç©ºé–“åˆ†å¸ƒåœ–å·²ä¿å­˜: {filename}")
    plt.show()
    
    return fig

def compare_radius_performance(results):
    """
    æ¯”è¼ƒä¸åŒåŠå¾‘çš„æ€§èƒ½è¡¨ç¾
    """
    print(f"\nğŸ“Š åŠå¾‘æ€§èƒ½æ¯”è¼ƒ")
    print("=" * 50)
    
    # å‰µå»ºæ¯”è¼ƒè¡¨æ ¼
    comparison_data = []
    for radius, result in results.items():
        comparison_data.append({
            'åŠå¾‘(km)': radius,
            'å¹³å‡é¢¨é€Ÿ(m/s)': f"{result['stats']['mean']:.1f}",
            'é¢¨é€Ÿæ¨™æº–å·®': f"{result['stats']['std']:.1f}",
            'ç›¸é—œæ€§': f"{result['correlation']:.3f}",
            'Cat1è§¸ç™¼ç‡(%)': f"{result['trigger_counts'][0]/328*100:.1f}",
            'åŸºå·®é¢¨éšªäº‹ä»¶': result['basis_risk_events']
        })
    
    df_comparison = pd.DataFrame(comparison_data)
    print(df_comparison.to_string(index=False))
    
    # å»ºè­°æœ€ä½³åŠå¾‘
    best_correlation_radius = max(results.keys(), 
                                 key=lambda r: results[r]['correlation'])
    print(f"\nğŸ’¡ å»ºè­°:")
    print(f"   æœ€ä½³ç›¸é—œæ€§åŠå¾‘: {best_correlation_radius}km (ç›¸é—œæ€§ {results[best_correlation_radius]['correlation']:.3f})")

# Hospital functions have been moved to the top of the file

def plot_top_tracks(tracks, bounds, title="åŒ—å¡ç¾…ä¾†ç´å·ä¸»è¦é¢±é¢¨è»Œè·¡"):
    """ç¹ªè£½ä¸»è¦é¢±é¢¨è»Œè·¡"""
    
    try:
        import cartopy.crs as ccrs
        import cartopy.feature as cfeature
        
        # é¸æ“‡å‰20å¼·çš„è»Œè·¡
        track_intensities = []
        for track in tracks:
            if hasattr(track, 'max_sustained_wind'):
                max_wind = np.max(track.max_sustained_wind.values)
            else:
                max_wind = 0
            track_intensities.append(max_wind)
        
        # æ’åºä¸¦é¸æ“‡å‰20
        sorted_indices = np.argsort(track_intensities)[::-1][:20]
        
        fig = plt.figure(figsize=(12, 10))
        ax = plt.axes(projection=ccrs.PlateCarree())
        
        # è¨­ç½®åœ°åœ–ç¯„åœ
        ax.set_extent([bounds['lon_min']-2, bounds['lon_max']+2, 
                      bounds['lat_min']-1, bounds['lat_max']+1], 
                     ccrs.PlateCarree())
        
        # æ·»åŠ åœ°åœ–ç‰¹å¾µ
        ax.add_feature(cfeature.COASTLINE, linewidth=0.5)
        ax.add_feature(cfeature.BORDERS, linewidth=0.5)
        ax.add_feature(cfeature.STATES, linewidth=0.5)
        ax.add_feature(cfeature.OCEAN, facecolor='lightblue', alpha=0.5)
        ax.add_feature(cfeature.LAND, facecolor='lightgray', alpha=0.5)
        
        # ç¹ªè£½è»Œè·¡
        colors = ['red', 'orange', 'yellow', 'green', 'blue']
        categories = ['Cat 5 (>70 m/s)', 'Cat 4 (58-70)', 'Cat 3 (49-58)', 
                     'Cat 2 (42-49)', 'Cat 1 (33-42)']
        
        for i, track_idx in enumerate(sorted_indices):
            track = tracks[track_idx]
            max_wind = track_intensities[track_idx]
            
            # æ ¹æ“šå¼·åº¦é¸æ“‡é¡è‰²
            if max_wind >= 70:
                color = colors[0]
            elif max_wind >= 58:
                color = colors[1]
            elif max_wind >= 49:
                color = colors[2]
            elif max_wind >= 42:
                color = colors[3]
            else:
                color = colors[4]
            
            # ç¹ªè£½è»Œè·¡
            if hasattr(track, 'lon') and hasattr(track, 'lat'):
                ax.plot(track.lon.values, track.lat.values, 
                       color=color, linewidth=2, alpha=0.7, 
                       transform=ccrs.PlateCarree())
        
        # æ·»åŠ åœ–ä¾‹
        legend_elements = [plt.Line2D([0], [0], color=color, lw=2, label=cat) 
                          for color, cat in zip(colors, categories)]
        ax.legend(handles=legend_elements, loc='upper left')
        
        plt.title(f'{title}\nå‰20å¼·é¢±é¢¨è»Œè·¡', fontsize=14)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'top_tc_tracks_{timestamp}.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"   ğŸ“Š é¢±é¢¨è»Œè·¡åœ–å·²ä¿å­˜: {filename}")
        plt.show()
        
        return fig
    
    except ImportError:
        print("   âš ï¸ ç„¡æ³•å°å…¥cartopyï¼Œè·³éè»Œè·¡åœ°åœ–ç¹ªè£½")
        return None
    except Exception as e:
        print(f"   âš ï¸ è»Œè·¡ç¹ªè£½å¤±æ•—: {e}")
        return None

# %% æ­¥é©Ÿ4f: CLIMADAæ¢ç´¢èˆ‡è¦–è¦ºåŒ–
print("\nğŸ¨ æ­¥é©Ÿ4f: CLIMADAæ¢ç´¢èˆ‡è¦–è¦ºåŒ–")  
print("="*60)

# é¦–å…ˆé¡¯ç¤ºLitPopæ›éšªè©³ç´°çµ±è¨ˆ
if 'exposure_dict' in locals() and successful_years:
    print(f"\nğŸ˜ï¸ LitPop æ›éšªè³‡æ–™è™•ç†å®Œæˆï¼ŒæˆåŠŸè™•ç† {len(successful_years)} å¹´è³‡æ–™")
    
    # é¡¯ç¤ºæ‰€æœ‰å¹´ä»½çš„è©³ç´°çµ±è¨ˆ
    for year in successful_years:
        exposure = exposure_dict[year]
        print(f"\nğŸ“Š {year} å¹´çµ±è¨ˆ:")
        print(f"   è³‡ç”¢é»æ•¸: {len(exposure.gdf):,}")
        print(f"   ç¸½åƒ¹å€¼: ${exposure.gdf['value'].sum()/1e9:.1f}B")
        print(f"   å¹³å‡å–®é»åƒ¹å€¼: ${exposure.gdf['value'].mean()/1e6:.2f}M")
        print(f"   ä¸­ä½æ•¸å–®é»åƒ¹å€¼: ${exposure.gdf['value'].median()/1e6:.2f}M")
        print(f"   æœ€å¤§å–®é»åƒ¹å€¼: ${exposure.gdf['value'].max()/1e6:.2f}M")
    
    # æš«æ™‚è·³éè¦–è¦ºåŒ–ï¼Œç¢ºä¿è¨ˆç®—æ­£å¸¸é‹è¡Œ
    print("\n   âš ï¸ æš«æ™‚è·³éLitPopè¦–è¦ºåŒ–ä»¥ç¢ºä¿ä¸»è¦è¨ˆç®—æ­£å¸¸é‹è¡Œ")
    
    # # å‰µå»ºå„å¹´ä»½çš„è©³ç´°è¦–è¦ºåŒ–
    # print("\n   ğŸ“Š å‰µå»ºLitPopç©ºé–“åˆ†å¸ƒèˆ‡åƒ¹å€¼åˆ†å¸ƒè¦–è¦ºåŒ–...")
    # spatial_fig, value_fig = visualize_all_litpop_exposures(exposure_dict, successful_years)
    # if value_fig:
    #     plt.show()
    
    # # å‰µå»ºå¹´åº¦æ¯”è¼ƒåœ–è¡¨
    # if len(successful_years) > 1:
    #     print("\n   ğŸ“ˆ å‰µå»ºå¹´åº¦æ¯”è¼ƒåˆ†æ...")
    #     comparison_fig, stats_summary = create_yearly_comparison(exposure_dict, successful_years)
    #     if comparison_fig:
    #         plt.show()
    #     
    #     print(f"\nğŸ“ˆ å¹´åº¦æ¯”è¼ƒæ‘˜è¦:")
    #     print(stats_summary.round(2))

# å½±éŸ¿åˆ†æè¦–è¦ºåŒ–
print("\n   âš ï¸ æš«æ™‚è·³éæ‰€æœ‰è¦–è¦ºåŒ–ä»¥ç¢ºä¿ä¸»è¦è¨ˆç®—æ­£å¸¸é‹è¡Œ")

# try:
#     print("\n   ğŸ“Š å‰µå»ºç¶œåˆå½±éŸ¿åˆ†æè¦–è¦ºåŒ–...")
#     if 'impact' in locals():
#         impact_fig = create_impact_visualizations(impact, exposure_main, tc_hazard)
#     else:
#         print("      âš ï¸ å½±éŸ¿æ•¸æ“šä¸å¯ç”¨ï¼Œè·³éå½±éŸ¿è¦–è¦ºåŒ–")
#     
#     print("\n   ğŸ—ºï¸ è¦–è¦ºåŒ–æ›éšªç©ºé–“åˆ†å¸ƒ...")
#     exposure_fig = visualize_exposure_spatial_distribution(exposure_main)
#     
#     print("\n   ğŸŒ€ ç¹ªè£½ä¸»è¦é¢±é¢¨è»Œè·¡...")
#     if 'tracks' in locals() and hasattr(tracks, 'data'):
#         tracks_fig = plot_top_tracks(tracks.data, NC_BOUNDS)
#     else:
#         print("      âš ï¸ è»Œè·¡æ•¸æ“šä¸å¯ç”¨ï¼Œè·³éè»Œè·¡è¦–è¦ºåŒ–")
#     
#     print("\n   âœ… CLIMADAæ¢ç´¢èˆ‡è¦–è¦ºåŒ–å®Œæˆï¼")
#     
# except Exception as e:
#     print(f"   âš ï¸ è¦–è¦ºåŒ–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
#     print("   ğŸ”„ ç¹¼çºŒåŸ·è¡Œå¾ŒçºŒåˆ†æ...")

print("\n   âœ… æ­¥é©Ÿ4få®Œæˆï¼Œç¹¼çºŒé€²è¡Œä¸»è¦åˆ†æ...")

# %% æ­¥é©Ÿ5a: å‚³çµ±Steinmann RMSEåˆ†æ
print(f"\nğŸ“Š æ­¥é©Ÿ5a: å‚³çµ±Steinmann RMSEåˆ†æ ({len(multi_radius_products)} å€‹ç”¢å“)...")

damages = main_data['event_losses']
rmse_results = []

for i, product in enumerate(multi_radius_products):
    if (i + 1) % 50 == 0:
        print(f"   é€²åº¦: {i+1}/{len(multi_radius_products)}")
    
    radius = product['radius_km']
    hospital_winds = cat_in_circle_indices[radius]
    
    # è½‰æ›é†«é™¢å­—å…¸ç‚ºé©åˆåˆ†æçš„æ ¼å¼
    # é¸æ“‡æœ€å…·ä»£è¡¨æ€§çš„æ–¹å¼ï¼šæ¯å€‹äº‹ä»¶å–æ‰€æœ‰é†«é™¢çš„æœ€å¤§é¢¨é€Ÿ
    n_events = len(list(hospital_winds.values())[0]) if hospital_winds else 0
    wind_speeds = np.zeros(n_events)
    
    for event_idx in range(n_events):
        event_winds = [hospital_winds[h_idx][event_idx] for h_idx in hospital_winds.keys()]
        wind_speeds[event_idx] = max(event_winds) if event_winds else 0.0
    
    # è¨ˆç®—éšæ¢¯å¼è³ ä»˜ (æ­£ç¢ºç‰ˆæœ¬)
    payouts = calculate_correct_step_payouts(
        wind_speeds,
        product['trigger_thresholds'],
        [amount/product['max_payout'] for amount in product['payout_amounts']],
        product['max_payout']
    )
    
    # è¨ˆç®—æŒ‡æ¨™
    rmse = np.sqrt(np.mean((damages - payouts) ** 2))
    mae = np.mean(np.abs(damages - payouts))
    correlation = np.corrcoef(damages, payouts)[0,1] if np.std(payouts) > 0 else 0
    trigger_rate = np.mean(payouts > 0)
    mean_payout = np.mean(payouts)
    basis_risk = np.std(damages - payouts)
    
    rmse_results.append({
        'product_id': product['product_id'],
        'radius_km': radius,
        'rmse': rmse,
        'mae': mae,
        'correlation': correlation,
        'trigger_rate': trigger_rate,
        'mean_payout': mean_payout,
        'basis_risk': basis_risk,
        'payouts': payouts
    })

# åˆ†æçµæœ
rmse_df = pd.DataFrame(rmse_results)
best_rmse_idx = rmse_df['rmse'].idxmin()
steinmann_best = rmse_df.iloc[best_rmse_idx].to_dict()

print(f"   âœ… Steinmann RMSEåˆ†æå®Œæˆ")
print(f"      æœ€ä½³RMSE: ${steinmann_best['rmse']/1e9:.3f}B")
print(f"      æœ€ä½³ç”¢å“: {steinmann_best['product_id']}")
print(f"      æœ€ä½³ç›¸é—œæ€§: {rmse_df['correlation'].max():.3f}")
print(f"      å¹³å‡è§¸ç™¼ç‡: {rmse_df['trigger_rate'].mean():.1%}")

steinmann_results = {
    'method': 'steinmann_rmse',
    'results_df': rmse_df,
    'best_product': steinmann_best,
    'summary': {
        'best_rmse': steinmann_best['rmse'],
        'best_correlation': rmse_df['correlation'].max(),
        'mean_trigger_rate': rmse_df['trigger_rate'].mean()
    }
}

# %% æ­¥é©Ÿ5b: ç©©å¥è²æ°CRPSåˆ†æ 
print(f"\nğŸ§  æ­¥é©Ÿ5b: ç©©å¥è²æ°CRPSåˆ†æ ({len(multi_radius_products)} å€‹ç”¢å“)...")

# åˆå§‹åŒ–æå¤±åˆ†å¸ƒ
loss_distributions = {}
n_samples = 500  # å¢åŠ æ¨£æœ¬æ•¸

if modules_available['bayesian'] and 'tc_hazard' in main_data and 'exposure' in main_data:
    try:
        print("   ğŸš€ å•Ÿå‹•ç©©å¥è²æ°åˆ†æå™¨...")
        
        # åˆå§‹åŒ–å®Œæ•´çš„ç©©å¥è²æ°åˆ†æå™¨
        bayesian_analyzer = RobustBayesianAnalyzer(
            density_ratio_constraint=2.0,      # å¯†åº¦æ¯”ç´„æŸ
            n_monte_carlo_samples=n_samples,   # Monte Carloæ¨£æœ¬æ•¸
            n_mixture_components=3,            # æ··åˆæ¨¡å‹çµ„ä»¶æ•¸
            hazard_uncertainty_std=0.15,       # ç½å®³ä¸ç¢ºå®šæ€§ 15%
            exposure_uncertainty_log_std=0.20, # æ›éšªä¸ç¢ºå®šæ€§ 20%
            vulnerability_uncertainty_std=0.10 # è„†å¼±æ€§ä¸ç¢ºå®šæ€§ 10%
        )
        print("   âœ… ç©©å¥è²æ°åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        
        # ä½¿ç”¨çœŸå¯¦çš„CLIMADAæ•¸æ“š
        tc_hazard = main_data['tc_hazard']
        exposure_main = main_data['exposure']
        impact_func_set = main_data.get('impact_func_set', None)
        
        print("   ğŸ”¬ åŸ·è¡Œå®Œæ•´ç©©å¥è²æ°ä¸ç¢ºå®šæ€§é‡åŒ–...")
        print(f"      ğŸ“Š ç½å®³äº‹ä»¶æ•¸: {tc_hazard.size}")
        print(f"      ğŸ˜ï¸ æ›éšªé»æ•¸: {len(exposure_main.gdf)}")
        print(f"      ğŸ² Monte Carloæ¨£æœ¬æ•¸: {n_samples}")
        
        # åŸ·è¡Œå®Œæ•´çš„ç©©å¥è²æ°åˆ†æ
        try:
            # ä½¿ç”¨é†«é™¢æ›éšªæ•¸æ“šä¾†ç”Ÿæˆèˆ‡Cat-in-a-Circleä¸€è‡´çš„æå¤±åˆ†å¸ƒ
            print("      ğŸ¥ ä½¿ç”¨é†«é™¢æ›éšªæ•¸æ“šé€²è¡Œè²æ°å»ºæ¨¡...")
            
            # å¦‚æœæœ‰é†«é™¢æ›éšªæ•¸æ“šï¼Œå„ªå…ˆä½¿ç”¨
            if 'hospital_exposures' in locals() and hospital_exposures is not None:
                exposure_for_bayesian = hospital_exposures
                print(f"      ğŸ¯ ä½¿ç”¨ {len(exposure_for_bayesian.gdf)} å€‹é†«é™¢é»é€²è¡Œå»ºæ¨¡")
            else:
                exposure_for_bayesian = exposure_main
                print(f"      âš ï¸ ä½¿ç”¨å®Œæ•´LitPopæ•¸æ“š ({len(exposure_for_bayesian.gdf)} é»)")
            
            # ç¢ºä¿damagesæ˜¯æ­£ç¢ºçš„numpyæ•¸çµ„æ ¼å¼
            damages_array = np.array(damages, dtype=np.float64)
            print(f"      ğŸ“Š æå¤±æ•¸æ“šæ ¼å¼: {type(damages_array)}, å½¢ç‹€: {damages_array.shape}")
            
            bayesian_results = bayesian_analyzer.comprehensive_bayesian_analysis(
                tc_hazard=tc_hazard,                    # CLIMADAç½å®³å°è±¡
                exposure_main=exposure_for_bayesian,    # ä½¿ç”¨é†«é™¢æ›éšªæ•¸æ“š
                impact_func_set=impact_func_set,        # CLIMADAå½±éŸ¿å‡½æ•¸
                observed_losses=damages_array,          # ä½¿ç”¨æ ¼å¼åŒ–çš„æå¤±æ•¸æ“š
                parametric_products=multi_radius_products[:5]  # å‚³å…¥éƒ¨åˆ†ç”¢å“ç”¨æ–¼åˆ†æ
            )
            
            # æ­£ç¢ºæå–æ¦‚ç‡åˆ†å¸ƒ
            uncertainty_results = bayesian_results.get('uncertainty_quantification', {})
            probabilistic_results = uncertainty_results.get('probabilistic_loss_distributions', {})
            
            if 'event_loss_distributions' in probabilistic_results:
                # è½‰æ›æ ¼å¼ç‚ºèˆ‡æ—¢æœ‰ä»£ç¢¼å…¼å®¹çš„å­—å…¸
                event_distributions = probabilistic_results['event_loss_distributions']
                loss_distributions = {}
                
                for event_id, event_data in event_distributions.items():
                    if isinstance(event_id, str) and event_id.startswith('event_'):
                        event_idx = int(event_id.split('_')[1])
                    else:
                        event_idx = int(event_id) if isinstance(event_id, (int, str)) else 0
                    
                    # æå–æ¨£æœ¬æ•¸æ“š
                    if isinstance(event_data, dict):
                        samples = event_data.get('samples', event_data.get('distribution_samples', []))
                    else:
                        samples = event_data if hasattr(event_data, '__iter__') else []
                    
                    # å®‰å…¨åœ°æª¢æŸ¥ samples æ˜¯å¦ç‚ºç©º
                    if isinstance(samples, np.ndarray):
                        loss_distributions[event_idx] = samples if samples.size > 0 else np.array([0])
                    elif isinstance(samples, (list, tuple)):
                        loss_distributions[event_idx] = np.array(samples) if len(samples) > 0 else np.array([0])
                    else:
                        loss_distributions[event_idx] = np.array([0])
                
                print(f"      âœ… ä½¿ç”¨å®Œæ•´ç©©å¥è²æ°æ¦‚ç‡åˆ†å¸ƒ (é†«é™¢ç´šåˆ¥)")
                print(f"      ğŸ§® æå–äº† {len(loss_distributions)} å€‹äº‹ä»¶çš„åˆ†å¸ƒ")
                
            elif bayesian_results:
                print("      âš ï¸ å®Œæ•´åˆ†æå™¨çµæœæ ¼å¼ä¸ç¬¦ï¼Œä½†åŒ…å«å…¶ä»–æ•¸æ“š")
                print(f"      ğŸ“‹ çµæœåŒ…å«: {list(bayesian_results.keys())}")
                if uncertainty_results:
                    print(f"      ğŸ² ä¸ç¢ºå®šæ€§çµæœ: {list(uncertainty_results.keys())}")
                raise ValueError("Distribution format mismatch - no event_loss_distributions")
            else:
                print("      âš ï¸ å®Œæ•´åˆ†æå™¨æœªè¿”å›çµæœ")
                raise ValueError("No results from comprehensive analyzer")
                
        except Exception as comprehensive_error:
            print(f"      âš ï¸ å®Œæ•´åˆ†æå™¨å¤±æ•—: {comprehensive_error}")
            print(f"      ğŸ” éŒ¯èª¤é¡å‹: {type(comprehensive_error).__name__}")
            import traceback
            tb_lines = traceback.format_exc().split('\n')
            error_location = tb_lines[-3] if len(tb_lines) > 2 else 'Unknown'
            print(f"      ğŸ“ éŒ¯èª¤ä½ç½®: {error_location}")
            print("      ğŸ”„ ä½¿ç”¨åŸºç¤æ¦‚ç‡åˆ†å¸ƒç”Ÿæˆå™¨...")
            
            # å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨åŸºç¤çš„æ¦‚ç‡åˆ†å¸ƒç”Ÿæˆå™¨
            enhanced_distributions = generate_probabilistic_loss_distributions(
                tc_hazard=tc_hazard,                    # CLIMADAç½å®³å°è±¡
                exposure=exposure_for_bayesian,         # ä½¿ç”¨èˆ‡ä¸»åˆ†æç›¸åŒçš„æ›éšªæ•¸æ“š
                impact_func_set=impact_func_set,        # CLIMADAå½±éŸ¿å‡½æ•¸
                n_samples=n_samples                     # Monte Carloæ¨£æœ¬æ•¸
            )
            
            if enhanced_distributions is not None and isinstance(enhanced_distributions, dict) and 'event_loss_distributions' in enhanced_distributions:
                # æ­£ç¢ºæå–æ¦‚ç‡åˆ†å¸ƒ
                event_distributions = enhanced_distributions['event_loss_distributions']
                loss_distributions = {}
                
                for event_id, event_data in event_distributions.items():
                    if isinstance(event_id, str) and event_id.startswith('event_'):
                        event_idx = int(event_id.split('_')[1])
                    else:
                        event_idx = int(event_id) if isinstance(event_id, (int, str)) else 0
                    
                    # æå–æ¨£æœ¬æ•¸æ“š
                    if isinstance(event_data, dict):
                        samples = event_data.get('samples', event_data.get('distribution_samples', []))
                    else:
                        samples = event_data if hasattr(event_data, '__iter__') else []
                    
                    # å®‰å…¨åœ°æª¢æŸ¥ samples æ˜¯å¦ç‚ºç©º
                    if isinstance(samples, np.ndarray):
                        loss_distributions[event_idx] = samples if samples.size > 0 else np.array([0])
                    elif isinstance(samples, (list, tuple)):
                        loss_distributions[event_idx] = np.array(samples) if len(samples) > 0 else np.array([0])
                    else:
                        loss_distributions[event_idx] = np.array([0])
                
                print("      âœ… ä½¿ç”¨åŸºæ–¼CLIMADAçš„å¢å¼·è²æ°åˆ†å¸ƒ")
            else:
                raise ValueError("Enhanced distributions failed")
        
        print(f"      ğŸ§® ç”Ÿæˆäº† {len(loss_distributions)} å€‹äº‹ä»¶çš„æ¦‚ç‡åˆ†å¸ƒ")
        if isinstance(loss_distributions, dict) and len(loss_distributions) > 0:
            sample_dist = list(loss_distributions.values())[0]
            print(f"      ğŸ“ˆ æ¯å€‹åˆ†å¸ƒåŒ…å« {len(sample_dist) if hasattr(sample_dist, '__len__') else 'N/A'} å€‹æ¨£æœ¬")
            
    except Exception as e:
        print(f"      âŒ ç©©å¥è²æ°åˆ†æå¤±æ•—: {e}")
        print("      ğŸ”„ é€€å›æ¨™æº–æ¦‚ç‡åˆ†å¸ƒ...")
        
        # æ¨™æº–åˆ†å¸ƒä½œç‚ºæœ€çµ‚å‚™ç”¨
        uncertainty_ratio = 0.25
        for i, damage in enumerate(damages):
            if damage > 0:
                std = damage * uncertainty_ratio
                mu = np.log(damage) - 0.5 * np.log(1 + (std/damage)**2)
                sigma = np.sqrt(np.log(1 + (std/damage)**2))
                samples = np.random.lognormal(mu, sigma, n_samples)
            else:
                samples = np.random.exponential(1e6, n_samples)
            loss_distributions[i] = samples
        print("      âœ… ä½¿ç”¨æ¨™æº–å°æ•¸æ­£æ…‹åˆ†å¸ƒ")
        
else:
    print("   âš ï¸ ç©©å¥è²æ°æ¨¡çµ„ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨™æº–æ¦‚ç‡åˆ†å¸ƒ")
    uncertainty_ratio = 0.25
    for i, damage in enumerate(damages):
        if damage > 0:
            std = damage * uncertainty_ratio
            mu = np.log(damage) - 0.5 * np.log(1 + (std/damage)**2)
            sigma = np.sqrt(np.log(1 + (std/damage)**2))
            samples = np.random.lognormal(mu, sigma, n_samples)
        else:
            samples = np.random.exponential(1e6, n_samples)
        loss_distributions[i] = samples

# åŸ·è¡ŒCRPSåˆ†æ
print(f"   âš–ï¸ ä½¿ç”¨é†«é™¢æ¨™æº–åŒ–æå¤±ä½œç‚ºå°æ¯”åŸºæº–...")
crps_results = []

for i, product in enumerate(multi_radius_products):
    if (i + 1) % 50 == 0:
        print(f"   é€²åº¦: {i+1}/{len(multi_radius_products)}")
    
    radius = product['radius_km']
    hospital_winds = cat_in_circle_indices[radius]
    
    # è½‰æ›é†«é™¢å­—å…¸ç‚ºé©åˆåˆ†æçš„æ ¼å¼
    # æ¯å€‹äº‹ä»¶å–æ‰€æœ‰é†«é™¢çš„æœ€å¤§é¢¨é€Ÿ
    n_events = len(list(hospital_winds.values())[0]) if hospital_winds else 0
    wind_speeds = np.zeros(n_events)
    
    for event_idx in range(n_events):
        event_winds = [hospital_winds[h_idx][event_idx] for h_idx in hospital_winds.keys()]
        wind_speeds[event_idx] = max(event_winds) if event_winds else 0.0
    
    # è¨ˆç®—è³ ä»˜
    payouts = calculate_correct_step_payouts(
        wind_speeds,
        product['trigger_thresholds'],
        [amount/product['max_payout'] for amount in product['payout_amounts']],
        product['max_payout']
    )
    
    # ç²å–å°æ‡‰åŠå¾‘çš„é†«é™¢æ¨™æº–åŒ–æå¤±ä½œç‚ºåŸºæº–
    if radius in hospital_damages_by_radius:
        hospital_damages = hospital_damages_by_radius[radius]
        reference_losses = hospital_damages  # ä½¿ç”¨é†«é™¢æ¨™æº–åŒ–æå¤±
        comparison_type = "é†«é™¢æ¨™æº–åŒ–æå¤±"
    else:
        reference_losses = damages  # å‚™ç”¨ï¼šä½¿ç”¨æ•´é«”ç¶“æ¿Ÿæå¤±
        comparison_type = "æ•´é«”ç¶“æ¿Ÿæå¤±"
    
    # è¨ˆç®—CRPSåˆ†æ•¸
    crps_scores = []
    for j, payout in enumerate(payouts):
        if j in loss_distributions:
            crps = calculate_crps_score(payout, loss_distributions[j])
        else:
            # å¦‚æœæ²’æœ‰åˆ†å¸ƒæ•¸æ“šï¼Œä½¿ç”¨åƒè€ƒæå¤±çš„é»ä¼°è¨ˆ
            reference_loss = reference_losses[j] if j < len(reference_losses) else 0
            crps = abs(payout - reference_loss)
        
        # ç¢ºä¿ crps æ˜¯æ¨™é‡å€¼
        if isinstance(crps, np.ndarray):
            crps = float(crps.flatten()[0]) if crps.size > 0 else 0.0
        else:
            crps = float(crps)
        
        crps_scores.append(crps)
    
    mean_crps = np.mean(crps_scores)
    
    # ä½¿ç”¨ç›¸åŒçš„åŸºæº–æå¤±è¨ˆç®—ç›¸é—œæ€§
    if len(reference_losses) == len(payouts):
        correlation = np.corrcoef(reference_losses, payouts)[0,1] if np.std(payouts) > 0 else 0
    else:
        correlation = np.corrcoef(damages, payouts)[0,1] if np.std(payouts) > 0 else 0
    
    crps_results.append({
        'product_id': product['product_id'],
        'radius_km': radius,
        'crps': mean_crps,
        'correlation': correlation,
        'trigger_rate': np.mean(payouts > 0),
        'mean_payout': np.mean(payouts),
        'basis_risk_probabilistic': mean_crps,
        'comparison_type': comparison_type,  # è¨˜éŒ„ä½¿ç”¨çš„åŸºæº–é¡å‹
        'payouts': payouts
    })

# åˆ†æçµæœ
crps_df = pd.DataFrame(crps_results)
best_crps_idx = crps_df['crps'].idxmin()
bayesian_best = crps_df.iloc[best_crps_idx].to_dict()

print(f"   âœ… è²æ°CRPSåˆ†æå®Œæˆ")
print(f"      æœ€ä½³CRPS: ${bayesian_best['crps']/1e9:.3f}B")
print(f"      æœ€ä½³ç”¢å“: {bayesian_best['product_id']}")
print(f"      æœ€ä½³ç›¸é—œæ€§: {crps_df['correlation'].max():.3f}")
print(f"      å¹³å‡è§¸ç™¼ç‡: {crps_df['trigger_rate'].mean():.1%}")

bayesian_results = {
    'method': 'bayesian_crps',
    'results_df': crps_df,
    'best_product': bayesian_best,
    'summary': {
        'best_crps': bayesian_best['crps'],
        'best_correlation': crps_df['correlation'].max(),
        'mean_trigger_rate': crps_df['trigger_rate'].mean()
    }
}

# %% æ­¥é©Ÿ6: é›™è»Œæ–¹æ³•æ¯”è¼ƒ
print("\nğŸ” æ­¥é©Ÿ6: é›™è»Œæ–¹æ³•æ¯”è¼ƒ (Steinmann RMSE vs Bayesian CRPS)...")

comparison_results = {
    'steinmann_rmse': {
        'best_product': steinmann_best['product_id'],
        'best_radius': steinmann_best['radius_km'],
        'rmse': steinmann_best['rmse'],
        'correlation': steinmann_best['correlation'],
        'trigger_rate': steinmann_best['trigger_rate']
    },
    'bayesian_crps': {
        'best_product': bayesian_best['product_id'],
        'best_radius': bayesian_best['radius_km'],
        'crps': bayesian_best['crps'],
        'correlation': bayesian_best['correlation'],
        'trigger_rate': bayesian_best['trigger_rate']
    },
    'comparison_metrics': {
        'same_best_product': steinmann_best['product_id'] == bayesian_best['product_id'],
        'same_best_radius': steinmann_best['radius_km'] == bayesian_best['radius_km'],
        'correlation_improvement': bayesian_best['correlation'] - steinmann_best['correlation'],
        'rmse_vs_crps_ratio': steinmann_best['rmse'] / bayesian_best['crps'],
        'trigger_rate_difference': bayesian_best['trigger_rate'] - steinmann_best['trigger_rate']
    }
}

print(f"   âœ… é›™è»Œæ–¹æ³•æ¯”è¼ƒå®Œæˆ")
print(f"      Steinmannæœ€ä½³: {comparison_results['steinmann_rmse']['best_product']}")
print(f"        - åŠå¾‘: {comparison_results['steinmann_rmse']['best_radius']}km")
print(f"        - RMSE: ${comparison_results['steinmann_rmse']['rmse']/1e9:.3f}B")
print(f"      Bayesianæœ€ä½³: {comparison_results['bayesian_crps']['best_product']}")
print(f"        - åŠå¾‘: {comparison_results['bayesian_crps']['best_radius']}km")
print(f"        - CRPS: ${comparison_results['bayesian_crps']['crps']/1e9:.3f}B")
print(f"      ä¸€è‡´æ€§: ç›¸åŒæœ€ä½³ç”¢å“: {comparison_results['comparison_metrics']['same_best_product']}")
print(f"      ç›¸é—œæ€§æå‡: {comparison_results['comparison_metrics']['correlation_improvement']:.3f}")

# %% æ­¥é©Ÿ7: ç¶œåˆåˆ†æçµæœå±•ç¤º
print("\nğŸ“Š æ­¥é©Ÿ7: ç¶œåˆåˆ†æçµæœå±•ç¤º")
print("=" * 80)

# æ•´åˆæ‰€æœ‰çµæœ
all_results = {
    'steinmann_results': steinmann_results,
    'bayesian_results': bayesian_results,
    'comparison_results': comparison_results,
    'data_metadata': {
        'data_source': data_source,
        'n_products_analyzed': len(multi_radius_products),
        'radii_analyzed': radii,
        'n_events': len(damages)
    }
}

# é¡¯ç¤ºåˆ†æè¦æ¨¡æ¦‚è¦½
print(f"\nğŸ” åˆ†æè¦æ¨¡æ¦‚è¦½:")
print(f"   â€¢ ç”¢å“ç¸½æ•¸: {len(multi_radius_products)} å€‹")
print(f"   â€¢ åŠå¾‘ç¯„åœ: {radii} km")
print(f"   â€¢ åˆ†æäº‹ä»¶: {len(damages)} å€‹")
print(f"   â€¢ æ•¸æ“šä¾†æº: {data_source}")
print(f"   â€¢ ç¸½æå¤±: ${np.sum(damages)/1e9:.2f}B")
print(f"   â€¢ å¹³å‡å¹´æå¤±: ${np.mean(damages)/1e9:.4f}B")

# é¡¯ç¤ºç’°å¢ƒè³‡æ–™çµ±è¨ˆ
print(f"\nğŸŒªï¸ é¢¨é€Ÿç’°å¢ƒè³‡æ–™çµ±è¨ˆ:")
for radius in radii:
    if radius in cat_analysis_results:
        radius_data = cat_analysis_results[radius]
        print(f"   åŠå¾‘ {radius}km:")
        print(f"     - å¹³å‡é¢¨é€Ÿ: {radius_data.get('mean_wind_speed', 0):.1f} m/s")
        print(f"     - æœ€å¤§é¢¨é€Ÿ: {radius_data.get('max_wind_speed', 0):.1f} m/s")
        print(f"     - é¢¨é€Ÿæ¨™æº–å·®: {radius_data.get('wind_std', 0):.1f} m/s")
        print(f"     - ç›¸é—œæ€§: {radius_data.get('correlation', 0):.3f}")

# é¡¯ç¤ºCat-in-a-Circleåˆ†æçµæœ
print(f"\nğŸ¯ Cat-in-a-Circle åˆ†æçµæœ:")
best_radius = max(cat_analysis_results.keys(), key=lambda r: cat_analysis_results[r]['correlation'])
print(f"   æœ€ä½³åŠå¾‘: {best_radius}km")
print(f"   æœ€é«˜ç›¸é—œæ€§: {cat_analysis_results[best_radius]['correlation']:.3f}")
print(f"   æœ€ä½³RMSE: ${cat_analysis_results[best_radius].get('rmse', 0)/1e9:.3f}B")
print(f"\nğŸ“ˆ å„åŠå¾‘ç¸¾æ•ˆæ¯”è¼ƒ:")
for radius in sorted(radii):
    if radius in cat_analysis_results:
        data = cat_analysis_results[radius]
        print(f"   {radius}km: ç›¸é—œæ€§={data.get('correlation', 0):.3f}, RMSE=${data.get('rmse', 0)/1e9:.3f}B")

print(f"\n   âœ… çµæœå·²æ•´åˆå®Œæˆ (ç„¡éœ€ä¿å­˜JSONæ–‡ä»¶)")

# %% æ­¥é©Ÿ7: åŸºå·®é¢¨éšªè©³ç´°åˆ†æèˆ‡ç”¢å“è³‡è¨Š
print("\nğŸ“Š æ­¥é©Ÿ7: åŸºå·®é¢¨éšªè©³ç´°åˆ†æèˆ‡ç”¢å“è³‡è¨Š")
print("=" * 60)

# å°å…¥åŸºå·®é¢¨éšªåˆ†ææ¨¡çµ„
try:
    from basis_risk_analysis import analyze_basis_risk_detailed
    basis_risk_module_available = True
except ImportError:
    print("   âš ï¸ åŸºå·®é¢¨éšªåˆ†ææ¨¡çµ„ä¸å¯ç”¨ï¼Œä½¿ç”¨ç°¡åŒ–åˆ†æ")
    basis_risk_module_available = False

# è¨ˆç®—ç”¢å“è©³ç´°è³‡è¨Šçš„å‡½æ•¸
def calculate_product_statistics(payouts, damages, product):
    """è¨ˆç®—ç”¢å“çš„è©³ç´°çµ±è¨ˆè³‡è¨Š"""
    
    # åŸºæœ¬çµ±è¨ˆ
    non_zero_payouts = payouts[payouts > 0]
    max_payout = product.get('max_payout', np.max(payouts))
    max_single_payout = np.max(payouts)
    mean_payout = np.mean(payouts)
    total_payouts = np.sum(payouts)
    
    # ä¿è²»è¨ˆç®— (ç°¡åŒ–ç‰ˆæœ¬)
    expected_payout = mean_payout
    risk_load_factor = 0.20  # 20% é¢¨éšªé™„åŠ 
    expense_load_factor = 0.15  # 15% è²»ç”¨é™„åŠ 
    
    technical_premium = expected_payout * (1 + risk_load_factor)
    commercial_premium = technical_premium * (1 + expense_load_factor)
    
    # æ•ˆèƒ½æŒ‡æ¨™
    payout_efficiency = total_payouts / np.sum(damages) if np.sum(damages) > 0 else 0
    trigger_frequency = np.mean(payouts > 0)
    loss_ratio = total_payouts / (commercial_premium * len(payouts)) if commercial_premium > 0 else 0
    
    # åŸºå·®é¢¨éšª
    basis_risk = damages - payouts
    basis_risk_rmse = np.sqrt(np.mean(basis_risk**2))
    basis_risk_mae = np.mean(np.abs(basis_risk))
    
    return {
        'max_possible_payout': max_payout,
        'max_single_payout': max_single_payout,
        'mean_payout': mean_payout,
        'total_payouts': total_payouts,
        'expected_payout': expected_payout,
        'technical_premium': technical_premium,
        'commercial_premium': commercial_premium,
        'payout_efficiency': payout_efficiency,
        'trigger_frequency': trigger_frequency,
        'loss_ratio': loss_ratio,
        'premium_to_exposure_ratio': commercial_premium / (np.sum(damages) / len(damages)) if np.mean(damages) > 0 else 0,
        'basis_risk_rmse': basis_risk_rmse,
        'basis_risk_mae': basis_risk_mae,
        'correlation': np.corrcoef(damages, payouts)[0,1] if np.std(payouts) > 0 else 0,
        'overpayment_events': np.sum((payouts > damages) & (damages > 0)),
        'underpayment_events': np.sum((payouts < damages) & (damages > 0)),
        'false_triggers': np.sum((payouts > 0) & (damages == 0)),
        'missed_events': np.sum((payouts == 0) & (damages > 0))
    }

# åˆ†ææœ€ä½³ç”¢å“çš„åŸºå·®é¢¨éšª
print("\nğŸ¯ æœ€ä½³ç”¢å“åŸºå·®é¢¨éšªåˆ†æ:")
print("-" * 40)

# Steinmannæœ€ä½³ç”¢å“
steinmann_payouts = rmse_df.iloc[best_rmse_idx]['payouts']
print(f"\n1. Steinmann RMSE æœ€ä½³ç”¢å“ ({steinmann_best['product_id']}):")

if basis_risk_module_available:
    steinmann_basis_risk = analyze_basis_risk_detailed(
        damages, 
        steinmann_payouts,
        f"Steinmann Best - {steinmann_best['product_id']}"
    )
else:
    # ç°¡åŒ–çš„åŸºå·®é¢¨éšªåˆ†æ
    basis_risk = damages - steinmann_payouts
    print(f"   â€¢ ç¸½åŸºå·®é¢¨éšª: ${np.sum(np.abs(basis_risk))/1e9:.2f}B")
    print(f"   â€¢ å¹³å‡åŸºå·®é¢¨éšª: ${np.mean(np.abs(basis_risk))/1e9:.3f}B")
    print(f"   â€¢ åŸºå·®é¢¨éšªæ¨™æº–å·®: ${np.std(basis_risk)/1e9:.3f}B")
    print(f"   â€¢ æœ€å¤§æ­£åŸºå·®é¢¨éšª: ${np.max(basis_risk)/1e9:.3f}B (æå¤±æœªå……åˆ†è£œå„Ÿ)")
    print(f"   â€¢ æœ€å¤§è² åŸºå·®é¢¨éšª: ${np.min(basis_risk)/1e9:.3f}B (éåº¦è£œå„Ÿ)")

# Bayesianæœ€ä½³ç”¢å“
if 'crps_df' in locals():
    best_crps_idx = crps_df['crps'].idxmin()
    bayesian_payouts = crps_df.iloc[best_crps_idx]['payouts']
    
    print(f"\n2. Bayesian CRPS æœ€ä½³ç”¢å“ ({bayesian_best['product_id']}):")
    
    if basis_risk_module_available:
        bayesian_basis_risk = analyze_basis_risk_detailed(
            damages,
            bayesian_payouts,
            f"Bayesian Best - {bayesian_best['product_id']}"
        )
    else:
        basis_risk = damages - bayesian_payouts
        print(f"   â€¢ ç¸½åŸºå·®é¢¨éšª: ${np.sum(np.abs(basis_risk))/1e9:.2f}B")
        print(f"   â€¢ å¹³å‡åŸºå·®é¢¨éšª: ${np.mean(np.abs(basis_risk))/1e9:.3f}B")
        print(f"   â€¢ åŸºå·®é¢¨éšªæ¨™æº–å·®: ${np.std(basis_risk)/1e9:.3f}B")
        print(f"   â€¢ æœ€å¤§æ­£åŸºå·®é¢¨éšª: ${np.max(basis_risk)/1e9:.3f}B")
        print(f"   â€¢ æœ€å¤§è² åŸºå·®é¢¨éšª: ${np.min(basis_risk)/1e9:.3f}B")

# åˆ†æå‰10å€‹æœ€ä½³ç”¢å“
print("\nğŸ† å‰10å€‹æœ€ä½³ç”¢å“è©³ç´°è³‡è¨Š:")
print("-" * 40)

# Steinmannå‰10
top10_steinmann = rmse_df.nsmallest(10, 'rmse')
print("\nğŸ“Š Steinmann RMSE å‰10ç”¢å“:")
for idx, (i, row) in enumerate(top10_steinmann.iterrows(), 1):
    product = multi_radius_products[i]
    payouts = row['payouts']
    stats = calculate_product_statistics(payouts, damages, product)
    
    print(f"\n{idx}. {row['product_id']} (åŠå¾‘: {row['radius_km']}km)")
    print(f"   æ•ˆèƒ½æŒ‡æ¨™:")
    print(f"   â€¢ RMSE: ${row['rmse']/1e9:.3f}B")
    print(f"   â€¢ ç›¸é—œæ€§: {stats['correlation']:.3f}")
    print(f"   â€¢ è§¸ç™¼é »ç‡: {stats['trigger_frequency']:.1%}")
    print(f"   ä¿éšªè¨­è¨ˆ:")
    print(f"   â€¢ æœ€å¤§è³ ä»˜: ${stats['max_possible_payout']/1e9:.2f}B")
    print(f"   â€¢ å¹³å‡è³ ä»˜: ${stats['mean_payout']/1e9:.3f}B")
    print(f"   â€¢ æŠ€è¡“ä¿è²»: ${stats['technical_premium']/1e9:.3f}B")
    print(f"   â€¢ å•†æ¥­ä¿è²»: ${stats['commercial_premium']/1e9:.3f}B")
    print(f"   â€¢ è³ ä»˜æ•ˆç‡: {stats['payout_efficiency']:.1%}")
    print(f"   åŸºå·®é¢¨éšª:")
    print(f"   â€¢ åŸºå·®é¢¨éšªRMSE: ${stats['basis_risk_rmse']/1e9:.3f}B")
    print(f"   â€¢ éŒ¯èª¤è§¸ç™¼: {stats['false_triggers']} æ¬¡")
    print(f"   â€¢ éºæ¼äº‹ä»¶: {stats['missed_events']} æ¬¡")

# Bayesianå‰10 (å¦‚æœæœ‰)
if 'crps_df' in locals():
    top10_bayesian = crps_df.nsmallest(10, 'crps')
    print("\n\nğŸ§  Bayesian CRPS å‰10ç”¢å“:")
    for idx, (i, row) in enumerate(top10_bayesian.iterrows(), 1):
        product = multi_radius_products[i]
        payouts = row['payouts']
        stats = calculate_product_statistics(payouts, damages, product)
        
        print(f"\n{idx}. {row['product_id']} (åŠå¾‘: {row['radius_km']}km)")
        print(f"   æ•ˆèƒ½æŒ‡æ¨™:")
        print(f"   â€¢ CRPS: ${row['crps']/1e9:.3f}B")
        print(f"   â€¢ ç›¸é—œæ€§: {stats['correlation']:.3f}")
        print(f"   â€¢ è§¸ç™¼é »ç‡: {stats['trigger_frequency']:.1%}")
        print(f"   ä¿éšªè¨­è¨ˆ:")
        print(f"   â€¢ æœ€å¤§è³ ä»˜: ${stats['max_possible_payout']/1e9:.2f}B")
        print(f"   â€¢ å¹³å‡è³ ä»˜: ${stats['mean_payout']/1e9:.3f}B")
        print(f"   â€¢ æŠ€è¡“ä¿è²»: ${stats['technical_premium']/1e9:.3f}B")
        print(f"   â€¢ å•†æ¥­ä¿è²»: ${stats['commercial_premium']/1e9:.3f}B")
        print(f"   â€¢ è³ ä»˜æ•ˆç‡: {stats['payout_efficiency']:.1%}")
        print(f"   åŸºå·®é¢¨éšª:")
        print(f"   â€¢ åŸºå·®é¢¨éšªRMSE: ${stats['basis_risk_rmse']/1e9:.3f}B")
        print(f"   â€¢ éŒ¯èª¤è§¸ç™¼: {stats['false_triggers']} æ¬¡")
        print(f"   â€¢ éºæ¼äº‹ä»¶: {stats['missed_events']} æ¬¡")

# å„²å­˜è©³ç´°åˆ†æçµæœ
detailed_results = {
    'top10_steinmann': [],
    'top10_bayesian': [],
    'basis_risk_analysis': {}
}

for i, row in top10_steinmann.iterrows():
    product = multi_radius_products[i]
    payouts = row['payouts']
    stats = calculate_product_statistics(payouts, damages, product)
    detailed_results['top10_steinmann'].append({
        'product_id': row['product_id'],
        'radius_km': row['radius_km'],
        'rmse': row['rmse'],
        **stats
    })

if 'crps_df' in locals():
    for i, row in top10_bayesian.iterrows():
        product = multi_radius_products[i]
        payouts = row['payouts']
        stats = calculate_product_statistics(payouts, damages, product)
        detailed_results['top10_bayesian'].append({
            'product_id': row['product_id'],
            'radius_km': row['radius_km'],
            'crps': row['crps'],
            **stats
        })

# é¡¯ç¤ºè©³ç´°çš„å‰10åç”¢å“åˆ†æçµæœ
print(f"\nğŸ† å‰10åç”¢å“è©³ç´°åˆ†æ:")
print("=" * 80)

# é¡¯ç¤ºSteinmann RMSEå‰10å
print(f"\nğŸ“Š Steinmann RMSE å‰10åç”¢å“:")
for i, product_detail in enumerate(detailed_results['top10_steinmann'][:10]):
    print(f"\n{i+1}. ç”¢å“ {product_detail['product_id']}")
    print(f"   ğŸ¯ åŸºæœ¬è³‡è¨Š:")
    print(f"     - åŠå¾‘: {product_detail['radius_km']}km")
    print(f"     - RMSE: ${product_detail['rmse']/1e9:.4f}B")
    print(f"     - ç›¸é—œæ€§: {product_detail.get('correlation', 0):.4f}")
    
    print(f"   ğŸ’° è²¡å‹™è³‡è¨Š:")
    print(f"     - æŠ€è¡“ä¿è²»: ${product_detail.get('technical_premium', 0)/1e9:.4f}B")
    print(f"     - å•†æ¥­ä¿è²»: ${product_detail.get('commercial_premium', 0)/1e9:.4f}B")
    print(f"     - æœ€å¤§è³ ä»˜: ${product_detail.get('max_single_payout', 0)/1e9:.4f}B")
    print(f"     - è³ ä»˜æ•ˆç‡: {product_detail.get('payout_efficiency', 0):.2%}")
    
    print(f"   ğŸ“Š çµ±è¨ˆæŒ‡æ¨™:")
    print(f"     - åŸºå·®é¢¨éšª: ${product_detail.get('basis_risk', 0)/1e9:.4f}B")
    print(f"     - è§¸ç™¼æ©Ÿç‡: {product_detail.get('trigger_probability', 0):.2%}")
    print(f"     - è¦†è“‹ç‡: {product_detail.get('coverage_ratio', 0):.2%}")
    
    print(f"   ğŸ”§ ç”¢å“è¨­è¨ˆ:")
    print(f"     - è§¸ç™¼é–¾å€¼: {product_detail.get('trigger_thresholds', [])} m/s")
    print(f"     - è³ ä»˜æ¯”ä¾‹: {product_detail.get('payout_ratios', [])}")
    
    if i < 9:  # ä¸æ˜¯æœ€å¾Œä¸€å€‹æ™‚æ‰å°åˆ†éš”ç·š
        print("   " + "-" * 60)

# å¦‚æœæœ‰Bayesiançµæœï¼Œä¹Ÿé¡¯ç¤º
if detailed_results['top10_bayesian']:
    print(f"\nğŸ§  Bayesian CRPS å‰10åç”¢å“:")
    for i, product_detail in enumerate(detailed_results['top10_bayesian'][:10]):
        print(f"\n{i+1}. ç”¢å“ {product_detail['product_id']}")
        print(f"   ğŸ¯ åŸºæœ¬è³‡è¨Š:")
        print(f"     - åŠå¾‘: {product_detail['radius_km']}km")
        print(f"     - CRPS: ${product_detail['crps']/1e9:.4f}B")
        print(f"     - ç›¸é—œæ€§: {product_detail.get('correlation', 0):.4f}")
        
        print(f"   ğŸ’° è²¡å‹™è³‡è¨Š:")
        print(f"     - æŠ€è¡“ä¿è²»: ${product_detail.get('technical_premium', 0)/1e9:.4f}B")
        print(f"     - å•†æ¥­ä¿è²»: ${product_detail.get('commercial_premium', 0)/1e9:.4f}B")
        print(f"     - æœ€å¤§è³ ä»˜: ${product_detail.get('max_single_payout', 0)/1e9:.4f}B")
        print(f"     - è³ ä»˜æ•ˆç‡: {product_detail.get('payout_efficiency', 0):.2%}")
        
        print(f"   ğŸ“Š çµ±è¨ˆæŒ‡æ¨™:")
        print(f"     - åŸºå·®é¢¨éšª: ${product_detail.get('basis_risk', 0)/1e9:.4f}B")
        print(f"     - è§¸ç™¼æ©Ÿç‡: {product_detail.get('trigger_probability', 0):.2%}")
        print(f"     - è¦†è“‹ç‡: {product_detail.get('coverage_ratio', 0):.2%}")
        
        print(f"   ğŸ”§ ç”¢å“è¨­è¨ˆ:")
        print(f"     - è§¸ç™¼é–¾å€¼: {product_detail.get('trigger_thresholds', [])} m/s")
        print(f"     - è³ ä»˜æ¯”ä¾‹: {product_detail.get('payout_ratios', [])}")
        
        if i < 9:  # ä¸æ˜¯æœ€å¾Œä¸€å€‹æ™‚æ‰å°åˆ†éš”ç·š
            print("   " + "-" * 60)
else:
    print(f"\nâš ï¸ Bayesian CRPS åˆ†ææœªèƒ½å®Œæˆï¼Œåƒ…é¡¯ç¤º Steinmann RMSE çµæœ")

print(f"\nâœ… å·²é¡¯ç¤ºå‰10åç”¢å“çš„å®Œæ•´è³‡è¨Š (ç„¡éœ€ä¿å­˜æª”æ¡ˆ)")

# %% æœ€çµ‚åˆ†ææ‘˜è¦
print("\n" + "=" * 80)
print("ğŸ‰ åŒ—å¡ç¾…ä¾†ç´å·ç†±å¸¶æ°£æ—‹åƒæ•¸å‹ä¿éšªåˆ†æå®Œæˆ!")
print("=" * 80)

print(f"ğŸ“Š åˆ†æè¦æ¨¡:")
print(f"   â€¢ ç”¢å“æ•¸é‡: {len(multi_radius_products)} å€‹ (5åŠå¾‘ Ã— 70å‡½æ•¸)")
print(f"   â€¢ åˆ†æäº‹ä»¶: {len(damages)} å€‹")
print(f"   â€¢ æ•¸æ“šä¾†æº: {data_source}")
print(f"   â€¢ åˆ†ææ–¹æ³•: Steinmann RMSE + Bayesian CRPS")

print(f"\nğŸ† æœ€ä½³ç”¢å“:")
if 'steinmann_best' in locals():
    print(f"   â€¢ Steinmann RMSE: {steinmann_best['product_id']}")
    print(f"     - RMSE: ${steinmann_best['rmse']/1e9:.3f}B")
    print(f"     - ç›¸é—œæ€§: {steinmann_best['correlation']:.3f}")
    print(f"     - åŠå¾‘: {steinmann_best['radius_km']}km")
else:
    print(f"   â€¢ Steinmann RMSE: åˆ†ææœªå®Œæˆ")

if 'bayesian_best' in locals():
    print(f"   â€¢ Bayesian CRPS: {bayesian_best['product_id']}")
    print(f"     - CRPS: ${bayesian_best['crps']/1e9:.3f}B") 
    print(f"     - ç›¸é—œæ€§: {bayesian_best['correlation']:.3f}")
    print(f"     - åŠå¾‘: {bayesian_best['radius_km']}km")
else:
    print(f"   â€¢ Bayesian CRPS: åˆ†ææœªå®Œæˆæˆ–ä½¿ç”¨ç°¡åŒ–æ–¹æ³•")

print(f"\nğŸ¯ Cat-in-a-Circle é—œéµç™¼ç¾:")
best_radius = max(cat_analysis_results.keys(), key=lambda r: cat_analysis_results[r]['correlation'])
print(f"   â€¢ æœ€ä½³ç›¸é—œæ€§åŠå¾‘: {best_radius}km (ç›¸é—œæ€§ {cat_analysis_results[best_radius]['correlation']:.3f})")
print(f"   â€¢ æœ€é«˜è§¸ç™¼é »ç‡åŠå¾‘: {max(cat_analysis_results.keys(), key=lambda r: cat_analysis_results[r]['trigger_counts'][0])}km")
print(f"   â€¢ åŸºå·®é¢¨éšªæœ€ä½åŠå¾‘: {min(cat_analysis_results.keys(), key=lambda r: cat_analysis_results[r]['basis_risk_events'])}km")

print(f"\nğŸ“ˆ æ–¹æ³•å°æ¯”ç™¼ç¾:")
if 'comparison_results' in locals() and 'comparison_metrics' in comparison_results:
    print(f"   â€¢ å…©æ–¹æ³•æœ€ä½³ç”¢å“{'ç›¸åŒ' if comparison_results['comparison_metrics']['same_best_product'] else 'ä¸åŒ'}")
    print(f"   â€¢ ç›¸é—œæ€§æå‡: {comparison_results['comparison_metrics']['correlation_improvement']:.3f}")
    print(f"   â€¢ è§¸ç™¼ç‡å·®ç•°: {comparison_results['comparison_metrics']['trigger_rate_difference']:.1%}")
else:
    print(f"   â€¢ æ–¹æ³•å°æ¯”åˆ†æ: éœ€è¦å®Œæ•´çš„Bayesian CRPSçµæœæ‰èƒ½é€²è¡Œå°æ¯”")
    print(f"   â€¢ å»ºè­°: åœ¨æœ‰å®Œæ•´è²æ°åˆ†æçµæœæ™‚é‡æ–°é‹è¡Œå°æ¯”")

print(f"\nğŸ” æ–¹æ³•è«–å·®ç•°åˆ†æ:")
print(f"   â€¢ RMSE (é»ä¼°è¨ˆ): è¡¡é‡è³ ä»˜èˆ‡æ•´é«”ç¶“æ¿Ÿæå¤±çš„å·®ç•°")
print(f"   â€¢ CRPS (åˆ†å¸ƒé æ¸¬): è¡¡é‡è³ ä»˜èˆ‡é†«é™¢æ¨™æº–åŒ–æå¤±åˆ†å¸ƒçš„æº–ç¢ºæ€§")
print(f"   â€¢ CRPSè€ƒæ…®äº†é†«é™¢ç´šåˆ¥çš„ä¸ç¢ºå®šæ€§å’Œç©ºé–“ç›¸é—œæ€§")
print(f"   â€¢ å…©æ–¹æ³•é¸æ“‡ç›¸åŒç”¢å“èªªæ˜çµæœåœ¨ä¸åŒåŸºæº–ä¸‹éƒ½ç©©å¥")

# æª¢æŸ¥ä½¿ç”¨çš„æ¯”è¼ƒåŸºæº–
if 'crps_df' in locals() and len(crps_df) > 0:
    comparison_types = crps_df['comparison_type'].value_counts()
    print(f"\nğŸ“Š CRPSåˆ†æåŸºæº–:")
    for comp_type, count in comparison_types.items():
        print(f"   â€¢ {comp_type}: {count} å€‹ç”¢å“")
        
print(f"\nğŸ¥ Cat-in-a-Circle ä¸€è‡´æ€§:")
print(f"   â€¢ è³ ä»˜è§¸ç™¼: åŸºæ–¼é†«é™¢å‘¨åœåœ“åœˆå…§çš„æœ€å¤§é¢¨é€Ÿ")
print(f"   â€¢ æå¤±åŸºæº–: ä½¿ç”¨é†«é™¢æ¨™æº–åŒ–æå¤± (1å–®ä½Ã—è„†å¼±åº¦å‡½æ•¸)")  
print(f"   â€¢ è²æ°å»ºæ¨¡: å°é†«é™¢æ›éšªé€²è¡Œä¸ç¢ºå®šæ€§é‡åŒ–")
print(f"   â€¢ ç©ºé–“ä¸€è‡´æ€§: è³ ä»˜è§¸ç™¼èˆ‡æå¤±è©•ä¼°ä½¿ç”¨ç›¸åŒçš„é†«é™¢ä½ç½®")

print(f"\nâš¡ è¨ˆç®—æ•ˆç‡èˆ‡æ–¹æ³•æ¯”è¼ƒ:")
try:
    if 'rmse_df' in locals() and 'best_rmse_idx' in locals():
        steinmann_payouts = rmse_df.iloc[best_rmse_idx]['payouts']
        rmse_calc = np.sqrt(np.mean((damages - steinmann_payouts) ** 2))
        print(f"   â€¢ RMSEé‡æ–°é©—è­‰: ${rmse_calc/1e9:.3f}B")
    
    if 'crps_df' in locals() and 'best_crps_idx' in locals() and len(crps_df) > 0:
        bayesian_payouts = crps_df.iloc[best_crps_idx]['payouts']
        if 'steinmann_payouts' in locals():
            mae_diff = np.mean(np.abs(steinmann_payouts - bayesian_payouts))
            print(f"   â€¢ å…©ç¨®æœ€ä½³ç”¢å“çš„è³ ä»˜å·®ç•° (MAE): ${mae_diff/1e9:.3f}B")
    
    if 'n_samples' in locals():
        print(f"   â€¢ è²æ°æ–¹æ³•ä½¿ç”¨ {n_samples} æ¨£æœ¬é€²è¡Œæ©Ÿç‡åˆ†å¸ƒå»ºæ¨¡")
        print(f"   â€¢ CRPSè¨ˆç®—è€ƒæ…®äº†æå¤±çš„ä¸ç¢ºå®šæ€§åˆ†å¸ƒ")
    else:
        print(f"   â€¢ ä½¿ç”¨ç°¡åŒ–æ¦‚ç‡åˆ†å¸ƒé€²è¡ŒCRPSè¨ˆç®—")
        
except Exception as e:
    print(f"   â€¢ âš ï¸ æ•ˆç‡æ¯”è¼ƒè¨ˆç®—å¤±æ•—: {e}")
    print(f"   â€¢ åˆ†æä»ç„¶æˆåŠŸå®Œæˆï¼Œè«‹åƒè€ƒä¸Šè¿°çµæœ")

# æª¢æŸ¥æ˜¯å¦æˆåŠŸä½¿ç”¨äº†ç©©å¥è²æ°åˆ†æ
if modules_available['bayesian'] and isinstance(loss_distributions, dict) and len(loss_distributions) > 0:
    sample_size = len(list(loss_distributions.values())[0]) if isinstance(loss_distributions, dict) else 0
    if sample_size >= 500:
        print(f"   â€¢ âœ… æˆåŠŸä½¿ç”¨ç©©å¥è²æ°åˆ†æå™¨ (æ¯å€‹åˆ†å¸ƒ{sample_size}æ¨£æœ¬)")
        print(f"   â€¢ ğŸ§® åŒ…å« Monte Carlo æ¨¡æ“¬ã€å¯†åº¦æ¯”æ–¹æ³•å’Œéšå±¤æ¨¡å‹")
    else:
        print(f"   â€¢ âš ï¸ ä½¿ç”¨ç°¡åŒ–è²æ°åˆ†å¸ƒ ({sample_size}æ¨£æœ¬)")
else:
    print(f"   â€¢ â„¹ï¸ ä½¿ç”¨æ¨™æº–å°æ•¸æ­£æ…‹åˆ†å¸ƒä½œç‚ºå‚™ç”¨æ–¹æ¡ˆ")

print(f"\nğŸ“Š åˆ†æè¼¸å‡º:")
print(f"   â€¢ åœ–è¡¨å·²åœ¨å„å€‹cellä¸­é¡¯ç¤º")
print(f"   â€¢ é¢¨é€Ÿåˆ†å¸ƒåœ–: cat_in_circle_wind_distributions_*.png")
print(f"   â€¢ æ‰€æœ‰çµæœå·²åœ¨notebookä¸­å®Œæ•´å‘ˆç¾")

print(f"\nğŸ’¡ ä½¿ç”¨å»ºè­°:")
print(f"   â€¢ æ¨è–¦ä½¿ç”¨ {best_radius}km åŠå¾‘ä½œç‚ºä¸»è¦è¨­è¨ˆåƒæ•¸")
print(f"   â€¢ çµåˆå…©ç¨®æ–¹æ³•çš„å„ªå‹¢é€²è¡Œç”¢å“çµ„åˆ")
print(f"   â€¢ é—œæ³¨åŸºå·®é¢¨éšªè¼ƒé«˜çš„åŠå¾‘è¨­å®š")
print(f"   â€¢ è€ƒæ…®ä¸åŒåŠå¾‘åœ¨è§¸ç™¼é »ç‡ä¸Šçš„æ¬Šè¡¡")

print(f"\nâœ… åŠŸèƒ½å¼åˆ†æå®Œæˆï¼æ¯å€‹ cell éƒ½åœ¨è‡ªå·±çš„ cell ä¸­ç”¢ç”Ÿäº†çµæœ")

# %%
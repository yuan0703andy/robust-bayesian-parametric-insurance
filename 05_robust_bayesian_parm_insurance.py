#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
05_robust_bayesian_parm_insurance.py
=====================================
Complete Robust Hierarchical Bayesian Parametric Insurance Analysis
å®Œæ•´å¼·å¥éšå±¤è²æ°åƒæ•¸ä¿éšªåˆ†æ

Integrates the full bayesian/ module framework for comprehensive analysis:
æ•´åˆå®Œæ•´bayesian/æ¨¡çµ„æ¡†æ¶é€²è¡Œç¶œåˆåˆ†æï¼š

â€¢ Hierarchical Bayesian Model (4-level + MPE) éšå±¤è²æ°æ¨¡å‹(å››å±¤+æ··åˆé æ¸¬ä¼°è¨ˆ)
â€¢ Robust Bayesian Framework (Density Ratio) å¼·å¥è²æ°æ¡†æ¶(å¯†åº¦æ¯”)
â€¢ Uncertainty Quantification ä¸ç¢ºå®šæ€§é‡åŒ–
â€¢ Weight Sensitivity Analysis æ¬Šé‡æ•æ„Ÿåº¦åˆ†æ
â€¢ Integration with skill_scores and insurance modules æ•´åˆæŠ€èƒ½åˆ†æ•¸å’Œä¿éšªæ¨¡çµ„
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import pickle
import pandas as pd
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Import complete bayesian framework åŒ¯å…¥å®Œæ•´è²æ°æ¡†æ¶
from bayesian import (
    RobustBayesianAnalyzer,                    # Main analyzer ä¸»åˆ†æå™¨
    RobustBayesianFramework,                   # Density ratio framework å¯†åº¦æ¯”æ¡†æ¶
    HierarchicalBayesianModel,                 # 4-level hierarchical model å››å±¤éšå±¤æ¨¡å‹
    HierarchicalModelConfig,                   # Hierarchical configuration éšå±¤é…ç½®
    ProbabilisticLossDistributionGenerator,    # Uncertainty quantification ä¸ç¢ºå®šæ€§é‡åŒ–
    WeightSensitivityAnalyzer,                 # Weight sensitivity analysis æ¬Šé‡æ•æ„Ÿåº¦åˆ†æ
    MixedPredictiveEstimation,                 # MPE implementation MPEå¯¦ç¾
    get_default_config,                        # Default configuration é è¨­é…ç½®
    validate_installation                       # Installation validation å®‰è£é©—è­‰
)

# Import external integrations åŒ¯å…¥å¤–éƒ¨æ•´åˆ
from skill_scores.basis_risk_functions import (
    BasisRiskCalculator, BasisRiskConfig, BasisRiskType
)


def main():
    """
    Complete Robust Hierarchical Bayesian Analysis
    å®Œæ•´å¼·å¥éšå±¤è²æ°åˆ†æä¸»ç¨‹å¼
    
    Implements comprehensive Bayesian framework with:
    å¯¦ç¾åŒ…å«ä»¥ä¸‹å®Œæ•´è²æ°æ¡†æ¶ï¼š
    â€¢ 4-level hierarchical Bayesian model å››å±¤éšå±¤è²æ°æ¨¡å‹
    â€¢ Mixed Predictive Estimation (MPE) æ··åˆé æ¸¬ä¼°è¨ˆ
    â€¢ Density ratio robustness constraints å¯†åº¦æ¯”å¼·å¥æ€§ç´„æŸ
    â€¢ Complete uncertainty quantification å®Œæ•´ä¸ç¢ºå®šæ€§é‡åŒ–
    â€¢ Weight sensitivity analysis æ¬Šé‡æ•æ„Ÿåº¦åˆ†æ
    """
    print("=" * 100)
    print("ğŸ§  Complete Robust Hierarchical Bayesian Parametric Insurance Analysis")
    print("   å®Œæ•´å¼·å¥éšå±¤è²æ°åƒæ•¸ä¿éšªåˆ†æ")
    print("=" * 100)
    print("ğŸ“‹ Analysis Components åˆ†æçµ„ä»¶:")
    print("   â€¢ RobustBayesianAnalyzer (Main Interface) å¼·å¥è²æ°åˆ†æå™¨(ä¸»ä»‹é¢)")
    print("   â€¢ HierarchicalBayesianModel (4-level + MPE) éšå±¤è²æ°æ¨¡å‹(å››å±¤+MPE)")
    print("   â€¢ ProbabilisticLossDistributionGenerator (Uncertainty) æ©Ÿç‡æå¤±åˆ†å¸ƒç”Ÿæˆå™¨(ä¸ç¢ºå®šæ€§)")
    print("   â€¢ WeightSensitivityAnalyzer (Sensitivity) æ¬Šé‡æ•æ„Ÿåº¦åˆ†æå™¨")
    print("   â€¢ Integration with skill_scores & insurance modules æ•´åˆæŠ€èƒ½åˆ†æ•¸å’Œä¿éšªæ¨¡çµ„")
    print("=" * 100)
    
    # Validate installation é©—è­‰å®‰è£
    print("\nğŸ” Validating installation é©—è­‰å®‰è£...")
    validation = validate_installation()
    print(f"   â€¢ Core bayesian modules: {'âœ…' if validation['core_modules'] else 'âŒ'}")
    print(f"   â€¢ skill_scores integration: {'âœ…' if validation['skill_scores'] else 'âš ï¸'}")
    print(f"   â€¢ insurance_analysis_refactored: {'âœ…' if validation['insurance_module'] else 'âš ï¸'}")
    print(f"   â€¢ CLIMADA integration: {'âœ…' if validation['climada'] else 'âš ï¸'}")
    
    if validation['dependencies']:
        print("   Dependencies missing:")
        for dep in validation['dependencies']:
            print(f"     - {dep}")
    print()
    
    # Load required data
    print("\nğŸ“‚ Loading data...")
    
    # Load products
    try:
        with open("results/insurance_products/products.pkl", 'rb') as f:
            products = pickle.load(f)
        print(f"âœ… Loaded {len(products)} insurance products")
    except FileNotFoundError:
        print("âŒ Products not found. Run 03_insurance_product.py first.")
        return
    
    # Load spatial analysis results  
    try:
        with open("results/spatial_analysis/cat_in_circle_results.pkl", 'rb') as f:
            spatial_results = pickle.load(f)
        wind_indices_dict = spatial_results['indices']
        # Extract main wind index for analysis (using 30km max as primary)
        wind_indices = wind_indices_dict.get('cat_in_circle_30km_max', np.array([]))
        print("âœ… Loaded spatial analysis results")
        print(f"   Using primary index: cat_in_circle_30km_max ({len(wind_indices)} events)")
    except FileNotFoundError:
        print("âŒ Spatial results not found. Run 02_spatial_analysis.py first.")
        return
    
    # Load CLIMADA data
    try:
        with open("climada_complete_data.pkl", 'rb') as f:
            climada_data = pickle.load(f)
        print("âœ… Loaded CLIMADA data")
    except FileNotFoundError:
        print("âš ï¸ Using synthetic loss data")
        np.random.seed(42)
        # Match the length of wind indices
        n_events = len(wind_indices) if len(wind_indices) > 0 else 1000
        climada_data = {
            'impact': type('MockImpact', (), {
                'at_event': np.random.lognormal(15, 1, n_events) * 1e6
            })()
        }
    
    # Ensure data arrays have matching lengths
    observed_losses = climada_data.get('impact').at_event if 'impact' in climada_data else np.array([])
    
    # Truncate to minimum length to ensure compatibility
    min_length = min(len(wind_indices), len(observed_losses))
    if min_length > 0:
        wind_indices = wind_indices[:min_length]
        observed_losses = observed_losses[:min_length]
        print(f"   Aligned data to {min_length} events")
    else:
        print("âŒ No valid data found")
        return
    
    # =============================================================================
    # Phase 1: Initialize Complete Bayesian Framework
    # ç¬¬ä¸€éšæ®µï¼šåˆå§‹åŒ–å®Œæ•´è²æ°æ¡†æ¶
    # =============================================================================
    
    print("\nğŸš€ Phase 1: Initializing Complete Bayesian Framework")
    print("   ç¬¬ä¸€éšæ®µï¼šåˆå§‹åŒ–å®Œæ•´è²æ°æ¡†æ¶")
    
    # Get default configuration ç²å–é è¨­é…ç½®
    config = get_default_config()
    print(f"   Using configuration ä½¿ç”¨é…ç½®: {config}")
    
    # Initialize main analyzer åˆå§‹åŒ–ä¸»åˆ†æå™¨
    print("\nğŸ“Š Initializing RobustBayesianAnalyzer åˆå§‹åŒ–å¼·å¥è²æ°åˆ†æå™¨...")
    main_analyzer = RobustBayesianAnalyzer(
        density_ratio_constraint=config['density_ratio_constraint'],  # 2.0
        n_monte_carlo_samples=config['n_monte_carlo_samples'],        # 500
        n_mixture_components=config['n_mixture_components'],           # 3
        mcmc_samples=config['mcmc_samples'],                          # 2000
        mcmc_warmup=config['mcmc_warmup'],                           # 1000
        mcmc_chains=config['mcmc_chains']                            # 4
    )
    print("   âœ… RobustBayesianAnalyzer initialized with full configuration")
    
    # Initialize hierarchical Bayesian model åˆå§‹åŒ–éšå±¤è²æ°æ¨¡å‹
    print("\nğŸ—ï¸ Initializing HierarchicalBayesianModel åˆå§‹åŒ–éšå±¤è²æ°æ¨¡å‹...")
    hierarchical_config = HierarchicalModelConfig(
        n_mixture_components=config['n_mixture_components'],
        mcmc_samples=config['mcmc_samples'],
        mcmc_warmup=config['mcmc_warmup'],
        mcmc_chains=config['mcmc_chains']
    )
    hierarchical_model = HierarchicalBayesianModel(hierarchical_config)
    print("   âœ… 4-level Hierarchical Bayesian Model with MPE initialized")
    
    # Initialize uncertainty quantification åˆå§‹åŒ–ä¸ç¢ºå®šæ€§é‡åŒ–
    print("\nğŸ² Initializing Uncertainty Quantification åˆå§‹åŒ–ä¸ç¢ºå®šæ€§é‡åŒ–...")
    uncertainty_generator = ProbabilisticLossDistributionGenerator(
        n_monte_carlo_samples=config['n_monte_carlo_samples'],
        hazard_uncertainty_std=config['hazard_uncertainty_std'],
        exposure_uncertainty_log_std=config['exposure_uncertainty_log_std'],
        vulnerability_uncertainty_std=config['vulnerability_uncertainty_std']
    )
    print("   âœ… Probabilistic Loss Distribution Generator initialized")
    
    # Initialize weight sensitivity analyzer åˆå§‹åŒ–æ¬Šé‡æ•æ„Ÿåº¦åˆ†æå™¨
    print("\nâš–ï¸ Initializing Weight Sensitivity Analyzer åˆå§‹åŒ–æ¬Šé‡æ•æ„Ÿåº¦åˆ†æå™¨...")
    weight_analyzer = WeightSensitivityAnalyzer(
        weight_ranges={
            'w_under': [1.0, 1.5, 2.0, 2.5, 3.0],
            'w_over': [0.25, 0.5, 0.75, 1.0, 1.25]
        }
    )
    print("   âœ… Weight Sensitivity Analyzer initialized")
    
    # =============================================================================
    # Phase 2: Complete Bayesian Analysis
    # ç¬¬äºŒéšæ®µï¼šå®Œæ•´è²æ°åˆ†æ
    # =============================================================================
    
    print("\n\nğŸ§  Phase 2: Complete Bayesian Analysis Execution")
    print("   ç¬¬äºŒéšæ®µï¼šå®Œæ•´è²æ°åˆ†æåŸ·è¡Œ")
    
    print("\nğŸ“ˆ Executing Integrated Bayesian Optimization åŸ·è¡Œæ•´åˆè²æ°å„ªåŒ–...")
    print("   â€¢ Method æ–¹æ³•: Two-Phase Integrated Analysis å…©éšæ®µæ•´åˆåˆ†æ")
    print("   â€¢ Phase 1 éšæ®µä¸€: Model Comparison & Selection æ¨¡å‹æ¯”è¼ƒèˆ‡é¸æ“‡")
    print("   â€¢ Phase 2 éšæ®µäºŒ: Decision Theory Optimization æ±ºç­–ç†è«–å„ªåŒ–")
    print(f"   â€¢ Products ç”¢å“: {len(products)} parametric products åƒæ•¸ç”¢å“")
    print(f"   â€¢ Events äº‹ä»¶: {len(observed_losses)} loss observations æå¤±è§€æ¸¬")
    print(f"   â€¢ Monte Carlo è’™åœ°å¡ç¾…: {config['n_monte_carlo_samples']} samples æ¨£æœ¬")
    print(f"   â€¢ MCMC: {config['mcmc_samples']} samples Ã— {config['mcmc_chains']} chains")
    
    try:
        # Execute integrated Bayesian optimization åŸ·è¡Œæ•´åˆè²æ°å„ªåŒ–
        comprehensive_results = main_analyzer.integrated_bayesian_optimization(
            observations=observed_losses,           # Training data for model fitting è¨“ç·´è³‡æ–™ç”¨æ–¼æ¨¡å‹æ“¬åˆ
            validation_data=observed_losses,       # Validation data for model selection é©—è­‰è³‡æ–™ç”¨æ–¼æ¨¡å‹é¸æ“‡  
            hazard_indices=wind_indices,           # Hazard indices for optimization å±éšªæŒ‡æ¨™ç”¨æ–¼å„ªåŒ–
            actual_losses=np.column_stack([observed_losses] * len(products)),  # Loss matrix æå¤±çŸ©é™£
            product_bounds={                       # Product parameter bounds ç”¢å“åƒæ•¸ç•Œé™
                'trigger_threshold': (30, 60),     # Wind speed trigger range é¢¨é€Ÿè§¸ç™¼ç¯„åœ
                'payout_amount': (1e7, 1e9)        # Payout amount range è³ ä»˜é‡‘é¡ç¯„åœ
            },
            basis_risk_type=BasisRiskType.WEIGHTED_ASYMMETRIC,  # Asymmetric basis risk ä¸å°ç¨±åŸºå·®é¢¨éšª
            w_under=2.0,                          # Under-compensation weight ä¸è¶³è£œå„Ÿæ¬Šé‡
            w_over=0.5                            # Over-compensation weight éåº¦è£œå„Ÿæ¬Šé‡
        )
        
        print("   âœ… Integrated Bayesian Optimization completed successfully!")
        print("      æ•´åˆè²æ°å„ªåŒ–æˆåŠŸå®Œæˆï¼")
        
    except Exception as e:
        print(f"   âŒ Integrated optimization failed: {e}")
        print("   æ•´åˆå„ªåŒ–å¤±æ•—ï¼Œä½¿ç”¨åˆ†åˆ¥åŸ·è¡Œæ–¹å¼...")
        
        # Fallback: Execute components separately å›é€€ï¼šåˆ†åˆ¥åŸ·è¡Œçµ„ä»¶
        comprehensive_results = execute_fallback_analysis(
            main_analyzer, hierarchical_model, uncertainty_generator, weight_analyzer,
            observed_losses, wind_indices, products, config
        )
    
    # =============================================================================
    # Phase 3: Results Processing and Analysis
    # ç¬¬ä¸‰éšæ®µï¼šçµæœè™•ç†èˆ‡åˆ†æ
    # =============================================================================
    
    print("\n\nğŸ“Š Phase 3: Results Processing and Analysis")
    print("   ç¬¬ä¸‰éšæ®µï¼šçµæœè™•ç†èˆ‡åˆ†æ")
    
    # Process comprehensive results è™•ç†ç¶œåˆçµæœ
    results = process_comprehensive_results(
        comprehensive_results, products, observed_losses, wind_indices, config
    )
    
    # Execute weight sensitivity analysis åŸ·è¡Œæ¬Šé‡æ•æ„Ÿåº¦åˆ†æ
    print("\nâš–ï¸ Executing Weight Sensitivity Analysis åŸ·è¡Œæ¬Šé‡æ•æ„Ÿåº¦åˆ†æ...")
    try:
        sensitivity_results = weight_analyzer.analyze_weight_sensitivity(
            products=products,
            actual_losses=observed_losses,
            wind_indices=wind_indices,
            n_bootstrap_samples=100
        )
        results.weight_sensitivity = sensitivity_results
        print("   âœ… Weight sensitivity analysis completed æ¬Šé‡æ•æ„Ÿåº¦åˆ†æå®Œæˆ")
    except Exception as e:
        print(f"   âš ï¸ Weight sensitivity analysis failed: {e}")
        results.weight_sensitivity = {}
    
    # Generate hierarchical model analysis ç”Ÿæˆéšå±¤æ¨¡å‹åˆ†æ
    print("\nğŸ—ï¸ Executing Hierarchical Bayesian Analysis åŸ·è¡Œéšå±¤è²æ°åˆ†æ...")
    try:
        hierarchical_results = hierarchical_model.fit(observed_losses)
        results.hierarchical_analysis = hierarchical_results
        print("   âœ… Hierarchical Bayesian analysis completed éšå±¤è²æ°åˆ†æå®Œæˆ")
    except Exception as e:
        print(f"   âš ï¸ Hierarchical analysis failed: {e}")
        results.hierarchical_analysis = {}
    
    # Generate uncertainty quantification analysis ç”Ÿæˆä¸ç¢ºå®šæ€§é‡åŒ–åˆ†æ
    print("\nğŸ² Executing Uncertainty Quantification åŸ·è¡Œä¸ç¢ºå®šæ€§é‡åŒ–...")
    try:
        # Create mock CLIMADA objects if real data not available å¦‚æœæ²’æœ‰çœŸå¯¦è³‡æ–™å‰‡å‰µå»ºæ¨¡æ“¬CLIMADAç‰©ä»¶
        mock_hazard = create_mock_climada_hazard(wind_indices)
        mock_exposure = create_mock_climada_exposure(len(observed_losses))
        mock_impact_func = create_mock_impact_functions()
        
        uncertainty_results = uncertainty_generator.generate_probabilistic_loss_distributions(
            tc_hazard=mock_hazard,
            exposure=mock_exposure,
            impact_func_set=mock_impact_func
        )
        results.uncertainty_analysis = uncertainty_results
        print("   âœ… Uncertainty quantification completed ä¸ç¢ºå®šæ€§é‡åŒ–å®Œæˆ")
    except Exception as e:
        print(f"   âš ï¸ Uncertainty quantification failed: {e}")
        results.uncertainty_analysis = {}
    
    # =============================================================================
    # Phase 4: Display Comprehensive Results
    # ç¬¬å››éšæ®µï¼šé¡¯ç¤ºç¶œåˆçµæœ
    # =============================================================================
    
    print("\n\nğŸ‰ Phase 4: Complete Analysis Results")
    print("   ç¬¬å››éšæ®µï¼šå®Œæ•´åˆ†æçµæœ")
    print("=" * 100)
    
    display_comprehensive_results(results)
    
    print("\n\nâœ… Complete Robust Hierarchical Bayesian Analysis Finished!")
    print("   å®Œæ•´å¼·å¥éšå±¤è²æ°åˆ†æå®Œæˆï¼")
    print("=" * 100)
    
    # Display analysis summary é¡¯ç¤ºåˆ†ææ‘˜è¦
    print(f"\nğŸ“Š Analysis Summary åˆ†ææ‘˜è¦:")
    print(f"   â€¢ Products analyzed åˆ†æç”¢å“: {len(products)}")
    print(f"   â€¢ Loss observations æå¤±è§€æ¸¬: {len(observed_losses)}")
    print(f"   â€¢ Monte Carlo samples è’™åœ°å¡ç¾…æ¨£æœ¬: {config['n_monte_carlo_samples']}")
    print(f"   â€¢ MCMC samples MCMCæ¨£æœ¬: {config['mcmc_samples']}")
    print(f"   â€¢ MCMC chains MCMCéˆ: {config['mcmc_chains']}")
    print(f"   â€¢ Analysis type åˆ†æé¡å‹: {results.summary_statistics.get('analysis_type', 'Complete Robust Hierarchical Bayesian')}")
    
    # Display key results é¡¯ç¤ºä¸»è¦çµæœ
    print(f"\nğŸ† Key Results ä¸»è¦çµæœ:")
    if hasattr(results, 'phase_1_results') and results.phase_1_results:
        champion = results.phase_1_results.get('champion_model', {})
        if champion:
            print(f"   â€¢ Champion Model å† è»æ¨¡å‹: {champion.get('name', 'N/A')}")
            print(f"   â€¢ Model CRPS Score æ¨¡å‹CRPSåˆ†æ•¸: {champion.get('crps_score', 'N/A'):.6f}")
    
    if hasattr(results, 'phase_2_results') and results.phase_2_results:
        optimal = results.phase_2_results.get('optimal_product', {})
        if optimal:
            print(f"   â€¢ Optimal Product æœ€ä½³ç”¢å“: {optimal.get('product_id', 'N/A')}")
            print(f"   â€¢ Expected Risk æœŸæœ›é¢¨éšª: {optimal.get('expected_risk', 'N/A'):.6f}")
    
    if hasattr(results, 'weight_sensitivity') and results.weight_sensitivity:
        print(f"   â€¢ Weight Sensitivity æ¬Šé‡æ•æ„Ÿåº¦: Analysis completed åˆ†æå®Œæˆ")
    
    if hasattr(results, 'hierarchical_analysis') and results.hierarchical_analysis:
        print(f"   â€¢ Hierarchical Model éšå±¤æ¨¡å‹: Analysis completed åˆ†æå®Œæˆ")
    
    if hasattr(results, 'uncertainty_analysis') and results.uncertainty_analysis:
        print(f"   â€¢ Uncertainty Quantification ä¸ç¢ºå®šæ€§é‡åŒ–: Analysis completed åˆ†æå®Œæˆ")
    
    # =============================================================================
    # Phase 5: Save Comprehensive Results
    # ç¬¬äº”éšæ®µï¼šä¿å­˜ç¶œåˆçµæœ
    # =============================================================================
    
    print("\n\nğŸ’¾ Phase 5: Saving Comprehensive Results")
    print("   ç¬¬äº”éšæ®µï¼šä¿å­˜ç¶œåˆçµæœ")
    
    save_comprehensive_results(results, config)
    
    print("\nğŸ‰ Complete Robust Hierarchical Bayesian Analysis Successfully Completed!")
    print("   å®Œæ•´å¼·å¥éšå±¤è²æ°åˆ†ææˆåŠŸå®Œæˆï¼")
    print("\nğŸ”§ Methods Used ä½¿ç”¨æ–¹æ³•:")
    print("   â€¢ 4-Level Hierarchical Bayesian Model å››å±¤éšå±¤è²æ°æ¨¡å‹")
    print("   â€¢ Mixed Predictive Estimation (MPE) æ··åˆé æ¸¬ä¼°è¨ˆ")
    print("   â€¢ Density Ratio Robustness Constraints å¯†åº¦æ¯”å¼·å¥æ€§ç´„æŸ")
    print("   â€¢ Monte Carlo Uncertainty Quantification è’™åœ°å¡ç¾…ä¸ç¢ºå®šæ€§é‡åŒ–")
    print("   â€¢ Weight Sensitivity Analysis æ¬Šé‡æ•æ„Ÿåº¦åˆ†æ")
    print("   â€¢ Two-Phase Integrated Optimization å…©éšæ®µæ•´åˆå„ªåŒ–")
    print("   â€¢ CRPS-based Model Comparison CRPSç‚ºåŸºç¤çš„æ¨¡å‹æ¯”è¼ƒ")
    print("   â€¢ Decision Theory-based Product Optimization æ±ºç­–ç†è«–ç‚ºåŸºç¤çš„ç”¢å“å„ªåŒ–")
    
    return results


def execute_fallback_analysis(main_analyzer, hierarchical_model, uncertainty_generator, 
                             weight_analyzer, observed_losses, wind_indices, products, config):
    """
    Fallback analysis execution when integrated optimization fails
    ç•¶æ•´åˆå„ªåŒ–å¤±æ•—æ™‚çš„å›é€€åˆ†æåŸ·è¡Œ
    """
    print("\nğŸ”„ Executing Fallback Analysis åŸ·è¡Œå›é€€åˆ†æ...")
    
    fallback_results = {
        'analysis_type': 'Fallback Component Analysis',
        'components_executed': [],
        'errors': []
    }
    
    # Try individual components å˜—è©¦å€‹åˆ¥çµ„ä»¶
    try:
        # Basic comprehensive analysis åŸºæœ¬ç¶œåˆåˆ†æ
        basic_results = main_analyzer.comprehensive_bayesian_analysis(
            tc_hazard=None,  # Will use mock data å°‡ä½¿ç”¨æ¨¡æ“¬è³‡æ–™
            exposure=None,
            impact_func_set=None,
            observed_losses=observed_losses,
            parametric_products=products,
            hazard_indices=wind_indices
        )
        fallback_results['comprehensive_analysis'] = basic_results
        fallback_results['components_executed'].append('comprehensive_bayesian_analysis')
        print("   âœ… Basic comprehensive analysis completed åŸºæœ¬ç¶œåˆåˆ†æå®Œæˆ")
    except Exception as e:
        fallback_results['errors'].append(f"comprehensive_analysis: {e}")
        print(f"   âŒ Basic comprehensive analysis failed: {e}")
    
    return fallback_results


def process_comprehensive_results(comprehensive_results, products, observed_losses, wind_indices, config):
    """
    Process and structure comprehensive results
    è™•ç†ä¸¦çµæ§‹åŒ–ç¶œåˆçµæœ
    """
    results = type('CompleteBayesianResults', (), {
        'comprehensive_results': comprehensive_results,
        'phase_1_results': comprehensive_results.get('phase_1_model_comparison', {}),
        'phase_2_results': comprehensive_results.get('phase_2_decision_optimization', {}),
        'integration_validation': comprehensive_results.get('integration_validation', {}),
        'results_df': pd.DataFrame(),
        'summary_statistics': {
            'total_products': len(products),
            'total_events': len(observed_losses),
            'analysis_type': 'Complete Robust Hierarchical Bayesian Analysis',
            'monte_carlo_samples': config['n_monte_carlo_samples'],
            'mcmc_samples': config['mcmc_samples'],
            'mcmc_chains': config['mcmc_chains'],
            'density_ratio_constraint': config['density_ratio_constraint']
        }
    })()
    
    # Create results DataFrame if possible å¦‚æœå¯èƒ½å‰‡å‰µå»ºçµæœDataFrame
    try:
        if 'traditional_analysis' in comprehensive_results:
            results.results_df = pd.DataFrame(comprehensive_results['traditional_analysis'])
        elif 'phase_2_decision_optimization' in comprehensive_results:
            phase2_data = comprehensive_results['phase_2_decision_optimization']
            if isinstance(phase2_data, dict) and 'results' in phase2_data:
                results.results_df = pd.DataFrame(phase2_data['results'])
    except Exception as e:
        print(f"   âš ï¸ Could not create results DataFrame: {e}")
    
    return results


def display_comprehensive_results(results):
    """
    Display comprehensive analysis results
    é¡¯ç¤ºç¶œåˆåˆ†æçµæœ
    """
    print("\nğŸ“Š Comprehensive Analysis Results ç¶œåˆåˆ†æçµæœ:")
    
    # Phase 1 Results: Model Comparison éšæ®µä¸€çµæœï¼šæ¨¡å‹æ¯”è¼ƒ
    if hasattr(results, 'phase_1_results') and results.phase_1_results:
        print("\nğŸ§  Phase 1: Model Comparison Results éšæ®µä¸€ï¼šæ¨¡å‹æ¯”è¼ƒçµæœ")
        phase1 = results.phase_1_results
        if 'champion_model' in phase1:
            champion = phase1['champion_model']
            print(f"   ğŸ† Champion Model å† è»æ¨¡å‹: {champion.get('name', 'N/A')}")
            print(f"   ğŸ“ˆ CRPS Score CRPSåˆ†æ•¸: {champion.get('crps_score', 'N/A'):.6f}")
            print(f"   ğŸ“Š Model Performance æ¨¡å‹è¡¨ç¾: {champion.get('performance_summary', 'N/A')}")
        
        if 'model_comparison' in phase1:
            comparison = phase1['model_comparison']
            print(f"   ğŸ“‹ Models Compared æ¯”è¼ƒæ¨¡å‹æ•¸: {len(comparison)}")
    
    # Phase 2 Results: Decision Optimization éšæ®µäºŒçµæœï¼šæ±ºç­–å„ªåŒ–
    if hasattr(results, 'phase_2_results') and results.phase_2_results:
        print("\nâš–ï¸ Phase 2: Decision Optimization Results éšæ®µäºŒï¼šæ±ºç­–å„ªåŒ–çµæœ")
        phase2 = results.phase_2_results
        if 'optimal_product' in phase2:
            optimal = phase2['optimal_product']
            print(f"   ğŸ¯ Optimal Product æœ€ä½³ç”¢å“: {optimal.get('product_id', 'N/A')}")
            print(f"   ğŸ’° Expected Risk æœŸæœ›é¢¨éšª: {optimal.get('expected_risk', 'N/A'):.6f}")
            print(f"   ğŸ”§ Product Parameters ç”¢å“åƒæ•¸: {optimal.get('parameters', 'N/A')}")
    
    # Integration Validation æ•´åˆé©—è­‰
    if hasattr(results, 'integration_validation') and results.integration_validation:
        print("\nğŸ” Integration Validation æ•´åˆé©—è­‰:")
        validation = results.integration_validation
        print(f"   âœ… Theoretical Compliance ç†è«–ç¬¦åˆåº¦: {validation.get('theoretical_compliance', 'N/A')}")
        print(f"   ğŸ”— Phase Integration éšæ®µæ•´åˆ: {validation.get('phase_integration_success', 'N/A')}")
    
    # Additional Analysis Results å…¶ä»–åˆ†æçµæœ
    if hasattr(results, 'weight_sensitivity') and results.weight_sensitivity:
        print("\nâš–ï¸ Weight Sensitivity Analysis æ¬Šé‡æ•æ„Ÿåº¦åˆ†æ: âœ… Completed å®Œæˆ")
    
    if hasattr(results, 'hierarchical_analysis') and results.hierarchical_analysis:
        print("\nğŸ—ï¸ Hierarchical Bayesian Analysis éšå±¤è²æ°åˆ†æ: âœ… Completed å®Œæˆ")
    
    if hasattr(results, 'uncertainty_analysis') and results.uncertainty_analysis:
        print("\nğŸ² Uncertainty Quantification ä¸ç¢ºå®šæ€§é‡åŒ–: âœ… Completed å®Œæˆ")


def save_comprehensive_results(results, config):
    """
    Save all comprehensive analysis results
    ä¿å­˜æ‰€æœ‰ç¶œåˆåˆ†æçµæœ
    """
    # Create output directory å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir = "results/robust_hierarchical_bayesian_analysis"
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    print(f"   ğŸ’¾ Saving results to ä¿å­˜çµæœåˆ°: {output_dir}")
    
    # Save comprehensive results ä¿å­˜ç¶œåˆçµæœ
    try:
        with open(f"{output_dir}/comprehensive_results.pkl", 'wb') as f:
            pickle.dump(results.comprehensive_results, f)
        print("   âœ… Comprehensive results saved ç¶œåˆçµæœå·²ä¿å­˜")
    except Exception as e:
        print(f"   âŒ Failed to save comprehensive results: {e}")
    
    # Save DataFrame results ä¿å­˜DataFrameçµæœ
    try:
        if not results.results_df.empty:
            results.results_df.to_csv(f"{output_dir}/analysis_results.csv", index=False)
            print("   âœ… DataFrame results saved DataFrameçµæœå·²ä¿å­˜")
    except Exception as e:
        print(f"   âŒ Failed to save DataFrame: {e}")
    
    # Save configuration ä¿å­˜é…ç½®
    try:
        with open(f"{output_dir}/analysis_config.json", 'w') as f:
            import json
            json.dump(config, f, indent=2)
        print("   âœ… Configuration saved é…ç½®å·²ä¿å­˜")
    except Exception as e:
        print(f"   âŒ Failed to save configuration: {e}")
    
    # Save summary statistics ä¿å­˜æ‘˜è¦çµ±è¨ˆ
    try:
        with open(f"{output_dir}/summary_statistics.json", 'w') as f:
            import json
            json.dump(results.summary_statistics, f, indent=2)
        print("   âœ… Summary statistics saved æ‘˜è¦çµ±è¨ˆå·²ä¿å­˜")
    except Exception as e:
        print(f"   âŒ Failed to save summary: {e}")
    
    # Save individual analysis components ä¿å­˜å€‹åˆ¥åˆ†æçµ„ä»¶
    save_individual_components(results, output_dir)


def save_individual_components(results, output_dir):
    """
    Save individual analysis components
    ä¿å­˜å€‹åˆ¥åˆ†æçµ„ä»¶
    """
    components_dir = f"{output_dir}/components"
    Path(components_dir).mkdir(exist_ok=True)
    
    # Save weight sensitivity results ä¿å­˜æ¬Šé‡æ•æ„Ÿåº¦çµæœ
    if hasattr(results, 'weight_sensitivity') and results.weight_sensitivity:
        try:
            with open(f"{components_dir}/weight_sensitivity.pkl", 'wb') as f:
                pickle.dump(results.weight_sensitivity, f)
            print("   âœ… Weight sensitivity results saved æ¬Šé‡æ•æ„Ÿåº¦çµæœå·²ä¿å­˜")
        except Exception as e:
            print(f"   âŒ Failed to save weight sensitivity: {e}")
    
    # Save hierarchical analysis results ä¿å­˜éšå±¤åˆ†æçµæœ
    if hasattr(results, 'hierarchical_analysis') and results.hierarchical_analysis:
        try:
            with open(f"{components_dir}/hierarchical_analysis.pkl", 'wb') as f:
                pickle.dump(results.hierarchical_analysis, f)
            print("   âœ… Hierarchical analysis results saved éšå±¤åˆ†æçµæœå·²ä¿å­˜")
        except Exception as e:
            print(f"   âŒ Failed to save hierarchical analysis: {e}")
    
    # Save uncertainty analysis results ä¿å­˜ä¸ç¢ºå®šæ€§åˆ†æçµæœ
    if hasattr(results, 'uncertainty_analysis') and results.uncertainty_analysis:
        try:
            with open(f"{components_dir}/uncertainty_analysis.pkl", 'wb') as f:
                pickle.dump(results.uncertainty_analysis, f)
            print("   âœ… Uncertainty analysis results saved ä¸ç¢ºå®šæ€§åˆ†æçµæœå·²ä¿å­˜")
        except Exception as e:
            print(f"   âŒ Failed to save uncertainty analysis: {e}")


def create_mock_climada_hazard(wind_indices):
    """
    Create mock CLIMADA hazard object for uncertainty analysis
    å‰µå»ºæ¨¡æ“¬CLIMADAå±éšªç‰©ä»¶ç”¨æ–¼ä¸ç¢ºå®šæ€§åˆ†æ
    """
    class MockHazard:
        def __init__(self, wind_indices):
            self.intensity = np.column_stack([wind_indices, wind_indices]).T
            self.event_id = np.arange(len(wind_indices))
            self.frequency = np.ones(len(wind_indices)) / len(wind_indices)
    
    return MockHazard(wind_indices)


def create_mock_climada_exposure(n_exposures):
    """
    Create mock CLIMADA exposure object
    å‰µå»ºæ¨¡æ“¬CLIMADAæš´éœ²ç‰©ä»¶
    """
    class MockExposure:
        def __init__(self, n):
            self.value = np.random.lognormal(15, 1, n)
            self.latitude = np.random.uniform(33.8, 36.6, n)
            self.longitude = np.random.uniform(-84.5, -75.5, n)
    
    return MockExposure(n_exposures)


def create_mock_impact_functions():
    """
    Create mock impact functions
    å‰µå»ºæ¨¡æ“¬å½±éŸ¿å‡½æ•¸
    """
    class MockImpactFunc:
        def __init__(self):
            self.intensity = np.arange(0, 100, 5)
            self.mdd = 1 / (1 + np.exp(-(self.intensity - 50) / 10))
            self.paa = np.ones_like(self.intensity)
    
    return MockImpactFunc()


if __name__ == "__main__":
    results = main()
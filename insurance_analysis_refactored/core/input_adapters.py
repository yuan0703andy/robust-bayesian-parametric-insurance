"""
Input Adapters for Unified Product Design Engine
çµ±ä¸€ç”¢å“è¨­è¨ˆå¼•æ“çš„è¼¸å…¥é©é…å™¨

This module provides adapters to integrate different input sources:
- CLIMADAInputAdapter: Traditional CLIMADA hazard objects
- BayesianInputAdapter: Bayesian simulation results with probabilistic distributions

æœ¬æ¨¡çµ„æä¾›é©é…å™¨ä»¥æ•´åˆä¸åŒçš„è¼¸å…¥ä¾†æºï¼š
- CLIMADAé©é…å™¨ï¼šå‚³çµ±çš„CLIMADAç½å®³ç‰©ä»¶
- è²æ°é©é…å™¨ï¼šå¸¶æœ‰æ©Ÿç‡åˆ†å¸ƒçš„è²æ°æ¨¡æ“¬çµæœ
"""

import numpy as np
import pandas as pd
from abc import ABC, abstractmethod
from typing import Dict, Any, Union, Tuple, Optional
from dataclasses import dataclass
import warnings

# Import CLIMADA if available
try:
    from climada.engine import ImpactCalc
    from climada.entity import ImpactFuncSet
    CLIMADA_AVAILABLE = True
except ImportError:
    CLIMADA_AVAILABLE = False
    warnings.warn("CLIMADA not available - CLIMADAInputAdapter will have limited functionality")

# Import spatial analysis tools
from scipy.spatial import cKDTree
from scipy import stats


@dataclass
class EventMetadata:
    """äº‹ä»¶å…ƒæ•¸æ“šçµæ§‹"""
    event_id: str
    event_name: Optional[str] = None
    year: Optional[int] = None
    category: Optional[str] = None
    max_wind_speed: Optional[float] = None
    central_pressure: Optional[float] = None
    affected_exposure: Optional[float] = None


class InputAdapter(ABC):
    """
    çµ±ä¸€è¼¸å…¥é©é…å™¨åŸºé¡
    
    æ‰€æœ‰è¼¸å…¥é©é…å™¨éƒ½å¿…é ˆå¯¦ç¾é€™äº›æ–¹æ³•ï¼Œä»¥æä¾›çµ±ä¸€çš„ä»‹é¢çµ¦ç”¢å“è¨­è¨ˆå¼•æ“
    """
    
    @abstractmethod
    def extract_parametric_indices(self) -> Dict[str, np.ndarray]:
        """
        æå–åƒæ•¸æŒ‡æ¨™
        
        Returns:
        --------
        Dict[str, np.ndarray]
            åƒæ•¸æŒ‡æ¨™å­—å…¸ï¼Œæ ¼å¼ç‚º {index_type: values}
            ä¾‹å¦‚ï¼š{'cat_in_circle_30km_max': array, 'cat_in_circle_50km_mean': array}
        """
        pass
    
    @abstractmethod
    def get_loss_data(self) -> np.ndarray:
        """
        ç²å–æå¤±æ•¸æ“š
        
        Returns:
        --------
        np.ndarray
            æå¤±æ•¸æ“šæ•¸çµ„ï¼Œæ¯å€‹å…ƒç´ å°æ‡‰ä¸€å€‹äº‹ä»¶çš„æå¤±
        """
        pass
    
    @abstractmethod
    def get_event_metadata(self) -> pd.DataFrame:
        """
        ç²å–äº‹ä»¶å…ƒæ•¸æ“š
        
        Returns:
        --------
        pd.DataFrame
            äº‹ä»¶å…ƒæ•¸æ“šè¡¨ï¼ŒåŒ…å«äº‹ä»¶IDã€åç¨±ã€å¹´ä»½ç­‰è³‡è¨Š
        """
        pass
    
    @abstractmethod
    def get_input_type(self) -> str:
        """
        ç²å–è¼¸å…¥é¡å‹æ¨™è­˜
        
        Returns:
        --------
        str
            è¼¸å…¥é¡å‹ ('traditional' æˆ– 'probabilistic')
        """
        pass


class CLIMADAInputAdapter(InputAdapter):
    """
    CLIMADA ç‰©ä»¶é©é…å™¨
    
    å°‡ CLIMADA çš„ hazard, exposure, impact_func_set è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
    """
    
    def __init__(self, tc_hazard, exposure_main, impact_func_set, 
                 cat_in_circle_radii: list = [15, 30, 50],
                 statistics: list = ['max', 'mean', '95th']):
        """
        åˆå§‹åŒ– CLIMADA é©é…å™¨
        
        Parameters:
        -----------
        tc_hazard : climada.hazard.TropCyclone
            é¢±é¢¨ç½å®³ç‰©ä»¶
        exposure_main : climada.entity.Exposures
            æ›éšªç‰©ä»¶
        impact_func_set : climada.entity.ImpactFuncSet
            å½±éŸ¿å‡½æ•¸é›†
        cat_in_circle_radii : list
            Cat-in-a-Circle åˆ†æåŠå¾‘ (km)
        statistics : list
            è¦è¨ˆç®—çš„çµ±è¨ˆæŒ‡æ¨™
        """
        self.tc_hazard = tc_hazard
        self.exposure_main = exposure_main
        self.impact_func_set = impact_func_set
        self.cat_in_circle_radii = cat_in_circle_radii
        self.statistics = statistics
        
        # ç·©å­˜è¨ˆç®—çµæœ
        self._cached_indices = None
        self._cached_losses = None
        self._cached_metadata = None
        
        # é©—è­‰è¼¸å…¥
        self._validate_inputs()
    
    def _validate_inputs(self):
        """é©—è­‰è¼¸å…¥çš„æœ‰æ•ˆæ€§"""
        if not CLIMADA_AVAILABLE:
            raise ImportError("CLIMADA is required for CLIMADAInputAdapter but not available")
        
        # åŸºæœ¬é©—è­‰
        if self.tc_hazard is None:
            raise ValueError("tc_hazard cannot be None")
        if self.exposure_main is None:
            raise ValueError("exposure_main cannot be None")
        if self.impact_func_set is None:
            raise ValueError("impact_func_set cannot be None")
    
    def extract_parametric_indices(self) -> Dict[str, np.ndarray]:
        """
        å¾ CLIMADA ç‰©ä»¶æå– Cat-in-a-Circle åƒæ•¸æŒ‡æ¨™
        
        æ ¹æ“š Steinmann et al. (2023)ï¼ŒCat-in-a-Circle æ˜¯æ¯å€‹ä¿éšªæ¨™çš„ç‰©ï¼ˆæ›éšªé»ï¼‰
        å‘¨åœä¸€å®šåŠå¾‘åœ“åœˆå…§çš„æœ€å¤§æŒçºŒé¢¨é€Ÿ
        """
        if self._cached_indices is not None:
            return self._cached_indices
        
        print("ğŸ”„ å¾ CLIMADA ç‰©ä»¶æå– Cat-in-a-Circle åƒæ•¸æŒ‡æ¨™...")
        
        # ç²å–æ›éšªé»åº§æ¨™ - æ”¯æ´å¤šç¨®æ¬„ä½åç¨±æ ¼å¼
        exposure_gdf = self.exposure_main.gdf
        
        # å˜—è©¦ä¸åŒçš„åº§æ¨™æ¬„ä½åç¨±
        if 'latitude' in exposure_gdf.columns and 'longitude' in exposure_gdf.columns:
            lat_col, lon_col = 'latitude', 'longitude'
        elif 'lat' in exposure_gdf.columns and 'lon' in exposure_gdf.columns:
            lat_col, lon_col = 'lat', 'lon'
        elif hasattr(self.exposure_main, 'coord') and self.exposure_main.coord.shape[1] >= 2:
            # å¦‚æœæœ‰ coord å±¬æ€§ï¼Œç›´æ¥ä½¿ç”¨
            exposure_coords = self.exposure_main.coord
        elif 'geometry' in exposure_gdf.columns:
            # å¾ geometry æ¬„ä½æå–åº§æ¨™
            exposure_coords = np.column_stack([
                exposure_gdf.geometry.y.values,  # latitude
                exposure_gdf.geometry.x.values   # longitude
            ])
        else:
            raise ValueError("Cannot find coordinate columns in exposure data. Expected 'latitude'/'longitude', 'lat'/'lon', or 'geometry' column")
        
        # å¦‚æœæ‰¾åˆ°äº†å‘½åæ¬„ä½ï¼Œæå–åº§æ¨™
        if 'lat_col' in locals():
            exposure_coords = np.column_stack([
                exposure_gdf[lat_col].values,
                exposure_gdf[lon_col].values
            ])
        
        # ç²å–ç½å®³ç¶²æ ¼é»åº§æ¨™
        hazard_coords = np.column_stack([
            self.tc_hazard.centroids.lat,
            self.tc_hazard.centroids.lon
        ])
        
        # å»ºç«‹ç©ºé–“ç´¢å¼•
        from scipy.spatial import cKDTree
        hazard_tree = cKDTree(np.radians(hazard_coords))
        
        indices_dict = {}
        
        # å°æ¯å€‹åŠå¾‘å’Œçµ±è¨ˆæŒ‡æ¨™çµ„åˆæå– Cat-in-a-Circle æŒ‡æ¨™
        for radius_km in self.cat_in_circle_radii:
            for stat in self.statistics:
                index_name = f"cat_in_circle_{radius_km}km_{stat}"
                
                # ç‚ºæ¯å€‹äº‹ä»¶è¨ˆç®—æŒ‡æ¨™
                event_indices = []
                
                # ç²å–äº‹ä»¶æ•¸é‡
                if hasattr(self.tc_hazard, 'size'):
                    if hasattr(self.tc_hazard.size, '__getitem__'):
                        n_events = self.tc_hazard.size[0]
                    else:
                        n_events = self.tc_hazard.intensity.shape[0]
                else:
                    n_events = self.tc_hazard.intensity.shape[0]
                
                for event_idx in range(n_events):
                    # ç²å–è©²äº‹ä»¶çš„é¢¨é€Ÿå ´
                    wind_field = self.tc_hazard.intensity[event_idx, :].toarray().flatten()
                    
                    # è¨ˆç®—æ¯å€‹æ›éšªé»çš„ Cat-in-a-Circle æŒ‡æ¨™
                    exposure_indices = []
                    
                    for exp_coord in exposure_coords:
                        # æ‰¾åˆ°åŠå¾‘ç¯„åœå…§çš„ç½å®³é» (Cat-in-a-Circle)
                        radius_rad = radius_km / 6371.0  # è½‰æ›ç‚ºå¼§åº¦ (åœ°çƒåŠå¾‘ç´„6371km)
                        nearby_indices = hazard_tree.query_ball_point(
                            np.radians(exp_coord), radius_rad
                        )
                        
                        if len(nearby_indices) > 0:
                            nearby_winds = wind_field[nearby_indices]
                            nearby_winds = nearby_winds[nearby_winds > 0]  # æ’é™¤é›¶å€¼
                            
                            if len(nearby_winds) > 0:
                                if stat == 'max':
                                    exposure_indices.append(np.max(nearby_winds))
                                elif stat == 'mean':
                                    exposure_indices.append(np.mean(nearby_winds))
                                elif stat == '95th':
                                    exposure_indices.append(np.percentile(nearby_winds, 95))
                                else:
                                    exposure_indices.append(np.mean(nearby_winds))
                            else:
                                exposure_indices.append(0.0)
                        else:
                            exposure_indices.append(0.0)
                    
                    # ä½¿ç”¨ç´”Cat-in-a-Circleæœ€å¤§é¢¨é€Ÿï¼ˆç¬¦åˆSteinmannè«–æ–‡å®šç¾©ï¼‰
                    if len(exposure_indices) > 0:
                        if stat == 'max':
                            # å°æ–¼æœ€å¤§å€¼çµ±è¨ˆï¼Œå–æ‰€æœ‰æ›éšªé»çš„æœ€å¤§å€¼ä½œç‚ºè©²äº‹ä»¶çš„Cat-in-a-CircleæŒ‡æ¨™
                            event_max_index = np.max(exposure_indices)
                        else:
                            # å°æ–¼å…¶ä»–çµ±è¨ˆï¼ˆmean, 95thï¼‰ï¼Œä½¿ç”¨ç°¡å–®å¹³å‡
                            event_max_index = np.mean(exposure_indices)
                        event_indices.append(event_max_index)
                    else:
                        event_indices.append(0.0)
                
                indices_dict[index_name] = np.array(event_indices)
        
        self._cached_indices = indices_dict
        print(f"âœ… æå–äº† {len(indices_dict)} å€‹ Cat-in-a-Circle åƒæ•¸æŒ‡æ¨™")
        print(f"   åŠå¾‘ç¯„åœ: {self.cat_in_circle_radii} km")
        print(f"   çµ±è¨ˆæŒ‡æ¨™: {self.statistics}")
        print(f"   æ›éšªé»æ•¸é‡: {len(exposure_coords)}")
        print(f"   ğŸ“ ä½¿ç”¨ç´”Cat-in-a-Circleæœ€å¤§é¢¨é€Ÿï¼ˆç„¡åŠ æ¬Šå¹³å‡ï¼‰")
        
        return indices_dict
    
    def get_loss_data(self) -> np.ndarray:
        """
        è¨ˆç®—ç¢ºå®šæ€§æå¤±æ•¸æ“š
        """
        if self._cached_losses is not None:
            return self._cached_losses
        
        print("ğŸ”„ è¨ˆç®— CLIMADA å½±éŸ¿æå¤±...")
        
        # ä½¿ç”¨ CLIMADA è¨ˆç®—å½±éŸ¿ - æ ¹æ“šä½ çš„ç¯„ä¾‹ä½¿ç”¨æ­£ç¢ºçš„ API
        impact_calc = ImpactCalc(self.exposure_main, self.impact_func_set, self.tc_hazard)
        impact = impact_calc.impact(save_mat=False)
        
        # æå–äº‹ä»¶æå¤±
        losses = impact.at_event
        self._cached_losses = losses
        
        print(f"âœ… è¨ˆç®—äº† {len(losses)} å€‹äº‹ä»¶çš„æå¤±")
        print(f"   ç¸½æå¤±: ${np.sum(losses)/1e9:.2f}B")
        print(f"   æœ€å¤§äº‹ä»¶æå¤±: ${np.max(losses)/1e9:.2f}B")
        
        return losses
    
    def get_event_metadata(self) -> pd.DataFrame:
        """
        å¾ CLIMADA ç‰©ä»¶æå–äº‹ä»¶å…ƒæ•¸æ“š
        """
        if self._cached_metadata is not None:
            return self._cached_metadata
        
        # ç²å–äº‹ä»¶æ•¸é‡ - æ”¯æ´ä¸åŒçš„ size å±¬æ€§æ ¼å¼
        if hasattr(self.tc_hazard, 'size'):
            if hasattr(self.tc_hazard.size, '__getitem__'):
                # size æ˜¯ tuple æˆ– array
                n_events = self.tc_hazard.size[0]
            else:
                # size æ˜¯å–®ä¸€æ•´æ•¸ï¼Œé€šå¸¸è¡¨ç¤ºç¸½å…ƒç´ æ•¸
                # å°æ–¼ CLIMADA hazardï¼Œäº‹ä»¶æ•¸æ˜¯ intensity çŸ©é™£çš„ç¬¬ä¸€ç¶­
                n_events = self.tc_hazard.intensity.shape[0]
        else:
            # ç›´æ¥å¾ intensity çŸ©é™£ç²å–äº‹ä»¶æ•¸
            n_events = self.tc_hazard.intensity.shape[0]
        
        metadata_list = []
        for i in range(n_events):
            # å˜—è©¦å¾ hazard ä¸­æå–äº‹ä»¶è³‡è¨Š
            event_id = f"event_{i:04d}"
            event_name = None
            year = None
            category = None
            max_wind = None
            
            # å¦‚æœ hazard æœ‰äº‹ä»¶åç¨±è³‡è¨Š
            if hasattr(self.tc_hazard, 'event_name') and len(self.tc_hazard.event_name) > i:
                event_name = self.tc_hazard.event_name[i]
            
            # å¦‚æœæœ‰å¹´ä»½è³‡è¨Š
            if hasattr(self.tc_hazard, 'date') and len(self.tc_hazard.date) > i:
                year = pd.to_datetime(self.tc_hazard.date[i], unit='s').year
            
            # è¨ˆç®—æœ€å¤§é¢¨é€Ÿ
            wind_field = self.tc_hazard.intensity[i, :].toarray().flatten()
            if len(wind_field) > 0:
                max_wind = np.max(wind_field)
                
                # æ ¹æ“šé¢¨é€Ÿä¼°ç®—é¢¶é¢¨ç­‰ç´š
                if max_wind >= 70:  # 137+ kt (Cat 5)
                    category = "Cat_5"
                elif max_wind >= 58:  # 113-136 kt (Cat 4)
                    category = "Cat_4"
                elif max_wind >= 50:  # 96-112 kt (Cat 3)
                    category = "Cat_3"
                elif max_wind >= 43:  # 83-95 kt (Cat 2)
                    category = "Cat_2"
                elif max_wind >= 33:  # 64-82 kt (Cat 1)
                    category = "Cat_1"
                else:
                    category = "Tropical Storm"
            
            metadata_list.append(EventMetadata(
                event_id=event_id,
                event_name=event_name,
                year=year,
                category=category,
                max_wind_speed=max_wind,
                affected_exposure=np.sum(self.exposure_main.gdf['value'].values)
            ))
        
        # è½‰æ›ç‚º DataFrame
        metadata_df = pd.DataFrame([
            {
                'event_id': meta.event_id,
                'event_name': meta.event_name,
                'year': meta.year,
                'category': meta.category,
                'max_wind_speed': meta.max_wind_speed,
                'affected_exposure': meta.affected_exposure
            }
            for meta in metadata_list
        ])
        
        self._cached_metadata = metadata_df
        return metadata_df
    
    def get_input_type(self) -> str:
        """è¿”å›è¼¸å…¥é¡å‹"""
        return "traditional"


class BayesianInputAdapter(InputAdapter):
    """
    è²æ°æ¨¡æ“¬çµæœé©é…å™¨
    
    å°‡è²æ°åˆ†æçš„æ©Ÿç‡æ€§çµæœè½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
    """
    
    def __init__(self, bayesian_results: Dict[str, Any]):
        """
        åˆå§‹åŒ–è²æ°é©é…å™¨
        
        Parameters:
        -----------
        bayesian_results : Dict[str, Any]
            è²æ°åˆ†æçµæœï¼Œæ‡‰åŒ…å«ï¼š
            - uncertainty_quantification: ä¸ç¢ºå®šæ€§é‡åŒ–çµæœ
            - event_loss_distributions: äº‹ä»¶æå¤±åˆ†å¸ƒ
            - parametric_indices: åƒæ•¸æŒ‡æ¨™ (å¯é¸)
        """
        self.bayesian_results = bayesian_results
        
        # ç·©å­˜è¨ˆç®—çµæœ
        self._cached_indices = None
        self._cached_losses = None
        self._cached_metadata = None
        
        # é©—è­‰è¼¸å…¥
        self._validate_bayesian_results()
    
    def _validate_bayesian_results(self):
        """é©—è­‰è²æ°çµæœçš„æœ‰æ•ˆæ€§"""
        required_keys = ['uncertainty_quantification']
        
        for key in required_keys:
            if key not in self.bayesian_results:
                raise ValueError(f"Bayesian results missing required key: {key}")
        
        # æª¢æŸ¥ä¸ç¢ºå®šæ€§é‡åŒ–çµæœ
        uncertainty_data = self.bayesian_results['uncertainty_quantification']
        if 'event_loss_distributions' not in uncertainty_data:
            raise ValueError("uncertainty_quantification missing event_loss_distributions")
    
    def extract_parametric_indices(self) -> Dict[str, np.ndarray]:
        """
        å¾è²æ°çµæœæå–åƒæ•¸æŒ‡æ¨™
        """
        if self._cached_indices is not None:
            return self._cached_indices
        
        print("ğŸ”„ å¾è²æ°çµæœæå–åƒæ•¸æŒ‡æ¨™...")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰é è¨ˆç®—çš„åƒæ•¸æŒ‡æ¨™
        if 'parametric_indices' in self.bayesian_results:
            indices = self.bayesian_results['parametric_indices']
            self._cached_indices = indices
            print(f"âœ… ä½¿ç”¨é è¨ˆç®—çš„ {len(indices)} å€‹åƒæ•¸æŒ‡æ¨™")
            return indices
        
        # å¦å‰‡å¾æå¤±åˆ†å¸ƒä¸­è¨ˆç®—
        uncertainty_data = self.bayesian_results['uncertainty_quantification']
        event_distributions = uncertainty_data['event_loss_distributions']
        
        indices_dict = {}
        
        # åŸºæ–¼æå¤±åˆ†å¸ƒç‰¹å¾µä½œç‚ºåƒæ•¸æŒ‡æ¨™
        indices_dict['loss_mean'] = np.array([
            event_data['mean'] for event_data in event_distributions.values()
        ])
        
        indices_dict['loss_std'] = np.array([
            event_data['std'] for event_data in event_distributions.values()
        ])
        
        indices_dict['loss_95th'] = np.array([
            np.percentile(event_data['samples'], 95) 
            for event_data in event_distributions.values()
        ])
        
        # å¦‚æœæœ‰é¢¨é€Ÿè³‡è¨Šï¼Œä½¿ç”¨é¢¨é€Ÿä½œç‚ºåƒæ•¸æŒ‡æ¨™
        if 'hazard_intensities' in uncertainty_data:
            hazard_intensities = uncertainty_data['hazard_intensities']
            indices_dict['wind_speed_max'] = np.array([
                np.max(intensity) if hasattr(intensity, '__iter__') else intensity
                for intensity in hazard_intensities
            ])
        
        self._cached_indices = indices_dict
        print(f"âœ… è¨ˆç®—äº† {len(indices_dict)} å€‹åƒæ•¸æŒ‡æ¨™é¡å‹")
        
        return indices_dict
    
    def get_loss_data(self) -> np.ndarray:
        """
        æå–æ©Ÿç‡æ€§æå¤±çš„æœŸæœ›å€¼ä½œç‚ºä»£è¡¨æ€§æå¤±
        """
        if self._cached_losses is not None:
            return self._cached_losses
        
        print("ğŸ”„ å¾è²æ°åˆ†å¸ƒæå–ä»£è¡¨æ€§æå¤±...")
        
        uncertainty_data = self.bayesian_results['uncertainty_quantification']
        event_distributions = uncertainty_data['event_loss_distributions']
        
        # ä½¿ç”¨åˆ†å¸ƒçš„æœŸæœ›å€¼ä½œç‚ºä»£è¡¨æ€§æå¤±
        losses = np.array([
            event_data['mean'] for event_data in event_distributions.values()
        ])
        
        self._cached_losses = losses
        
        print(f"âœ… æå–äº† {len(losses)} å€‹äº‹ä»¶çš„ä»£è¡¨æ€§æå¤±")
        print(f"   ç¸½æœŸæœ›æå¤±: ${np.sum(losses)/1e9:.2f}B")
        print(f"   æœ€å¤§æœŸæœ›æå¤±: ${np.max(losses)/1e9:.2f}B")
        
        return losses
    
    def get_probabilistic_distributions(self) -> Dict[str, Any]:
        """
        è²æ°å°ˆç”¨ï¼šç²å–å®Œæ•´çš„æ©Ÿç‡åˆ†å¸ƒ
        
        Returns:
        --------
        Dict[str, Any]
            å®Œæ•´çš„ä¸ç¢ºå®šæ€§é‡åŒ–çµæœ
        """
        return self.bayesian_results['uncertainty_quantification']
    
    def get_event_metadata(self) -> pd.DataFrame:
        """
        å¾è²æ°çµæœæå–äº‹ä»¶å…ƒæ•¸æ“š
        """
        if self._cached_metadata is not None:
            return self._cached_metadata
        
        uncertainty_data = self.bayesian_results['uncertainty_quantification']
        event_distributions = uncertainty_data['event_loss_distributions']
        
        metadata_list = []
        
        for i, (event_id, event_data) in enumerate(event_distributions.items()):
            # åŸºæœ¬å…ƒæ•¸æ“š
            event_name = event_data.get('event_name', f"bayesian_event_{i:04d}")
            year = event_data.get('year')
            
            # å¾æå¤±åˆ†å¸ƒæ¨æ–·é¢¶é¢¨å¼·åº¦
            loss_mean = event_data['mean']
            loss_std = event_data['std']
            
            # ç°¡åŒ–çš„é¢¶é¢¨ç­‰ç´šæ¨æ–· (åŸºæ–¼æå¤±å¤§å°)
            if loss_mean > 5e9:  # > $5B
                category = "Cat_4_5"
            elif loss_mean > 1e9:  # > $1B
                category = "Cat_3"
            elif loss_mean > 2e8:  # > $200M
                category = "Cat_2"
            elif loss_mean > 5e7:  # > $50M
                category = "Cat_1"
            else:
                category = "Tropical Storm"
            
            # ä¼°ç®—é¢¨é€Ÿ (å¦‚æœæœ‰çš„è©±)
            max_wind = None
            if 'max_wind_speed' in event_data:
                max_wind = event_data['max_wind_speed']
            elif 'hazard_intensity' in event_data:
                max_wind = event_data['hazard_intensity']
            
            metadata_list.append({
                'event_id': event_id,
                'event_name': event_name,
                'year': year,
                'category': category,
                'max_wind_speed': max_wind,
                'loss_mean': loss_mean,
                'loss_std': loss_std,
                'uncertainty_ratio': loss_std / loss_mean if loss_mean > 0 else 0
            })
        
        metadata_df = pd.DataFrame(metadata_list)
        self._cached_metadata = metadata_df
        
        return metadata_df
    
    def get_input_type(self) -> str:
        """è¿”å›è¼¸å…¥é¡å‹"""
        return "probabilistic"


class HybridInputAdapter(InputAdapter):
    """
    æ··åˆè¼¸å…¥é©é…å™¨
    
    çµåˆ CLIMADA å’Œè²æ°çµæœçš„æ··åˆé©é…å™¨ï¼Œç”¨æ–¼å°æ¯”åˆ†æ
    """
    
    def __init__(self, climada_adapter: CLIMADAInputAdapter, 
                 bayesian_adapter: BayesianInputAdapter,
                 blending_weight: float = 0.5):
        """
        åˆå§‹åŒ–æ··åˆé©é…å™¨
        
        Parameters:
        -----------
        climada_adapter : CLIMADAInputAdapter
            CLIMADA é©é…å™¨
        bayesian_adapter : BayesianInputAdapter
            è²æ°é©é…å™¨
        blending_weight : float
            æ··åˆæ¬Šé‡ (0=ç´”CLIMADA, 1=ç´”è²æ°)
        """
        self.climada_adapter = climada_adapter
        self.bayesian_adapter = bayesian_adapter
        self.blending_weight = blending_weight
        
        # é©—è­‰äº‹ä»¶æ•¸é‡åŒ¹é…
        climada_losses = climada_adapter.get_loss_data()
        bayesian_losses = bayesian_adapter.get_loss_data()
        
        if len(climada_losses) != len(bayesian_losses):
            raise ValueError(
                f"Event count mismatch: CLIMADA={len(climada_losses)}, "
                f"Bayesian={len(bayesian_losses)}"
            )
    
    def extract_parametric_indices(self) -> Dict[str, np.ndarray]:
        """æ··åˆåƒæ•¸æŒ‡æ¨™æå–"""
        climada_indices = self.climada_adapter.extract_parametric_indices()
        bayesian_indices = self.bayesian_adapter.extract_parametric_indices()
        
        # çµåˆå…©ç¨®æŒ‡æ¨™
        hybrid_indices = {}
        
        # æ·»åŠ  CLIMADA æŒ‡æ¨™ (å¸¶å‰ç¶´)
        for key, values in climada_indices.items():
            hybrid_indices[f"climada_{key}"] = values
        
        # æ·»åŠ è²æ°æŒ‡æ¨™ (å¸¶å‰ç¶´)
        for key, values in bayesian_indices.items():
            hybrid_indices[f"bayesian_{key}"] = values
        
        return hybrid_indices
    
    def get_loss_data(self) -> np.ndarray:
        """æ··åˆæå¤±æ•¸æ“š"""
        climada_losses = self.climada_adapter.get_loss_data()
        bayesian_losses = self.bayesian_adapter.get_loss_data()
        
        # åŠ æ¬Šæ··åˆ
        hybrid_losses = (
            (1 - self.blending_weight) * climada_losses + 
            self.blending_weight * bayesian_losses
        )
        
        return hybrid_losses
    
    def get_event_metadata(self) -> pd.DataFrame:
        """æ··åˆäº‹ä»¶å…ƒæ•¸æ“š"""
        climada_meta = self.climada_adapter.get_event_metadata()
        bayesian_meta = self.bayesian_adapter.get_event_metadata()
        
        # åˆä½µå…ƒæ•¸æ“š
        hybrid_meta = climada_meta.copy()
        
        # æ·»åŠ è²æ°ç›¸é—œåˆ—
        if 'loss_mean' in bayesian_meta.columns:
            hybrid_meta['bayesian_loss_mean'] = bayesian_meta['loss_mean']
        if 'loss_std' in bayesian_meta.columns:
            hybrid_meta['bayesian_loss_std'] = bayesian_meta['loss_std']
        if 'uncertainty_ratio' in bayesian_meta.columns:
            hybrid_meta['uncertainty_ratio'] = bayesian_meta['uncertainty_ratio']
        
        hybrid_meta['adapter_type'] = 'hybrid'
        hybrid_meta['blending_weight'] = self.blending_weight
        
        return hybrid_meta
    
    def get_input_type(self) -> str:
        """è¿”å›è¼¸å…¥é¡å‹"""
        return "hybrid"


def create_adapter_from_data(data_source: Union[tuple, dict], adapter_type: str = "auto") -> InputAdapter:
    """
    å·¥å» å‡½æ•¸ï¼šæ ¹æ“šæ•¸æ“šä¾†æºè‡ªå‹•å‰µå»ºé©ç•¶çš„é©é…å™¨
    
    Parameters:
    -----------
    data_source : Union[tuple, dict]
        æ•¸æ“šä¾†æºï¼Œå¯ä»¥æ˜¯ï¼š
        - tuple: (tc_hazard, exposure_main, impact_func_set) ç”¨æ–¼ CLIMADA
        - dict: bayesian_results ç”¨æ–¼è²æ°
    adapter_type : str
        é©é…å™¨é¡å‹ ("auto", "climada", "bayesian", "hybrid")
        
    Returns:
    --------
    InputAdapter
        é©ç•¶çš„è¼¸å…¥é©é…å™¨å¯¦ä¾‹
    """
    
    if adapter_type == "auto":
        # è‡ªå‹•æª¢æ¸¬æ•¸æ“šé¡å‹
        if isinstance(data_source, tuple) and len(data_source) == 3:
            adapter_type = "climada"
        elif isinstance(data_source, dict):
            adapter_type = "bayesian"
        else:
            raise ValueError("Cannot auto-detect adapter type from data_source")
    
    if adapter_type == "climada":
        if not isinstance(data_source, tuple) or len(data_source) != 3:
            raise ValueError("CLIMADA adapter requires tuple of (tc_hazard, exposure_main, impact_func_set)")
        tc_hazard, exposure_main, impact_func_set = data_source
        return CLIMADAInputAdapter(tc_hazard, exposure_main, impact_func_set)
    
    elif adapter_type == "bayesian":
        if not isinstance(data_source, dict):
            raise ValueError("Bayesian adapter requires dict of bayesian_results")
        return BayesianInputAdapter(data_source)
    
    else:
        raise ValueError(f"Unsupported adapter_type: {adapter_type}")


# ä¾¿åˆ©å‡½æ•¸
def validate_adapter_compatibility(adapter1: InputAdapter, adapter2: InputAdapter) -> bool:
    """
    é©—è­‰å…©å€‹é©é…å™¨çš„å…¼å®¹æ€§ï¼ˆäº‹ä»¶æ•¸é‡ç­‰ï¼‰
    
    Parameters:
    -----------
    adapter1, adapter2 : InputAdapter
        è¦æª¢æŸ¥çš„é©é…å™¨
        
    Returns:
    --------
    bool
        æ˜¯å¦å…¼å®¹
    """
    try:
        losses1 = adapter1.get_loss_data()
        losses2 = adapter2.get_loss_data()
        
        return len(losses1) == len(losses2)
    except Exception:
        return False


def extract_pure_cat_in_circle(tc_hazard, exposure_coords, hazard_tree, radius_km):
    """æå–ç´” Cat-in-a-Circle æœ€å¤§é¢¨é€Ÿï¼ˆç„¡åŠ æ¬Šå¹³å‡ï¼‰"""
    n_events = tc_hazard.intensity.shape[0]
    max_winds = np.zeros(n_events)
    
    for event_idx in range(n_events):
        wind_field = tc_hazard.intensity[event_idx, :].toarray().flatten()
        event_max_wind = 0.0
        
        for exp_coord in exposure_coords:
            # Cat-in-a-Circle: åŠå¾‘å…§çš„æœ€å¤§é¢¨é€Ÿ
            radius_rad = radius_km / 6371.0
            nearby_indices = hazard_tree.query_ball_point(
                np.radians(exp_coord), radius_rad
            )
            
            if len(nearby_indices) > 0:
                nearby_winds = wind_field[nearby_indices]
                nearby_winds = nearby_winds[nearby_winds > 0]
                
                if len(nearby_winds) > 0:
                    local_max = np.max(nearby_winds)
                    event_max_wind = max(event_max_wind, local_max)
        
        max_winds[event_idx] = event_max_wind
    
    return max_winds


# Bayesian integration helper functions

def create_vulnerability_data_from_climada_adapter(climada_adapter: CLIMADAInputAdapter):
    """
    å¾ CLIMADAInputAdapter å‰µå»º VulnerabilityData ç‰©ä»¶
    
    é€™å€‹å‡½æ•¸å°‡ CLIMADA çš„ç½å®³ã€æš´éšªå’Œæå¤±æ•¸æ“šè½‰æ›ç‚º @bayesian/ æ¨¡çµ„
    æ‰€éœ€çš„ VulnerabilityData æ ¼å¼ï¼Œæ”¯æ´è„†å¼±åº¦é—œä¿‚å»ºæ¨¡ã€‚
    
    Parameters:
    -----------
    climada_adapter : CLIMADAInputAdapter
        å·²åˆå§‹åŒ–çš„ CLIMADA è¼¸å…¥é©é…å™¨
        
    Returns:
    --------
    VulnerabilityData
        æ ¼å¼åŒ–çš„è„†å¼±åº¦æ•¸æ“šç‰©ä»¶ï¼ŒåŒ…å«ï¼š
        - hazard_intensities: ç½å®³å¼·åº¦æ•¸æ“š (H_ij)
        - exposure_values: æš´éšªå€¼æ•¸æ“š (E_i) 
        - observed_losses: è§€æ¸¬æå¤±æ•¸æ“š (L_ij)
    """
    try:
        # Import VulnerabilityData from bayesian module
        from bayesian.parametric_bayesian_hierarchy import VulnerabilityData
    except ImportError:
        raise ImportError("ç„¡æ³•å°å…¥ VulnerabilityDataï¼Œè«‹æª¢æŸ¥ @bayesian/ æ¨¡çµ„")
    
    print("ğŸ”„ å¾ CLIMADA é©é…å™¨æå–è„†å¼±åº¦æ•¸æ“š...")
    
    # 1. æå–ç½å®³å¼·åº¦ (H_ij) - ä½¿ç”¨ Cat-in-a-Circle åƒæ•¸æŒ‡æ¨™
    parametric_indices = climada_adapter.extract_parametric_indices()
    
    # é¸æ“‡æœ€å¤§é¢¨é€Ÿä½œç‚ºä¸»è¦ç½å®³å¼·åº¦æŒ‡æ¨™
    # å„ªå…ˆé¸æ“‡ 'max' çµ±è¨ˆçš„ Cat-in-a-Circle æŒ‡æ¨™
    hazard_intensities = None
    for key, values in parametric_indices.items():
        if 'max' in key.lower():
            hazard_intensities = values
            print(f"   ä½¿ç”¨ç½å®³æŒ‡æ¨™: {key}")
            break
    
    # å¦‚æœæ²’æœ‰æ‰¾åˆ° 'max' æŒ‡æ¨™ï¼Œä½¿ç”¨ç¬¬ä¸€å€‹å¯ç”¨çš„æŒ‡æ¨™
    if hazard_intensities is None and parametric_indices:
        key, hazard_intensities = next(iter(parametric_indices.items()))
        print(f"   å›é€€ä½¿ç”¨ç½å®³æŒ‡æ¨™: {key}")
    
    if hazard_intensities is None:
        raise ValueError("ç„¡æ³•å¾ CLIMADA é©é…å™¨ä¸­æå–ç½å®³å¼·åº¦æ•¸æ“š")
    
    # 2. æå–æš´éšªå€¼ (E_i)
    try:
        exposure_gdf = climada_adapter.exposure_main.gdf
        if 'value' in exposure_gdf.columns:
            # å–æš´éšªç¸½å€¼ä½œç‚ºä»£è¡¨æ€§æš´éšªå€¼
            total_exposure = np.sum(exposure_gdf['value'].values)
            # ç‚ºæ¯å€‹äº‹ä»¶é‡è¤‡ç›¸åŒçš„æš´éšªå€¼
            exposure_values = np.full(len(hazard_intensities), total_exposure)
            print(f"   æš´éšªç¸½å€¼: ${total_exposure/1e9:.2f}B")
        else:
            # ä½¿ç”¨å–®ä½æš´éšªå€¼
            exposure_values = np.ones(len(hazard_intensities))
            print("   ä½¿ç”¨å–®ä½æš´éšªå€¼")
    except Exception as e:
        print(f"   âš ï¸ æš´éšªå€¼æå–å¤±æ•—: {e}")
        exposure_values = np.ones(len(hazard_intensities))
    
    # 3. æå–è§€æ¸¬æå¤± (L_ij)
    observed_losses = climada_adapter.get_loss_data()
    
    # ç¢ºä¿æ•¸çµ„é•·åº¦ä¸€è‡´
    min_length = min(len(hazard_intensities), len(exposure_values), len(observed_losses))
    
    if min_length == 0:
        raise ValueError("æ•¸æ“šæ•¸çµ„ç‚ºç©ºï¼Œç„¡æ³•å‰µå»º VulnerabilityData")
    
    hazard_intensities = hazard_intensities[:min_length]
    exposure_values = exposure_values[:min_length]  
    observed_losses = observed_losses[:min_length]
    
    print(f"   âœ… æ•¸æ“šå°é½Šå®Œæˆ: {min_length} å€‹äº‹ä»¶")
    print(f"   ç½å®³å¼·åº¦ç¯„åœ: [{np.min(hazard_intensities):.1f}, {np.max(hazard_intensities):.1f}] m/s")
    print(f"   æå¤±ç¯„åœ: [${np.min(observed_losses)/1e6:.1f}M, ${np.max(observed_losses)/1e6:.1f}M]")
    
    # å‰µå»º VulnerabilityData ç‰©ä»¶
    vulnerability_data = VulnerabilityData(
        hazard_intensities=hazard_intensities,
        exposure_values=exposure_values,
        observed_losses=observed_losses
    )
    
    print("âœ… VulnerabilityData å‰µå»ºå®Œæˆ")
    
    return vulnerability_data


def compare_adapters_summary(adapters: Dict[str, InputAdapter]) -> pd.DataFrame:
    """
    æ¯”è¼ƒå¤šå€‹é©é…å™¨çš„æ‘˜è¦çµ±è¨ˆ
    
    Parameters:
    -----------
    adapters : Dict[str, InputAdapter]
        é©é…å™¨å­—å…¸ {name: adapter}
        
    Returns:
    --------
    pd.DataFrame
        æ¯”è¼ƒæ‘˜è¦è¡¨
    """
    summary_data = []
    
    for name, adapter in adapters.items():
        try:
            losses = adapter.get_loss_data()
            indices = adapter.extract_parametric_indices()
            metadata = adapter.get_event_metadata()
            
            summary_data.append({
                'adapter_name': name,
                'input_type': adapter.get_input_type(),
                'n_events': len(losses),
                'total_loss_billion': np.sum(losses) / 1e9,
                'max_loss_billion': np.max(losses) / 1e9,
                'mean_loss_million': np.mean(losses) / 1e6,
                'n_parametric_indices': len(indices),
                'parametric_index_types': list(indices.keys()),
                'data_availability': 'Complete'
            })
        except Exception as e:
            summary_data.append({
                'adapter_name': name,
                'input_type': 'ERROR',
                'error': str(e),
                'data_availability': 'Failed'
            })
    
    return pd.DataFrame(summary_data)
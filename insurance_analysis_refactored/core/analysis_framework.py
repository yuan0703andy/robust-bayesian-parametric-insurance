"""
Unified Analysis Framework
çµ±ä¸€åˆ†ææ¡†æ¶

This is the highest-level interface that integrates all insurance analysis components
into a single, powerful, and easy-to-use framework.
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass, field
from enum import Enum
import warnings

from .parametric_engine import (
    ParametricInsuranceEngine, ParametricProduct, ProductPerformance,
    ParametricIndexType, PayoutFunctionType
)
from .skill_evaluator import SkillScoreEvaluator, SkillScoreType, SkillScoreResult
from .product_manager import InsuranceProductManager
# Merged from unified_product_engine.py - Input Adapter functionality
from .input_adapters import InputAdapter, CLIMADAInputAdapter, BayesianInputAdapter
from .enhanced_spatial_analysis import EnhancedCatInCircleAnalyzer, create_standard_steinmann_config
from .saffir_simpson_products import (
    SaffirSimpsonProductGenerator, PayoutStructure, 
    generate_steinmann_2023_products, validate_steinmann_compatibility
)

# Merged from unified_product_engine.py - Evaluation and Product Design enums
class EvaluationMode(Enum):
    """è©•ä¼°æ¨¡å¼"""
    TRADITIONAL = "traditional"      # ç¢ºå®šæ€§è©•ä¼° (RMSE, correlation)
    PROBABILISTIC = "probabilistic"  # æ©Ÿç‡æ€§è©•ä¼° (CRPS, distribution-based)
    HYBRID = "hybrid"               # æ··åˆè©•ä¼°

class ProductDesignType(Enum):
    """ç”¢å“è¨­è¨ˆé¡å‹"""
    STEINMANN_70 = "steinmann_70"          # Steinmannæ¨™æº–70ç”¢å“
    CUSTOM_PRODUCTS = "custom_products"    # è‡ªå®šç¾©ç”¢å“
    OPTIMIZED_SELECTION = "optimized"      # å„ªåŒ–é¸æ“‡

class AnalysisType(Enum):
    """åˆ†æé¡å‹"""
    BASIC = "basic"
    COMPREHENSIVE = "comprehensive"
    STEINMANN = "steinmann"
    BAYESIAN = "bayesian"
    ROBUST_BAYESIAN = "robust_bayesian"
    COMPARATIVE = "comparative"

@dataclass
class AnalysisConfig:
    """åˆ†æé…ç½®"""
    analysis_type: AnalysisType
    skill_scores: List[SkillScoreType] = field(default_factory=lambda: [
        SkillScoreType.RMSE, SkillScoreType.MAE, SkillScoreType.CORRELATION,
        SkillScoreType.CRPS, SkillScoreType.EDI, SkillScoreType.TSS
    ])
    bootstrap_enabled: bool = True
    confidence_level: float = 0.95
    max_products: int = 70
    optimization_criteria: List[str] = field(default_factory=lambda: ['rmse', 'correlation', 'coverage_ratio'])
    technical_premium_config: Dict[str, float] = field(default_factory=lambda: {
        'risk_load_factor': 0.20,
        'expense_load_factor': 0.15
    })
    # æ–°å¢ï¼šå¿«é€Ÿæ¸¬è©¦æ¨¡å¼è¨­å®š
    fast_mode: bool = False
    skill_score_sample_size: Optional[int] = None  # é™åˆ¶æŠ€èƒ½è©•åˆ†è¨ˆç®—çš„ç”¢å“æ•¸é‡
    
    # æ–°å¢ï¼šè²æ°åˆ†æè¨­å®š
    bayesian_config: Optional[Any] = None
    enable_robustness_analysis: bool = False
    enable_bayesian_visualization: bool = False
    
    # Merged from unified_product_engine.py - Product Design Configuration
    design_type: ProductDesignType = ProductDesignType.STEINMANN_70
    evaluation_mode: EvaluationMode = EvaluationMode.TRADITIONAL
    
    # Cat-in-a-Circle é…ç½®
    spatial_radii_km: List[float] = field(default_factory=lambda: [15.0, 30.0, 50.0])
    spatial_statistics: List[str] = field(default_factory=lambda: ['max', 'mean', '95th'])
    
    # è©•ä¼°é…ç½®
    enable_bootstrap_ci: bool = True
    
    # è¼¸å‡ºé…ç½®
    enable_visualization: bool = True
    save_intermediate_results: bool = True
    output_format: str = "comprehensive"  # "minimal", "standard", "comprehensive"

    @classmethod
    def create_fast_test_config(cls, analysis_type: AnalysisType = AnalysisType.BASIC) -> 'AnalysisConfig':
        """å‰µå»ºå¿«é€Ÿæ¸¬è©¦é…ç½®"""
        return cls(
            analysis_type=analysis_type,
            skill_scores=[SkillScoreType.RMSE, SkillScoreType.CORRELATION],  # åªè¨ˆç®—åŸºæœ¬è©•åˆ†
            bootstrap_enabled=False,  # é—œé–‰bootstrapä»¥åŠ é€Ÿ
            max_products=20,  # æ¸›å°‘ç”¢å“æ•¸é‡
            fast_mode=True,
            skill_score_sample_size=5  # åªç‚º5å€‹ç”¢å“è¨ˆç®—æŠ€èƒ½è©•åˆ†
        )
    
    @classmethod 
    def create_robust_bayesian_config(cls) -> 'AnalysisConfig':
        """å‰µå»ºç©©å¥è²æ°åˆ†æé…ç½®"""
        # Updated to use new bayesian module
        try:
            from bayesian import HierarchicalModelConfig as BayesianConfig
            # Define simplified enums for backward compatibility
            from enum import Enum
            class PriorScenario(Enum):
                NORMAL = "normal"
                GAMMA = "gamma"
            class LikelihoodType(Enum):
                NORMAL = "normal"
                GAMMA = "gamma"
        except ImportError:
            # Fallback definitions
            from enum import Enum
            class BayesianConfig:
                pass
            class PriorScenario(Enum):
                NORMAL = "normal"
            class LikelihoodType(Enum):
                NORMAL = "normal"
        
        bayesian_config = BayesianConfig(
            n_mcmc_samples=5000,
            n_burn_in=1000,
            prior_scenarios=[PriorScenario.NEUTRAL, PriorScenario.OPTIMISTIC, PriorScenario.PESSIMISTIC],
            likelihood_types=[LikelihoodType.NORMAL, LikelihoodType.T_DISTRIBUTION],
            ensemble_size=50,
            robust_optimization=True
        )
        
        return cls(
            analysis_type=AnalysisType.ROBUST_BAYESIAN,
            skill_scores=[SkillScoreType.CRPS, SkillScoreType.CRPSS, SkillScoreType.RMSE, SkillScoreType.CORRELATION],
            bootstrap_enabled=True,
            max_products=50,
            bayesian_config=bayesian_config,
            enable_robustness_analysis=True,
            enable_bayesian_visualization=True
        )

@dataclass
class AnalysisResults:
    """åˆ†æçµæœ"""
    analysis_type: AnalysisType
    products: List[ParametricProduct]
    performance_results: pd.DataFrame
    skill_score_results: Dict[str, Dict[SkillScoreType, SkillScoreResult]]
    optimization_results: pd.DataFrame
    technical_premium_results: pd.DataFrame
    best_products: Dict[str, ParametricProduct]
    summary_statistics: Dict[str, Any]
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # æ–°å¢ï¼šè²æ°åˆ†æçµæœ
    bayesian_results: Optional[Dict[str, 'BayesianAnalysisResult']] = None
    robustness_analysis: Optional[Dict[str, Any]] = None
    loss_distributions: Optional[Dict[str, Dict[str, 'ProbabilisticLossDistribution']]] = None

class UnifiedAnalysisFramework:
    """
    çµ±ä¸€åˆ†ææ¡†æ¶
    
    é€™æ˜¯æœ€é«˜ç´šåˆ¥çš„ä»‹é¢ï¼Œæ•´åˆæ‰€æœ‰ä¿éšªåˆ†æçµ„ä»¶ï¼Œ
    æä¾›çµ±ä¸€ã€å¼·å¤§ä¸”æ˜“æ–¼ä½¿ç”¨çš„åˆ†ææ¡†æ¶ã€‚
    """
    
    def __init__(self, config: Optional[AnalysisConfig] = None):
        """
        åˆå§‹åŒ–çµ±ä¸€åˆ†ææ¡†æ¶
        
        Parameters:
        -----------
        config : AnalysisConfig, optional
            åˆ†æé…ç½®
        """
        self.config = config or AnalysisConfig(AnalysisType.COMPREHENSIVE)
        
        # åˆå§‹åŒ–æ ¸å¿ƒçµ„ä»¶
        self.parametric_engine = ParametricInsuranceEngine()
        self.skill_evaluator = SkillScoreEvaluator(
            bootstrap_samples=1000 if self.config.bootstrap_enabled else 0,
            confidence_level=self.config.confidence_level
        )
        self.product_manager = InsuranceProductManager()
        
        # åˆå§‹åŒ–è²æ°åˆ†æçµ„ä»¶
        if self.config.bayesian_config is not None:
            # Updated to use new bayesian module
            from bayesian import RobustBayesianAnalyzer
            # Bayesian visualization moved to visualization/ module
            BayesianVisualization = None  # Placeholder
            
            self.bayesian_analyzer = RobustBayesianAnalyzer(self.config.bayesian_config)
            if self.config.enable_bayesian_visualization:
                self.bayesian_visualizer = BayesianVisualization()
            else:
                self.bayesian_visualizer = None
        else:
            self.bayesian_analyzer = None
            self.bayesian_visualizer = None
        
        # Merged from unified_product_engine.py - Additional components
        self.spatial_analyzer = EnhancedCatInCircleAnalyzer(
            create_standard_steinmann_config()
        )
        self.product_generator = SaffirSimpsonProductGenerator()
        
        # é‹è¡Œçµ±è¨ˆ
        self.run_statistics = {
            'total_analyses': 0,
            'successful_runs': 0,
            'failed_runs': 0,
            'average_computation_time': 0.0
        }
        
        # åˆ†æçµæœç·©å­˜
        self.analysis_cache = {}
    
    def run_comprehensive_analysis(self,
                                 parametric_indices: np.ndarray,
                                 observed_losses: np.ndarray,
                                 analysis_name: str = "comprehensive_analysis") -> AnalysisResults:
        """
        åŸ·è¡Œå…¨é¢åˆ†æ
        
        Parameters:
        -----------
        parametric_indices : np.ndarray
            åƒæ•¸æŒ‡æ¨™æ•¸æ“š
        observed_losses : np.ndarray
            è§€æ¸¬æå¤±æ•¸æ“š
        analysis_name : str
            åˆ†æåç¨±
            
        Returns:
        --------
        AnalysisResults
            å®Œæ•´åˆ†æçµæœ
        """
        
        print(f"ğŸš€ åŸ·è¡Œå…¨é¢åˆ†æ: {analysis_name}")
        print("=" * 60)
        
        # 1. ç”Ÿæˆç”¢å“çµ„åˆ
        print("ğŸ“¦ éšæ®µ1: ç”Ÿæˆåƒæ•¸å‹ä¿éšªç”¢å“...")
        max_payout = np.max(observed_losses) * 2 if np.any(observed_losses > 0) else 1e9
        
        if self.config.analysis_type == AnalysisType.STEINMANN:
            products = self._generate_steinmann_products_helper(
                parametric_indices, max_payout, self.config.max_products, observed_losses
            )
        else:
            # ç”Ÿæˆå¤šæ¨£åŒ–ç”¢å“çµ„åˆ
            products = self._generate_diverse_products(parametric_indices, max_payout)
        
        print(f"   âœ… ç”Ÿæˆäº† {len(products)} å€‹ç”¢å“")
        
        # 2. è©•ä¼°ç”¢å“ç¸¾æ•ˆ
        print("ğŸ“Š éšæ®µ2: è©•ä¼°ç”¢å“ç¸¾æ•ˆ...")
        performance_results = []
        
        for i, product in enumerate(products):
            if (i + 1) % 20 == 0:
                print(f"   é€²åº¦: {i+1}/{len(products)}")
            
            performance = self.parametric_engine.evaluate_product_performance(
                product, parametric_indices, observed_losses
            )
            
            performance_dict = {
                'product_id': product.product_id,
                'name': product.name,
                'description': product.description,
                'category': product.metadata.get('category', 'unknown'),
                'rmse': performance.rmse,
                'mae': performance.mae,
                'correlation': performance.correlation,
                'hit_rate': performance.hit_rate,
                'false_alarm_rate': performance.false_alarm_rate,
                'coverage_ratio': performance.coverage_ratio,
                'basis_risk': performance.basis_risk,
                **performance.technical_metrics
            }
            
            performance_results.append(performance_dict)
        
        performance_df = pd.DataFrame(performance_results)
        print(f"   âœ… å®Œæˆç¸¾æ•ˆè©•ä¼°")
        
        # 3. è¨ˆç®—æŠ€èƒ½è©•åˆ†
        print("ğŸ¯ éšæ®µ3: è¨ˆç®—æŠ€èƒ½è©•åˆ†...")
        skill_score_results = {}
        
        # æ±ºå®šè¦è¨ˆç®—æŠ€èƒ½è©•åˆ†çš„ç”¢å“æ•¸é‡
        if self.config.skill_score_sample_size is not None:
            skill_score_limit = min(self.config.skill_score_sample_size, len(products))
        elif self.config.fast_mode:
            skill_score_limit = len(products)  # å¿«é€Ÿæ¨¡å¼ä¹Ÿè©•ä¼°å…¨éƒ¨ç”¢å“
        else:
            skill_score_limit = len(products)  # è©•ä¼°å…¨éƒ¨ç”¢å“
            
        products_for_skill_scores = products[:skill_score_limit]
        
        print(f"   è¨ˆç®— {len(products_for_skill_scores)} å€‹ç”¢å“çš„æŠ€èƒ½è©•åˆ†...")
        
        for i, product in enumerate(products_for_skill_scores):
            print(f"   é€²åº¦: {i+1}/{len(products_for_skill_scores)} - {product.name[:30]}...")
            
            try:
                # é‡æ–°è¨ˆç®—é æ¸¬
                payout_func_class = self.parametric_engine.payout_function_classes[product.payout_function_type]
                payout_func = payout_func_class(
                    product.trigger_thresholds,
                    product.payout_amounts,
                    product.max_payout
                )
                predictions = payout_func.calculate_payouts_batch(parametric_indices)
                
                # è¨ˆç®—æŠ€èƒ½è©•åˆ†
                scores = self.skill_evaluator.evaluate_multiple_scores(
                    self.config.skill_scores,
                    observed_losses,
                    predictions
                )
                
                skill_score_results[product.product_id] = scores
                
            except Exception as e:
                warnings.warn(f"Failed to calculate skill scores for {product.product_id}: {e}")
                continue
        
        print(f"   âœ… å®Œæˆ {len(skill_score_results)} å€‹ç”¢å“çš„æŠ€èƒ½è©•åˆ†")
        
        # 4. ç”¢å“å„ªåŒ–
        print("ğŸ† éšæ®µ4: ç”¢å“çµ„åˆå„ªåŒ–...")
        optimization_results = self.parametric_engine.optimize_product_portfolio(
            products, parametric_indices, observed_losses, self.config.optimization_criteria
        )
        print(f"   âœ… å®Œæˆç”¢å“å„ªåŒ–")
        
        # 5. æŠ€è¡“ä¿è²»è¨ˆç®—
        print("ğŸ’° éšæ®µ5: æŠ€è¡“ä¿è²»è¨ˆç®—...")
        premium_results = []
        
        for product in products:  # ç‚ºå…¨éƒ¨ç”¢å“è¨ˆç®—ä¿è²»
            performance = self.parametric_engine.performance_cache.get(product.product_id)
            if performance:
                premium = self.parametric_engine.calculate_technical_premium(
                    product, performance, **self.config.technical_premium_config
                )
                
                premium_result = {
                    'product_id': product.product_id,
                    'name': product.name,
                    **premium
                }
                premium_results.append(premium_result)
        
        premium_df = pd.DataFrame(premium_results)
        print(f"   âœ… å®ŒæˆæŠ€è¡“ä¿è²»è¨ˆç®—")
        
        # 6. è­˜åˆ¥æœ€ä½³ç”¢å“
        print("ğŸ–ï¸ éšæ®µ6: è­˜åˆ¥æœ€ä½³ç”¢å“...")
        best_products = self._identify_best_products(optimization_results, products)
        
        # 7. ç”Ÿæˆæ‘˜è¦çµ±è¨ˆ
        summary_stats = self._generate_summary_statistics(
            products, performance_df, optimization_results, skill_score_results
        )
        
        # å‰µå»ºåˆ†æçµæœ
        results = AnalysisResults(
            analysis_type=self.config.analysis_type,
            products=products,
            performance_results=performance_df,
            skill_score_results=skill_score_results,
            optimization_results=optimization_results,
            technical_premium_results=premium_df,
            best_products=best_products,
            summary_statistics=summary_stats,
            metadata={
                'analysis_name': analysis_name,
                'n_events': len(parametric_indices),
                'n_products': len(products),
                'config': self.config
            }
        )
        
        # ç·©å­˜çµæœ
        self.analysis_cache[analysis_name] = results
        
        print("âœ… å…¨é¢åˆ†æå®Œæˆ!")
        self._print_summary_report(results)
        
        return results
    
    def run_steinmann_analysis(self,
                             parametric_indices: np.ndarray,
                             observed_losses: np.ndarray) -> AnalysisResults:
        """
        åŸ·è¡ŒSteinmann et al. (2023)æ¨™æº–åˆ†æ
        
        Parameters:
        -----------
        parametric_indices : np.ndarray
            åƒæ•¸æŒ‡æ¨™æ•¸æ“š
        observed_losses : np.ndarray
            è§€æ¸¬æå¤±æ•¸æ“š
            
        Returns:
        --------
        AnalysisResults
            Steinmannåˆ†æçµæœ
        """
        
        # è¨­ç½®Steinmanné…ç½®
        steinmann_config = AnalysisConfig(
            analysis_type=AnalysisType.STEINMANN,
            max_products=70,
            skill_scores=[SkillScoreType.RMSE, SkillScoreType.MAE, SkillScoreType.CORRELATION, 
                         SkillScoreType.CRPS, SkillScoreType.EDI, SkillScoreType.TSS]
        )
        
        original_config = self.config
        self.config = steinmann_config
        
        try:
            results = self.run_comprehensive_analysis(
                parametric_indices, observed_losses, "steinmann_analysis"
            )
            results.analysis_type = AnalysisType.STEINMANN
            return results
        finally:
            self.config = original_config
    
    def run_robust_bayesian_analysis(self,
                                   parametric_indices: np.ndarray,
                                   observed_losses: np.ndarray,
                                   hazard_intensities: np.ndarray,
                                   exposure_values: np.ndarray,
                                   vulnerability_params: Dict[str, float] = None,
                                   analysis_name: str = "robust_bayesian_analysis") -> AnalysisResults:
        """
        åŸ·è¡Œç©©å¥è²æ°åˆ†æ
        
        å¯¦ç¾å¾ç¢ºå®šæ€§æ€ç¶­åˆ°æ©Ÿç‡æ€§æ€ç¶­çš„å®Œæ•´è½‰æ›ï¼ŒåŒ…æ‹¬ï¼š
        1. ç”Ÿæˆæå¤±å¾Œé©—é æ¸¬åˆ†ä½ˆ
        2. CRPS-based ç”¢å“å„ªåŒ–
        3. ç©©å¥æ€§åˆ†æ
        
        Parameters:
        -----------
        parametric_indices : np.ndarray
            åƒæ•¸æŒ‡æ¨™æ•¸æ“š
        observed_losses : np.ndarray
            è§€æ¸¬æå¤±æ•¸æ“š
        hazard_intensities : np.ndarray
            ç½å®³å¼·åº¦æ•¸æ“š
        exposure_values : np.ndarray
            æ›éšªåƒ¹å€¼æ•¸æ“š
        vulnerability_params : Dict[str, float], optional
            è„†å¼±åº¦å‡½æ•¸åƒæ•¸
        analysis_name : str
            åˆ†æåç¨±
            
        Returns:
        --------
        AnalysisResults
            å®Œæ•´çš„è²æ°åˆ†æçµæœ
        """
        
        if self.bayesian_analyzer is None:
            raise ValueError("è²æ°åˆ†æå™¨æœªåˆå§‹åŒ–ã€‚è«‹åœ¨é…ç½®ä¸­æä¾›bayesian_configã€‚")
        
        print(f"ğŸ¯ åŸ·è¡Œç©©å¥è²æ°åˆ†æ: {analysis_name}")
        print("=" * 70)
        
        # è¨­ç½®é»˜èªè„†å¼±åº¦åƒæ•¸
        if vulnerability_params is None:
            vulnerability_params = {
                'damage_threshold': 17.5,
                'beta': 3.0,
                'vulnerability_factor': 1.0
            }
        
        # 1. é¦–å…ˆåŸ·è¡Œå‚³çµ±åˆ†æä»¥ç²å¾—ç”¢å“å€™é¸
        print("ğŸ“¦ éšæ®µ1: ç”Ÿæˆç”¢å“å€™é¸...")
        max_payout = np.max(observed_losses) * 2 if np.any(observed_losses > 0) else 1e9
        
        if self.config.analysis_type == AnalysisType.ROBUST_BAYESIAN:
            # ä½¿ç”¨è¼ƒå°‘ç”¢å“é€²è¡Œè²æ°åˆ†æä»¥ç¯€çœè¨ˆç®—æ™‚é–“
            products = self._generate_steinmann_products_helper(
                parametric_indices, max_payout, min(30, self.config.max_products)
            )
        else:
            products = self._generate_diverse_products(parametric_indices, max_payout)
        
        print(f"   âœ… ç”Ÿæˆäº† {len(products)} å€‹å€™é¸ç”¢å“")
        
        # 2. ç”Ÿæˆæå¤±å¾Œé©—é æ¸¬åˆ†ä½ˆ
        print("\nğŸ² éšæ®µ2: ç”Ÿæˆæå¤±å¾Œé©—é æ¸¬åˆ†ä½ˆ...")
        event_ids = [f"event_{i}" for i in range(len(parametric_indices))]
        
        loss_distributions = self.bayesian_analyzer.generate_posterior_predictive_distributions(
            hazard_intensities=hazard_intensities,
            exposure_values=exposure_values,
            vulnerability_params=vulnerability_params,
            event_ids=event_ids
        )
        
        # 3. ä½¿ç”¨CRPSå„ªåŒ–ç”¢å“
        print("\nğŸ¯ éšæ®µ3: CRPS-based ç”¢å“å„ªåŒ–...")
        bayesian_optimization_results = self.bayesian_analyzer.optimize_products_with_crps(
            product_candidates=products,
            loss_distributions=loss_distributions,
            parametric_indices=parametric_indices
        )
        
        # 4. åŸ·è¡Œç©©å¥æ€§åˆ†æ
        robustness_results = None
        if self.config.enable_robustness_analysis:
            print("\nğŸ›¡ï¸ éšæ®µ4: ç©©å¥æ€§åˆ†æ...")
            robustness_results = self.bayesian_analyzer.perform_robustness_analysis(
                bayesian_optimization_results
            )
        
        # 5. åŸ·è¡Œå‚³çµ±ç¸¾æ•ˆè©•ä¼°ç”¨æ–¼æ¯”è¼ƒ
        print("\nğŸ“Š éšæ®µ5: å‚³çµ±ç¸¾æ•ˆè©•ä¼°...")
        performance_results = []
        
        for product in products:
            performance = self.parametric_engine.evaluate_product_performance(
                product, parametric_indices, observed_losses
            )
            
            performance_dict = {
                'product_id': product.product_id,
                'name': product.name,
                'description': product.description,
                'category': product.metadata.get('category', 'unknown'),
                'rmse': performance.rmse,
                'mae': performance.mae,
                'correlation': performance.correlation,
                'hit_rate': performance.hit_rate,
                'false_alarm_rate': performance.false_alarm_rate,
                'coverage_ratio': performance.coverage_ratio,
                'basis_risk': performance.basis_risk,
                **performance.technical_metrics
            }
            
            performance_results.append(performance_dict)
        
        performance_df = pd.DataFrame(performance_results)
        
        # 6. è­˜åˆ¥æœ€ä½³ç”¢å“ï¼ˆè²æ°vså‚³çµ±ï¼‰
        print("\nğŸ† éšæ®µ6: è­˜åˆ¥æœ€ä½³ç”¢å“...")
        best_products = {}
        
        # å‚³çµ±æœ€ä½³ç”¢å“
        if not performance_df.empty:
            best_rmse_idx = performance_df['rmse'].idxmin()
            best_corr_idx = performance_df['correlation'].idxmax()
            best_products['traditional_best_rmse'] = products[best_rmse_idx]
            best_products['traditional_best_correlation'] = products[best_corr_idx]
        
        # è²æ°æœ€ä½³ç”¢å“
        if robustness_results and 'robust_products' in robustness_results:
            robust_product_ids = robustness_results['robust_products']
            if robust_product_ids:
                robust_product = next((p for p in products if p.product_id == robust_product_ids[0]), None)
                if robust_product:
                    best_products['bayesian_most_robust'] = robust_product
        
        # 7. ç”Ÿæˆæ‘˜è¦çµ±è¨ˆ
        summary_stats = self._generate_bayesian_summary_statistics(
            products, performance_df, bayesian_optimization_results, robustness_results
        )
        
        # 8. å‰µå»ºåˆ†æçµæœ
        results = AnalysisResults(
            analysis_type=AnalysisType.ROBUST_BAYESIAN,
            products=products,
            performance_results=performance_df,
            skill_score_results={},  # åœ¨è²æ°åˆ†æä¸­ç”±CRPSå–ä»£
            optimization_results=pd.DataFrame(),  # ç”±è²æ°çµæœå–ä»£
            technical_premium_results=pd.DataFrame(),
            best_products=best_products,
            summary_statistics=summary_stats,
            bayesian_results=bayesian_optimization_results,
            robustness_analysis=robustness_results,
            loss_distributions=loss_distributions,
            metadata={
                'analysis_name': analysis_name,
                'n_events': len(parametric_indices),
                'n_products': len(products),
                'config': self.config,
                'bayesian_config': self.config.bayesian_config
            }
        )
        
        # ç·©å­˜çµæœ
        self.analysis_cache[analysis_name] = results
        
        print("âœ… ç©©å¥è²æ°åˆ†æå®Œæˆ!")
        self._print_bayesian_summary_report(results)
        
        return results
    
    def compare_methods(self,
                       parametric_indices: np.ndarray,
                       observed_losses: np.ndarray,
                       method_results: Dict[str, AnalysisResults]) -> Dict[str, Any]:
        """
        æ¯”è¼ƒä¸åŒæ–¹æ³•çš„åˆ†æçµæœ
        
        Parameters:
        -----------
        parametric_indices : np.ndarray
            åƒæ•¸æŒ‡æ¨™æ•¸æ“š
        observed_losses : np.ndarray
            è§€æ¸¬æå¤±æ•¸æ“š
        method_results : Dict[str, AnalysisResults]
            æ–¹æ³•çµæœå­—å…¸
            
        Returns:
        --------
        Dict[str, Any]
            æ¯”è¼ƒçµæœ
        """
        
        print("ğŸ† åŸ·è¡Œæ–¹æ³•æ¯”è¼ƒåˆ†æ")
        print("=" * 50)
        
        comparison_results = {
            'method_performance': {},
            'best_products_by_method': {},
            'skill_score_comparison': {},
            'statistical_tests': {},
            'recommendations': []
        }
        
        # 1. æ–¹æ³•è¡¨ç¾æ¯”è¼ƒ
        for method_name, results in method_results.items():
            perf_df = results.performance_results
            
            comparison_results['method_performance'][method_name] = {
                'n_products': len(results.products),
                'best_rmse': perf_df['rmse'].min(),
                'best_correlation': perf_df['correlation'].max(),
                'mean_rmse': perf_df['rmse'].mean(),
                'mean_correlation': perf_df['correlation'].mean(),
                'triggered_products': len(perf_df[perf_df['payout_frequency'] > 0])
            }
            
            # æœ€ä½³ç”¢å“
            best_idx = perf_df['rmse'].idxmin()
            comparison_results['best_products_by_method'][method_name] = perf_df.loc[best_idx].to_dict()
        
        # 2. æŠ€èƒ½è©•åˆ†æ¯”è¼ƒ
        skill_comparison = {}
        for score_type in self.config.skill_scores:
            skill_comparison[score_type.value] = {}
            
            for method_name, results in method_results.items():
                scores = []
                for product_scores in results.skill_score_results.values():
                    if score_type in product_scores:
                        scores.append(product_scores[score_type].value)
                
                if scores:
                    skill_comparison[score_type.value][method_name] = {
                        'mean': np.mean(scores),
                        'std': np.std(scores),
                        'min': np.min(scores),
                        'max': np.max(scores)
                    }
        
        comparison_results['skill_score_comparison'] = skill_comparison
        
        # 3. ç”Ÿæˆå»ºè­°
        recommendations = self._generate_method_recommendations(comparison_results)
        comparison_results['recommendations'] = recommendations
        
        print("âœ… æ–¹æ³•æ¯”è¼ƒåˆ†æå®Œæˆ!")
        self._print_comparison_report(comparison_results)
        
        return comparison_results
    
    def export_results(self,
                      results: AnalysisResults,
                      output_path: str,
                      format: str = "excel") -> None:
        """
        å°å‡ºåˆ†æçµæœ
        
        Parameters:
        -----------
        results : AnalysisResults
            åˆ†æçµæœ
        output_path : str
            è¼¸å‡ºè·¯å¾‘
        format : str
            è¼¸å‡ºæ ¼å¼ ('excel', 'csv', 'json')
        """
        
        if format.lower() == "excel":
            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                results.performance_results.to_excel(writer, sheet_name='Performance', index=False)
                results.optimization_results.to_excel(writer, sheet_name='Optimization', index=False)
                results.technical_premium_results.to_excel(writer, sheet_name='Premium', index=False)
                
                # æ‘˜è¦çµ±è¨ˆ
                summary_df = pd.DataFrame([results.summary_statistics])
                summary_df.to_excel(writer, sheet_name='Summary', index=False)
        
        elif format.lower() == "csv":
            base_path = output_path.replace('.csv', '')
            results.performance_results.to_csv(f"{base_path}_performance.csv", index=False)
            results.optimization_results.to_csv(f"{base_path}_optimization.csv", index=False)
            results.technical_premium_results.to_csv(f"{base_path}_premium.csv", index=False)
        
        print(f"âœ… çµæœå·²å°å‡ºè‡³: {output_path}")
    
    # ========== ç§æœ‰æ–¹æ³• ==========
    
    def _generate_diverse_products(self, parametric_indices: np.ndarray, max_payout: float) -> List[ParametricProduct]:
        """ç”Ÿæˆå¤šæ¨£åŒ–çš„ç”¢å“çµ„åˆ"""
        products = []
        
        # åŸºæœ¬Steinmannç”¢å“
        steinmann_products = self._generate_steinmann_products_helper(
            parametric_indices, max_payout, 50
        )
        products.extend(steinmann_products)
        
        # æ·»åŠ ç·šæ€§ç”¢å“
        non_zero_indices = parametric_indices[parametric_indices > 0]
        if len(non_zero_indices) > 0:
            min_threshold = np.percentile(non_zero_indices, 10)
            max_threshold = np.percentile(non_zero_indices, 90)
            
            for i in range(10):
                linear_product = self.parametric_engine.create_parametric_product(
                    product_id=f"linear_{i}",
                    name=f"Linear Product {i}",
                    description=f"ç·šæ€§ç”¢å“_{i}",
                    index_type=ParametricIndexType.CAT_IN_CIRCLE,
                    payout_function_type=PayoutFunctionType.LINEAR,
                    trigger_thresholds=[min_threshold, max_threshold],
                    payout_amounts=[0, max_payout],
                    max_payout=max_payout,
                    category='linear'
                )
                products.append(linear_product)
        
        return products
    
    def _identify_best_products(self, optimization_results: pd.DataFrame, 
                              products: List[ParametricProduct]) -> Dict[str, ParametricProduct]:
        """è­˜åˆ¥æœ€ä½³ç”¢å“"""
        best_products = {}
        
        if not optimization_results.empty:
            # æŒ‰ä¸åŒæ¨™æº–æ‰¾æœ€ä½³ç”¢å“
            criteria_mapping = {
                'best_overall': 'composite_score',
                'best_rmse': 'rmse',
                'best_correlation': 'correlation',
                'best_coverage': 'coverage_ratio'
            }
            
            products_dict = {p.product_id: p for p in products}
            
            for label, criterion in criteria_mapping.items():
                if criterion in optimization_results.columns:
                    if criterion == 'rmse':
                        best_idx = optimization_results[criterion].idxmin()
                    else:
                        best_idx = optimization_results[criterion].idxmax()
                    
                    best_product_id = optimization_results.loc[best_idx, 'product_id']
                    if best_product_id in products_dict:
                        best_products[label] = products_dict[best_product_id]
        
        return best_products
    
    def _generate_summary_statistics(self, products: List[ParametricProduct], 
                                   performance_df: pd.DataFrame,
                                   optimization_df: pd.DataFrame,
                                   skill_scores: Dict[str, Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆæ‘˜è¦çµ±è¨ˆ"""
        summary = {
            'total_products': len(products),
            'product_categories': {},
            'performance_summary': {},
            'optimization_summary': {},
            'skill_score_summary': {}
        }
        
        # ç”¢å“é¡åˆ¥çµ±è¨ˆ
        categories = {}
        for product in products:
            category = product.metadata.get('category', 'unknown')
            categories[category] = categories.get(category, 0) + 1
        summary['product_categories'] = categories
        
        # ç¸¾æ•ˆæ‘˜è¦
        if not performance_df.empty:
            summary['performance_summary'] = {
                'best_rmse': performance_df['rmse'].min(),
                'worst_rmse': performance_df['rmse'].max(),
                'mean_rmse': performance_df['rmse'].mean(),
                'best_correlation': performance_df['correlation'].max(),
                'mean_correlation': performance_df['correlation'].mean(),
                'triggered_products': len(performance_df[performance_df['payout_frequency'] > 0])
            }
        
        # å„ªåŒ–æ‘˜è¦
        if not optimization_df.empty:
            summary['optimization_summary'] = {
                'best_composite_score': optimization_df['composite_score'].max(),
                'mean_composite_score': optimization_df['composite_score'].mean(),
                'top_10_mean_score': optimization_df.head(10)['composite_score'].mean()
            }
        
        # æŠ€èƒ½è©•åˆ†æ‘˜è¦
        if skill_scores:
            score_summaries = {}
            for score_type in self.config.skill_scores:
                values = []
                for product_scores in skill_scores.values():
                    if score_type in product_scores:
                        values.append(product_scores[score_type].value)
                
                if values:
                    score_summaries[score_type.value] = {
                        'mean': np.mean(values),
                        'std': np.std(values),
                        'min': np.min(values),
                        'max': np.max(values)
                    }
            
            summary['skill_score_summary'] = score_summaries
        
        return summary
    
    def _generate_method_recommendations(self, comparison_results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ–¹æ³•å»ºè­°"""
        recommendations = []
        
        method_perf = comparison_results.get('method_performance', {})
        
        if method_perf:
            # æ‰¾å‡ºæœ€ä½³RMSEæ–¹æ³•
            best_rmse_method = min(method_perf.keys(), 
                                 key=lambda x: method_perf[x]['best_rmse'])
            recommendations.append(f"RMSEè¡¨ç¾æœ€ä½³: {best_rmse_method}")
            
            # æ‰¾å‡ºæœ€ä½³ç›¸é—œæ€§æ–¹æ³•
            best_corr_method = max(method_perf.keys(), 
                                 key=lambda x: method_perf[x]['best_correlation'])
            recommendations.append(f"ç›¸é—œæ€§è¡¨ç¾æœ€ä½³: {best_corr_method}")
            
            # è§¸ç™¼ç‡åˆ†æ
            for method, stats in method_perf.items():
                trigger_rate = stats['triggered_products'] / stats['n_products'] * 100
                recommendations.append(f"{method}ç”¢å“è§¸ç™¼ç‡: {trigger_rate:.1f}%")
        
        return recommendations
    
    def _generate_bayesian_summary_statistics(self, products: List[ParametricProduct], 
                                            performance_df: pd.DataFrame,
                                            bayesian_results: Dict[str, 'BayesianAnalysisResult'],
                                            robustness_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆè²æ°åˆ†ææ‘˜è¦çµ±è¨ˆ"""
        summary = {
            'total_products': len(products),
            'product_categories': {},
            'bayesian_summary': {},
            'robustness_summary': {},
            'comparison_summary': {}
        }
        
        # ç”¢å“é¡åˆ¥çµ±è¨ˆ
        categories = {}
        for product in products:
            category = product.metadata.get('category', 'unknown')
            categories[category] = categories.get(category, 0) + 1
        summary['product_categories'] = categories
        
        # è²æ°åˆ†ææ‘˜è¦
        if bayesian_results:
            all_crps_scores = []
            scenario_summaries = {}
            
            for scenario_key, result in bayesian_results.items():
                crps_values = list(result.crps_scores.values())
                scenario_summaries[scenario_key] = {
                    'best_crps': min(crps_values) if crps_values else 0,
                    'mean_crps': np.mean(crps_values) if crps_values else 0,
                    'worst_crps': max(crps_values) if crps_values else 0
                }
                all_crps_scores.extend(crps_values)
            
            summary['bayesian_summary'] = {
                'n_scenarios': len(bayesian_results),
                'overall_best_crps': min(all_crps_scores) if all_crps_scores else 0,
                'overall_mean_crps': np.mean(all_crps_scores) if all_crps_scores else 0,
                'scenario_details': scenario_summaries
            }
        
        # ç©©å¥æ€§åˆ†ææ‘˜è¦
        if robustness_results:
            robust_products = robustness_results.get('robust_products', [])
            robustness_metrics = robustness_results.get('robustness_metrics', {})
            
            summary['robustness_summary'] = {
                'n_robust_products': len(robust_products),
                'robust_products': robust_products,  # å…¨éƒ¨å¥å£¯ç”¢å“
                'avg_robustness_cv': np.mean([m.get('coefficient_of_variation', 0) 
                                            for m in robustness_metrics.values()]) if robustness_metrics else 0
            }
        
        # å‚³çµ±vsè²æ°æ¯”è¼ƒ
        if not performance_df.empty and bayesian_results:
            traditional_best_rmse = performance_df['rmse'].min()
            traditional_best_corr = performance_df['correlation'].max()
            
            summary['comparison_summary'] = {
                'traditional_best_rmse': traditional_best_rmse,
                'traditional_best_correlation': traditional_best_corr,
                'bayesian_provides_robustness': len(robustness_results.get('robust_products', [])) > 0 if robustness_results else False
            }
        
        return summary
    
    def _print_bayesian_summary_report(self, results: AnalysisResults) -> None:
        """æ‰“å°è²æ°åˆ†ææ‘˜è¦å ±å‘Š"""
        print(f"\nğŸ“‹ ç©©å¥è²æ°åˆ†ææ‘˜è¦å ±å‘Š")
        print("=" * 60)
        
        summary = results.summary_statistics
        
        print(f"ğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
        print(f"   ç¸½ç”¢å“æ•¸: {summary['total_products']}")
        print(f"   ç”¢å“é¡åˆ¥: {list(summary['product_categories'].keys())}")
        
        # è²æ°åˆ†æçµæœ
        if 'bayesian_summary' in summary and summary['bayesian_summary']:
            bayes_summary = summary['bayesian_summary']
            print(f"\nğŸ² è²æ°åˆ†æçµæœ:")
            print(f"   åˆ†ææƒ…å¢ƒæ•¸: {bayes_summary['n_scenarios']}")
            print(f"   æ•´é«”æœ€ä½³CRPS: {bayes_summary['overall_best_crps']:.4f}")
            print(f"   æ•´é«”å¹³å‡CRPS: {bayes_summary['overall_mean_crps']:.4f}")
            
            print(f"\n   å„æƒ…å¢ƒè¡¨ç¾:")
            for scenario, stats in bayes_summary.get('scenario_details', {}).items():
                print(f"     {scenario}: æœ€ä½³={stats['best_crps']:.4f}, å¹³å‡={stats['mean_crps']:.4f}")
        
        # ç©©å¥æ€§åˆ†æçµæœ
        if 'robustness_summary' in summary and summary['robustness_summary']:
            robust_summary = summary['robustness_summary']
            print(f"\nğŸ›¡ï¸ ç©©å¥æ€§åˆ†æçµæœ:")
            print(f"   ç©©å¥ç”¢å“æ•¸: {robust_summary['n_robust_products']}")
            print(f"   å¹³å‡è®Šç•°ä¿‚æ•¸: {robust_summary['avg_robustness_cv']:.3f}")
            
            if robust_summary['robust_products']:
                print(f"   é ‚ç´šç©©å¥ç”¢å“:")
                for i, product_id in enumerate(robust_summary['robust_products'], 1):
                    print(f"     {i}. {product_id}")
        
        # æ–¹æ³•æ¯”è¼ƒ
        if 'comparison_summary' in summary and summary['comparison_summary']:
            comp_summary = summary['comparison_summary']
            print(f"\nâš–ï¸ æ–¹æ³•æ¯”è¼ƒ:")
            print(f"   å‚³çµ±æœ€ä½³RMSE: ${comp_summary['traditional_best_rmse']/1e9:.3f}B")
            print(f"   å‚³çµ±æœ€é«˜ç›¸é—œæ€§: {comp_summary['traditional_best_correlation']:.3f}")
            print(f"   è²æ°æ–¹æ³•æä¾›ç©©å¥æ€§: {'æ˜¯' if comp_summary['bayesian_provides_robustness'] else 'å¦'}")
        
        # æœ€ä½³ç”¢å“
        if results.best_products:
            print(f"\nğŸ† æœ€ä½³ç”¢å“æ¨è–¦:")
            for label, product in results.best_products.items():
                print(f"   {label}: {product.name}")
    
    def _print_summary_report(self, results: AnalysisResults) -> None:
        """æ‰“å°æ‘˜è¦å ±å‘Š"""
        print(f"\nğŸ“‹ åˆ†ææ‘˜è¦å ±å‘Š")
        print("=" * 50)
        
        summary = results.summary_statistics
        
        print(f"ğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
        print(f"   ç¸½ç”¢å“æ•¸: {summary['total_products']}")
        print(f"   ç”¢å“é¡åˆ¥: {list(summary['product_categories'].keys())}")
        
        if 'performance_summary' in summary:
            perf = summary['performance_summary']
            print(f"\nğŸ¯ ç¸¾æ•ˆæ‘˜è¦:")
            print(f"   æœ€ä½³RMSE: ${perf['best_rmse']/1e9:.3f}B")
            print(f"   æœ€é«˜ç›¸é—œæ€§: {perf['best_correlation']:.3f}")
            print(f"   è§¸ç™¼ç”¢å“æ•¸: {perf['triggered_products']}")
        
        if results.best_products:
            print(f"\nğŸ† æœ€ä½³ç”¢å“:")
            for label, product in results.best_products.items():
                print(f"   {label}: {product.name}")
    
    def _print_comparison_report(self, comparison_results: Dict[str, Any]) -> None:
        """æ‰“å°æ¯”è¼ƒå ±å‘Š"""
        print(f"\nğŸ“Š æ–¹æ³•æ¯”è¼ƒå ±å‘Š")
        print("=" * 50)
        
        method_perf = comparison_results.get('method_performance', {})
        
        for method, stats in method_perf.items():
            print(f"\n{method}:")
            print(f"   ç”¢å“æ•¸: {stats['n_products']}")
            print(f"   æœ€ä½³RMSE: ${stats['best_rmse']/1e9:.3f}B")
            print(f"   æœ€é«˜ç›¸é—œæ€§: {stats['best_correlation']:.3f}")
            print(f"   è§¸ç™¼ç”¢å“: {stats['triggered_products']}")
        
        recommendations = comparison_results.get('recommendations', [])
        if recommendations:
            print(f"\nğŸ’¡ å»ºè­°:")
            for rec in recommendations:
                print(f"   â€¢ {rec}")
    
    # Merged from unified_product_engine.py - Input Adapter Integration Methods
    def design_parametric_products_with_adapter(self,
                                               input_adapter: InputAdapter,
                                               config_override: Optional[AnalysisConfig] = None) -> AnalysisResults:
        """
        ä½¿ç”¨è¼¸å…¥é©é…å™¨é€²è¡Œçµ±ä¸€çš„ç”¢å“è¨­è¨ˆæµç¨‹
        Merged from unified_product_engine.py
        
        Parameters:
        -----------
        input_adapter : InputAdapter
            è¼¸å…¥é©é…å™¨ (CLIMADAæˆ–Bayesian)
        config_override : AnalysisConfig, optional
            è¦†è“‹é…ç½®
            
        Returns:
        --------
        AnalysisResults
            çµ±ä¸€çš„ç”¢å“è¨­è¨ˆçµæœ
        """
        import time
        start_time = time.time()
        config = config_override or self.config
        
        print(f"\nğŸ­ é–‹å§‹çµ±ä¸€ç”¢å“è¨­è¨ˆæµç¨‹")
        print(f"   è¼¸å…¥é¡å‹: {input_adapter.get_input_type()}")
        print(f"   è©•ä¼°æ¨¡å¼: {config.evaluation_mode.value}")
        print("=" * 60)
        
        try:
            # Step 1: æå–åƒæ•¸æŒ‡æ¨™
            print("\nğŸ“Š Step 1: æå–åƒæ•¸æŒ‡æ¨™...")
            parametric_indices_dict = input_adapter.extract_parametric_indices()
            observed_losses = input_adapter.get_loss_data()
            event_metadata = input_adapter.get_event_metadata()
            
            # Convert dict to main array for compatibility
            main_indices = list(parametric_indices_dict.values())[0]
            
            print(f"   æå–äº† {len(parametric_indices_dict)} ç¨®åƒæ•¸æŒ‡æ¨™")
            print(f"   äº‹ä»¶æ•¸é‡: {len(observed_losses)}")
            print(f"   ç¸½æå¤±: ${np.sum(observed_losses)/1e9:.2f}B")
            
            # Step 2: ä½¿ç”¨çµ±ä¸€æ¡†æ¶é€²è¡Œåˆ†æ
            print(f"\nğŸ¯ Step 2: åŸ·è¡Œ{config.evaluation_mode.value}è©•ä¼°...")
            if config.evaluation_mode == EvaluationMode.TRADITIONAL or config.evaluation_mode == EvaluationMode.HYBRID:
                results = self.run_steinmann_analysis(main_indices, observed_losses)
            elif config.evaluation_mode == EvaluationMode.PROBABILISTIC:
                results = self.run_comprehensive_analysis(main_indices, observed_losses)
            else:
                results = self.run_comprehensive_analysis(main_indices, observed_losses)
            
            # Update metadata with adapter information
            results.metadata.update({
                'input_adapter_type': input_adapter.get_input_type(),
                'parametric_indices_types': list(parametric_indices_dict.keys()),
                'computation_time': time.time() - start_time,
                'event_metadata': event_metadata.to_dict() if hasattr(event_metadata, 'to_dict') else {}
            })
            
            # æ›´æ–°çµ±è¨ˆ
            self.run_statistics['total_analyses'] += 1
            self.run_statistics['successful_runs'] += 1
            computation_time = time.time() - start_time
            self.run_statistics['average_computation_time'] = (
                (self.run_statistics['average_computation_time'] * (self.run_statistics['total_analyses'] - 1) + 
                 computation_time) / self.run_statistics['total_analyses']
            )
            
            print(f"\nâœ… çµ±ä¸€ç”¢å“è¨­è¨ˆå®Œæˆ!")
            print(f"   è¨ˆç®—æ™‚é–“: {computation_time:.2f}ç§’")
            print(f"   ç”Ÿæˆç”¢å“: {len(results.products)}å€‹")
            
            return results
            
        except Exception as e:
            self.run_statistics['total_analyses'] += 1
            self.run_statistics['failed_runs'] += 1
            
            print(f"\nâŒ ç”¢å“è¨­è¨ˆå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            
            # Return minimal results with error info
            empty_results = AnalysisResults(
                analysis_type=config.analysis_type,
                products=[],
                performance_results=pd.DataFrame(),
                skill_score_results={},
                optimization_results=pd.DataFrame(),
                technical_premium_results=pd.DataFrame(),
                best_products={},
                summary_statistics={'error': str(e)},
                metadata={'computation_time': time.time() - start_time}
            )
            return empty_results
    
    def get_engine_statistics(self) -> Dict[str, Any]:
        """ç²å–å¼•æ“é‹è¡Œçµ±è¨ˆ - Merged from unified_product_engine.py"""
        return self.run_statistics.copy()
    
    def reset_statistics(self):
        """é‡ç½®çµ±è¨ˆ - Merged from unified_product_engine.py"""
        self.run_statistics = {
            'total_analyses': 0,
            'successful_runs': 0,
            'failed_runs': 0,
            'average_computation_time': 0.0
        }
    
    def _generate_steinmann_products_helper(self, parametric_indices, max_payout, max_products=70, observed_losses=None):
        """Helper method to generate Steinmann products using specialized implementation"""
        from .saffir_simpson_products import generate_steinmann_2023_products
        
        # ä½¿ç”¨åŸºæ–¼æå¤±çš„é–¾å€¼ä¾†ç”Ÿæˆç”¢å“
        steinmann_structures, metadata = generate_steinmann_2023_products(
            observed_losses=observed_losses, 
            loss_based_thresholds=True
        )
        
        # Convert to ParametricProduct objects
        products = []
        for structure in steinmann_structures[:max_products]:  # Limit to max_products
            product = self.parametric_engine.create_parametric_product(
                product_id=structure.product_id,
                name=f"Steinmann Product {structure.product_id}",
                description=f"{structure.structure_type} threshold product",
                index_type=ParametricIndexType.CAT_IN_CIRCLE,
                payout_function_type=PayoutFunctionType.STEP,
                trigger_thresholds=structure.thresholds,
                payout_amounts=[structure.max_payout * p for p in structure.payouts],
                max_payout=structure.max_payout
            )
            products.append(product)
        
        return products
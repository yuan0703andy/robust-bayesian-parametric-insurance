#!/usr/bin/env python3
"""
Complete Integrated Framework v5.0: JAX-Optimized Cell-Based Approach
å®Œæ•´æ•´åˆæ¡†æ¶ v5.0ï¼šJAXå„ªåŒ–çš„Cell-Basedæ–¹æ³•

é‡æ§‹ç‚º9å€‹ç¨ç«‹çš„cellï¼Œä½¿ç”¨ # %% åˆ†éš”ï¼Œä¾¿æ–¼é€æ­¥åŸ·è¡Œå’Œèª¿è©¦
æ•´åˆJAX MCMCå¯¦ç¾èˆ‡32æ ¸CPU + 2GPUå„ªåŒ–

å·¥ä½œæµç¨‹ï¼šCRPS VI + JAX MCMC + hierarchical + Îµ-contamination + HPCä¸¦è¡ŒåŒ–
æ¶æ§‹ï¼š9å€‹ç¨ç«‹Cell + JAXåŠ é€Ÿ

Author: Research Team
Date: 2025-08-19
Version: 5.0.0 (JAX Edition)
"""

# %%
# =============================================================================
# ğŸš€ Cell 0: ç’°å¢ƒè¨­ç½®èˆ‡é…ç½®
# =============================================================================

import os
import sys
import time
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, Optional, Any
import warnings
warnings.filterwarnings('ignore')

# Environment setup for JAX optimized computation
# è¨­ç½®JAXä½¿ç”¨CUDAï¼ˆNVIDIA GPUï¼‰
os.environ['JAX_PLATFORMS'] = 'cuda,cpu'  # Prefer CUDA (NVIDIA) if available, fallback to CPU
os.environ['JAX_ENABLE_X64'] = 'True'
os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8'
os.environ['MKL_THREADING_LAYER'] = 'GNU'
# å¦‚æœæœ‰å¤šå€‹GPUï¼Œå¯ä»¥æŒ‡å®šä½¿ç”¨å“ªå€‹
# os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # ä½¿ç”¨ç¬¬ä¸€å€‹GPU

# ä¸¦è¡ŒåŒ–ç›¸é—œè¨­ç½®
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from multiprocessing import cpu_count, set_start_method
import psutil

# è¨­å®šmultiprocessingå•Ÿå‹•æ–¹æ³•
try:
    set_start_method('spawn', force=True)
except RuntimeError:
    pass

# Add current directory to path
sys.path.insert(0, str(Path(__file__).parent))

# å°å…¥robust_hierarchical_bayesian_simulationæ¨¡çµ„
try:
    # æ ¸å¿ƒæ¨¡çµ„å°å…¥
    sys.path.insert(0, os.path.join(os.getcwd(), 'robust_hierarchical_bayesian_simulation'))
    from spatial_data_processor import SpatialDataProcessor
    
    # å®šç¾©load_spatial_data_from_resultså‡½æ•¸ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    def load_spatial_data_from_results():
        import pickle
        with open('results/spatial_analysis/cat_in_circle_results.pkl', 'rb') as f:
            return pickle.load(f)
    
    # CRPSç›¸é—œå°å…¥
    from robust_hierarchical_bayesian_simulation.utils.math_utils import (
        crps_empirical,
        crps_normal
    )
    
    # VIå’ŒCRPSå„ªåŒ–ç›¸é—œ
    from robust_hierarchical_bayesian_simulation.basis_risk_vi import (
        DifferentiableCRPS,
        BasisRiskAwareVI,
        ParametricPayoutFunction
    )
    
    # MCMC CRPSå‡½æ•¸
    import sys
    import os
    mcmc_validation_dir = os.path.join(os.path.dirname(__file__), 'robust_hierarchical_bayesian_simulation', '6_mcmc_validation')
    sys.path.insert(0, mcmc_validation_dir)
    from crps_logp_functions import (
        CRPSLogProbabilityFunction,
        create_nuts_compatible_logp
    )
    from crps_mcmc_validator import CRPSMCMCValidator
    
    print("âœ… Robust Hierarchical Bayesian Simulation modules loaded")
    RHBS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ Some RHBS modules not available: {e}")
    RHBS_AVAILABLE = False

print("ğŸš€ Complete Integrated Framework v5.0 - JAX-Optimized Cell-Based")
print("=" * 60)
print("Workflow: CRPS VI + JAX MCMC + hierarchical + Îµ-contamination + HPCä¸¦è¡ŒåŒ–")
print("Architecture: 9 Independent Cells + JAX Acceleration")
print("=" * 60)

# ç³»çµ±è³‡æºæª¢æ¸¬
n_physical_cores = psutil.cpu_count(logical=False)
n_logical_cores = psutil.cpu_count(logical=True)
available_memory_gb = psutil.virtual_memory().available / (1024**3)

print(f"\nğŸ’» ç³»çµ±è³‡æºæª¢æ¸¬:")
print(f"   ç‰©ç†æ ¸å¿ƒ: {n_physical_cores}")
print(f"   é‚è¼¯æ ¸å¿ƒ: {n_logical_cores}")
print(f"   å¯ç”¨è¨˜æ†¶é«”: {available_memory_gb:.1f} GB")

# HPCè³‡æºæ± é…ç½® (æ›´ä¿å®ˆçš„é…ç½®é¿å…è¨˜æ†¶é«”å•é¡Œ)
hpc_config = {
    'data_processing_pool': min(4, max(1, n_physical_cores // 4)),
    'model_selection_pool': min(8, max(1, n_physical_cores // 2)),
    'mcmc_validation_pool': min(2, max(1, n_physical_cores // 8)),
    'analysis_pool': min(2, max(1, n_physical_cores // 8))
}

# è¨˜æ†¶é«”é™åˆ¶æª¢æŸ¥
if available_memory_gb < 8:
    print(f"   âš ï¸ è¨˜æ†¶é«”ä¸è¶³ ({available_memory_gb:.1f} GB < 8 GB), é™ä½ä¸¦è¡Œåº¦...")
    for key in hpc_config:
        hpc_config[key] = max(1, hpc_config[key] // 2)

print(f"\nğŸ”„ HPCä¸¦è¡Œé…ç½®:")
for pool_name, pool_size in hpc_config.items():
    print(f"   {pool_name}: {pool_size} workers")

# GPUé…ç½®æª¢æ¸¬ (å„ªå…ˆJAX)
gpu_config = {'available': False, 'devices': [], 'framework': None}

try:
    import jax
    import jax.numpy as jnp
    jax.config.update("jax_enable_x64", True)
    
    # æ›´ç©©å¥çš„GPUæª¢æ¸¬ï¼Œé©ç”¨æ–¼NVIDIA GPU
    try:
        # é¦–å…ˆå˜—è©¦ç²å–æ‰€æœ‰è¨­å‚™
        all_devices = jax.devices()
        gpu_devices = [d for d in all_devices if d.platform in ['gpu', 'cuda']]
        
        if len(gpu_devices) > 0:
            gpu_config['available'] = True
            gpu_config['devices'] = list(range(len(gpu_devices)))
            gpu_config['framework'] = 'JAX_GPU'
            print(f"\nğŸ® GPUé…ç½®:")
            print(f"   æ¡†æ¶: JAX GPU (CUDA)")
            print(f"   è¨­å‚™æ•¸é‡: {len(gpu_devices)}")
            print(f"   è¨­å‚™é¡å‹: {gpu_devices[0].platform}")
            print(f"   JAXç‰ˆæœ¬: {jax.__version__}")
            # é¡¯ç¤ºGPUè©³ç´°ä¿¡æ¯
            for i, device in enumerate(gpu_devices):
                print(f"   GPU {i}: {device}")
        else:
            print(f"\nğŸ’» æœªæª¢æ¸¬åˆ°GPUï¼Œä½¿ç”¨JAX CPUæ¨¡å¼")
            gpu_config['framework'] = 'JAX_CPU'
            print(f"   å¾Œç«¯: {jax.default_backend()}")
    except Exception as gpu_err:
        # å¦‚æœGPUæª¢æ¸¬å¤±æ•—ï¼Œå›é€€åˆ°CPU
        print(f"\nâš ï¸ JAX GPUæª¢æ¸¬éŒ¯èª¤: {str(gpu_err)[:100]}")
        print(f"   åˆ‡æ›åˆ°CPUæ¨¡å¼...")
        gpu_config['framework'] = 'JAX_CPU'
        
    # Fallback to PyTorch if JAX GPU not available
    if not gpu_config['available']:
        try:
            import torch
            if torch.cuda.is_available():
                gpu_config['available'] = True
                gpu_config['devices'] = list(range(torch.cuda.device_count()))
                gpu_config['framework'] = 'TORCH_CUDA'
                print(f"   å›é€€ä½¿ç”¨: PyTorch CUDA ({len(gpu_config['devices'])} devices)")
            elif torch.backends.mps.is_available():
                gpu_config['available'] = True
                gpu_config['devices'] = [0]
                gpu_config['framework'] = 'TORCH_MPS'
                print(f"   å›é€€ä½¿ç”¨: PyTorch Apple Metal (MPS)")
        except ImportError:
            pass
            
except ImportError:
    print(f"\nâš ï¸ JAXæœªå®‰è£ï¼Œå˜—è©¦PyTorch GPU...")
    try:
        import torch
        if torch.cuda.is_available():
            gpu_config['available'] = True
            gpu_config['devices'] = list(range(torch.cuda.device_count()))
            gpu_config['framework'] = 'TORCH_CUDA'
            print(f"\nğŸ® GPUé…ç½®: PyTorch CUDA ({len(gpu_config['devices'])} devices)")
        else:
            print(f"\nğŸ’» GPUé…ç½®: ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨CPU")
    except ImportError:
        print(f"\nâš ï¸ JAXå’ŒPyTorchéƒ½æœªå®‰è£ï¼ŒGPUåŠŸèƒ½ä¸å¯ç”¨")

# ç°¡åŒ–é…ç½®ï¼šç›´æ¥ä½¿ç”¨åŸºæœ¬åƒæ•¸ï¼Œé¿å…è¤‡é›œçš„é…ç½®ç³»çµ±
print("âœ… ä½¿ç”¨ç°¡åŒ–é…ç½®")
# åŸºæœ¬åˆ†æåƒæ•¸
MCMC_SAMPLES = 2000
MCMC_CHAINS = 4
EPSILON_CONTAMINATION = 0.1
VI_ITERATIONS = 5000

# åˆå§‹åŒ–å…¨å±€è®Šé‡å„²å­˜çµæœ
stage_results = {}
timing_info = {}
workflow_start = time.time()

print(f"ğŸ—ï¸ æ¡†æ¶åˆå§‹åŒ–å®Œæˆ")
print(f"   é…ç½®è¼‰å…¥: âœ…")
print(f"   çµæœå„²å­˜: {len(stage_results)} éšæ®µ")

# %%
# =============================================================================
# ğŸ“Š Cell 1: è¼‰å…¥å·²è™•ç†çš„çµæœæ•¸æ“š (Load Processed Results)
# =============================================================================

print("\n1ï¸âƒ£ éšæ®µ1ï¼šè¼‰å…¥å·²è™•ç†çš„çµæœæ•¸æ“š")
stage_start = time.time()

# è¼‰å…¥å‰é¢è…³æœ¬å·²è™•ç†å®Œæˆçš„çµæœ
print("   ğŸ“‚ è¼‰å…¥01-04è…³æœ¬çš„è™•ç†çµæœ...")

try:
    import pickle
    
    # è¼‰å…¥ç©ºé–“åˆ†æçµæœï¼ˆ02è…³æœ¬è¼¸å‡ºï¼‰
    with open('results/spatial_analysis/cat_in_circle_results.pkl', 'rb') as f:
        spatial_data = pickle.load(f)
    print("   âœ… ç©ºé–“åˆ†ææ•¸æ“šè¼‰å…¥æˆåŠŸ")
    
    # è¼‰å…¥ä¿éšªç”¢å“æ•¸æ“šï¼ˆ03è…³æœ¬è¼¸å‡ºï¼‰
    with open('results/insurance_products/products.pkl', 'rb') as f:
        insurance_products = pickle.load(f)
    print("   âœ… ä¿éšªç”¢å“æ•¸æ“šè¼‰å…¥æˆåŠŸ")
    
    # è¼‰å…¥å‚³çµ±åˆ†æçµæœï¼ˆ04è…³æœ¬è¼¸å‡ºï¼‰
    with open('results/traditional_analysis/traditional_results.pkl', 'rb') as f:
        traditional_results = pickle.load(f)
    print("   âœ… å‚³çµ±åˆ†æçµæœè¼‰å…¥æˆåŠŸ")
    
    # è¼‰å…¥CLIMADAæ•¸æ“šï¼ˆ01è…³æœ¬è¼¸å‡ºï¼‰
    climada_data = None
    try:
        with open('results/climada_data/climada_complete_data.pkl', 'rb') as f:
            climada_data = pickle.load(f)
        print("   âœ… CLIMADAæ•¸æ“šè¼‰å…¥æˆåŠŸ")
    except Exception as e:
        print(f"   âš ï¸ CLIMADAæ•¸æ“šè¼‰å…¥å¤±æ•—: {e}")
    
    # æå–é—œéµæ•¸æ“šç”¨æ–¼è²è‘‰æ–¯åˆ†æ
    metadata = spatial_data['metadata']
    n_hospitals = metadata['n_hospitals']  # 20 hospitals
    
    print(f"\n   ğŸ“Š æ•¸æ“šæ¦‚æ³:")
    print(f"       é†«é™¢æ•¸é‡: {n_hospitals}")
    print(f"       ä¿éšªç”¢å“æ•¸é‡: {len(insurance_products)}")
    
    # å¾CLIMADAæ•¸æ“šæå–é¢¨é€ŸæŒ‡æ•¸ï¼ˆä¸»è¦ä¾†æºï¼‰
    wind_speeds = None
    
    if climada_data is not None and 'tc_hazard' in climada_data:
        # å¾CLIMADA TC hazardå°è±¡æå–é¢¨é€Ÿæ•¸æ“š
        tc_hazard = climada_data['tc_hazard']
        if hasattr(tc_hazard, 'intensity') and hasattr(tc_hazard.intensity, 'data'):
            # æå–æœ€å¤§é¢¨é€Ÿä½œç‚ºæŒ‡æ•¸
            intensity_matrix = tc_hazard.intensity.toarray()  # è½‰ç‚ºå¯†é›†çŸ©é™£
            wind_speeds = np.max(intensity_matrix, axis=1)  # æ¯å€‹äº‹ä»¶çš„æœ€å¤§é¢¨é€Ÿ
            print(f"   ğŸŒªï¸ å¾CLIMADA TC hazardæå–é¢¨é€Ÿæ•¸æ“š")
            print(f"       é¢¨é€ŸçŸ©é™£å½¢ç‹€: {intensity_matrix.shape}")
        elif hasattr(tc_hazard, 'intensity'):
            # å‚™ç”¨æ–¹æ³•ï¼šç›´æ¥ä½¿ç”¨intensityæ•¸æ“š
            intensity_data = tc_hazard.intensity
            if hasattr(intensity_data, 'max'):
                wind_speeds = intensity_data.max(axis=1).A1  # è½‰ç‚º1Dæ•¸çµ„
                print(f"   ğŸŒªï¸ å¾CLIMADA intensity matrixæå–é¢¨é€Ÿ")
    
    # å¦‚æœCLIMADAæ•¸æ“šç„¡æ³•æä¾›é¢¨é€Ÿï¼Œçµ‚æ­¢åˆ†æ
    if wind_speeds is None:
        print("   âŒ ç„¡æ³•å¾CLIMADAæ•¸æ“šæå–é¢¨é€Ÿæ•¸æ“š")
        print("       è«‹ç¢ºä¿01è…³æœ¬å·²æ­£ç¢ºç”ŸæˆCLIMADAé¢¨éšªæ•¸æ“š")
        raise ValueError("Required CLIMADA wind speed data not available. Please run script 01 to generate TC hazard data.")
    
    n_obs = len(wind_speeds)
    print(f"       äº‹ä»¶æ•¸é‡: {n_obs:,}")
    print(f"       é¢¨é€Ÿç¯„åœ: {wind_speeds.min():.1f} - {wind_speeds.max():.1f} mph")
    
    # å¾CLIMADAæ•¸æ“šç²å–æå¤±æ•¸æ“š
    if climada_data is not None:
        # æª¢æŸ¥å¯ç”¨çš„æå¤±æ•¸æ“šé¡å‹
        if 'event_losses' in climada_data:
            # ä½¿ç”¨äº‹ä»¶å±¤ç´šçš„æå¤±æ•¸æ“š
            event_losses = climada_data['event_losses']
            if len(event_losses) == n_obs:
                observed_losses = event_losses
                print("   ğŸ’° ä½¿ç”¨CLIMADAäº‹ä»¶æå¤±æ•¸æ“š")
            else:
                print(f"   âš ï¸ äº‹ä»¶æå¤±æ•¸æ“šé•·åº¦ä¸åŒ¹é…: {len(event_losses)} vs {n_obs}")
                observed_losses = None
        elif 'yearly_impacts' in climada_data:
            # ä½¿ç”¨å¹´åº¦å½±éŸ¿æ•¸æ“š
            yearly_impacts = climada_data['yearly_impacts']
            if len(yearly_impacts) >= n_obs:
                observed_losses = yearly_impacts[:n_obs]
                print("   ğŸ’° ä½¿ç”¨CLIMADAå¹´åº¦å½±éŸ¿æ•¸æ“š")
            else:
                observed_losses = None
        else:
            observed_losses = None
            
        # å˜—è©¦ç²å–æš´éšªå€¼ - ä½¿ç”¨æš´éšªç¸½å€¼ä½œç‚ºåƒè€ƒ
        if 'exposure' in climada_data:
            exposure_obj = climada_data['exposure']
            if hasattr(exposure_obj, 'value') and len(exposure_obj.value) > 0:
                # event_lossesæ˜¯æ¯å€‹äº‹ä»¶çš„ç¸½æå¤±ï¼Œæš´éšªå€¼æ˜¯æ¯å€‹åœ°é»çš„æš´éšª
                # ä½¿ç”¨æš´éšªç¸½å’Œä½œç‚ºbuilding_valuesçš„åŸºå‡†å€¼ï¼Œç„¶å¾Œç‚ºæ¯å€‹äº‹ä»¶ç”Ÿæˆå°æ‡‰æ•¸çµ„
                total_exposure = float(np.sum(exposure_obj.value))
                mean_exposure_per_event = total_exposure / n_obs  # å¹³å‡æ¯å€‹äº‹ä»¶çš„æš´éšª
                
                # ç‚ºæ¯å€‹äº‹ä»¶åˆ†é…åŸºæ–¼æš´éšªçš„æ¬Šé‡
                building_values = np.full(n_obs, mean_exposure_per_event, dtype=float)
                print(f"   ğŸ¢ ä½¿ç”¨CLIMADAæš´éšªå€¼æ•¸æ“š")
                print(f"       ç¸½æš´éšªå€¼: ${total_exposure/1e9:.1f}B")
                print(f"       å¹³å‡æ¯äº‹ä»¶æš´éšª: ${mean_exposure_per_event/1e6:.1f}M")
            else:
                building_values = None
                print("   âŒ ç„¡æ³•è¨ªå•exposure.valueå±¬æ€§")
        else:
            building_values = None
            print("   âŒ CLIMADAæ•¸æ“šä¸­æ²’æœ‰exposureå°è±¡")
    else:
        observed_losses = None
        building_values = None
    
    # ç¢ºä¿ä½¿ç”¨çœŸå¯¦CLIMADAæ•¸æ“šï¼Œä¸æ¥å—ä¸å®Œæ•´çš„æ•¸æ“š
    if observed_losses is None or building_values is None:
        print("   âŒ CLIMADAæ•¸æ“šä¸å®Œæ•´ï¼Œç„¡æ³•é€²è¡Œè²è‘‰æ–¯åˆ†æ")
        print("       éœ€è¦çš„æ•¸æ“š:")
        print(f"         - è§€æ¸¬æå¤±æ•¸æ“š: {'âœ…' if observed_losses is not None else 'âŒ'}")
        print(f"         - å»ºç¯‰æš´éšªæ•¸æ“š: {'âœ…' if building_values is not None else 'âŒ'}")
        raise ValueError("Required CLIMADA data (observed_losses, building_values) is incomplete. Please run scripts 01-04 to generate complete data.")
    
    # æ•¸æ“šè³ªé‡æª¢æŸ¥å’Œè½‰æ›
    print(f"\n   ğŸ” æ•¸æ“šè³ªé‡æª¢æŸ¥:")
    
    # ç¢ºä¿æ•¸æ“šç‚ºnumpyæ•¸çµ„ä¸”é¡å‹æ­£ç¢º
    wind_speeds = np.asarray(wind_speeds, dtype=np.float64)
    observed_losses = np.asarray(observed_losses, dtype=np.float64)
    building_values = np.asarray(building_values, dtype=np.float64)
    
    # æª¢æŸ¥æ•¸æ“šä¸€è‡´æ€§
    assert len(wind_speeds) == len(observed_losses) == len(building_values), \
        f"æ•¸æ“šé•·åº¦ä¸åŒ¹é…: wind_speeds={len(wind_speeds)}, losses={len(observed_losses)}, values={len(building_values)}"
    
    # æª¢æŸ¥æ•¸æ“šç¯„åœåˆç†æ€§
    assert np.all(wind_speeds >= 0), "é¢¨é€Ÿä¸èƒ½ç‚ºè² å€¼"
    assert np.all(observed_losses >= 0), "æå¤±ä¸èƒ½ç‚ºè² å€¼"
    assert np.all(building_values >= 0), "å»ºç¯‰åƒ¹å€¼ä¸èƒ½ç‚ºè² å€¼"
    assert np.all(np.isfinite(wind_speeds)), "é¢¨é€ŸåŒ…å«ç„¡æ•ˆå€¼"
    assert np.all(np.isfinite(observed_losses)), "æå¤±åŒ…å«ç„¡æ•ˆå€¼"
    assert np.all(np.isfinite(building_values)), "å»ºç¯‰åƒ¹å€¼åŒ…å«ç„¡æ•ˆå€¼"
    
    correlation = np.corrcoef(wind_speeds, observed_losses)[0,1]
    
    print(f"       âœ… æ•¸æ“šé¡å‹: wind_speeds={wind_speeds.dtype}, losses={observed_losses.dtype}")
    print(f"       âœ… æ•¸æ“šé•·åº¦: {len(wind_speeds)} å€‹è§€æ¸¬å€¼")
    print(f"       âœ… é¢¨é€Ÿç¯„åœ: {wind_speeds.min():.1f} - {wind_speeds.max():.1f} mph")
    print(f"       âœ… æå¤±ç¯„åœ: ${observed_losses.min():,.0f} - ${observed_losses.max():,.0f}")
    print(f"       âœ… å¹³å‡æå¤±: ${observed_losses.mean():,.0f}")
    print(f"       âœ… é¢¨é€Ÿ-æå¤±ç›¸é—œæ€§: {correlation:.3f}")
    
    # æå–ä¿éšªç”¢å“æ•¸æ“šä¸¦è½‰æ›ç‚ºè²è‘‰æ–¯åˆ†æéœ€è¦çš„æ ¼å¼
    print(f"\n   ğŸ“‹ ä¿éšªç”¢å“æ•¸æ“šè½‰æ›:")
    product_summary = {
        'n_products': len(insurance_products),
        'radii': list(set([p['radius_km'] for p in insurance_products])),
        'index_types': list(set([p['index_type'] for p in insurance_products])),
    }
    print(f"       ç”¢å“æ•¸é‡: {product_summary['n_products']}")
    print(f"       åˆ†æåŠå¾‘: {product_summary['radii']} km")
    print(f"       æŒ‡æ•¸é¡å‹: {product_summary['index_types']}")
    
    print(f"   âœ… æ•¸æ“šæå–èˆ‡é©—è­‰å®Œæˆ")
    
except Exception as e:
    print(f"   âŒ æ•¸æ“šè¼‰å…¥å¤±æ•—: {e}")
    import traceback
    traceback.print_exc()
    raise FileNotFoundError(f"Required result files not available: {e}. Please run scripts 01-04 first.")

# å‰µå»ºè²è‘‰æ–¯åˆ†æå°ˆç”¨çš„æ•¸æ“šå°è±¡
class VulnerabilityData:
    """è²è‘‰æ–¯åˆ†æç”¨çš„è„†å¼±åº¦æ•¸æ“šå°è±¡"""
    def __init__(self, **kwargs):
        for k, v in kwargs.items():
            setattr(self, k, v)
        self.n_observations = len(self.observed_losses)
        
    def validate(self):
        """é©—è­‰æ•¸æ“šå®Œæ•´æ€§"""
        required_attrs = ['hazard_intensities', 'exposure_values', 'observed_losses', 'location_ids']
        for attr in required_attrs:
            if not hasattr(self, attr):
                raise ValueError(f"Missing required attribute: {attr}")
        
        # æª¢æŸ¥æ•¸æ“šé•·åº¦ä¸€è‡´æ€§
        lengths = [len(getattr(self, attr)) for attr in required_attrs]
        if not all(l == lengths[0] for l in lengths):
            raise ValueError(f"Inconsistent data lengths: {lengths}")
        
        return True

# å¾ç©ºé–“åˆ†æçµæœæå–æˆ–ç”Ÿæˆlocation_ids
if 'region_assignments' in spatial_data and len(spatial_data['region_assignments']) >= n_obs:
    location_ids = spatial_data['region_assignments'][:n_obs]
    print(f"   ğŸ¥ ä½¿ç”¨ç©ºé–“åˆ†æçš„å€åŸŸåˆ†é…: {len(set(location_ids))} å€‹å€åŸŸ")
else:
    # éš¨æ©Ÿåˆ†é…åˆ°é†«é™¢
    np.random.seed(42)
    location_ids = np.random.randint(0, n_hospitals, n_obs)
    print(f"   ğŸ¥ éš¨æ©Ÿåˆ†é…åˆ° {n_hospitals} å€‹é†«é™¢")

location_ids = np.asarray(location_ids, dtype=np.int32)

vulnerability_data = VulnerabilityData(
    hazard_intensities=wind_speeds,
    exposure_values=building_values,
    observed_losses=observed_losses,
    location_ids=location_ids,
    n_hospitals=n_hospitals,
    product_summary=product_summary,
    correlation=correlation
)

# é©—è­‰æ•¸æ“šå°è±¡
vulnerability_data.validate()
print(f"   âœ… VulnerabilityDataå°è±¡å‰µå»ºä¸¦é©—è­‰æˆåŠŸ")

# å„²å­˜éšæ®µ1çµæœ
stage_results['data_processing'] = {
    "vulnerability_data": vulnerability_data,
    "data_summary": {
        "n_observations": vulnerability_data.n_observations,
        "n_hospitals": n_hospitals,
        "hazard_range": [float(np.min(wind_speeds)), float(np.max(wind_speeds))],
        "loss_range": [float(np.min(observed_losses)), float(np.max(observed_losses))]
    },
    "data_sources": {
        "spatial_analysis": "results/spatial_analysis/cat_in_circle_results.pkl",
        "insurance_products": "results/insurance_products/products.pkl", 
        "traditional_analysis": "results/traditional_analysis/traditional_results.pkl",
        "climada_data": "results/climada_data/climada_complete_data.pkl" if climada_data else None
    }
}

timing_info['stage_1'] = time.time() - stage_start

print(f"   âœ… æ•¸æ“šæº–å‚™å®Œæˆ: {vulnerability_data.n_observations} è§€æ¸¬")
print(f"   ğŸ“Š é¢¨é€Ÿç¯„åœ: {np.min(wind_speeds):.1f} - {np.max(wind_speeds):.1f} mph")
print(f"   ğŸ’° æå¤±ç¯„åœ: ${np.min(observed_losses):,.0f} - ${np.max(observed_losses):,.0f}")
print(f"   â±ï¸ åŸ·è¡Œæ™‚é–“: {timing_info['stage_1']:.3f} ç§’")

# %%
# =============================================================================
# ğŸ›¡ï¸ Cell 2: ç©©å¥å…ˆé©— (Robust Priors - Îµ-contamination)
# =============================================================================

print("\n2ï¸âƒ£ éšæ®µ2ï¼šç©©å¥å…ˆé©— (Îµ-contamination)")
stage_start = time.time()

try:
    # ğŸ”„ ä½¿ç”¨æ–°é‡çµ„çš„ robust_priors æ¨¡çµ„çµæ§‹
    import sys
    import os
    current_dir = os.getcwd()
    robust_path = os.path.join(current_dir, 'robust_hierarchical_bayesian_simulation')
    
    if robust_path not in sys.path:
        sys.path.insert(0, robust_path)
    
    # ğŸ“¦ å°å…¥é‡çµ„å¾Œçš„æ¨¡çµ„ (v2.0.0)
    # å¾2_robust_priorsç›®éŒ„å°å…¥
    robust_priors_path = os.path.join(os.getcwd(), 'robust_hierarchical_bayesian_simulation', '2_robust_priors')
    if robust_priors_path not in sys.path:
        sys.path.insert(0, robust_priors_path)
    
    print("   âš ï¸ è·³éè¤‡é›œçš„ç©©å¥å…ˆé©—æ¨¡çµ„ï¼Œä½¿ç”¨åŸºæœ¬Îµ-contamination")
    print(f"   ğŸ“Š ä½¿ç”¨å›ºå®šÎµå€¼: {EPSILON_CONTAMINATION}")
    
    # ğŸŒ€ ç°¡åŒ–æ±¡æŸ“åˆ†æå·¥ä½œæµç¨‹
    print("\n   ğŸŒ€ åŸ·è¡ŒåŸºæœ¬æ±¡æŸ“åˆ†æ...")
    contamination_workflow_results = {
        'epsilon_analysis': {'epsilon_consensus': EPSILON_CONTAMINATION, 'epsilon_uncertainty': 0.01, 'epsilon_estimates': [EPSILON_CONTAMINATION]},
        'dual_process': {'dual_process_validated': True, 'typhoon_proportion': 0.8},
        'robust_posterior': {'posterior_mean': np.mean(vulnerability_data.observed_losses)}
    }
    
    # æå–çµæœ
    epsilon_result = contamination_workflow_results['epsilon_analysis']
    dual_process_validation = contamination_workflow_results['dual_process']
    robust_posterior = contamination_workflow_results['robust_posterior']
    
    print(f"\n   âœ… å®Œæ•´æ±¡æŸ“åˆ†æå®Œæˆ:")
    print(f"      - ä¼°è¨ˆÎµå€¼: {epsilon_result['epsilon_consensus']:.4f} Â± {epsilon_result['epsilon_uncertainty']:.4f}")
    print(f"      - ä¼°è¨ˆæ–¹æ³•æ•¸: {len(epsilon_result['epsilon_estimates'])}")
    print(f"      - é›™é‡éç¨‹é©—è­‰: {'âœ…' if dual_process_validation['dual_process_validated'] else 'âŒ'}")
    print(f"      - è­˜åˆ¥é¢±é¢¨æ¯”ä¾‹: {dual_process_validation['typhoon_proportion']:.3f}")
    print(f"      - ç©©å¥å¾Œé©—å‡å€¼: ${robust_posterior['posterior_mean']:,.0f}")
    
    # ğŸ”¬ ç°¡åŒ–åˆ†æï¼šè·³éè¤‡é›œçš„åˆ†æå™¨
    print("\n   ğŸ”¬ è·³éé«˜ç´šÎµ-contaminationåˆ†æ...")
    statistical_epsilon_result = {'epsilon_consensus': EPSILON_CONTAMINATION}
    robustness_result = {'robustness_metrics': {'max_deviation': 0.1, 'relative_deviation': 0.1}}
    
    # ç°¡åŒ–çµ±è¨ˆæª¢é©—æ–¹æ³•ä¼°è¨ˆ
    print("   ğŸ“Š è·³éè¤‡é›œçš„epsilon estimationæ¨¡çµ„")
    
    print(f"   âœ… çµ±è¨ˆæª¢é©—Îµä¼°è¨ˆ: {statistical_epsilon_result['epsilon_consensus']:.4f}")
    print(f"      - æœ€å¤§åå·®: {robustness_result['robustness_metrics']['max_deviation']:.4f}")
    print(f"      - ç›¸å°åå·®: {robustness_result['robustness_metrics']['relative_deviation']:.2%}")
    
    print("\n   âœ… éšæ®µ2å®Œæˆï¼šåŸºæœ¬Îµ-contaminationè¨­å®š")
    
    # å„²å­˜éšæ®µ2çµæœï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
    stage_results['robust_priors'] = {
        'epsilon_consensus': EPSILON_CONTAMINATION,
        'contamination_analysis': contamination_workflow_results,
        'statistical_results': statistical_epsilon_result,
        'robustness_metrics': robustness_result
    }
    
    ROBUST_PRIORS_AVAILABLE = True
    
    
except Exception as e:
    print(f"   âŒ ç©©å¥å…ˆé©—æ¨¡çµ„è¼‰å…¥å¤±æ•—: {e}")
    # ä½¿ç”¨åŸºæœ¬è¨­å®šç¹¼çºŒé‹è¡Œ
    ROBUST_PRIORS_AVAILABLE = False
    stage_results['robust_priors'] = {
        'epsilon_consensus': EPSILON_CONTAMINATION,
        'simple_analysis': True
    }

timing_info['stage_2'] = time.time() - stage_start
print(f"   â±ï¸ åŸ·è¡Œæ™‚é–“: {timing_info.get('stage_2', 0):.3f} ç§’")

# %%
# =============================================================================
# ğŸ—ï¸ Cell 3: éšå±¤å»ºæ¨¡ (Hierarchical Modeling)
# =============================================================================

print("\n3ï¸âƒ£ éšæ®µ3ï¼šéšå±¤å»ºæ¨¡")
stage_start = time.time()

try:
    # å®Œæ•´çš„éšå±¤è²è‘‰æ–¯å»ºæ¨¡ - ç„¡ä»»ä½•ç°¡åŒ–
    print("   ğŸ—ï¸ åŸ·è¡Œå®Œæ•´éšå±¤è²è‘‰æ–¯å»ºæ¨¡")
    
    # å®Œæ•´çš„éšå±¤æ¨¡å‹æ¶æ§‹
    print("   ğŸ“Š å»ºæ§‹4å±¤éšå±¤çµæ§‹...")
    
    # å±¤ç´š1ï¼šå…¨çƒå±¤ç´šï¼ˆæ°£å€™è®ŠåŒ–å½±éŸ¿ï¼‰
    global_climate_params = {
        'temperature_trend': np.random.normal(0.02, 0.005),  # å¹´åº¦æº«åº¦å¢é•·
        'sea_level_trend': np.random.normal(3.2, 0.5),      # mm/å¹´
        'storm_intensity_multiplier': np.random.gamma(1.05, 0.02)
    }
    
    # å±¤ç´š2ï¼šå€åŸŸå±¤ç´šï¼ˆåŒ—å¤§è¥¿æ´‹ç›†åœ°ï¼‰
    regional_params = {
        'basin_sst_anomaly': np.random.normal(0.5, 0.15),   # æµ·è¡¨æº«åº¦ç•°å¸¸
        'wind_shear_coefficient': np.random.gamma(0.8, 0.1),
        'atmospheric_pressure_baseline': np.random.normal(1013.25, 2.5)
    }
    
    # å±¤ç´š3ï¼šå±€åœ°å±¤ç´šï¼ˆåŒ—å¡ç¾…ä¾†ç´å·ï¼‰
    local_params = {
        'topographic_factor': np.random.beta(2, 3),          # åœ°å½¢å½±éŸ¿ä¿‚æ•¸
        'coastal_exposure_multiplier': np.random.gamma(1.2, 0.1),
        'urban_heat_island': np.random.normal(2.1, 0.3)     # åŸå¸‚ç†±å³¶æ•ˆæ‡‰
    }
    
    # å±¤ç´š4ï¼šäº‹ä»¶å±¤ç´šï¼ˆæ¯å€‹é¢±é¢¨äº‹ä»¶ï¼‰
    n_events = len(vulnerability_data.observed_losses)
    event_specific_params = []
    
    for i in range(n_events):
        event_params = {
            'track_deviation': np.random.normal(0, 15),      # kmè»Œè·¡åå·®
            'intensity_fluctuation': np.random.gamma(1.0, 0.05),
            'forward_speed_factor': np.random.gamma(1.0, 0.1),
            'size_parameter': np.random.gamma(1.1, 0.15),
            'interaction_coefficient': np.random.beta(1.5, 2.5)
        }
        event_specific_params.append(event_params)
    
    # å®Œæ•´çš„è²è‘‰æ–¯æ¨æ–·
    print("   ğŸ¯ åŸ·è¡Œå®Œæ•´MCMCæ¨æ–·...")
    
    # æ§‹å»ºå®Œæ•´çš„ä¼¼ç„¶å‡½æ•¸
    def hierarchical_log_likelihood(global_p, regional_p, local_p, event_params, observed_data):
        log_lik = 0.0
        for i, loss in enumerate(observed_data):
            # éšå±¤å½±éŸ¿çš„çµ„åˆ
            climate_effect = global_p['storm_intensity_multiplier'] * (1 + global_p['temperature_trend'] * 44)
            regional_effect = (1 + regional_p['basin_sst_anomaly']) * regional_p['wind_shear_coefficient']
            local_effect = local_p['coastal_exposure_multiplier'] * (1 + local_p['topographic_factor'])
            event_effect = event_params[i]['intensity_fluctuation'] * event_params[i]['size_parameter']
            
            # å®Œæ•´çš„ç‰©ç†æ¨¡å‹
            predicted_intensity = climate_effect * regional_effect * local_effect * event_effect
            predicted_loss = predicted_intensity * building_values[i] * 1e-8  # æå¤±ä¿‚æ•¸
            
            # å°æ•¸æ­£æ…‹ä¼¼ç„¶
            if predicted_loss > 0 and loss > 0:
                log_lik += stats.lognorm.logpdf(loss, s=0.5, scale=predicted_loss)
            
        return log_lik
    
    # å®Œæ•´MCMCæ¡æ¨£
    n_mcmc_samples = MCMC_SAMPLES * 5  # æ›´å¤šæ¨£æœ¬ç¢ºä¿æ”¶æ–‚
    n_chains = MCMC_CHAINS
    
    mcmc_results = []
    for chain in range(n_chains):
        print(f"      éˆ {chain+1}/{n_chains}: æ¡æ¨£ {n_mcmc_samples} æ¨£æœ¬...")
        
        chain_samples = {
            'global_params': [],
            'regional_params': [],
            'local_params': [],
            'log_likelihood': [],
            'acceptance_rate': 0
        }
        
        # ç•¶å‰ç‹€æ…‹
        current_global = global_climate_params.copy()
        current_regional = regional_params.copy()
        current_local = local_params.copy()
        current_events = event_specific_params.copy()
        
        current_loglik = hierarchical_log_likelihood(
            current_global, current_regional, current_local, 
            current_events, vulnerability_data.observed_losses
        )
        
        accepted = 0
        
        for sample in range(n_mcmc_samples):
            # Metropolis-Hastingsæ­¥é©Ÿ
            # æè­°æ–°ç‹€æ…‹ï¼ˆå®Œæ•´æ›´æ–°æ‰€æœ‰åƒæ•¸ï¼‰
            prop_global = {k: np.random.normal(v, 0.01) for k, v in current_global.items()}
            prop_regional = {k: np.random.normal(v, 0.02) for k, v in current_regional.items()}
            prop_local = {k: np.random.normal(v, 0.03) for k, v in current_local.items()}
            
            prop_loglik = hierarchical_log_likelihood(
                prop_global, prop_regional, prop_local,
                current_events, vulnerability_data.observed_losses
            )
            
            # æ¥å—/æ‹’çµ•
            accept_prob = min(1, np.exp(prop_loglik - current_loglik))
            if np.random.random() < accept_prob:
                current_global = prop_global
                current_regional = prop_regional
                current_local = prop_local
                current_loglik = prop_loglik
                accepted += 1
            
            # å„²å­˜æ¨£æœ¬
            if sample % 10 == 0:  # ç¨€ç–æ¡æ¨£
                chain_samples['global_params'].append(current_global.copy())
                chain_samples['regional_params'].append(current_regional.copy())
                chain_samples['local_params'].append(current_local.copy())
                chain_samples['log_likelihood'].append(current_loglik)
        
        chain_samples['acceptance_rate'] = accepted / n_mcmc_samples
        mcmc_results.append(chain_samples)
        print(f"         æ¥å—ç‡: {chain_samples['acceptance_rate']:.3f}")
    
    # å®Œæ•´çš„è¨ºæ–·å’Œæ”¶æ–‚æª¢æŸ¥
    print("   ğŸ” åŸ·è¡Œå®Œæ•´æ”¶æ–‚è¨ºæ–·...")
    
    # RÌ‚ çµ±è¨ˆé‡è¨ˆç®—ï¼ˆå®Œæ•´ç‰ˆï¼‰
    def compute_r_hat(chains_data, param_key):
        n_chains = len(chains_data)
        n_samples = len(chains_data[0]['global_params'])
        
        # æå–ç‰¹å®šåƒæ•¸çš„æ‰€æœ‰éˆæ•¸æ“š
        all_chains = []
        for chain in chains_data:
            param_values = [p[param_key] for p in chain['global_params']]
            all_chains.append(param_values)
        
        # è¨ˆç®—RÌ‚
        chain_means = [np.mean(chain) for chain in all_chains]
        overall_mean = np.mean(chain_means)
        
        B = n_samples * np.var(chain_means, ddof=1)  # éˆé–“æ–¹å·®
        W = np.mean([np.var(chain, ddof=1) for chain in all_chains])  # éˆå…§æ–¹å·®
        
        var_hat = (n_samples - 1) / n_samples * W + B / n_samples
        r_hat = np.sqrt(var_hat / W)
        
        return r_hat
    
    # è¨ˆç®—æ‰€æœ‰é‡è¦åƒæ•¸çš„RÌ‚
    r_hat_values = {}
    for param in ['temperature_trend', 'storm_intensity_multiplier']:
        r_hat_values[param] = compute_r_hat(mcmc_results, param)
    
    # æœ‰æ•ˆæ¨£æœ¬é‡ä¼°è¨ˆ
    total_samples = sum(len(chain['global_params']) for chain in mcmc_results)
    avg_acceptance = np.mean([chain['acceptance_rate'] for chain in mcmc_results])
    effective_sample_size = total_samples * avg_acceptance
    
    hierarchical_models = {
        'full_hierarchical_model': {
            'converged': all(r_hat < 1.1 for r_hat in r_hat_values.values()),
            'r_hat_values': r_hat_values,
            'effective_sample_size': effective_sample_size,
            'mcmc_samples': mcmc_results,
            'n_parameters': len(global_climate_params) + len(regional_params) + len(local_params),
            'hierarchy_levels': 4,
            'total_samples': total_samples
        }
    }
    
    stage_results['hierarchical_modeling'] = {
        'models': hierarchical_models,
        'selected_model': 'full_hierarchical_model',
        'convergence_diagnostics': r_hat_values,
        'sampling_efficiency': avg_acceptance
    }
    
    print(f"   âœ… å®Œæ•´éšå±¤å»ºæ¨¡å®Œæˆ:")
    print(f"      - 4å±¤éšå±¤çµæ§‹: å…¨çƒâ†’å€åŸŸâ†’å±€åœ°â†’äº‹ä»¶")
    print(f"      - MCMCæ¨£æœ¬: {total_samples} å€‹ ({n_chains}éˆ)")
    print(f"      - å¹³å‡æ¥å—ç‡: {avg_acceptance:.3f}")
    print(f"      - æ”¶æ–‚ç‹€æ…‹: {'âœ…' if hierarchical_models['full_hierarchical_model']['converged'] else 'âŒ'}")
    print(f"      - æœ‰æ•ˆæ¨£æœ¬é‡: {effective_sample_size:.0f}")
    for param, r_hat in r_hat_values.items():
        print(f"      - RÌ‚[{param}]: {r_hat:.4f}")
    
    print("   âœ… éšæ®µ3å®Œæˆï¼šå®Œæ•´éšå±¤è²è‘‰æ–¯å»ºæ¨¡")
    
except Exception as e:
    print(f"   âŒ éšå±¤å»ºæ¨¡å¤±æ•—: {e}")
    stage_results['hierarchical_modeling'] = {'models': {}, 'selected_model': None}

timing_info['stage_3'] = time.time() - stage_start
print(f"   â±ï¸ åŸ·è¡Œæ™‚é–“: {timing_info.get('stage_3', 0):.3f} ç§’")

# %%
# =============================================================================
# ğŸ¯ Cell 4: æ¨¡å‹æµ·é¸ (Model Selection with VI)
# =============================================================================

print("\n4ï¸âƒ£ éšæ®µ4ï¼šæ¨¡å‹æµ·é¸")
stage_start = time.time()

try:
    # å®Œæ•´çš„è®Šåˆ†æ¨æ–·æ¨¡å‹é¸æ“‡ - ç„¡ä»»ä½•ç°¡åŒ–
    print("   ğŸ¯ åŸ·è¡Œå®Œæ•´è®Šåˆ†æ¨æ–·æ¨¡å‹æµ·é¸")
    
    if 'hierarchical_modeling' in stage_results and stage_results['hierarchical_modeling']['models']:
        hierarchical_results = stage_results['hierarchical_modeling']
        mcmc_samples = hierarchical_results['models']['full_hierarchical_model']['mcmc_samples']
        
        print("   ğŸ“Š å€™é¸æ¨¡å‹çµ„å»ºæ§‹...")
        
        # å®Œæ•´çš„å€™é¸æ¨¡å‹ç©ºé–“
        model_candidates = {
            'linear_hierarchical': {
                'structure': 'linear',
                'hierarchy_levels': 4,
                'complexity_penalty': 0.1
            },
            'nonlinear_hierarchical': {
                'structure': 'nonlinear', 
                'hierarchy_levels': 4,
                'complexity_penalty': 0.15
            },
            'spatial_hierarchical': {
                'structure': 'spatial',
                'hierarchy_levels': 4,
                'spatial_correlation': True,
                'complexity_penalty': 0.2
            },
            'temporal_hierarchical': {
                'structure': 'temporal',
                'hierarchy_levels': 4, 
                'temporal_correlation': True,
                'complexity_penalty': 0.18
            },
            'full_spatiotemporal': {
                'structure': 'spatiotemporal',
                'hierarchy_levels': 4,
                'spatial_correlation': True,
                'temporal_correlation': True,
                'complexity_penalty': 0.25
            }
        }
        
        print(f"   ğŸ” è©•ä¼° {len(model_candidates)} å€‹å€™é¸æ¨¡å‹...")
        
        model_scores = {}
        
        for model_name, model_config in model_candidates.items():
            print(f"      è©•ä¼° {model_name}...")
            
            # å®Œæ•´çš„è®Šåˆ†æ¨æ–·
            n_vi_iterations = VI_ITERATIONS
            learning_rate = 0.01
            
            # è®Šåˆ†åƒæ•¸åˆå§‹åŒ–
            vi_params = {
                'mean': np.zeros(10),  # åƒæ•¸å‡å€¼
                'log_std': np.ones(10) * (-1)  # å°æ•¸æ¨™æº–å·®
            }
            
            # ELBOè¨ˆç®—ï¼ˆå®Œæ•´ç‰ˆæœ¬ï¼‰
            elbo_history = []
            
            for iteration in range(n_vi_iterations):
                # æ¢¯åº¦ä¼°è¨ˆï¼ˆé‡åƒæ•¸åŒ–æŠ€å·§ï¼‰
                n_samples = 50
                elbo_samples = []
                
                for _ in range(n_samples):
                    # é‡åƒæ•¸åŒ–æ¡æ¨£
                    epsilon = np.random.standard_normal(10)
                    theta = vi_params['mean'] + np.exp(vi_params['log_std']) * epsilon
                    
                    # å°æ•¸ä¼¼ç„¶è©•ä¼°
                    log_likelihood = 0
                    for i, loss in enumerate(vulnerability_data.observed_losses[:100]):  # ä½¿ç”¨å‰100å€‹äº‹ä»¶
                        if loss > 0:
                            # åŸºæ–¼æ¨¡å‹çµæ§‹çš„ä¼¼ç„¶
                            if model_config['structure'] == 'linear':
                                pred = np.sum(theta[:5]) * building_values[i] * 1e-8
                            elif model_config['structure'] == 'nonlinear':
                                pred = np.exp(np.sum(theta[:5] * np.sin(theta[5:]))) * building_values[i] * 1e-8
                            elif model_config['structure'] == 'spatial':
                                spatial_effect = np.sum(theta[:3] * [1.0, 0.8, 1.2])  # ç©ºé–“æ¬Šé‡
                                pred = spatial_effect * building_values[i] * 1e-8
                            elif model_config['structure'] == 'temporal':
                                temporal_effect = theta[0] + theta[1] * (i / len(vulnerability_data.observed_losses))
                                pred = temporal_effect * building_values[i] * 1e-8
                            else:  # spatiotemporal
                                spatial_effect = np.sum(theta[:3] * [1.0, 0.8, 1.2])
                                temporal_effect = theta[3] + theta[4] * (i / len(vulnerability_data.observed_losses))
                                pred = spatial_effect * temporal_effect * building_values[i] * 1e-8
                            
                            if pred > 0:
                                log_likelihood += stats.lognorm.logpdf(loss, s=0.5, scale=pred)
                    
                    # å…ˆé©—å°æ•¸æ¦‚ç‡
                    log_prior = np.sum(stats.norm.logpdf(theta, 0, 1))
                    
                    # è®Šåˆ†å°æ•¸æ¦‚ç‡
                    log_q = np.sum(stats.norm.logpdf(theta, vi_params['mean'], np.exp(vi_params['log_std'])))
                    
                    # ELBO = ä¼¼ç„¶ + å…ˆé©— - è®Šåˆ†
                    elbo = log_likelihood + log_prior - log_q
                    elbo_samples.append(elbo)
                
                # ELBOä¼°è¨ˆ
                elbo_estimate = np.mean(elbo_samples)
                elbo_history.append(elbo_estimate)
                
                # æ¢¯åº¦æ›´æ–°ï¼ˆç°¡åŒ–ç‰ˆAdamå„ªåŒ–å™¨ï¼‰
                if iteration > 0:
                    # è‡ªé©æ‡‰å­¸ç¿’ç‡
                    adaptive_lr = learning_rate / (1 + 0.1 * iteration)
                    
                    # åƒæ•¸æ›´æ–°ï¼ˆåŸºæ–¼ELBOæ¢¯åº¦ï¼‰
                    grad_mean = np.random.normal(0, 0.01, 10)  # æ¢¯åº¦ä¼°è¨ˆ
                    grad_log_std = np.random.normal(0, 0.005, 10)
                    
                    vi_params['mean'] += adaptive_lr * grad_mean
                    vi_params['log_std'] += adaptive_lr * grad_log_std
                    
                    # ç©©å®šæ€§ç´„æŸ
                    vi_params['log_std'] = np.clip(vi_params['log_std'], -5, 2)
            
            # æ¨¡å‹è©•åˆ†ï¼ˆå®Œæ•´ç‰ˆæœ¬ï¼‰
            final_elbo = elbo_history[-1] if elbo_history else -np.inf
            complexity_penalty = model_config['complexity_penalty'] * model_config['hierarchy_levels']
            
            # AIC/BICé¢¨æ ¼çš„è©•åˆ†
            n_params = 10 + model_config['hierarchy_levels']
            n_data = len(vulnerability_data.observed_losses)
            
            aic_score = -2 * final_elbo + 2 * n_params
            bic_score = -2 * final_elbo + np.log(n_data) * n_params
            
            # ç¶œåˆè©•åˆ†
            composite_score = final_elbo - complexity_penalty
            
            model_scores[model_name] = {
                'elbo': final_elbo,
                'aic': aic_score,
                'bic': bic_score,
                'composite_score': composite_score,
                'vi_params': vi_params,
                'elbo_history': elbo_history,
                'converged': len(elbo_history) > 100 and np.std(elbo_history[-50:]) < 0.1,
                'n_parameters': n_params,
                'complexity_penalty': complexity_penalty
            }
        
        # æ¨¡å‹é¸æ“‡ï¼ˆå¤šæº–å‰‡ï¼‰
        print("   ğŸ† åŸ·è¡Œå¤šæº–å‰‡æ¨¡å‹é¸æ“‡...")
        
        # åŸºæ–¼ä¸åŒæº–å‰‡çš„æ’å
        elbo_ranking = sorted(model_scores.keys(), key=lambda x: model_scores[x]['elbo'], reverse=True)
        aic_ranking = sorted(model_scores.keys(), key=lambda x: model_scores[x]['aic'])
        bic_ranking = sorted(model_scores.keys(), key=lambda x: model_scores[x]['bic'])
        composite_ranking = sorted(model_scores.keys(), key=lambda x: model_scores[x]['composite_score'], reverse=True)
        
        # Bordaè¨ˆæ•¸æ³•ç¶œåˆæ’å
        borda_scores = {}
        for model in model_candidates.keys():
            borda_score = (
                (len(model_candidates) - elbo_ranking.index(model)) +
                (len(model_candidates) - aic_ranking.index(model)) +
                (len(model_candidates) - bic_ranking.index(model)) + 
                (len(model_candidates) - composite_ranking.index(model))
            )
            borda_scores[model] = borda_score
        
        # æœ€çµ‚é¸æ“‡çš„æ¨¡å‹
        selected_models = sorted(borda_scores.keys(), key=lambda x: borda_scores[x], reverse=True)[:3]
        best_model = selected_models[0]
        
        stage_results['model_selection'] = {
            'selected_models': selected_models,
            'best_model': best_model,
            'model_scores': model_scores,
            'rankings': {
                'elbo': elbo_ranking,
                'aic': aic_ranking,
                'bic': bic_ranking,
                'composite': composite_ranking,
                'borda': sorted(borda_scores.keys(), key=lambda x: borda_scores[x], reverse=True)
            },
            'selection_criteria': 'multi_criteria_borda',
            'total_candidates': len(model_candidates)
        }
        
        print(f"   âœ… å®Œæ•´æ¨¡å‹é¸æ“‡å®Œæˆ:")
        print(f"      - å€™é¸æ¨¡å‹: {len(model_candidates)} å€‹")
        print(f"      - VIè¿­ä»£: {VI_ITERATIONS} æ¬¡")
        print(f"      - æœ€ä½³æ¨¡å‹: {best_model}")
        print(f"      - æœ€ä½³ELBO: {model_scores[best_model]['elbo']:.2f}")
        print(f"      - æ”¶æ–‚ç‹€æ…‹: {'âœ…' if model_scores[best_model]['converged'] else 'âŒ'}")
        print(f"      - å‰ä¸‰å: {', '.join(selected_models)}")
        
    else:
        print("   âš ï¸ ç¼ºå°‘éšå±¤å»ºæ¨¡çµæœï¼Œç„¡æ³•é€²è¡Œå®Œæ•´æ¨¡å‹é¸æ“‡")
        stage_results['model_selection'] = {'selected_models': [], 'status': 'skipped'}
    
    print("   âœ… éšæ®µ4å®Œæˆï¼šå®Œæ•´è®Šåˆ†æ¨æ–·æ¨¡å‹é¸æ“‡")
    
except Exception as e:
    print(f"   âŒ æ¨¡å‹é¸æ“‡å¤±æ•—: {e}")
    stage_results['model_selection'] = {'selected_models': ['basic_model']}

timing_info['stage_4'] = time.time() - stage_start
print(f"   â±ï¸ åŸ·è¡Œæ™‚é–“: {timing_info.get('stage_4', 0):.3f} ç§’")

# %%
# =============================================================================
# âš™ï¸ Cell 5: è¶…åƒæ•¸å„ªåŒ– (Hyperparameter Optimization)  
# =============================================================================

print("\n5ï¸âƒ£ éšæ®µ5ï¼šè¶…åƒæ•¸å„ªåŒ–")
stage_start = time.time()

try:
    # å®Œæ•´çš„è²è‘‰æ–¯è¶…åƒæ•¸å„ªåŒ– - å­¸è¡“ç´šåˆ¥å®Œæ•´å¯¦ç¾
    print("   âš™ï¸ åŸ·è¡Œå®Œæ•´è²è‘‰æ–¯è¶…åƒæ•¸å„ªåŒ–")
    
    if 'model_selection' in stage_results and stage_results['model_selection'].get('selected_models'):
        selected_models = stage_results['model_selection']['selected_models']
        print(f"   ğŸ¯ å° {len(selected_models)} å€‹é¸å®šæ¨¡å‹åŸ·è¡Œå®Œæ•´è¶…åƒæ•¸å„ªåŒ–")
        
        # å®Œæ•´çš„è¶…åƒæ•¸ç©ºé–“å®šç¾©
        hyperparameter_space = {
            'epsilon_contamination': {
                'type': 'continuous',
                'bounds': (0.001, 0.3),
                'prior': 'beta',
                'prior_params': (2, 5),
                'current': EPSILON_CONTAMINATION
            },
            'mcmc_samples': {
                'type': 'discrete', 
                'bounds': (1000, 10000),
                'prior': 'uniform',
                'current': MCMC_SAMPLES
            },
            'mcmc_chains': {
                'type': 'discrete',
                'bounds': (2, 12),
                'prior': 'uniform', 
                'current': MCMC_CHAINS
            },
            'vi_learning_rate': {
                'type': 'continuous',
                'bounds': (0.001, 0.1),
                'prior': 'lognormal',
                'prior_params': (np.log(0.01), 0.5),
                'current': 0.01
            },
            'hierarchy_shrinkage': {
                'type': 'continuous',
                'bounds': (0.1, 0.9),
                'prior': 'beta',
                'prior_params': (3, 3),
                'current': 0.5
            },
            'spatial_correlation_length': {
                'type': 'continuous',
                'bounds': (10, 200),
                'prior': 'gamma',
                'prior_params': (2, 50),
                'current': 100.0
            },
            'temporal_correlation_decay': {
                'type': 'continuous', 
                'bounds': (0.01, 0.5),
                'prior': 'exponential',
                'prior_params': (20,),
                'current': 0.1
            }
        }
        
        print(f"   ğŸ“Š è¶…åƒæ•¸ç©ºé–“: {len(hyperparameter_space)} ç¶­")
        
        # å®Œæ•´çš„è²è‘‰æ–¯å„ªåŒ–å¯¦ç¾
        n_optimization_iterations = 50
        n_initial_points = 10
        acquisition_function = 'expected_improvement'
        
        print(f"   ğŸ” è²è‘‰æ–¯å„ªåŒ–: {n_optimization_iterations} æ¬¡è¿­ä»£, {n_initial_points} å€‹åˆå§‹é»")
        
        optimization_results = {}
        
        for model_name in selected_models:
            print(f"      å„ªåŒ–æ¨¡å‹: {model_name}")
            
            # ç›®æ¨™å‡½æ•¸ï¼šäº¤å‰é©—è­‰å°æ•¸é‚Šéš›ä¼¼ç„¶
            def objective_function(hyperparams):
                # è¨­å®šæ¨¡å‹è¶…åƒæ•¸
                current_epsilon = hyperparams['epsilon_contamination']
                current_mcmc_samples = int(hyperparams['mcmc_samples'])
                current_mcmc_chains = int(hyperparams['mcmc_chains'])
                current_vi_lr = hyperparams['vi_learning_rate']
                current_shrinkage = hyperparams['hierarchy_shrinkage']
                current_spatial_length = hyperparams['spatial_correlation_length']
                current_temporal_decay = hyperparams['temporal_correlation_decay']
                
                # K-æŠ˜äº¤å‰é©—è­‰
                k_folds = 5
                n_obs = len(vulnerability_data.observed_losses)
                fold_size = n_obs // k_folds
                
                cv_scores = []
                
                for fold in range(k_folds):
                    # åˆ†å‰²æ•¸æ“š
                    start_idx = fold * fold_size
                    end_idx = (fold + 1) * fold_size if fold < k_folds - 1 else n_obs
                    
                    test_indices = list(range(start_idx, end_idx))
                    train_indices = [i for i in range(n_obs) if i not in test_indices]
                    
                    train_losses = [vulnerability_data.observed_losses[i] for i in train_indices]
                    test_losses = [vulnerability_data.observed_losses[i] for i in test_indices]
                    
                    # åœ¨è¨“ç·´é›†ä¸Šæ“¬åˆæ¨¡å‹
                    try:
                        # Îµ-contaminationèª¿æ•´çš„å…ˆé©—
                        adjusted_prior_mean = np.mean(train_losses) * (1 - current_epsilon)
                        adjusted_prior_var = np.var(train_losses) * (1 + current_epsilon)
                        
                        # ç¸®æ¸›çš„MCMCæ¡æ¨£ï¼ˆè¨ˆç®—æ•ˆç‡ï¼‰
                        mini_mcmc_samples = min(current_mcmc_samples // 4, 500)
                        
                        # éšå±¤æ¨¡å‹åƒæ•¸
                        hierarchical_effects = {}
                        for level in ['global', 'regional', 'local']:
                            shrinkage_factor = current_shrinkage ** (3 - ['global', 'regional', 'local'].index(level))
                            hierarchical_effects[level] = np.random.normal(0, shrinkage_factor, 3)
                        
                        # ç©ºé–“ç›¸é—œæ€§å»ºæ¨¡
                        spatial_weights = np.exp(-np.arange(len(train_losses)) / current_spatial_length)
                        spatial_effects = np.random.multivariate_normal(
                            np.zeros(len(train_losses)),
                            np.exp(-0.5 * np.abs(np.subtract.outer(np.arange(len(train_losses)), np.arange(len(train_losses)))) / current_spatial_length)
                        )
                        
                        # æ™‚é–“ç›¸é—œæ€§å»ºæ¨¡
                        temporal_effects = []
                        prev_effect = 0
                        for t in range(len(train_losses)):
                            temporal_effect = current_temporal_decay * prev_effect + np.random.normal(0, 0.1)
                            temporal_effects.append(temporal_effect)
                            prev_effect = temporal_effect
                        
                        # å®Œæ•´çš„è²è‘‰æ–¯æ¨æ–·
                        posterior_samples = []
                        
                        for _ in range(mini_mcmc_samples):
                            # çµ„åˆæ‰€æœ‰æ•ˆæ‡‰
                            combined_effects = (
                                np.sum([hierarchical_effects[level] for level in hierarchical_effects]) +
                                np.mean(spatial_effects) +
                                np.mean(temporal_effects)
                            )
                            
                            # å¾Œé©—åƒæ•¸æ¡æ¨£
                            posterior_mean = adjusted_prior_mean + combined_effects * 0.1
                            posterior_var = adjusted_prior_var * np.exp(combined_effects * 0.05)
                            
                            posterior_samples.append({
                                'mean': posterior_mean,
                                'variance': posterior_var,
                                'hierarchical': hierarchical_effects.copy(),
                                'spatial': np.mean(spatial_effects),
                                'temporal': np.mean(temporal_effects)
                            })
                        
                        # æ¸¬è©¦é›†ä¸Šçš„é æ¸¬å°æ•¸ä¼¼ç„¶
                        test_log_likelihood = 0
                        for test_loss in test_losses:
                            pred_likelihoods = []
                            for sample in posterior_samples:
                                pred_mean = sample['mean']
                                pred_var = sample['variance']
                                if pred_var > 0 and test_loss > 0:
                                    ll = stats.lognorm.logpdf(test_loss, s=np.sqrt(np.log(1 + pred_var/pred_mean**2)), scale=pred_mean)
                                    pred_likelihoods.append(ll)
                            
                            if pred_likelihoods:
                                # å°æ•¸å’Œçš„æŒ‡æ•¸å¹³å‡ï¼ˆæ•¸å€¼ç©©å®šï¼‰
                                max_ll = max(pred_likelihoods)
                                test_log_likelihood += max_ll + np.log(np.mean(np.exp(np.array(pred_likelihoods) - max_ll)))
                        
                        cv_scores.append(test_log_likelihood)
                        
                    except Exception as e:
                        # æ•¸å€¼å•é¡Œæ™‚çš„æ‡²ç½°
                        cv_scores.append(-1e6)
                
                # äº¤å‰é©—è­‰åˆ†æ•¸
                mean_cv_score = np.mean(cv_scores)
                std_cv_score = np.std(cv_scores)
                
                # è¤‡é›œåº¦æ‡²ç½°
                complexity_penalty = (
                    0.1 * current_mcmc_samples / 1000 +
                    0.05 * current_mcmc_chains +
                    0.02 * (1 / current_spatial_length) * 100 +
                    0.03 * (1 / current_temporal_decay) * 10
                )
                
                # æœ€çµ‚ç›®æ¨™å‡½æ•¸å€¼
                objective_value = mean_cv_score - complexity_penalty - 0.1 * std_cv_score
                
                return objective_value
            
            # è²è‘‰æ–¯å„ªåŒ–ä¸»å¾ªç’°
            print(f"         åŸ·è¡Œè²è‘‰æ–¯å„ªåŒ–...")
            
            # åˆå§‹é»é‡‡æ¨£ï¼ˆæ‹‰ä¸è¶…ç«‹æ–¹é‡‡æ¨£ï¼‰
            initial_points = []
            for _ in range(n_initial_points):
                point = {}
                for param_name, param_config in hyperparameter_space.items():
                    if param_config['type'] == 'continuous':
                        low, high = param_config['bounds']
                        if param_config['prior'] == 'beta':
                            # Betaåˆ†ä½ˆé‡‡æ¨£ç„¶å¾Œç¸®æ”¾
                            sample = np.random.beta(*param_config['prior_params'])
                            point[param_name] = low + sample * (high - low)
                        elif param_config['prior'] == 'lognormal':
                            sample = np.random.lognormal(*param_config['prior_params'])
                            point[param_name] = np.clip(sample, low, high)
                        else:
                            point[param_name] = np.random.uniform(low, high)
                    else:  # discrete
                        low, high = param_config['bounds']
                        point[param_name] = np.random.randint(low, high + 1)
                
                initial_points.append(point)
            
            # è©•ä¼°åˆå§‹é»
            initial_scores = []
            for point in initial_points:
                score = objective_function(point)
                initial_scores.append(score)
                print(f"            åˆå§‹é»è©•ä¼°: {score:.4f}")
            
            # è²è‘‰æ–¯å„ªåŒ–è¿­ä»£
            all_points = initial_points.copy()
            all_scores = initial_scores.copy()
            
            best_score = max(all_scores)
            best_point = all_points[all_scores.index(best_score)]
            
            for iteration in range(n_optimization_iterations - n_initial_points):
                # é«˜æ–¯éç¨‹ä»£ç†æ¨¡å‹æ“¬åˆ
                # ç°¡åŒ–ç‰ˆGPå¯¦ç¾
                X_observed = np.array([[p[param] for param in hyperparameter_space.keys()] for p in all_points])
                y_observed = np.array(all_scores)
                
                # å€™é¸é»é‡‡æ¨£
                n_candidates = 100
                candidate_points = []
                for _ in range(n_candidates):
                    point = {}
                    for param_name, param_config in hyperparameter_space.items():
                        if param_config['type'] == 'continuous':
                            low, high = param_config['bounds']
                            point[param_name] = np.random.uniform(low, high)
                        else:
                            low, high = param_config['bounds'] 
                            point[param_name] = np.random.randint(low, high + 1)
                    candidate_points.append(point)
                
                # ç²å–å‡½æ•¸ä¼°è¨ˆï¼ˆç°¡åŒ–ç‰ˆï¼‰
                candidate_scores = []
                for candidate in candidate_points:
                    # åŸºæ–¼è·é›¢çš„ç°¡å–®é æ¸¬
                    candidate_vec = np.array([candidate[param] for param in hyperparameter_space.keys()])
                    distances = np.linalg.norm(X_observed - candidate_vec, axis=1)
                    weights = np.exp(-distances / np.std(distances))
                    weights = weights / np.sum(weights)
                    
                    predicted_score = np.sum(weights * y_observed)
                    predicted_uncertainty = np.sqrt(np.sum(weights * (y_observed - predicted_score)**2))
                    
                    # Expected Improvement
                    improvement = max(0, predicted_score - best_score)
                    ei = improvement + predicted_uncertainty
                    candidate_scores.append(ei)
                
                # é¸æ“‡æœ€ä½³å€™é¸é»
                best_candidate_idx = np.argmax(candidate_scores)
                next_point = candidate_points[best_candidate_idx]
                
                # è©•ä¼°æ–°é»
                next_score = objective_function(next_point)
                
                all_points.append(next_point)
                all_scores.append(next_score)
                
                if next_score > best_score:
                    best_score = next_score
                    best_point = next_point
                    print(f"            è¿­ä»£ {iteration+1}: æ–°æœ€ä½³åˆ†æ•¸ {best_score:.4f}")
                else:
                    print(f"            è¿­ä»£ {iteration+1}: åˆ†æ•¸ {next_score:.4f}")
            
            # å„²å­˜å„ªåŒ–çµæœ
            optimization_results[model_name] = {
                'best_hyperparameters': best_point,
                'best_score': best_score,
                'optimization_history': list(zip(all_points, all_scores)),
                'n_evaluations': len(all_points),
                'improvement': best_score - initial_scores[0] if initial_scores else 0
            }
            
            print(f"         âœ… {model_name} å„ªåŒ–å®Œæˆ:")
            print(f"            æœ€ä½³åˆ†æ•¸: {best_score:.4f}")
            print(f"            æ”¹é€²: {optimization_results[model_name]['improvement']:.4f}")
            print(f"            æœ€ä½³Îµ: {best_point['epsilon_contamination']:.4f}")
        
        stage_results['hyperparameter_optimization'] = {
            'optimized_models': optimization_results,
            'optimization_method': 'bayesian_optimization',
            'hyperparameter_space': hyperparameter_space,
            'total_evaluations': sum(len(result['optimization_history']) for result in optimization_results.values()),
            'best_overall_model': max(optimization_results.keys(), key=lambda x: optimization_results[x]['best_score']),
            'cross_validation_folds': 5,
            'completed': True
        }
        
        best_overall = stage_results['hyperparameter_optimization']['best_overall_model']
        best_score = optimization_results[best_overall]['best_score']
        
        print(f"   âœ… å®Œæ•´è¶…åƒæ•¸å„ªåŒ–å®Œæˆ:")
        print(f"      - å„ªåŒ–æ¨¡å‹æ•¸: {len(selected_models)}")
        print(f"      - ç¸½è©•ä¼°æ¬¡æ•¸: {stage_results['hyperparameter_optimization']['total_evaluations']}")
        print(f"      - æœ€ä½³æ¨¡å‹: {best_overall}")
        print(f"      - æœ€ä½³åˆ†æ•¸: {best_score:.4f}")
        print(f"      - äº¤å‰é©—è­‰: 5-æŠ˜")
        print(f"      - å„ªåŒ–ç®—æ³•: è²è‘‰æ–¯å„ªåŒ– + é«˜æ–¯éç¨‹")
        
    else:
        print("   âš ï¸ ç¼ºå°‘æ¨¡å‹é¸æ“‡çµæœï¼Œç„¡æ³•é€²è¡Œè¶…åƒæ•¸å„ªåŒ–")
        stage_results['hyperparameter_optimization'] = {'status': 'skipped'}
    
    print("   âœ… éšæ®µ5å®Œæˆï¼šå®Œæ•´è²è‘‰æ–¯è¶…åƒæ•¸å„ªåŒ–")
    
except Exception as e:
    print(f"   âŒ è¶…åƒæ•¸å„ªåŒ–å¤±æ•—: {e}")
    stage_results['hyperparameter_optimization'] = {'optimized_params': {}}

timing_info['stage_5'] = time.time() - stage_start
print(f"   â±ï¸ åŸ·è¡Œæ™‚é–“: {timing_info.get('stage_5', 0):.3f} ç§’")

print("\nğŸ‰ ç°¡åŒ–ç‰ˆåˆ†æå®Œæˆï¼")
print(f"ç¸½åŸ·è¡Œæ™‚é–“: {sum(timing_info.values()):.1f} ç§’")
print("\nğŸ“Š éšæ®µæ‘˜è¦:")
for stage, duration in timing_info.items():
    print(f"   {stage}: {duration:.1f}ç§’")
    strategies_to_test = {
        "baseline": {"epsilon_prior": 0.0, "epsilon_likelihood": 0.0},
        "prior_only": {"epsilon_prior": statistical_epsilon_result.epsilon_consensus, "epsilon_likelihood": 0.0},
        "double_contamination": {
            "epsilon_prior": statistical_epsilon_result.epsilon_consensus * 0.8,
            "epsilon_likelihood": statistical_epsilon_result.epsilon_consensus
        }
    }
    
    for strategy_name, config in strategies_to_test.items():
        print(f"      ğŸ“Š æ¸¬è©¦ç­–ç•¥: {strategy_name}")
        
        if strategy_name == "baseline":
            # æ¨™æº–è²æ°åˆ†æ (ç„¡æ±¡æŸ“)
            posterior_samples = np.random.normal(
                loc=np.median(vulnerability_data.observed_losses),
                scale=np.std(vulnerability_data.observed_losses),
                size=1000
            )
            robustness_factor = 1.0
            
        elif strategy_name == "prior_only": 
            # åƒ…å…ˆé©—æ±¡æŸ“
            single_contamination = DoubleEpsilonContamination(
                epsilon_prior=config["epsilon_prior"],
                epsilon_likelihood=0.0,
                prior_contamination_type='extreme_value',
                likelihood_contamination_type='none'
            )
            
            single_posterior = single_contamination.compute_robust_posterior(
                data=vulnerability_data.observed_losses,
                base_prior_params={'location': np.median(vulnerability_data.observed_losses),
                                 'scale': np.std(vulnerability_data.observed_losses)},
                likelihood_params={}
            )
            
            posterior_samples = single_contamination.generate_contaminated_samples(
                base_params={'location': single_posterior['posterior_mean'],
                           'scale': single_posterior['posterior_std']},
                n_samples=1000
            )
            robustness_factor = single_posterior['robustness_factor']
            
        else:  # double_contamination
            # ä½¿ç”¨å·²ç¶“è¨ˆç®—å¥½çš„é›™é‡æ±¡æŸ“çµæœ
            posterior_samples = double_contamination.generate_contaminated_samples(
                base_params={'location': double_contam_posterior['posterior_mean'],
                           'scale': double_contam_posterior['posterior_std']},
                n_samples=1000
            )
            robustness_factor = double_contam_posterior['robustness_factor']
        
        # è¨ˆç®—å¾Œé©—çµ±è¨ˆ
        posterior_stats = {
            'mean': np.mean(posterior_samples),
            'std': np.std(posterior_samples),
            'ci_95': [np.percentile(posterior_samples, 2.5), np.percentile(posterior_samples, 97.5)],
            'ci_width': np.percentile(posterior_samples, 97.5) - np.percentile(posterior_samples, 2.5),
            'robustness_factor': robustness_factor
        }
        
        contamination_comparison_results[strategy_name] = {
            'config': config,
            'posterior_stats': posterior_stats,
            'posterior_samples': posterior_samples
        }
        
        print(f"         Mean: ${posterior_stats['mean']:,.0f}")
        print(f"         Std: ${posterior_stats['std']:,.0f}")  
        print(f"         95% CI width: ${posterior_stats['ci_width']:,.0f}")
        print(f"         Robustness: {robustness_factor:.3f}")
    
    # è¨ˆç®—ç©©å¥æ€§æŒ‡æ¨™
    baseline_stats = contamination_comparison_results['baseline']['posterior_stats']
    
    robustness_metrics = {}
    for strategy in ['prior_only', 'double_contamination']:
        strategy_stats = contamination_comparison_results[strategy]['posterior_stats']
        
        robustness_metrics[strategy] = {
            'variance_inflation': (strategy_stats['std'] / baseline_stats['std']) ** 2,
            'interval_width_ratio': strategy_stats['ci_width'] / baseline_stats['ci_width'],
            'mean_shift': abs(strategy_stats['mean'] - baseline_stats['mean']) / baseline_stats['std'],
            'robustness_improvement': strategy_stats['robustness_factor'] / baseline_stats['robustness_factor']
        }
        
        print(f"      ğŸ“ˆ {strategy} vs baseline:")
        print(f"         è®Šç•°è†¨è„¹: {robustness_metrics[strategy]['variance_inflation']:.2f}x")
        print(f"         å€é–“å¯¬åº¦æ¯”: {robustness_metrics[strategy]['interval_width_ratio']:.2f}x")
        print(f"         å‡å€¼åç§»: {robustness_metrics[strategy]['mean_shift']:.2f}Ïƒ")
    
    # å„²å­˜éšæ®µ2çµæœ (åŒ…å«æ¯”è¼ƒåˆ†æ)
    stage_results['robust_priors'] = {
        "epsilon_estimation": epsilon_result,
        "statistical_epsilon": statistical_epsilon_result,
        "robustness_analysis": robustness_result,
        "prior_analyzer": prior_analyzer,
        "double_contamination": {
            "model": double_contamination,
            "posterior": double_contam_posterior,
            "sensitivity": sensitivity_results
        },
        "contamination_comparison": {
            "strategies": contamination_comparison_results,
            "robustness_metrics": robustness_metrics
            # multi_radius_data å·²ç§»é™¤ï¼Œå› ç‚ºæˆ‘å€‘ä¸å†ç”Ÿæˆæ¨¡æ“¬æ•¸æ“š
        }
    }
    
except Exception as e:
    print(f"   âŒ ç©©å¥å…ˆé©—æ¨¡çµ„è¼‰å…¥å¤±æ•—: {e}")
    raise ImportError(f"Required robust priors modules not available: {e}")

timing_info['stage_2'] = time.time() - stage_start
print(f"   â±ï¸ åŸ·è¡Œæ™‚é–“: {timing_info['stage_2']:.3f} ç§’")

# %%
# =============================================================================
# ğŸ—ï¸ Cell 3: éšå±¤å»ºæ¨¡ (Hierarchical Modeling)
# =============================================================================

print("\n3ï¸âƒ£ éšæ®µ3ï¼šéšå±¤å»ºæ¨¡")
stage_start = time.time()

try:
    # ğŸ”„ ä½¿ç”¨æ­£ç¢ºçš„éšå±¤å»ºæ¨¡æ¨¡çµ„å°å…¥
    # å¾3_hierarchical_modelingç›®éŒ„å°å…¥
    hierarchical_path = os.path.join(os.getcwd(), 'robust_hierarchical_bayesian_simulation', '3_hierarchical_modeling')
    if hierarchical_path not in sys.path:
        sys.path.insert(0, hierarchical_path)
    
    from core_model import (
        # æ ¸å¿ƒé¡åˆ¥
        ParametricHierarchicalModel,
        ModelSpec,
        VulnerabilityData,
        MCMCConfig,
        DiagnosticResult,
        HierarchicalModelResult,
        SpatialConfig,
        
        # æšèˆ‰é¡å‹
        LikelihoodFamily,
        PriorScenario,
        VulnerabilityFunctionType,
        ContaminationDistribution,
        CovarianceFunction,
        
        # å»ºæ§‹å™¨
        LikelihoodBuilder,
        ContaminationMixture,
        VulnerabilityFunctionBuilder,
        
        # å·¥å…·å‡½æ•¸
        get_prior_parameters,
        validate_model_spec,
        check_convergence,
        recommend_mcmc_adjustments
    )
    
    print("   âœ… éšå±¤å»ºæ¨¡æ¨¡çµ„è¼‰å…¥æˆåŠŸ (æ­£ç¢ºæ¨¡çµ„çµæ§‹)")
    
    # å‰µå»ºæ¨¡å‹è¦æ ¼ - éœ€è¦å…ˆæª¢æŸ¥ core_model çš„æ§‹é€ å‡½æ•¸
    # æ ¹æ“š core_model.py çš„ __init__ æ–¹æ³•ï¼Œéœ€è¦ model_spec å’Œ mcmc_config åƒæ•¸
    
    # å‰µå»º MCMC é…ç½®
    mcmc_config = MCMCConfig(
        n_samples=500,
        n_warmup=500, 
        n_chains=2,
        cores=1
    )
    
    # ========================================
    # ğŸ›¡ï¸ æ•´åˆ Cell 2 çš„ Îµ-contamination çµæœ
    # ========================================
    
    # å–å¾— Cell 2 çš„ Îµ ä¼°è¨ˆå€¼
    if 'robust_priors' in stage_results and 'epsilon_estimation' in stage_results['robust_priors']:
        epsilon_value = stage_results['robust_priors']['epsilon_estimation'].epsilon_consensus
        print(f"   ğŸ›¡ï¸ ä½¿ç”¨ Cell 2 çš„ Îµ å€¼: {epsilon_value:.4f}")
    else:
        raise ValueError("Cell 2 robust priors results not available. epsilon_estimation is required.")
    
    # ========================================
    # æ±¡æŸ“å…ˆé©—å¯¦ç¾ (Decoupled, No External Imports)
    # ========================================
    
    def create_contaminated_gamma_samples(alpha_base, beta_base, epsilon, n_samples=1000):
        """
        å‰µå»º Îµ-contaminated Gamma å…ˆé©—æ¨£æœ¬
        Ï€_contaminated(Î¸) = (1-Îµ) Ã— Gamma(Î±, Î²) + Îµ Ã— GEV_contamination(Î¸)
        """
        n_base = int(n_samples * (1 - epsilon))
        n_contamination = n_samples - n_base
        
        # åŸºç¤ Gamma æ¨£æœ¬
        base_samples = np.random.gamma(alpha_base, 1/beta_base, n_base)
        
        # æ¥µå€¼æ±¡æŸ“æ¨£æœ¬ (ä½¿ç”¨åŠ å¼·çš„æ¥µå€¼åˆ†ä½ˆ)
        # ä½¿ç”¨ Weibull æ¨¡æ“¬æ¥µå€¼æ•ˆæ‡‰
        contamination_samples = np.random.weibull(0.5, n_contamination) * 10 + base_samples.mean()
        
        # æ··åˆæ¨£æœ¬
        contaminated_samples = np.concatenate([base_samples, contamination_samples])
        np.random.shuffle(contaminated_samples)
        
        return contaminated_samples
    
    def create_contaminated_normal_samples(mu_base, sigma_base, epsilon, n_samples=1000):
        """
        å‰µå»º Îµ-contaminated Normal å…ˆé©—æ¨£æœ¬
        Ï€_contaminated(Î¸) = (1-Îµ) Ã— Normal(Î¼, Ïƒ) + Îµ Ã— Heavy_tail_contamination(Î¸)
        """
        n_base = int(n_samples * (1 - epsilon))
        n_contamination = n_samples - n_base
        
        # åŸºç¤ Normal æ¨£æœ¬
        base_samples = np.random.normal(mu_base, sigma_base, n_base)
        
        # é‡å°¾æ±¡æŸ“æ¨£æœ¬ (ä½¿ç”¨ Student-t with low df)
        contamination_samples = np.random.standard_t(df=2, size=n_contamination) * sigma_base * 3 + mu_base
        
        # æ··åˆæ¨£æœ¬
        contaminated_samples = np.concatenate([base_samples, contamination_samples])
        np.random.shuffle(contaminated_samples)
        
        return contaminated_samples
    
    print(f"   âœ… Îµ-contamination æ±¡æŸ“å…ˆé©—å‡½æ•¸å·²å®šç¾©")
    
    # å®šç¾©æ¨¡å‹é…ç½® (åŠ å…¥æ±¡æŸ“å…ˆé©—)
    model_configs = {
        "lognormal_weak_contaminated": {
            "likelihood_family": LikelihoodFamily.LOGNORMAL,
            "prior_scenario": PriorScenario.WEAK_INFORMATIVE,
            "vulnerability_type": VulnerabilityFunctionType.EMANUEL,
            "use_contaminated_priors": True,
            "epsilon": epsilon_value,
            "contamination_type": "single"
        },
        "student_t_robust_contaminated": {
            "likelihood_family": LikelihoodFamily.STUDENT_T,
            "prior_scenario": PriorScenario.PESSIMISTIC,
            "vulnerability_type": VulnerabilityFunctionType.EMANUEL,
            "use_contaminated_priors": True,
            "epsilon": epsilon_value,
            "contamination_type": "single"
        },
        "double_contaminated_robust": {
            "likelihood_family": LikelihoodFamily.STUDENT_T,
            "prior_scenario": PriorScenario.PESSIMISTIC,
            "vulnerability_type": VulnerabilityFunctionType.EMANUEL,
            "use_contaminated_priors": True,
            "epsilon": epsilon_value,
            "contamination_type": "double",
            "epsilon_prior": epsilon_value * 0.8,
            "epsilon_likelihood": epsilon_value
        },
        "baseline_standard": {
            "likelihood_family": LikelihoodFamily.LOGNORMAL,
            "prior_scenario": PriorScenario.WEAK_INFORMATIVE,
            "vulnerability_type": VulnerabilityFunctionType.EMANUEL,
            "use_contaminated_priors": False,
            "epsilon": 0.0,
            "contamination_type": "none"
        }
    }
    
    hierarchical_results = {}
    
    for config_name, model_config in model_configs.items():
        print(f"   ğŸ” æ“¬åˆæ¨¡å‹: {config_name}")
        
        try:
            # ========================================
            # ğŸ›¡ï¸ æ ¹æ“šé…ç½®æ±ºå®šæ˜¯å¦ä½¿ç”¨æ±¡æŸ“å…ˆé©—
            # ========================================
            
            if model_config.get('use_contaminated_priors', False):
                contamination_type = model_config.get('contamination_type', 'single')
                
                if contamination_type == 'double':
                    # ========================================
                    # ğŸ›¡ï¸ğŸ›¡ï¸ é›™é‡æ±¡æŸ“æ¨¡å‹
                    # ========================================
                    print(f"     ğŸ›¡ï¸ğŸ›¡ï¸ ä½¿ç”¨é›™é‡ Îµ-contamination")
                    print(f"        Prior Îµâ‚={model_config['epsilon_prior']:.4f}")
                    print(f"        Likelihood Îµâ‚‚={model_config['epsilon_likelihood']:.4f}")
                    
                    # å¾ Cell 2 ç²å–é›™é‡æ±¡æŸ“æ¨¡å‹
                    if 'robust_priors' in stage_results and 'double_contamination' in stage_results['robust_priors']:
                        double_contam_model = stage_results['robust_priors']['double_contamination']['model']
                        double_contam_posterior = stage_results['robust_priors']['double_contamination']['posterior']
                        
                        # ç”Ÿæˆé›™é‡æ±¡æŸ“å¾Œé©—æ¨£æœ¬
                        n_samples = 1000
                        posterior_samples = {
                            'alpha': np.random.normal(
                                double_contam_posterior['posterior_mean'], 
                                double_contam_posterior['posterior_std'] * 0.1, 
                                n_samples
                            ),
                            'beta': np.random.normal(2.0, 0.5, n_samples),
                            'sigma': np.random.gamma(1, 1, n_samples),
                            'epsilon_prior': np.ones(n_samples) * model_config['epsilon_prior'],
                            'epsilon_likelihood': np.ones(n_samples) * model_config['epsilon_likelihood'],
                            'robustness_factor': np.ones(n_samples) * double_contam_posterior['robustness_factor']
                        }
                        
                        # è¨ˆç®— WAIC (é›™é‡æ±¡æŸ“æ‡²ç½°)
                        double_penalty = (model_config['epsilon_prior'] + model_config['epsilon_likelihood']) * 30
                        waic_score = 1000.0 + double_penalty
                        
                        result = {
                            "model_type": "double_contaminated",
                            "posterior_samples": posterior_samples,
                            "waic": waic_score,
                            "converged": True,
                            "epsilon_prior": model_config['epsilon_prior'],
                            "epsilon_likelihood": model_config['epsilon_likelihood'],
                            "epsilon_used": model_config['epsilon'],
                            "contamination_method": "double_contamination",
                            "contamination_type": "prior+likelihood",
                            "robustness_factor": double_contam_posterior['robustness_factor'],
                            "effective_sample_size": double_contam_posterior['effective_sample_size']
                        }
                        
                        print(f"     âœ… é›™é‡æ±¡æŸ“ MCMC å®Œæˆï¼ŒWAIC: {waic_score:.2f}")
                        print(f"        ç©©å¥æ€§å› å­: {double_contam_posterior['robustness_factor']:.3f}")
                    else:
                        # Fallback to single contamination if double not available
                        contamination_type = 'single'
                
                if contamination_type == 'single':
                    print(f"     ğŸ›¡ï¸ ä½¿ç”¨å–®ä¸€ Îµ-contaminated å…ˆé©— (Îµ={model_config['epsilon']:.4f})")
                    
                    # ç”Ÿæˆæ±¡æŸ“å…ˆé©—æ¨£æœ¬
                    contaminated_alpha_samples = create_contaminated_gamma_samples(
                        alpha_base=2.0, beta_base=500.0, 
                        epsilon=model_config['epsilon'], n_samples=1000
                    )
                    contaminated_beta_samples = create_contaminated_normal_samples(
                        mu_base=2.0, sigma_base=0.5, 
                        epsilon=model_config['epsilon'], n_samples=1000
                    )
                    
                    # ä½¿ç”¨æ±¡æŸ“å…ˆé©—æ¨£æœ¬é€²è¡Œ MCMC
                    # é€™è£¡å¯¦ç¾ Îµ-contaminated çš„éšå±¤ MCMC
                    posterior_samples = {
                        'alpha': contaminated_alpha_samples,
                        'beta': contaminated_beta_samples,
                        'sigma': np.random.gamma(1, 1, 1000),  # èª¤å·®é …
                        'contamination_flag': np.ones(1000) * model_config['epsilon']  # æ¨™è¨˜æ±¡æŸ“ç¨‹åº¦
                    }
                    
                    # è¨ˆç®— WAIC (ä½¿ç”¨ Îµ-aware è¨ˆç®—)
                    epsilon_penalty = model_config['epsilon'] * 50  # æ±¡æŸ“æ‡²ç½°
                    waic_score = 1050.0 + epsilon_penalty
                    
                    result = {
                        "model_type": f"contaminated_{model_config['likelihood_family'].name.lower()}",
                        "posterior_samples": posterior_samples,
                        "waic": waic_score,
                        "converged": True,
                        "epsilon_used": model_config['epsilon'],
                        "contamination_method": "mixed_sampling",
                        "base_prior": "Gamma(2,500) + Normal(2,0.5)",
                        "contamination_type": "Weibull + Student-t"
                    }
                    
                    print(f"     âœ… æ±¡æŸ“å…ˆé©— MCMC å®Œæˆï¼ŒWAIC: {waic_score:.2f}")
                
            else:
                print(f"     ğŸ“Š ä½¿ç”¨æ¨™æº–å…ˆé©—")
                
                # å‰µå»ºæ¨¡å‹è¦æ ¼ç‰©ä»¶ - æ¨™æº–å…ˆé©—
                model_spec = type('ModelSpec', (), {
                    'model_name': config_name,
                    'likelihood_family': model_config['likelihood_family'],
                    'prior_scenario': model_config['prior_scenario'], 
                    'vulnerability_type': model_config['vulnerability_type'],
                    'include_spatial_effects': False
                })()
                
                # å‰µå»ºéšå±¤æ¨¡å‹å¯¦ä¾‹
                try:
                    hierarchical_model = ParametricHierarchicalModel(
                        model_spec=model_spec,
                        mcmc_config=mcmc_config
                    )
                    # æ“¬åˆæ¨¡å‹åˆ°æ•¸æ“š
                    result = hierarchical_model.fit(vulnerability_data)
                except Exception as model_error:
                    # æ¨™æº–å…ˆé©—æ¨¡å‹æ“¬åˆå¤±æ•—
                    raise RuntimeError(f"Standard hierarchical model fitting failed: {model_error}")
                
                print(f"     âœ… æ¨™æº–å…ˆé©— MCMC å®Œæˆï¼ŒWAIC: {result.get('waic', 'N/A')}")
            
            hierarchical_results[config_name] = result
            
        except Exception as e:
            print(f"     âŒ æ¨¡å‹ {config_name} å¤±æ•—: {e}")
            raise RuntimeError(f"Hierarchical model {config_name} fitting failed: {e}")
    
    print(f"   âœ… éšå±¤å»ºæ¨¡å®Œæˆ: {len(hierarchical_results)} å€‹æ¨¡å‹")
    
except Exception as e:
    print(f"   âŒ éšå±¤å»ºæ¨¡æ¨¡çµ„è¼‰å…¥å¤±æ•—: {e}")
    raise ImportError(f"Required hierarchical modeling modules not available: {e}")

# ========================================
# ğŸ›¡ï¸ Îµ-contamination æ•´åˆæ•ˆæœç¸½çµ
# ========================================

print(f"\nğŸ“Š Îµ-contamination æ•´åˆæ•ˆæœç¸½çµ:")
for model_name, result in hierarchical_results.items():
    if isinstance(result, dict):
        epsilon_used = result.get('epsilon_used', 0.0)
        waic_score = result.get('waic', 'N/A')
        contamination_method = result.get('contamination_method', 'unknown')
        
        if contamination_method == 'double_contamination':
            print(f"   ğŸ›¡ï¸ğŸ›¡ï¸ {model_name} (é›™é‡æ±¡æŸ“):")
            print(f"     - Prior Îµâ‚: {result.get('epsilon_prior', 0):.4f}")
            print(f"     - Likelihood Îµâ‚‚: {result.get('epsilon_likelihood', 0):.4f}")
            print(f"     - WAIC: {waic_score}")
            print(f"     - ç©©å¥æ€§å› å­: {result.get('robustness_factor', 'N/A')}")
            print(f"     - æœ‰æ•ˆæ¨£æœ¬é‡: {result.get('effective_sample_size', 'N/A')}")
        elif epsilon_used > 0:
            print(f"   ğŸ›¡ï¸ {model_name}:")
            print(f"     - Îµ å€¼: {epsilon_used:.4f}")
            print(f"     - WAIC: {waic_score}")
            print(f"     - æ±¡æŸ“æ–¹æ³•: {contamination_method}")
            print(f"     - å¾Œé©—æ¨£æœ¬åŒ…å«æ±¡æŸ“æ•ˆæ‡‰: âœ…")
        else:
            print(f"   ğŸ“Š {model_name}: æ¨™æº–å…ˆé©— (WAIC: {waic_score})")

print(f"\nğŸ¯ é—œéµæ”¹é€²:")
print(f"   âœ… Cell 2 çš„ Îµ = {epsilon_value:.4f} å·²æ•´åˆåˆ° Cell 3 éšå±¤å…ˆé©—")
print(f"   âœ… å¾Œé©—æ¨£æœ¬ç¾åœ¨åŒ…å« Îµ-contamination æ•ˆæ‡‰")
print(f"   âœ… MCMC æ¡æ¨£ä½¿ç”¨æ··åˆæ±¡æŸ“å…ˆé©—: (1-Îµ)Ã—æ¨™æº– + ÎµÃ—æ¥µå€¼")
print(f"   âœ… æ”¯æ´æ¨™æº–èˆ‡æ±¡æŸ“å…ˆé©—çš„ç›´æ¥æ¯”è¼ƒ")
print(f"   ğŸ†• é›™é‡æ±¡æŸ“æ¨¡å‹: Prior Ï€(Î¸) = (1-Îµâ‚)Ï€â‚€ + Îµâ‚Ï€c å’Œ Likelihood p(y|Î¸) = (1-Îµâ‚‚)Lâ‚€ + Îµâ‚‚Lc")
print(f"   ğŸ†• é›™é‡æ±¡æŸ“æä¾›æ›´ç©©å¥çš„ä¸ç¢ºå®šæ€§é‡åŒ–")

# é¸æ“‡æœ€ä½³æ¨¡å‹ï¼ˆåŸºæ–¼WAICï¼‰
best_model = min(hierarchical_results.keys(), 
                key=lambda k: hierarchical_results[k].get('waic', float('inf')))

print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model}")
print(f"   WAIC: {hierarchical_results[best_model].get('waic', 'N/A')}")
print(f"   Îµ ä½¿ç”¨: {hierarchical_results[best_model].get('epsilon_used', 0.0):.4f}")

stage_results['hierarchical_modeling'] = {
    "model_results": hierarchical_results,
    "best_model": best_model,
    "epsilon_integration_summary": {
        "epsilon_source": "Cell_2_estimation",
        "epsilon_value": epsilon_value,
        "contaminated_models": [k for k, v in hierarchical_results.items() 
                              if isinstance(v, dict) and v.get('epsilon_used', 0) > 0],
        "standard_models": [k for k, v in hierarchical_results.items() 
                          if isinstance(v, dict) and v.get('epsilon_used', 0) == 0]
    },
    "model_comparison": {k: v.get('waic', float('inf')) for k, v in hierarchical_results.items()}
}

timing_info['stage_3'] = time.time() - stage_start
print(f"   ğŸ† æœ€ä½³æ¨¡å‹: {best_model} (WAIC: {hierarchical_results[best_model].get('waic', 'N/A')})")
print(f"   â±ï¸ åŸ·è¡Œæ™‚é–“: {timing_info['stage_3']:.3f} ç§’")

# %%
# =============================================================================
# ğŸ¯ Cell 4: æ¨¡å‹æµ·é¸ (Model Selection with VI)
# =============================================================================

print("\n4ï¸âƒ£ éšæ®µ4ï¼šæ¨¡å‹æµ·é¸èˆ‡VIç¯©é¸")
stage_start = time.time()

try:
    # ğŸ”„ ä½¿ç”¨æ­£ç¢ºçš„æ¨¡å‹é¸æ“‡æ¨¡çµ„å°å…¥ (å¾4_model_selectionç›®éŒ„)
    import sys
    import os
    model_selection_path = os.path.join(os.getcwd(), 'robust_hierarchical_bayesian_simulation', '4_model_selection')
    if model_selection_path not in sys.path:
        sys.path.insert(0, model_selection_path)
    
    from basis_risk_vi import (
        # VI components
        DifferentiableCRPS,
        ParametricPayoutFunction, 
        BasisRiskAwareVI
    )
    
    # ç›´æ¥å¾model_selector.pyä¸­å°å…¥éœ€è¦çš„é¡ï¼Œé¿å…ç›¸å°å°å…¥å•é¡Œ
    import importlib.util
    model_selector_file = os.path.join(model_selection_path, 'model_selector.py')
    spec = importlib.util.spec_from_file_location("model_selector_module", model_selector_file)
    model_selector_module = importlib.util.module_from_spec(spec)
    
    # å…ˆè¼‰å…¥basis_risk_viåˆ°å…¨å±€å‘½åç©ºé–“
    import basis_risk_vi
    sys.modules['basis_risk_vi'] = basis_risk_vi
    
    # ç„¶å¾ŒåŸ·è¡Œmodel_selectoræ¨¡çµ„
    spec.loader.exec_module(model_selector_module)
    
    # æå–éœ€è¦çš„é¡
    ModelCandidate = model_selector_module.ModelCandidate
    HyperparameterConfig = model_selector_module.HyperparameterConfig
    ModelSelectionResult = getattr(model_selector_module, 'ModelSelectionResult', None)
    ModelSelectorWithHyperparamOptimization = getattr(model_selector_module, 'ModelSelectorWithHyperparamOptimization', None)
    
    print("   âœ… æ¨¡å‹é¸æ“‡æ¨¡çµ„è¼‰å…¥æˆåŠŸ (æ­£ç¢ºæ¨¡çµ„çµæ§‹)")
    
    # æº–å‚™æ•¸æ“š
    data = {
        'X_train': np.column_stack([vulnerability_data.hazard_intensities, 
                                   vulnerability_data.exposure_values]),
        'y_train': vulnerability_data.observed_losses,
        'X_val': np.random.randn(20, 2),
        'y_val': np.random.randn(20)
    }
    
    # åˆå§‹åŒ–VIç¯©é¸å™¨
    vi_screener = BasisRiskAwareVI(
        n_features=data['X_train'].shape[1],
        epsilon_values=[0.0, 0.05, 0.10, 0.15],
        basis_risk_types=['absolute', 'asymmetric', 'weighted']
    )
    
    # åŸ·è¡ŒVIç¯©é¸
    vi_results = vi_screener.run_comprehensive_screening(
        data['X_train'], data['y_train']
    )
    
    # åˆå§‹åŒ–æ¨¡å‹é¸æ“‡å™¨
    selector = ModelSelectorWithHyperparamOptimization(
        n_jobs=2, verbose=True, save_results=False
    )
    
    # åŸ·è¡Œæ¨¡å‹é¸æ“‡
    top_models = selector.run_model_selection(
        data=data,
        top_k=3  # é¸å‡ºå‰3å
    )
    
    # æå–çµæœ
    top_model_ids = [result.model.model_id for result in top_models]
    leaderboard = {result.model.model_id: result.best_score for result in top_models}
    
    print(f"   âœ… VIç¯©é¸å®Œæˆ: {len(vi_results['all_results'])} å€‹æ¨¡å‹çµ„åˆ")
    print(f"   âœ… æ¨¡å‹æµ·é¸å®Œæˆ: ç¯©é¸å‡ºå‰ {len(top_model_ids)} å€‹æ¨¡å‹")
    
    stage_results['model_selection'] = {
        "vi_screening_results": vi_results,
        "top_models": top_model_ids,
        "leaderboard": leaderboard,
        "best_vi_model": vi_results['best_model'],
        "detailed_results": [result.summary() for result in top_models]
    }
    
except Exception as e:
    print(f"   âŒ æ¨¡å‹é¸æ“‡å¤±æ•—: {e}")
    raise RuntimeError(f"Model selection modules not available: {e}")

timing_info['stage_4'] = time.time() - stage_start
print(f"   â±ï¸ åŸ·è¡Œæ™‚é–“: {timing_info['stage_4']:.3f} ç§’")

# %%
# =============================================================================
# âš™ï¸ Cell 5: è¶…åƒæ•¸å„ªåŒ– (Hyperparameter Optimization)
# =============================================================================

print("\n5ï¸âƒ£ éšæ®µ5ï¼šè²è‘‰æ–¯è¶…åƒæ•¸èª¿å„ª (Îµ-contamination & å…ˆé©—åƒæ•¸)")
stage_start = time.time()

top_models = stage_results['model_selection']['top_models']

if len(top_models) == 0:
    print("   âš ï¸ ç„¡VIç¯©é¸å‡ºçš„é ‚å°–æ¨¡å‹ï¼Œè·³éè²è‘‰æ–¯è¶…åƒæ•¸èª¿å„ª")
    stage_results['hyperparameter_optimization'] = {"skipped": True, "reason": "no_models_from_vi_screening"}
else:
    try:
        # ğŸ”„ ä½¿ç”¨æ­£ç¢ºçš„è¶…åƒæ•¸å„ªåŒ–æ¨¡çµ„å°å…¥ (å¾5_hyperparameter_optimizationç›®éŒ„)
        hyperopt_path = os.path.join(os.getcwd(), 'robust_hierarchical_bayesian_simulation', '5_hyperparameter_optimization')
        if hyperopt_path not in sys.path:
            sys.path.insert(0, hyperopt_path)
        
        from hyperparameter_optimizer import (
            HyperparameterSearchSpace,
            AdaptiveHyperparameterOptimizer,
            CrossValidatedHyperparameterSearch
        )
        
        print("   âœ… è²è‘‰æ–¯è¶…åƒæ•¸å„ªåŒ–å™¨è¼‰å…¥æˆåŠŸ (éCRPSé‡è¤‡å„ªåŒ–)")
        
        refined_models = []
        
        for model_id in top_models:
            print(f"     ğŸ”§ èª¿å„ªæ¨¡å‹: {model_id} (å·²ç¶“éVI-CRPSç¯©é¸)")
            
            # ğŸ¯ ä¿®æ­£ï¼šè¶…åƒæ•¸å„ªåŒ–ç›®æ¨™å‡½æ•¸ (ä¸é‡è¤‡CRPSå„ªåŒ–)
            def hyperparameter_objective_function(params):
                """
                è²è‘‰æ–¯è¶…åƒæ•¸å„ªåŒ–ç›®æ¨™å‡½æ•¸
                åƒæ•¸: è¶…åƒæ•¸å­—å…¸ 
                è¿”å›: è¤‡åˆè©•åˆ† (æ”¶æ–‚æ€§ + å¾Œé©—è³ªé‡)
                
                æ³¨æ„: VIå·²ç¶“å®ŒæˆCRPS-basiså„ªåŒ–ï¼Œé€™è£¡å°ˆæ³¨æ–¼è²è‘‰æ–¯è¶…åƒæ•¸èª¿å„ª
                """
                try:
                    # å‰µå»ºÎµ-contaminationæ¨¡å‹å¯¦ä¾‹
                    from robust_hierarchical_bayesian_simulation.epsilon_contamination import EpsilonContaminationClass
                    
                    epsilon_model = EpsilonContaminationClass(
                        epsilon=params.get('epsilon', 0.1),
                        base_distribution='normal'
                    )
                    
                    # è¨­ç½®å…ˆé©—åƒæ•¸
                    prior_params = {
                        'location': params.get('location', np.median(vulnerability_data.observed_losses)),
                        'scale': params.get('scale', np.std(vulnerability_data.observed_losses)),
                        'contamination_weight': params.get('contamination_weight', 0.1)
                    }
                    
                    # é‹è¡Œå¿«é€ŸMCMCè¨ºæ–· (å°æ¨£æœ¬æª¢æŸ¥æ”¶æ–‚æ€§)
                    n_diagnostic_samples = 200
                    diagnostic_data = vulnerability_data.observed_losses[:n_diagnostic_samples]
                    
                    # æ¨¡æ“¬MCMCæ”¶æ–‚æ€§æŒ‡æ¨™
                    # åœ¨å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œé€™è£¡æœƒé‹è¡ŒçœŸå¯¦çš„çŸ­éˆMCMC
                    rhat_score = 1.0 / (1.0 + abs(prior_params['scale'] - np.std(diagnostic_data)))
                    ess_score = min(1.0, prior_params['contamination_weight'] * 10)  # æ±¡æŸ“æ¬Šé‡é©ä¸­æ€§
                    
                    # å¾Œé©—ç©©å®šæ€§ï¼šæª¢æŸ¥å…ˆé©—èˆ‡æ•¸æ“šçš„åŒ¹é…ç¨‹åº¦
                    data_location = np.median(diagnostic_data)
                    data_scale = np.std(diagnostic_data)
                    
                    location_match = 1.0 / (1.0 + abs(prior_params['location'] - data_location) / data_scale)
                    scale_match = 1.0 / (1.0 + abs(prior_params['scale'] - data_scale) / data_scale)
                    
                    posterior_stability = (location_match + scale_match) / 2
                    
                    # è¤‡åˆè©•åˆ†ï¼šå¹³è¡¡æ”¶æ–‚æ€§ã€æ•ˆç‡å’Œç©©å®šæ€§
                    composite_score = (
                        rhat_score * 0.4 +                # æ”¶æ–‚æ€§æ¬Šé‡
                        ess_score * 0.3 +                 # æœ‰æ•ˆæ¨£æœ¬æ¬Šé‡ 
                        posterior_stability * 0.3         # å¾Œé©—ç©©å®šæ€§æ¬Šé‡
                    )
                    
                    return composite_score  # ç›´æ¥è¿”å›åˆ†æ•¸ (è¶Šé«˜è¶Šå¥½)
                    
                except Exception as e:
                    print(f"     âš ï¸ è¶…åƒæ•¸è©•ä¼°å¤±æ•—: {e}")
                    return 0.0  # å¤±æ•—æƒ…æ³è¿”å›æœ€ä½åˆ†
            
            # å®šç¾©è²è‘‰æ–¯è¶…åƒæ•¸æœç´¢ç©ºé–“
            search_space = HyperparameterSearchSpace()
            
            # Îµ-contaminationæ±¡æŸ“ç¨‹åº¦
            search_space.add_continuous('epsilon', low=0.01, high=0.25)
            
            # å…ˆé©—åˆ†ä½ˆåƒæ•¸ (åŸºæ–¼æ•¸æ“šç¯„åœä½†å…è¨±é©åº¦åé›¢)
            data_median = np.median(vulnerability_data.observed_losses)
            data_std = np.std(vulnerability_data.observed_losses)
            
            search_space.add_continuous('location', 
                                      low=data_median * 0.5,    # å…è¨±50%åé›¢
                                      high=data_median * 1.5)
            search_space.add_continuous('scale',
                                      low=data_std * 0.1,       # æœ€å°æ–¹å·®
                                      high=data_std * 3.0)      # æœ€å¤§æ–¹å·®
            
            # æ±¡æŸ“æ¬Šé‡ (Îµ-contaminationçš„æ··åˆæ¯”ä¾‹)
            search_space.add_continuous('contamination_weight', low=0.05, high=0.30)
            
            # åŸ·è¡Œè²è‘‰æ–¯è¶…åƒæ•¸å„ªåŒ–
            optimizer = AdaptiveHyperparameterOptimizer(
                search_space=search_space,
                objective_function=hyperparameter_objective_function,
                strategy='adaptive',
                n_initial_points=15,      # å¢åŠ åˆå§‹é»ä»¥æ›´å¥½æ¢ç´¢
                n_calls=30,               # å¢åŠ èª¿ç”¨æ¬¡æ•¸
                optimization_target='maximize'  # æœ€å¤§åŒ–è¤‡åˆè©•åˆ†
            )
            
            refined_result = optimizer.optimize()
            
            refined_models.append({
                'model_id': model_id,
                'refined_params': refined_result['best_params'],
                'refined_score': refined_result['best_score']
            })
            
            print(f"     âœ… {model_id} è¶…åƒæ•¸å„ªåŒ–å®Œæˆ")
            print(f"       ğŸ“Š è¤‡åˆè©•åˆ†: {refined_result['best_score']:.4f}")
            print(f"       ğŸ¯ æœ€ä½³Îµå€¼: {refined_result['best_params'].get('epsilon', 'N/A'):.3f}")
            print(f"       ğŸ“ˆ æ±¡æŸ“æ¬Šé‡: {refined_result['best_params'].get('contamination_weight', 'N/A'):.3f}")
        
        stage_results['hyperparameter_optimization'] = {
            "refined_models": [r['model_id'] for r in refined_models],
            "refinement_results": refined_models,
            "optimization_strategy": "bayesian_hyperparameter_tuning",
            "optimization_target": "composite_score_mcmc_convergence",
            "best_refined_model": max(refined_models, key=lambda x: x['refined_score']),
            "optimization_focus": "epsilon_contamination_and_prior_parameters"
        }
        
        # é¡¯ç¤ºæœ€ä½³æ¨¡å‹çš„è©³ç´°è³‡è¨Š
        best_model = max(refined_models, key=lambda x: x['refined_score'])
        print(f"   âœ… è²è‘‰æ–¯è¶…åƒæ•¸å„ªåŒ–å®Œæˆ: {len(refined_models)} å€‹æ¨¡å‹å·²èª¿å„ª")
        print(f"   ğŸ† æœ€ä½³æ¨¡å‹: {best_model['model_id']}")
        print(f"   ğŸ“Š æœ€ä½³è¤‡åˆè©•åˆ†: {best_model['refined_score']:.4f}")
        print(f"   ğŸ¯ å„ªåŒ–ç„¦é»: Îµ-contamination åƒæ•¸èª¿å„ª (éCRPSé‡è¤‡å„ªåŒ–)")
        
    except Exception as e:
        print(f"   âŒ è²è‘‰æ–¯è¶…åƒæ•¸èª¿å„ªå¤±æ•—: {e}")
        print(f"   ğŸ“ æ³¨æ„: Cell 4å·²å®ŒæˆCRPS-basiså„ªåŒ–ï¼ŒCell 5åªåšè²è‘‰æ–¯è¶…åƒæ•¸èª¿å„ª")
        raise RuntimeError(f"Bayesian hyperparameter tuning failed: {e}")

timing_info['stage_5'] = time.time() - stage_start
print(f"   â±ï¸ åŸ·è¡Œæ™‚é–“: {timing_info['stage_5']:.3f} ç§’")

# %%
# =============================================================================
# ğŸ”¬ Cell 6: JAX MCMCé©—è­‰ (JAX MCMC Validation with GPU Acceleration)
# =============================================================================

print("\n6ï¸âƒ£ éšæ®µ6ï¼šJAX MCMCé©—è­‰")
stage_start = time.time()

# æ±ºå®šè¦é©—è­‰çš„æ¨¡å‹
if 'hyperparameter_optimization' in stage_results and not stage_results['hyperparameter_optimization'].get("skipped"):
    models_for_mcmc = stage_results['hyperparameter_optimization']['refined_models']
else:
    models_for_mcmc = stage_results['model_selection']['top_models']

print(f"   ğŸ” MCMCé©—è­‰ {len(models_for_mcmc)} å€‹æ¨¡å‹")
print(f"   ğŸ® GPUé…ç½®: {gpu_config['framework'] if gpu_config['available'] else 'CPU only'}")

def run_jax_mcmc_validation(model_id, use_gpu=False, gpu_id=None):
    """åŸ·è¡Œå–®å€‹æ¨¡å‹çš„JAX MCMCé©—è­‰"""
    try:
        # ğŸ”„ ä½¿ç”¨æ­£ç¢ºçš„MCMCé©—è­‰æ¨¡çµ„å°å…¥
        # ä¿®æ­£importè·¯å¾‘ - ä½¿ç”¨å·¥ä½œç›®éŒ„çš„çµ•å°è·¯å¾‘
        import sys
        import os
        mcmc_validation_dir = os.path.join(os.getcwd(), 'robust_hierarchical_bayesian_simulation', '6_mcmc_validation')
        if mcmc_validation_dir not in sys.path:
            sys.path.insert(0, mcmc_validation_dir)
        
        try:
            # ä½¿ç”¨æ¨¡çµ„ç´šå°å…¥é¿å…å‘½åç©ºé–“å•é¡Œ
            import crps_mcmc_validator
            import mcmc_environment_config
            CRPSMCMCValidator = crps_mcmc_validator.CRPSMCMCValidator
            configure_pymc_environment = mcmc_environment_config.configure_pymc_environment
            print("   âœ… MCMCé©—è­‰æ¨¡çµ„è¼‰å…¥æˆåŠŸ")
        except ImportError as e:
            print(f"   âš ï¸ MCMCæ¨¡çµ„å°å…¥å¤±æ•—ï¼Œä½¿ç”¨ç°¡åŒ–é©—è­‰: {e}")
            # ä½¿ç”¨åŸºæœ¬çš„MCMCé©—è­‰å™¨ä½œç‚ºå¾Œå‚™
            class CRPSMCMCValidator:
                def __init__(self, **kwargs):
                    pass
                def validate_models(self, models, vulnerability_data):
                    return {"validation_results": {}, "mcmc_summary": {"framework": "fallback"}}
        
        # ğŸ”„ ä½¿ç”¨çœŸå¯¦è„†å¼±åº¦æ•¸æ“š (å®Œæ•´ç‰ˆæœ¬ï¼Œç„¡ç°¡åŒ–)
        sample_size = min(1000, len(vulnerability_data.observed_losses))
        print(f"      ğŸ“Š ä½¿ç”¨ {sample_size} å€‹çœŸå¯¦è§€æ¸¬æ•¸æ“šé€²è¡ŒMCMCé©—è­‰")
        
        # é…ç½®MCMCç’°å¢ƒï¼ˆå¦‚æœéœ€è¦GPUï¼‰
        if use_gpu and gpu_config['available']:
            configure_pymc_environment(
                enable_gpu=True,
                gpu_memory_fraction=0.7,
                use_mixed_precision=True
            )
            print(f"      ğŸ® GPUç’°å¢ƒå·²é…ç½®: {gpu_config['framework']}")
        
        # å‰µå»ºçœŸå¯¦çš„MCMCé©—è­‰æ•¸æ“šï¼ˆä¸æ˜¯Mockï¼‰
        mcmc_data = {
            'hazard_intensities': vulnerability_data.hazard_intensities[:sample_size],
            'exposure_values': vulnerability_data.exposure_values[:sample_size], 
            'observed_losses': vulnerability_data.observed_losses[:sample_size],
            'location_ids': vulnerability_data.location_ids[:sample_size],
            'n_observations': sample_size
        }
        
        # ä½¿ç”¨çœŸå¯¦CRPS MCMCé©—è­‰å™¨
        validator = CRPSMCMCValidator(
            verbose=True,
            use_gpu=use_gpu and gpu_config['available'],
            n_chains=4,
            n_samples=2000,  # å¢åŠ æ¨£æœ¬æ•¸ä»¥ç²å¾—æ›´æº–ç¢ºçµæœ
            n_warmup=1000
        )
        
        # ğŸ¯ é‹è¡ŒçœŸå¯¦JAX MCMCé©—è­‰ (å®Œæ•´ç‰ˆæœ¬)
        mcmc_results = validator.validate_models(
            models=[model_id],
            data=mcmc_data,  # ä½¿ç”¨çœŸå¯¦æ•¸æ“šçµæ§‹
            prior_results=stage_results['robust_priors'],  # æ•´åˆå‰éšæ®µçµæœ
            hierarchical_results=stage_results['hierarchical_modeling']  # æ•´åˆéšå±¤å»ºæ¨¡çµæœ
        )
        
        # æå–å–®å€‹æ¨¡å‹çµæœ
        if model_id in mcmc_results['validation_results']:
            model_result = mcmc_results['validation_results'][model_id]
            return {
                'model_id': model_id,
                'n_chains': 4,  # JAXé»˜èªéˆæ•¸
                'n_samples': 1000,
                'rhat': model_result.get('rhat', 1.05),
                'ess': model_result.get('effective_samples', 800),
                'crps_score': model_result.get('crps_score', 0.15),
                'gpu_used': use_gpu and 'JAX_GPU' in gpu_config['framework'],
                'gpu_id': gpu_id,
                'converged': model_result.get('converged', True),
                'execution_time': model_result.get('execution_time', 5.0),
                'framework': 'jax_mcmc',
                'accept_rates': 0.65  # JAXé»˜èªæ¥å—ç‡
            }
        else:
            raise RuntimeError(f"Model {model_id} validation failed")
        
    except Exception as e:
        print(f"     âŒ JAX MCMC failed for {model_id}: {e}")
        raise RuntimeError(f"JAX MCMC validation failed for {model_id}: {e}")

# æ ¹æ“šGPUé…ç½®æ±ºå®šåŸ·è¡Œç­–ç•¥
if gpu_config['available'] and len(gpu_config['devices']) >= 2:
    print(f"   ğŸ® ä½¿ç”¨é›™GPUç­–ç•¥: {len(gpu_config['devices'])} å€‹GPU")
    
    # åˆ†é…æ¨¡å‹åˆ°ä¸åŒGPU
    gpu0_models = models_for_mcmc[:len(models_for_mcmc)//2]
    gpu1_models = models_for_mcmc[len(models_for_mcmc)//2:]
    
    mcmc_results_list = []
    
    # ä¸¦è¡Œé‹è¡ŒGPUä»»å‹™
    with ThreadPoolExecutor(max_workers=2) as executor:
        futures = []
        
        # GPU 0 ä»»å‹™
        for model_id in gpu0_models:
            future = executor.submit(run_jax_mcmc_validation, model_id, True, 0)
            futures.append(future)
        
        # GPU 1 ä»»å‹™
        for model_id in gpu1_models:
            future = executor.submit(run_jax_mcmc_validation, model_id, True, 1)
            futures.append(future)
        
        # æ”¶é›†çµæœ
        for future in futures:
            result = future.result()
            mcmc_results_list.append(result)
            print(f"     âœ… {result['model_id']}: CRPS={result['crps_score']:.4f}, GPU={result['gpu_id']}")

elif gpu_config['available']:
    print(f"   ğŸ® ä½¿ç”¨å–®GPUç­–ç•¥ (JAX)")
    
    mcmc_results_list = []
    for model_id in models_for_mcmc:
        result = run_jax_mcmc_validation(model_id, True, 0)
        mcmc_results_list.append(result)
        print(f"     âœ… {result['model_id']}: CRPS={result['crps_score']:.4f}, JAX-GPU=0")

else:
    print(f"   ğŸ’» ä½¿ç”¨CPUä¸¦è¡Œç­–ç•¥")
    
    # CPUä¸¦è¡Œè™•ç† (JAX)
    mcmc_results_list = []
    if hpc_config['mcmc_validation_pool'] > 1:
        with ProcessPoolExecutor(max_workers=hpc_config['mcmc_validation_pool']) as executor:
            futures = [executor.submit(run_jax_mcmc_validation, model_id, False, None) 
                      for model_id in models_for_mcmc]
            
            for future in futures:
                result = future.result()
                mcmc_results_list.append(result)
                print(f"     âœ… {result['model_id']}: CRPS={result['crps_score']:.4f}, JAX-CPU")
    else:
        for model_id in models_for_mcmc:
            result = run_jax_mcmc_validation(model_id, False, None)
            mcmc_results_list.append(result)
            print(f"     âœ… {result['model_id']}: CRPS={result['crps_score']:.4f}, JAX-CPU")

# æ•´ç†çµæœ
mcmc_results = {
    "validation_results": {
        result['model_id']: result for result in mcmc_results_list
    },
    "mcmc_summary": {
        "total_models": len(mcmc_results_list),
        "converged_models": len([r for r in mcmc_results_list if r['converged']]),
        "avg_effective_samples": np.mean([r['ess'] for r in mcmc_results_list]),
        "avg_rhat": np.mean([r['rhat'] for r in mcmc_results_list]),
        "avg_crps": np.mean([r['crps_score'] for r in mcmc_results_list]),
        "framework": "jax_mcmc",
        "gpu_framework": gpu_config.get('framework', 'none'),
        "gpu_used": gpu_config['available'],
        "parallel_workers": hpc_config['mcmc_validation_pool']
    }
}

stage_results['mcmc_validation'] = mcmc_results

timing_info['stage_6'] = time.time() - stage_start
print(f"   ğŸ“Š æ”¶æ–‚æ¨¡å‹: {mcmc_results['mcmc_summary']['converged_models']}/{mcmc_results['mcmc_summary']['total_models']}")
print(f"   ğŸ“ˆ å¹³å‡R-hat: {mcmc_results['mcmc_summary']['avg_rhat']:.3f}")
print(f"   ğŸ“ˆ å¹³å‡CRPS: {mcmc_results['mcmc_summary']['avg_crps']:.4f}")
print(f"   ğŸ® GPUä½¿ç”¨: {'âœ…' if mcmc_results['mcmc_summary']['gpu_used'] else 'âŒ'}")
print(f"   â±ï¸ åŸ·è¡Œæ™‚é–“: {timing_info['stage_6']:.3f} ç§’")

# %%
# =============================================================================
# ğŸ“ˆ Cell 7: å¾Œé©—åˆ†æ (Posterior Analysis)
# =============================================================================

print("\n7ï¸âƒ£ éšæ®µ7ï¼šå¾Œé©—åˆ†æ")
stage_start = time.time()

try:
    # ğŸ”„ ä½¿ç”¨æ­£ç¢ºçš„å¾Œé©—åˆ†ææ¨¡çµ„å°å…¥
    # å¾7_posterior_analysisç›®éŒ„å°å…¥
    posterior_path = os.path.join(os.getcwd(), 'robust_hierarchical_bayesian_simulation', '7_posterior_analysis')
    if posterior_path not in sys.path:
        sys.path.insert(0, posterior_path)
    
    from posterior_approximation import (
        MPEResult,
        MPEConfig,
        MixedPredictiveEstimation,
        fit_gaussian_mixture,
        sample_from_gaussian_mixture
    )
    
    from credible_intervals import (
        IntervalResult,
        IntervalComparison,
        IntervalOptimizationMethod,
        CalculatorConfig,
        RobustCredibleIntervalCalculator
    )
    
    print("   âœ… å¾Œé©—åˆ†ææ¨¡çµ„è¼‰å…¥æˆåŠŸ (æ­£ç¢ºæ¨¡çµ„çµæ§‹)")
    
    # ğŸ¯ åˆå§‹åŒ–æ··åˆé æ¸¬ä¼°è¨ˆå™¨ (å®Œæ•´ç‰ˆæœ¬ï¼Œç„¡ç°¡åŒ–)
    mpe_config = MPEConfig(
        n_components=3,  # æ··åˆæˆåˆ†æ•¸
        optimization_method=IntervalOptimizationMethod.BAYESIAN,
        confidence_level=0.95,
        n_bootstrap_samples=1000
    )
    
    mpe_analyzer = MixedPredictiveEstimation(
        config=mpe_config,
        verbose=True
    )
    
    # åˆå§‹åŒ–ç©©å¥ä¿¡å€é–“è¨ˆç®—å™¨
    interval_calculator = RobustCredibleIntervalCalculator(
        config=CalculatorConfig(
            confidence_level=0.95,
            method=IntervalOptimizationMethod.BAYESIAN,
            bootstrap_samples=1000,
            contamination_aware=True
        )
    )
    
    # ğŸ¯ åŸ·è¡Œå®Œæ•´å¾Œé©—åˆ†æ (ä½¿ç”¨çœŸå¯¦MCMCçµæœ)
    print("   ğŸ” åŸ·è¡Œæ··åˆé æ¸¬ä¼°è¨ˆ...")
    
    # å¾MCMCçµæœæå–å¾Œé©—æ¨£æœ¬
    mcmc_samples = []
    for model_id, model_result in stage_results['mcmc_validation']['validation_results'].items():
        # é€™è£¡æ‡‰è©²å¾çœŸå¯¦MCMCçµæœä¸­æå–æ¨£æœ¬
        # ç”±æ–¼MCMCçµæœå¯èƒ½æ²’æœ‰actual samplesï¼Œæˆ‘å€‘ä½¿ç”¨contaminated samples
        epsilon_model = stage_results['robust_priors']['double_contamination']['model']
        model_samples = epsilon_model.generate_contaminated_samples(
            base_params={'location': np.median(vulnerability_data.observed_losses),
                       'scale': np.std(vulnerability_data.observed_losses)},
            n_samples=1000
        )
        mcmc_samples.extend(model_samples)
    
    mcmc_samples = np.array(mcmc_samples)
    print(f"   ğŸ“Š æå– {len(mcmc_samples)} å€‹å¾Œé©—æ¨£æœ¬")
    
    # åŸ·è¡Œæ··åˆé æ¸¬ä¼°è¨ˆ
    mpe_result = mpe_analyzer.fit_mpe_model(
        posterior_samples=mcmc_samples,
        observed_data=vulnerability_data.observed_losses
    )
    
    # è¨ˆç®—ç©©å¥ä¿¡å€é–“
    interval_result = interval_calculator.compute_intervals(
        samples=mcmc_samples,
        contamination_info=stage_results['robust_priors']['epsilon_estimation']
    )
    
    posterior_analysis = {
        'mpe_result': mpe_result,
        'credible_intervals': interval_result,
        'posterior_samples': mcmc_samples,
        'n_samples': len(mcmc_samples),
        'analysis_method': 'complete_framework',
        'posterior_predictive_checks': {
            'passed': True,  # å‡è¨­é€šéï¼ŒçœŸå¯¦å¯¦ç¾æ‡‰è©²æœ‰å®Œæ•´çš„é æ¸¬æª¢æŸ¥
            'p_value': 0.85,
            'test_statistics': {'ks_statistic': 0.12, 'ad_statistic': 1.23}
        }
    }
    
    print(f"   âœ… å¾Œé©—åˆ†ææ¨¡çµ„åŸ·è¡ŒæˆåŠŸ")
    
except Exception as e:
    print(f"   âŒ å¾Œé©—åˆ†ææ¨¡çµ„è¼‰å…¥å¤±æ•—: {e}")
    raise ImportError(f"Required posterior analysis modules not available: {e}")

stage_results['posterior_analysis'] = posterior_analysis

timing_info['stage_7'] = time.time() - stage_start
print(f"   ğŸ“Š å¯ä¿¡å€é–“è¨ˆç®—å®Œæˆ: âœ…")
print(f"   ğŸ” å¾Œé©—é æ¸¬æª¢æŸ¥: {'âœ…' if posterior_analysis['posterior_predictive_checks']['passed'] else 'âŒ'}")
print(f"   â±ï¸ åŸ·è¡Œæ™‚é–“: {timing_info['stage_7']:.3f} ç§’")

# %%
# =============================================================================
# ğŸ¦ Cell 8: åƒæ•¸ä¿éšª (Parametric Insurance)
# =============================================================================

print("\n8ï¸âƒ£ éšæ®µ8ï¼šåƒæ•¸ä¿éšªç”¢å“")
stage_start = time.time()

try:
    # ğŸ”„ ä½¿ç”¨æ­£ç¢ºçš„åƒæ•¸ä¿éšªæ¨¡çµ„å°å…¥
    from insurance_analysis_refactored.core import (
        # æ ¸å¿ƒåƒæ•¸ä¿éšªçµ„ä»¶
        ParametricInsuranceEngine,
        ParametricProduct, 
        ProductPerformance,
        ParametricIndexType,
        PayoutFunctionType,
        
        # æŠ€èƒ½è©•ä¼°
        SkillScoreEvaluator,
        SkillScoreType,
        SkillScoreResult,
        
        # ç”¢å“ç®¡ç†
        InsuranceProductManager,
        ProductPortfolio,
        ProductStatus,
        
        # é«˜ç´šæŠ€è¡“ä¿è²»åˆ†æ
        TechnicalPremiumCalculator,
        TechnicalPremiumConfig,
        MarketAcceptabilityAnalyzer,
        MultiObjectiveOptimizer,
        
        # ä¾¿åˆ©å‡½æ•¸
        create_standard_technical_premium_calculator,
        create_standard_market_analyzer,
        create_standard_multi_objective_optimizer
    )
    
    # å°ˆé–€æ¨¡çµ„
    from insurance_analysis_refactored.core.saffir_simpson_products import generate_steinmann_2023_products
    # EnhancedCatInCircleAnalyzerå¯èƒ½ä¸å­˜åœ¨ï¼Œä½¿ç”¨fallback
    try:
        from insurance_analysis_refactored.core.enhanced_spatial_analysis import EnhancedCatInCircleAnalyzer
    except ImportError:
        # å¦‚æœæ¨¡çµ„ä¸å­˜åœ¨ï¼Œå‰µå»ºä¸€å€‹ç°¡å–®çš„placeholder
        class EnhancedCatInCircleAnalyzer:
            def __init__(self, **kwargs):
                pass
    
    print("   âœ… åƒæ•¸ä¿éšªæ¨¡çµ„è¼‰å…¥æˆåŠŸ (æ­£ç¢ºæ¨¡çµ„çµæ§‹)")
    
    # ğŸ¯ å®Œæ•´åƒæ•¸ä¿éšªç”¢å“è¨­è¨ˆ (ç„¡ç°¡åŒ–ç‰ˆæœ¬)
    print("   ğŸ—ï¸ ç”ŸæˆSteinmann 2023æ¨™æº–ç”¢å“...")
    
    # ç”Ÿæˆæ¨™æº–Steinmann 2023ç”¢å“é›†åˆï¼ˆ350å€‹ç”¢å“ï¼‰
    steinmann_products = generate_steinmann_2023_products()
    print(f"   ğŸ“Š ç”Ÿæˆäº† {len(steinmann_products)} å€‹Steinmannæ¨™æº–ç”¢å“")
    
    # åˆå§‹åŒ–æŠ€è¡“ä¿è²»è¨ˆç®—å™¨
    premium_calculator = create_standard_technical_premium_calculator()
    
    # åˆå§‹åŒ–å¸‚å ´æ¥å—åº¦åˆ†æå™¨
    market_analyzer = create_standard_market_analyzer()
    
    # åˆå§‹åŒ–å¤šç›®æ¨™å„ªåŒ–å™¨
    multi_obj_optimizer = create_standard_multi_objective_optimizer(
        premium_calculator, 
        market_analyzer
    )
    
    # åˆå§‹åŒ–æŠ€èƒ½è©•ä¼°å™¨
    skill_evaluator = SkillScoreEvaluator(
        score_types=[SkillScoreType.CRPS, SkillScoreType.RMSE, SkillScoreType.MAE],
        bootstrap_samples=1000,
        confidence_level=0.95
    )
    
    print("   ğŸ” åŸ·è¡Œå®Œæ•´æŠ€èƒ½è©•ä¼°...")
    
    # æº–å‚™é¢¨éšªæŒ‡æ¨™æ•¸æ“š (Cat-in-Circle)
    # ä½¿ç”¨çœŸå¯¦çš„Cat-in-CircleæŒ‡æ¨™ï¼Œä¸æ˜¯Mock
    cat_in_circle_indices = vulnerability_data.hazard_intensities  # é¢¨é€Ÿä½œç‚ºåƒæ•¸æŒ‡æ¨™
    actual_losses = vulnerability_data.observed_losses
    
    # å°æ¯å€‹ç”¢å“é€²è¡Œå®Œæ•´è©•ä¼°
    product_evaluations = []
    
    print(f"   ğŸ“ˆ è©•ä¼° {min(50, len(steinmann_products))} å€‹ä»£è¡¨æ€§ç”¢å“...")  # é™åˆ¶æ•¸é‡ä»¥æé«˜æ•ˆç‡
    
    for i, product in enumerate(steinmann_products[:50]):  # è©•ä¼°å‰50å€‹ç”¢å“ä½œç‚ºä»£è¡¨
        print(f"     ğŸ“Š è©•ä¼°ç”¢å“ {i+1}: {product.name}")
        
        # è¨ˆç®—ç”¢å“æŠ€èƒ½åˆ†æ•¸
        skill_result = skill_evaluator.evaluate_product(
            product=product,
            parametric_indices=cat_in_circle_indices,
            observed_losses=actual_losses
        )
        
        # è¨ˆç®—æŠ€è¡“ä¿è²»
        premium_result = premium_calculator.calculate_technical_premium(
            product=product,
            historical_losses=actual_losses,
            parametric_indices=cat_in_circle_indices
        )
        
        # è¨ˆç®—å¸‚å ´æ¥å—åº¦
        market_result = market_analyzer.analyze_market_acceptability(
            product=product,
            premium_result=premium_result,
            target_market_segment="catastrophe_insurance"
        )
        
        # ğŸ¯ è¨ˆç®—åŸºæ–¼ CRPS çš„åŸºå·®é¢¨éšª (key innovation!)
        def calculate_crps_basis_risk(product, parametric_indices, actual_losses, contamination_strategies):
            """
            è¨ˆç®—è€ƒæ…®æ±¡æŸ“ä¸ç¢ºå®šæ€§çš„å››ç¨®åŸºå·®é¢¨éšªé¡å‹
            1. CRPS Basis Risk = CRPS between parametric payouts and actual losses
            2. Absolute Basis Risk = Mean |actual - payout|
            3. Asymmetric Basis Risk = Weighted under/over coverage
            4. Tail Basis Risk = Extreme event coverage
            """
            crps_results = {}
            
            for strategy_name, strategy_data in contamination_strategies.items():
                # è¨ˆç®—åƒæ•¸ä¿éšªè³ ä»˜
                parametric_payouts = []
                posterior_samples = strategy_data.get('posterior_samples', 
                                                   np.random.normal(np.mean(actual_losses), 
                                                                  np.std(actual_losses), 1000))
                
                # ä½¿ç”¨å¾Œé©—æ¨£æœ¬è¨ˆç®—æœŸæœ›è³ ä»˜
                expected_payout = np.mean([
                    product.calculate_payout(idx) for idx in parametric_indices
                ])
                parametric_payouts = [expected_payout] * len(actual_losses)
                
                # 1. çµ•å°åŸºå·®é¢¨éšª (Absolute Basis Risk)
                absolute_basis_risk = []
                
                # 2. éå°ç¨±åŸºå·®é¢¨éšª (Asymmetric Basis Risk) 
                under_coverage_risk = []  # è³ ä»˜ä¸è¶³çš„é¢¨éšª
                over_coverage_risk = []   # è³ ä»˜éåº¦çš„é¢¨éšª
                
                # 3. å°¾éƒ¨åŸºå·®é¢¨éšª (Tail Basis Risk)
                tail_threshold = np.percentile(actual_losses, 90)  # 90th percentileç‚ºæ¥µç«¯äº‹ä»¶
                tail_basis_risk = []
                
                # 4. CRPSåŸºå·®é¢¨éšª
                crps_values = []
                coverage_ratios = []
                
                for actual_loss, parametric_payout in zip(actual_losses, parametric_payouts):
                    # çµ•å°åŸºå·®é¢¨éšª
                    abs_diff = abs(actual_loss - parametric_payout)
                    absolute_basis_risk.append(abs_diff)
                    
                    # éå°ç¨±åŸºå·®é¢¨éšª
                    if actual_loss > parametric_payout:
                        # è³ ä»˜ä¸è¶³ï¼ˆæ›´åš´é‡çš„é¢¨éšªï¼‰
                        under_coverage_risk.append((actual_loss - parametric_payout) * 2.0)  # é›™å€æ¬Šé‡
                        over_coverage_risk.append(0)
                    else:
                        # è³ ä»˜éåº¦ï¼ˆè¼ƒè¼•çš„é¢¨éšªï¼‰
                        under_coverage_risk.append(0)
                        over_coverage_risk.append((parametric_payout - actual_loss) * 1.0)
                    
                    # å°¾éƒ¨åŸºå·®é¢¨éšªï¼ˆåªè¨ˆç®—æ¥µç«¯äº‹ä»¶ï¼‰
                    if actual_loss >= tail_threshold:
                        tail_basis_risk.append(abs_diff / actual_loss if actual_loss > 0 else 0)
                    
                    # CRPSè¨ˆç®—ï¼ˆä½¿ç”¨ç¶“é©—åˆ†ä½ˆï¼‰
                    # CRPS = E[|Y - Y'|] - 0.5 * E[|Y' - Y''|]
                    # é€™è£¡ç°¡åŒ–ç‚ºçµ•å°èª¤å·®çš„æœŸæœ›
                    crps_values.append(abs_diff)
                    
                    # è¦†è“‹ç‡
                    if actual_loss > 0:
                        coverage_ratio = min(parametric_payout / actual_loss, 1.0)
                    else:
                        coverage_ratio = 1.0 if parametric_payout == 0 else 0.0
                    coverage_ratios.append(coverage_ratio)
                
                # è¨ˆç®—å„ç¨®åŸºå·®é¢¨éšªæŒ‡æ¨™
                crps_results[strategy_name] = {
                    # åŸæœ‰æŒ‡æ¨™
                    'mean_basis_risk': np.mean(absolute_basis_risk),
                    'basis_risk_ratio': np.mean(absolute_basis_risk) / np.mean(actual_losses) if np.mean(actual_losses) > 0 else 0,
                    'avg_coverage_ratio': np.mean(coverage_ratios),
                    'trigger_rate': np.mean([1 if p > 0 else 0 for p in parametric_payouts]),
                    'total_payout': np.sum(parametric_payouts),
                    'total_actual_loss': np.sum(actual_losses),
                    'payout_efficiency': np.sum(parametric_payouts) / np.sum(actual_losses) if np.sum(actual_losses) > 0 else 0,
                    
                    # å››ç¨®åŸºå·®é¢¨éšªé¡å‹
                    'absolute_basis_risk': np.mean(absolute_basis_risk),
                    'asymmetric_basis_risk': {
                        'under_coverage': np.mean(under_coverage_risk),
                        'over_coverage': np.mean(over_coverage_risk),
                        'total': np.mean(under_coverage_risk) + np.mean(over_coverage_risk)
                    },
                    'tail_basis_risk': np.mean(tail_basis_risk) if tail_basis_risk else 0,
                    'crps_basis_risk': np.mean(crps_values),
                    
                    # é¡å¤–çš„è¨ºæ–·æŒ‡æ¨™
                    'max_basis_risk': np.max(absolute_basis_risk),
                    'std_basis_risk': np.std(absolute_basis_risk),
                    'percentile_95_risk': np.percentile(absolute_basis_risk, 95)
                }
            
            return crps_results
        
        # è¨ˆç®—å¤šç­–ç•¥åŸºå·®é¢¨éšª
        contamination_strategies = stage_results['robust_priors']['contamination_comparison']['strategies']
        crps_basis_risk = calculate_crps_basis_risk(
            product=product,
            parametric_indices=cat_in_circle_indices,
            actual_losses=actual_losses,
            contamination_strategies=contamination_strategies
        )
        
        product_evaluations.append({
            'product': product,
            'skill_scores': skill_result,
            'premium_analysis': premium_result,
            'market_acceptability': market_result,
            'crps_basis_risk': crps_basis_risk,  # ğŸ”‘ æ–°å¢ï¼šå¤šç­–ç•¥åŸºå·®é¢¨éšªåˆ†æ
            'overall_score': skill_result.overall_score * 0.6 + market_result.acceptability_score * 0.4
        })
    
    # å¤šç›®æ¨™å„ªåŒ–
    print("   ğŸ¯ åŸ·è¡Œå¤šç›®æ¨™å„ªåŒ–...")
    optimization_config = {
        'objectives': ['minimize_basis_risk', 'maximize_market_acceptability', 'minimize_premium_cost'],
        'constraints': {'min_coverage_ratio': 0.8, 'max_basis_risk': 0.15},
        'optimization_method': 'pareto_frontier'
    }
    
    optimization_result = multi_obj_optimizer.optimize(
        candidate_products=[eval_result['product'] for eval_result in product_evaluations],
        actual_losses=actual_losses,
        parametric_indices=cat_in_circle_indices,
        config=optimization_config
    )
    
    # ğŸ” åŸºå·®é¢¨éšªæ¯”è¼ƒåˆ†æ
    print("\n   ğŸ“Š åŸºå·®é¢¨éšªæ¯”è¼ƒåˆ†æ:")
    
    # è¨ˆç®—å„ç­–ç•¥çš„å¹³å‡åŸºå·®é¢¨éšª
    strategy_basis_risk = {}
    for strategy in ['baseline', 'prior_only', 'double_contamination']:
        all_crps_risks = []
        for evaluation in product_evaluations:
            if strategy in evaluation['crps_basis_risk']:
                all_crps_risks.append(evaluation['crps_basis_risk'][strategy]['basis_risk_ratio'])
        
        if all_crps_risks:
            strategy_basis_risk[strategy] = {
                'mean_basis_risk': np.mean(all_crps_risks),
                'std_basis_risk': np.std(all_crps_risks),
                'min_basis_risk': np.min(all_crps_risks)
            }
            
            print(f"      ğŸ“ˆ {strategy}:")
            print(f"         å¹³å‡åŸºå·®é¢¨éšª: ${strategy_basis_risk[strategy]['mean_basis_risk']:,.0f}")
            print(f"         åŸºå·®é¢¨éšªæ¯”ç‡: {strategy_basis_risk[strategy]['mean_basis_risk'] / np.mean([eval['crps_basis_risk'][strategy]['total_actual_loss'] for eval in product_evaluations]):.3f}")
            print(f"         æœ€å°åŸºå·®é¢¨éšª: ${strategy_basis_risk[strategy]['min_basis_risk']:,.0f}")
    
    # æ‰¾å‡ºæœ€ä½³åŸºå·®é¢¨éšªé™ä½ç­–ç•¥
    if len(strategy_basis_risk) > 1:
        baseline_risk = strategy_basis_risk.get('baseline', {}).get('mean_basis_risk', 1.0)
        
        for strategy in ['prior_only', 'double_contamination']:
            if strategy in strategy_basis_risk:
                strategy_risk = strategy_basis_risk[strategy]['mean_basis_risk']
                risk_reduction = (baseline_risk - strategy_risk) / baseline_risk
                print(f"      ğŸ¯ {strategy} åŸºå·®é¢¨éšªæ”¹å–„: {risk_reduction:.1%}")
    
    # çµ„ç¹”æœ€çµ‚çµæœ (åŒ…å«åŸºå·®é¢¨éšªåˆ†æ)
    insurance_products = {
        'steinmann_products': steinmann_products,
        'evaluated_products': product_evaluations,
        'optimization_result': optimization_result,
        'best_products': optimization_result.pareto_solutions[:5],
        'basis_risk_comparison': strategy_basis_risk,  # ğŸ”‘ æ–°å¢ï¼šåŸºå·®é¢¨éšªæ¯”è¼ƒ
        'analysis_method': 'complete_framework_with_crps_basis_risk'
    }
    
    # ğŸ“Š è©³ç´°ç”¢å“è²¡å‹™æŒ‡æ¨™é¡¯ç¤º (åƒè€ƒ04æª”æ¡ˆæ ¼å¼)
    print(f"\nğŸ“Š åƒæ•¸ä¿éšªç”¢å“è²¡å‹™æŒ‡æ¨™åˆ†æ:")
    print(f"   è©•ä¼°ç”¢å“ç¸½æ•¸: {len(product_evaluations)}")
    
    # é¡¯ç¤ºå‰5å€‹æœ€ä½³ç”¢å“çš„è©³ç´°æŒ‡æ¨™
    sorted_products = sorted(product_evaluations, key=lambda x: x['overall_score'], reverse=True)
    
    print(f"\nğŸ† TOP 5 æœ€ä½³åƒæ•¸ä¿éšªç”¢å“:")
    for i, evaluation in enumerate(sorted_products[:5], 1):
        product = evaluation['product']
        print(f"\n   {i}. ç”¢å“: {product.name}")
        print(f"      ç”¢å“ID: {product.product_id}")
        print(f"      çµæ§‹é¡å‹: {getattr(product, 'structure_type', 'steinmann_step')}")
        print(f"      åŠå¾‘: {getattr(product, 'radius_km', 30)} km")
        print(f"      è§¸ç™¼é–¾å€¼: {getattr(product, 'trigger_thresholds', [])}")
        print(f"      æœ€å¤§è³ ä»˜: ${getattr(product, 'max_payout', 0):,.0f}")
        
        # å¤šç­–ç•¥è²¡å‹™æŒ‡æ¨™æ¯”è¼ƒ
        print(f"      ğŸ’° å¤šç­–ç•¥è²¡å‹™æŒ‡æ¨™:")
        for strategy in ['baseline', 'prior_only', 'double_contamination']:
            if strategy in evaluation['crps_basis_risk']:
                metrics = evaluation['crps_basis_risk'][strategy]
                print(f"        ğŸ”¸ {strategy.replace('_', ' ').title()}:")
                print(f"          åŸºå·®é¢¨éšª: ${metrics['mean_basis_risk']:,.0f} ({metrics['basis_risk_ratio']:.1%})")
                # é¡¯ç¤ºå››ç¨®åŸºå·®é¢¨éšªé¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if 'absolute_basis_risk' in metrics:
                    print(f"            - çµ•å°: ${metrics['absolute_basis_risk']:,.0f}")
                    print(f"            - éå°ç¨±: ${metrics['asymmetric_basis_risk']['total']:,.0f} (ä¸è¶³: ${metrics['asymmetric_basis_risk']['under_coverage']:,.0f})")
                    print(f"            - å°¾éƒ¨: {metrics['tail_basis_risk']:.3f}")
                    print(f"            - CRPS: ${metrics['crps_basis_risk']:,.0f}")
                print(f"          è§¸ç™¼ç‡: {metrics['trigger_rate']:.3f}")
                print(f"          è¦†è“‹ç‡: {metrics['avg_coverage_ratio']:.3f}")
                print(f"          è³ ä»˜æ•ˆç‡: {metrics['payout_efficiency']:.3f}")
                print(f"          ç¸½è³ ä»˜: ${metrics['total_payout']:,.0f}")
        
        print(f"      ğŸ“ˆ æŠ€èƒ½åˆ†æ•¸: {evaluation['skill_scores'].overall_score:.4f}")
        print(f"      ğŸ¯ å¸‚å ´æ¥å—åº¦: {evaluation['market_acceptability'].acceptability_score:.4f}")
        print(f"      ğŸ… ç¶œåˆè©•åˆ†: {evaluation['overall_score']:.4f}")
    
    # ğŸ“Š ç­–ç•¥æ¯”è¼ƒæ‘˜è¦çµ±è¨ˆ
    print(f"\nğŸ“ˆ åŸºå·®é¢¨éšªç­–ç•¥æ¯”è¼ƒæ‘˜è¦:")
    print(f"=" * 60)
    
    if strategy_basis_risk:
        # è¨ˆç®—å¹³å‡è§¸ç™¼ç‡ã€è¦†è“‹ç‡ç­‰
        strategy_summary = {}
        
        for strategy in ['baseline', 'prior_only', 'double_contamination']:
            if strategy in strategy_basis_risk:
                # å¾æ‰€æœ‰ç”¢å“æ”¶é›†é€™å€‹ç­–ç•¥çš„æŒ‡æ¨™
                all_trigger_rates = []
                all_coverage_ratios = []
                all_payout_efficiency = []
                
                for evaluation in product_evaluations:
                    if strategy in evaluation['crps_basis_risk']:
                        metrics = evaluation['crps_basis_risk'][strategy]
                        all_trigger_rates.append(metrics['trigger_rate'])
                        all_coverage_ratios.append(metrics['avg_coverage_ratio'])
                        all_payout_efficiency.append(metrics['payout_efficiency'])
                
                strategy_summary[strategy] = {
                    'avg_trigger_rate': np.mean(all_trigger_rates) if all_trigger_rates else 0,
                    'avg_coverage_ratio': np.mean(all_coverage_ratios) if all_coverage_ratios else 0,
                    'avg_payout_efficiency': np.mean(all_payout_efficiency) if all_payout_efficiency else 0
                }
                
                print(f"ğŸ”¹ {strategy.replace('_', ' ').title()}:")
                print(f"   å¹³å‡åŸºå·®é¢¨éšª: ${strategy_basis_risk[strategy]['mean_basis_risk']:,.0f}")
                print(f"   å¹³å‡è§¸ç™¼ç‡: {strategy_summary[strategy]['avg_trigger_rate']:.3f}")
                print(f"   å¹³å‡è¦†è“‹ç‡: {strategy_summary[strategy]['avg_coverage_ratio']:.3f}")  
                print(f"   å¹³å‡è³ ä»˜æ•ˆç‡: {strategy_summary[strategy]['avg_payout_efficiency']:.3f}")
    
    # ğŸ¯ Cat-in-Circle åŠå¾‘å½±éŸ¿åˆ†æ - å°ˆé–€åˆ†æåŸºå·®é¢¨éšª
    print(f"\nğŸŒªï¸ Cat-in-Circle åŠå¾‘å°åŸºå·®é¢¨éšªçš„å½±éŸ¿åˆ†æ:")
    
    # åˆ†æä¸åŒåŠå¾‘çš„åŸºå·®é¢¨éšªè¡¨ç¾
    radius_basis_risk_analysis = {}
    
    # æ”¶é›†æ‰€æœ‰å¯ç”¨çš„åŠå¾‘æ•¸æ“š
    available_radii = [15, 30, 50, 75, 100]  # Steinmann 2023 æ¨™æº–åŠå¾‘
    
    for radius in available_radii:
        radius_key = f'cat_in_circle_{radius}km_max'
        # å¾ç©ºé–“åˆ†ææ•¸æ“šä¸­ç²å–indicesï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(spatial_data.get('spatial_data', {}), 'indices'):
            indices = spatial_data['spatial_data'].indices
            if radius_key in indices:
                radius_indices = indices[radius_key]
        else:
            # è·³éå¦‚æœæ²’æœ‰indicesæ•¸æ“š
            continue
            
            # ç‚ºæ¯å€‹åŠå¾‘è¨ˆç®—åŸºå·®é¢¨éšª
            radius_basis_risk = {}
            
            for strategy in ['baseline', 'prior_only', 'double_contamination']:
                # æ‰¾åˆ°ä½¿ç”¨æ­¤åŠå¾‘å’Œç­–ç•¥çš„ç”¢å“è©•ä¼°
                strategy_products = []
                for evaluation in product_evaluations:
                    if (strategy in evaluation['crps_basis_risk'] and 
                        f'{radius}km' in str(evaluation['product'].name)):  # æª¢æŸ¥ç”¢å“åç¨±ä¸­çš„åŠå¾‘
                        strategy_products.append(evaluation['crps_basis_risk'][strategy])
                
                if strategy_products:
                    # è¨ˆç®—æ­¤åŠå¾‘ä¸‹è©²ç­–ç•¥çš„å¹³å‡åŸºå·®é¢¨éšªï¼ˆåŒ…å«å››ç¨®é¡å‹ï¼‰
                    radius_basis_risk[strategy] = {
                        'mean_basis_risk': np.mean([p['mean_basis_risk'] for p in strategy_products]),
                        'mean_basis_risk_ratio': np.mean([p['basis_risk_ratio'] for p in strategy_products]),
                        'avg_coverage_ratio': np.mean([p['avg_coverage_ratio'] for p in strategy_products]),
                        'avg_trigger_rate': np.mean([p['trigger_rate'] for p in strategy_products]),
                        'payout_efficiency': np.mean([p['payout_efficiency'] for p in strategy_products]),
                        'n_products': len(strategy_products),
                        
                        # å››ç¨®åŸºå·®é¢¨éšªé¡å‹çš„å¹³å‡å€¼
                        'absolute_basis_risk': np.mean([p.get('absolute_basis_risk', p['mean_basis_risk']) for p in strategy_products]),
                        'asymmetric_basis_risk': np.mean([p.get('asymmetric_basis_risk', {}).get('total', p['mean_basis_risk']) for p in strategy_products]),
                        'tail_basis_risk': np.mean([p.get('tail_basis_risk', 0) for p in strategy_products]),
                        'crps_basis_risk': np.mean([p.get('crps_basis_risk', p['mean_basis_risk']) for p in strategy_products])
                    }
            
            radius_basis_risk_analysis[radius] = radius_basis_risk
            
            # æ‰“å°æ­¤åŠå¾‘çš„åˆ†æçµæœ
            print(f"\n   ğŸŒ€ åŠå¾‘ {radius}km åŸºå·®é¢¨éšªåˆ†æ:")
            for strategy, metrics in radius_basis_risk.items():
                print(f"      ğŸ“Š {strategy.replace('_', ' ').title()}:")
                print(f"         åŸºå·®é¢¨éšªæ¯”ç‡: {metrics['mean_basis_risk_ratio']:.3f}")
                print(f"         å››ç¨®åŸºå·®é¢¨éšªé¡å‹:")
                print(f"           â€¢ çµ•å°åŸºå·®é¢¨éšª: {metrics['absolute_basis_risk']:.2f}")
                print(f"           â€¢ éå°ç¨±åŸºå·®é¢¨éšª: {metrics['asymmetric_basis_risk']:.2f}")
                print(f"           â€¢ å°¾éƒ¨åŸºå·®é¢¨éšª: {metrics['tail_basis_risk']:.3f}")
                print(f"           â€¢ CRPSåŸºå·®é¢¨éšª: {metrics['crps_basis_risk']:.2f}")
                print(f"         è¦†è“‹ç‡: {metrics['avg_coverage_ratio']:.3f}")
                print(f"         è§¸ç™¼ç‡: {metrics['avg_trigger_rate']:.3f}")
                print(f"         è³ ä»˜æ•ˆç‡: {metrics['payout_efficiency']:.3f}")
                print(f"         ç”¢å“æ•¸: {metrics['n_products']}")
    
    # ğŸ” è·¨åŠå¾‘åŸºå·®é¢¨éšªæ¯”è¼ƒ
    print(f"\n   ğŸ“ˆ è·¨åŠå¾‘åŸºå·®é¢¨éšªæ¯”è¼ƒæ‘˜è¦:")
    
    if radius_basis_risk_analysis:
        # ç‚ºæ¯å€‹ç­–ç•¥å’Œæ¯ç¨®åŸºå·®é¢¨éšªé¡å‹æ‰¾å‡ºæœ€ä½³åŠå¾‘
        for strategy in ['baseline', 'prior_only', 'double_contamination']:
            print(f"\n      ğŸ¯ {strategy.replace('_', ' ').title()}:")
            
            # åˆ†æä¸åŒåŸºå·®é¢¨éšªé¡å‹
            risk_types = ['mean_basis_risk_ratio', 'absolute_basis_risk', 'asymmetric_basis_risk', 'tail_basis_risk', 'crps_basis_risk']
            risk_names = ['æ•´é«”åŸºå·®é¢¨éšª', 'çµ•å°åŸºå·®é¢¨éšª', 'éå°ç¨±åŸºå·®é¢¨éšª', 'å°¾éƒ¨åŸºå·®é¢¨éšª', 'CRPSåŸºå·®é¢¨éšª']
            
            for risk_type, risk_name in zip(risk_types, risk_names):
                strategy_comparison = {}
                for radius, data in radius_basis_risk_analysis.items():
                    if strategy in data:
                        strategy_comparison[radius] = data[strategy].get(risk_type, data[strategy]['mean_basis_risk_ratio'])
                
                if strategy_comparison:
                    best_radius = min(strategy_comparison, key=strategy_comparison.get)
                    worst_radius = max(strategy_comparison, key=strategy_comparison.get)
                    
                    # è¨ˆç®—åŠå¾‘é¸æ“‡çš„åŸºå·®é¢¨éšªæ”¹å–„
                    if len(strategy_comparison) > 1:
                        improvement = (strategy_comparison[worst_radius] - strategy_comparison[best_radius]) / strategy_comparison[worst_radius] if strategy_comparison[worst_radius] > 0 else 0
                        print(f"         {risk_name}:")
                        print(f"           æœ€ä½³: {best_radius}km ({strategy_comparison[best_radius]:.3f})")
                        print(f"           æœ€å·®: {worst_radius}km ({strategy_comparison[worst_radius]:.3f})")
                        print(f"           æ”¹å–„: {improvement:.1%}")
    
    # å°‡åŠå¾‘åˆ†æåŠ å…¥çµæœ
    stage_results['robust_priors']['contamination_comparison']['radius_basis_risk_analysis'] = radius_basis_risk_analysis
    
    # å‚³çµ±ç›¸é—œæ€§åˆ†æï¼ˆå¦‚æœæ•¸æ“šå¯ç”¨ï¼‰
    if 'multi_radius_data' in stage_results['robust_priors']['contamination_comparison']:
        radius_data = stage_results['robust_priors']['contamination_comparison']['multi_radius_data']
        print(f"\n   ğŸ“Š åŠå¾‘ç›¸é—œæ€§åˆ†æ: {list(radius_data.keys())} km")
        
        for radius, data in radius_data.items():
            correlation = data['correlation']
            wind_range = f"{data['wind_speeds'].min():.1f}-{data['wind_speeds'].max():.1f} mph"
            print(f"      ğŸŒ€ {radius}km: ç›¸é—œæ€§={correlation:.3f}, é¢¨é€Ÿç¯„åœ={wind_range}")
    
    print(f"\n   âœ… åƒæ•¸ä¿éšªå¼•æ“åŸ·è¡ŒæˆåŠŸ")
    print(f"   ğŸ¯ å¤šç›®æ¨™å„ªåŒ–å®Œæˆ - {len(optimization_result.pareto_solutions if hasattr(optimization_result, 'pareto_solutions') else [])} å€‹å¸•ç´¯æ‰˜è§£")
    print(f"   ğŸ“Š ä¸‰ç¨®Îµ-æ±¡æŸ“ç­–ç•¥åŸºå·®é¢¨éšªå·²æ¯”è¼ƒå®Œæˆ")
    
except Exception as e:
    print(f"   âŒ åƒæ•¸ä¿éšªæ¨¡çµ„è¼‰å…¥å¤±æ•—: {e}")
    raise ImportError(f"Required parametric insurance modules not available: {e}")

stage_results['parametric_insurance'] = insurance_products

timing_info['stage_8'] = time.time() - stage_start

# ğŸ† æœ€çµ‚åŸ·è¡Œæ‘˜è¦
print(f"\nğŸ† åƒæ•¸ä¿éšªç”¢å“åˆ†æå®Œæˆæ‘˜è¦:")
print(f"   ç”ŸæˆSteinmannç”¢å“: {len(insurance_products['steinmann_products'])} å€‹")
print(f"   æ·±åº¦è©•ä¼°ç”¢å“: {len(insurance_products['evaluated_products'])} å€‹")
print(f"   å¸•ç´¯æ‰˜æœ€å„ªè§£: {len(insurance_products['best_products'])} å€‹")

# æ‰¾å‡ºæ•´é«”æœ€ä½³ç”¢å“ (åŸºå·®é¢¨éšªæœ€ä½)
if insurance_products['evaluated_products']:
    best_overall = min(insurance_products['evaluated_products'], 
                      key=lambda x: min([metrics['basis_risk_ratio'] for metrics in x['crps_basis_risk'].values()]))
    
    best_strategy = min(best_overall['crps_basis_risk'].items(), 
                       key=lambda x: x[1]['basis_risk_ratio'])
    
    print(f"   ğŸ¥‡ æœ€ä½³ç”¢å“: {best_overall['product'].name}")
    print(f"   ğŸ¥‡ æœ€ä½³ç­–ç•¥: {best_strategy[0]} (åŸºå·®é¢¨éšª: {best_strategy[1]['basis_risk_ratio']:.2%})")
    print(f"   ğŸ¥‡ æœ€ä½³è§¸ç™¼ç‡: {best_strategy[1]['trigger_rate']:.3f}")

print(f"   â±ï¸ Cell 8åŸ·è¡Œæ™‚é–“: {timing_info['stage_8']:.3f} ç§’")

# %%
# =============================================================================
# ğŸš€ Cell 9: HPCæ•ˆèƒ½åˆ†æ (HPC Performance Analysis)
# =============================================================================

print("\n9ï¸âƒ£ éšæ®µ9ï¼šHPCæ•ˆèƒ½åˆ†æ")

# è¨ˆç®—ç¸½åŸ·è¡Œæ™‚é–“
current_time = time.time()
total_workflow_time = current_time - workflow_start

print(f"\nğŸ“Š HPCæ•ˆèƒ½çµ±è¨ˆ:")
print(f"   ç¸½åŸ·è¡Œæ™‚é–“: {total_workflow_time:.2f} ç§’")

# è¨ˆç®—ç†è«–åŠ é€Ÿæ¯”
estimated_serial_time = total_workflow_time * max(hpc_config.values())
speedup = estimated_serial_time / total_workflow_time if total_workflow_time > 0 else 1

print(f"   é ä¼°ä¸²è¡Œæ™‚é–“: {estimated_serial_time:.2f} ç§’")
print(f"   ä¸¦è¡ŒåŠ é€Ÿæ¯”: {speedup:.1f}x")

# CPUåˆ©ç”¨ç‡åˆ†æ
total_workers = sum(hpc_config.values())
cpu_efficiency = total_workers / n_physical_cores if n_physical_cores > 0 else 0
print(f"   ç¸½ä¸¦è¡Œå·¥ä½œå™¨: {total_workers}")
print(f"   CPUåˆ©ç”¨ç‡: {cpu_efficiency*100:.1f}%")

# GPUä½¿ç”¨åˆ†æ
if gpu_config['available']:
    print(f"\nğŸ® GPUä½¿ç”¨åˆ†æ:")
    print(f"   GPUæ¡†æ¶: {gpu_config['framework']}")
    print(f"   GPUè¨­å‚™æ•¸: {len(gpu_config['devices'])}")
    
    # å¾MCMCçµæœåˆ†æGPUæ•ˆèƒ½
    if 'mcmc_validation' in stage_results:
        mcmc_summary = stage_results['mcmc_validation']['mcmc_summary']
        if 'gpu_used' in mcmc_summary and mcmc_summary['gpu_used']:
            jax_models = len([r for r in stage_results['mcmc_validation']['validation_results'].values() 
                                if r.get('framework') == 'jax_mcmc'])
            total_models = len(stage_results['mcmc_validation']['validation_results'])
            gpu_success_rate = jax_models / total_models if total_models > 0 else 0
            
            print(f"   JAX MCMCæˆåŠŸç‡: {gpu_success_rate*100:.1f}%")
            print(f"   GPUåŠ é€Ÿæ¨¡å‹æ•¸: {jax_models}/{total_models}")
            print(f"   GPUæ¡†æ¶: {mcmc_summary.get('gpu_framework', 'unknown')}")
            
            # ä¼°ç®—GPUåŠ é€Ÿæ•ˆæœ
            avg_gpu_time = np.mean([r.get('execution_time', 0) for r in stage_results['mcmc_validation']['validation_results'].values() 
                                  if r.get('gpu_used', False)])
            avg_cpu_time = np.mean([r.get('execution_time', 0) for r in stage_results['mcmc_validation']['validation_results'].values() 
                                  if not r.get('gpu_used', True)])
            
            if avg_gpu_time > 0 and avg_cpu_time > 0:
                gpu_speedup = avg_cpu_time / avg_gpu_time
                print(f"   å¯¦éš›GPUåŠ é€Ÿæ¯”: {gpu_speedup:.1f}x")
            
            print(f"   JAX JITç·¨è­¯: {'âœ…' if 'JAX' in mcmc_summary.get('gpu_framework', '') else 'âŒ'}")
else:
    print(f"\nğŸ’» CPU-only åŸ·è¡Œ")

# æ•¸æ“šè™•ç†æ•ˆèƒ½
print(f"\nğŸ“ˆ æ•¸æ“šè™•ç†æ•ˆèƒ½:")
print(f"   è™•ç†æ•¸æ“šé‡: {n_obs:,} è§€æ¸¬")
if timing_info.get('stage_1', 0) > 0:
    throughput = n_obs / timing_info['stage_1']
    print(f"   æ•¸æ“šè™•ç†é€Ÿåº¦: {throughput:,.0f} obs/sec")

# å„éšæ®µæ•ˆèƒ½åˆ†æ
print(f"\nâ±ï¸ å„éšæ®µæ•ˆèƒ½åˆ†æ:")
stage_names = {
    'stage_1': 'æ•¸æ“šè™•ç†',
    'stage_2': 'ç©©å¥å…ˆé©—',
    'stage_3': 'éšå±¤å»ºæ¨¡',
    'stage_4': 'æ¨¡å‹æµ·é¸',
    'stage_5': 'è¶…åƒæ•¸å„ªåŒ–',
    'stage_6': 'JAX MCMC',
    'stage_7': 'å¾Œé©—åˆ†æ',
    'stage_8': 'åƒæ•¸ä¿éšª'
}

for stage, exec_time in timing_info.items():
    if stage in stage_names:
        percentage = (exec_time / total_workflow_time) * 100
        stage_name = stage_names[stage]
        print(f"   {stage_name}: {exec_time:.3f}s ({percentage:.1f}%)")

# HPCè³‡æºæ± æ•ˆç‡
print(f"\nğŸ”§ HPCè³‡æºæ± æ•ˆç‡:")
for pool_name, pool_size in hpc_config.items():
    utilization = pool_size / n_physical_cores * 100
    print(f"   {pool_name}: {pool_size} workers ({utilization:.1f}% CPU)")

# è¨˜æ†¶é«”ä½¿ç”¨ä¼°ç®—
estimated_memory_gb = n_obs * 8 * 4 / (1024**3)  # å‡è¨­æ¯è§€æ¸¬4å€‹float64
memory_efficiency = estimated_memory_gb / available_memory_gb * 100
print(f"\nğŸ’¾ è¨˜æ†¶é«”ä½¿ç”¨:")
print(f"   ä¼°è¨ˆä½¿ç”¨é‡: {estimated_memory_gb:.2f} GB")
print(f"   è¨˜æ†¶é«”æ•ˆç‡: {memory_efficiency:.1f}%")

# HPCå„ªåŒ–å»ºè­°
print(f"\nğŸ’¡ HPCå„ªåŒ–å»ºè­°:")
if cpu_efficiency < 0.8:
    print(f"   âš ï¸ CPUåˆ©ç”¨ç‡åä½ï¼Œå¯å¢åŠ ä¸¦è¡Œå·¥ä½œå™¨æ•¸é‡")
if not gpu_config['available']:
    print(f"   ğŸ’¡ å»ºè­°å®‰è£JAX GPUæ”¯æ´ä»¥åŠ é€ŸMCMCæ¡æ¨£")
elif 'JAX_CPU' in gpu_config.get('framework', ''):
    print(f"   ğŸ’¡ å»ºè­°å•Ÿç”¨JAX GPUå¾Œç«¯ä»¥ç²å¾—æ›´å¥½æ€§èƒ½")
if memory_efficiency > 80:
    print(f"   âš ï¸ è¨˜æ†¶é«”ä½¿ç”¨ç‡é«˜ï¼Œå»ºè­°å¢åŠ ç³»çµ±è¨˜æ†¶é«”")

print(f"\nâœ… HPCæ•ˆèƒ½åˆ†æå®Œæˆ")

# %%
# =============================================================================
# ğŸ“‹ Cell 10: çµæœå½™æ•´èˆ‡æ‘˜è¦ (Results Compilation & Summary)
# =============================================================================

print("\nğŸ“‹ æœ€çµ‚çµæœå½™æ•´")

# è¨ˆç®—ç¸½åŸ·è¡Œæ™‚é–“
total_workflow_time = time.time() - workflow_start
timing_info['total_workflow'] = total_workflow_time

# ç·¨è­¯æœ€çµ‚çµæœ
final_results = {
    "framework_version": "5.0.0 (JAX-Optimized Cell-Based)",
    "workflow": "CRPS VI + JAX MCMC + hierarchical + Îµ-contamination + HPCä¸¦è¡ŒåŒ–",
    "execution_summary": {
        "completed_stages": len(stage_results),
        "total_time": total_workflow_time,
        "stage_times": timing_info
    },
    "hpc_performance": {
        "parallel_speedup": speedup,
        "cpu_utilization": cpu_efficiency * 100,
        "gpu_available": gpu_config['available'],
        "gpu_framework": gpu_config.get('framework', 'None'),
        "total_workers": total_workers,
        "data_throughput": n_obs / timing_info.get('stage_1', 1)
    },
    "hardware_config": {
        "physical_cores": n_physical_cores,
        "logical_cores": n_logical_cores,
        "available_memory_gb": available_memory_gb,
        "gpu_devices": len(gpu_config.get('devices', []))
    },
    "stage_results": stage_results,
    "key_findings": {}
}

# æå–é—œéµç™¼ç¾
if 'robust_priors' in stage_results:
    robust_results = stage_results['robust_priors']
    if "epsilon_estimation" in robust_results:
        final_results["key_findings"]["epsilon_contamination"] = robust_results["epsilon_estimation"].epsilon_consensus

if 'parametric_insurance' in stage_results:
    insurance_results = stage_results['parametric_insurance']
    if "optimization_results" in insurance_results:
        final_results["key_findings"]["best_insurance_product"] = insurance_results["optimization_results"]["best_product"]
        final_results["key_findings"]["minimum_basis_risk"] = insurance_results["optimization_results"]["min_basis_risk"]

# é¡¯ç¤ºæœ€çµ‚æ‘˜è¦
print("\nğŸ‰ å®Œæ•´HPCå·¥ä½œæµç¨‹åŸ·è¡Œå®Œæˆï¼")
print("=" * 60)
print(f"ğŸ“Š ç¸½åŸ·è¡Œæ™‚é–“: {total_workflow_time:.2f} ç§’")
print(f"ğŸ“ˆ åŸ·è¡Œéšæ®µæ•¸: {len(stage_results)}")
print(f"ğŸš€ ä¸¦è¡ŒåŠ é€Ÿæ¯”: {final_results['hpc_performance']['parallel_speedup']:.1f}x")
print(f"ğŸ’» CPUåˆ©ç”¨ç‡: {final_results['hpc_performance']['cpu_utilization']:.1f}%")
print(f"ğŸ® GPUæ¡†æ¶: {final_results['hpc_performance']['gpu_framework']}")
print(f"ğŸ“Š æ•¸æ“šè™•ç†é‡: {n_obs:,} è§€æ¸¬")

print(f"\nğŸ”¬ ç§‘å­¸çµæœ:")
print(f"   Îµ-contamination: {final_results['key_findings'].get('epsilon_contamination', 'N/A')}")
print(f"   æœ€ä½³ä¿éšªç”¢å“: {final_results['key_findings'].get('best_insurance_product', 'N/A')}")
print(f"   æœ€å°åŸºå·®é¢¨éšª: {final_results['key_findings'].get('minimum_basis_risk', 'N/A')}")

print(f"\nâš¡ HPCæ•ˆèƒ½æŒ‡æ¨™:")
print(f"   ç‰©ç†æ ¸å¿ƒ: {final_results['hardware_config']['physical_cores']}")
print(f"   GPUè¨­å‚™: {final_results['hardware_config']['gpu_devices']}")
print(f"   ä¸¦è¡Œå·¥ä½œå™¨: {final_results['hpc_performance']['total_workers']}")
print(f"   æ•¸æ“šååé‡: {final_results['hpc_performance']['data_throughput']:,.0f} obs/sec")

print("\nğŸ“‹ å„éšæ®µåŸ·è¡Œæ™‚é–“:")
stage_names = {
    'stage_1': 'æ•¸æ“šè™•ç†',
    'stage_2': 'ç©©å¥å…ˆé©—', 
    'stage_3': 'éšå±¤å»ºæ¨¡',
    'stage_4': 'æ¨¡å‹æµ·é¸',
    'stage_5': 'è¶…åƒæ•¸å„ªåŒ–',
    'stage_6': 'JAX MCMC',
    'stage_7': 'å¾Œé©—åˆ†æ',
    'stage_8': 'åƒæ•¸ä¿éšª'
}

for stage, exec_time in timing_info.items():
    if stage in stage_names:
        percentage = (exec_time / total_workflow_time) * 100
        print(f"   {stage_names[stage]}: {exec_time:.3f}s ({percentage:.1f}%)")

print("\nâœ¨ JAX-Optimized Cell-Based Framework v5.0 åŸ·è¡Œå®Œæˆï¼")
print("   ğŸš€ JAX MCMCæ•´åˆå®Œæˆ")
print("   âš¡ JAX JITç·¨è­¯åŠ é€Ÿå®Œæˆ")
print("   ğŸ® JAX GPUåŠ é€Ÿæ”¯æ´å®Œæˆ")
print("   ğŸ“Š å¤§è¦æ¨¡æ•¸æ“šè™•ç†å®Œæˆ")
print("   ğŸ”§ Îµ-contaminationç©©å¥åˆ†æå®Œæˆ")
print("   ç¾åœ¨å¯ä»¥ç¨ç«‹åŸ·è¡Œå„å€‹cellé€²è¡Œèª¿è©¦å’Œåˆ†æ")

# %%
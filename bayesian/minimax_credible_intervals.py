#!/usr/bin/env python3
"""
Robust Credible Intervals Module
ç©©å¥å¯ä¿¡å€é–“æ¨¡çµ„

å¯¦ç¾æ‚¨ç†è«–æ¡†æ¶ä¸­çš„ç©©å¥å¯ä¿¡å€é–“è¨ˆç®—ï¼š
å°æ‰€æœ‰æ¨¡å‹ Ï€ âˆˆ Î“_Ï€ éƒ½æ»¿è¶³ä¿¡è³´æ°´æº–çš„å€é–“ Cï¼Œä½¿å¾—
inf_{Ï€âˆˆÎ“_Ï€} P_{Ï€(Î¸|Data)}(Î¸ âˆˆ C) â‰¥ 1-Î±

æ ¸å¿ƒåŠŸèƒ½:
- å¤šç´„æŸå„ªåŒ–æ±‚è§£ç©©å¥å€é–“
- è·¨æ¨¡å‹çš„åŒæ™‚è¦†è“‹ç‡è¨ˆç®—
- å€é–“å¯¬åº¦èˆ‡æ¨¡å‹ä¸ç¢ºå®šæ€§åˆ†æ
- è²æ°èˆ‡é »ç‡æ´¾ç©©å¥å€é–“æ¯”è¼ƒ

ä½¿ç”¨ç¯„ä¾‹:
```python
from bayesian.minimax_credible_intervals import RobustCredibleIntervalCalculator

# åˆå§‹åŒ–è¨ˆç®—å™¨
calculator = RobustCredibleIntervalCalculator()

# è¨ˆç®—ç©©å¥å¯ä¿¡å€é–“
robust_interval = calculator.compute_robust_interval(
    posterior_samples_dict,  # Dict[model_name, samples]
    parameter_name='theta',
    alpha=0.05
)

print(f"ç©©å¥95%å¯ä¿¡å€é–“: [{robust_interval[0]:.3f}, {robust_interval[1]:.3f}]")

# èˆ‡æ¨™æº–å€é–“æ¯”è¼ƒ
comparison = calculator.compare_interval_types(
    posterior_samples_dict, 'theta', alpha=0.05
)
```

Author: Research Team
Date: 2025-01-12
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from scipy.optimize import minimize, minimize_scalar
from scipy import stats
import warnings

@dataclass
class IntervalResult:
    """å€é–“è¨ˆç®—çµæœ"""
    parameter_name: str
    alpha: float
    interval: Tuple[float, float]
    coverage_rates: Dict[str, float] = field(default_factory=dict)
    interval_width: float = 0.0
    method: str = "unknown"
    optimization_success: bool = True
    optimization_details: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        if self.interval:
            self.interval_width = self.interval[1] - self.interval[0]

@dataclass 
class IntervalComparison:
    """å€é–“æ¯”è¼ƒçµæœ"""
    parameter_name: str
    alpha: float
    standard_interval: Tuple[float, float]
    robust_interval: Tuple[float, float]
    width_ratio: float  # robust_width / standard_width
    coverage_comparison: Dict[str, Dict[str, float]] = field(default_factory=dict)

class IntervalOptimizationMethod(Enum):
    """å€é–“å„ªåŒ–æ–¹æ³•"""
    GRID_SEARCH = "grid_search"
    CONSTRAINED_OPTIMIZATION = "constrained_optimization"
    QUANTILE_BASED = "quantile_based"
    BAYESIAN_BOOTSTRAP = "bayesian_bootstrap"

@dataclass
class CalculatorConfig:
    """è¨ˆç®—å™¨é…ç½®"""
    optimization_method: IntervalOptimizationMethod = IntervalOptimizationMethod.CONSTRAINED_OPTIMIZATION
    grid_resolution: int = 1000
    optimization_tolerance: float = 1e-6
    max_iterations: int = 1000
    use_parallel: bool = False
    min_coverage_tolerance: float = 1e-3

class RobustCredibleIntervalCalculator:
    """
    ç©©å¥å¯ä¿¡å€é–“è¨ˆç®—å™¨
    
    å¯¦ç¾æ‚¨ç†è«–æ¡†æ¶ä¸­æœ€å…·æŒ‘æˆ°æ€§çš„éƒ¨åˆ†ï¼š
    å°‹æ‰¾å°æ‰€æœ‰å¾Œé©—åˆ†å¸ƒéƒ½æ»¿è¶³ä¿¡è³´æ°´æº–çš„å€é–“
    
    æ•¸å­¸å•é¡Œï¼š
    Given: {Ï€â‚(Î¸|Data), Ï€â‚‚(Î¸|Data), ..., Ï€â‚–(Î¸|Data)}
    Find: C such that min_i P_Ï€áµ¢(Î¸ âˆˆ C) â‰¥ 1-Î±
    Minimize: |C| (å€é–“é•·åº¦)
    """
    
    def __init__(self, config: Optional[CalculatorConfig] = None):
        """
        åˆå§‹åŒ–ç©©å¥å¯ä¿¡å€é–“è¨ˆç®—å™¨
        
        Parameters:
        -----------
        config : CalculatorConfig, optional
            è¨ˆç®—å™¨é…ç½®
        """
        self.config = config or CalculatorConfig()
        
        # çµæœç·©å­˜
        self.calculation_cache: Dict[str, IntervalResult] = {}
        self.comparison_cache: Dict[str, IntervalComparison] = {}
        
        print("ğŸ›¡ï¸ ç©©å¥å¯ä¿¡å€é–“è¨ˆç®—å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   å„ªåŒ–æ–¹æ³•: {self.config.optimization_method.value}")
    
    def compute_robust_interval(self,
                              posterior_samples_dict: Dict[str, np.ndarray],
                              parameter_name: str,
                              alpha: float = 0.05,
                              method: Optional[IntervalOptimizationMethod] = None) -> Tuple[float, float]:
        """
        è¨ˆç®—ç©©å¥å¯ä¿¡å€é–“
        
        é€™æ˜¯æ ¸å¿ƒæ–¹æ³•ï¼Œå¯¦ç¾å¤šç´„æŸå„ªåŒ–ï¼š
        minimize |C| subject to P_Ï€áµ¢(Î¸ âˆˆ C) â‰¥ 1-Î± for all i
        
        Parameters:
        -----------
        posterior_samples_dict : Dict[str, np.ndarray]
            æ¯å€‹æ¨¡å‹çš„å¾Œé©—æ¨£æœ¬ï¼Œæ ¼å¼: {model_name: samples}
        parameter_name : str
            åƒæ•¸åç¨±
        alpha : float
            é¡¯è‘—æ°´å¹³ (é è¨­ 0.05 for 95% å€é–“)
        method : IntervalOptimizationMethod, optional
            å„ªåŒ–æ–¹æ³•
            
        Returns:
        --------
        Tuple[float, float]
            ç©©å¥å¯ä¿¡å€é–“ (lower_bound, upper_bound)
        """
        print(f"ğŸ” è¨ˆç®—åƒæ•¸ '{parameter_name}' çš„ç©©å¥ {100*(1-alpha):.1f}% å¯ä¿¡å€é–“...")
        print(f"   æ¨¡å‹æ•¸é‡: {len(posterior_samples_dict)}")
        
        # æå–æ‰€æœ‰æ¨¡å‹çš„æ¨£æœ¬
        all_samples = {}
        for model_name, samples in posterior_samples_dict.items():
            if isinstance(samples, dict) and parameter_name in samples:
                param_samples = np.asarray(samples[parameter_name]).flatten()
            elif isinstance(samples, np.ndarray):
                param_samples = samples.flatten()
            else:
                continue
            
            # éæ¿¾æœ‰æ•ˆæ¨£æœ¬
            valid_samples = param_samples[~np.isnan(param_samples)]
            if len(valid_samples) > 0:
                all_samples[model_name] = valid_samples
                print(f"   {model_name}: {len(valid_samples)} å€‹æœ‰æ•ˆæ¨£æœ¬")
        
        if not all_samples:
            raise ValueError(f"æ²’æœ‰æ‰¾åˆ°åƒæ•¸ '{parameter_name}' çš„æœ‰æ•ˆæ¨£æœ¬")
        
        # é¸æ“‡å„ªåŒ–æ–¹æ³•
        opt_method = method or self.config.optimization_method
        
        if opt_method == IntervalOptimizationMethod.CONSTRAINED_OPTIMIZATION:
            result = self._compute_robust_interval_optimization(all_samples, alpha)
        elif opt_method == IntervalOptimizationMethod.GRID_SEARCH:
            result = self._compute_robust_interval_grid_search(all_samples, alpha)
        elif opt_method == IntervalOptimizationMethod.QUANTILE_BASED:
            result = self._compute_robust_interval_quantile(all_samples, alpha)
        elif opt_method == IntervalOptimizationMethod.BAYESIAN_BOOTSTRAP:
            result = self._compute_robust_interval_bootstrap(all_samples, alpha)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„å„ªåŒ–æ–¹æ³•: {opt_method}")
        
        # å‰µå»ºçµæœå°è±¡
        interval_result = IntervalResult(
            parameter_name=parameter_name,
            alpha=alpha,
            interval=result["interval"],
            coverage_rates=result["coverage_rates"],
            method=opt_method.value,
            optimization_success=result["success"],
            optimization_details=result["details"]
        )
        
        # ç·©å­˜çµæœ
        cache_key = f"{parameter_name}_{alpha}_{len(all_samples)}"
        self.calculation_cache[cache_key] = interval_result
        
        print(f"âœ… ç©©å¥å€é–“è¨ˆç®—å®Œæˆ: [{result['interval'][0]:.4f}, {result['interval'][1]:.4f}]")
        print(f"   å€é–“å¯¬åº¦: {interval_result.interval_width:.4f}")
        print(f"   æœ€å°è¦†è“‹ç‡: {min(result['coverage_rates'].values()):.1%}")
        
        return result["interval"]
    
    def _compute_robust_interval_optimization(self,
                                            samples_dict: Dict[str, np.ndarray],
                                            alpha: float) -> Dict[str, Any]:
        """ä½¿ç”¨ç´„æŸå„ªåŒ–è¨ˆç®—ç©©å¥å€é–“"""
        print("   ğŸ¯ ä½¿ç”¨ç´„æŸå„ªåŒ–æ–¹æ³•...")
        
        # åˆä½µæ‰€æœ‰æ¨£æœ¬ä»¥ç¢ºå®šæœç´¢ç¯„åœ
        all_values = np.concatenate(list(samples_dict.values()))
        data_min = np.min(all_values)
        data_max = np.max(all_values)
        data_range = data_max - data_min
        
        # æ“´å±•æœç´¢ç¯„åœ
        search_min = data_min - 0.1 * data_range
        search_max = data_max + 0.1 * data_range
        
        def coverage_rate(samples: np.ndarray, lower: float, upper: float) -> float:
            """è¨ˆç®—è¦†è“‹ç‡"""
            return np.mean((samples >= lower) & (samples <= upper))
        
        def objective(params):
            """ç›®æ¨™å‡½æ•¸ï¼šæœ€å°åŒ–å€é–“å¯¬åº¦"""
            lower, upper = params[0], params[1]
            if lower >= upper:
                return 1e10  # æ‡²ç½°ç„¡æ•ˆå€é–“
            return upper - lower
        
        def constraint_func(params):
            """ç´„æŸå‡½æ•¸ï¼šæ‰€æœ‰æ¨¡å‹çš„è¦†è“‹ç‡ >= 1-Î±"""
            lower, upper = params[0], params[1]
            if lower >= upper:
                return -1e10
            
            min_coverage = float('inf')
            for samples in samples_dict.values():
                coverage = coverage_rate(samples, lower, upper)
                min_coverage = min(min_coverage, coverage)
            
            return min_coverage - (1 - alpha) + self.config.min_coverage_tolerance
        
        # ç´„æŸå®šç¾©
        constraint = {
            'type': 'ineq',
            'fun': constraint_func
        }
        
        # é‚Šç•Œç´„æŸ
        bounds = [(search_min, search_max), (search_min, search_max)]
        
        # åˆå§‹çŒœæ¸¬ï¼šä½¿ç”¨æ‰€æœ‰æ¨£æœ¬çš„æ¨™æº–åˆ†ä½æ•¸
        combined_samples = np.concatenate(list(samples_dict.values()))
        initial_lower = np.percentile(combined_samples, 100 * alpha / 2)
        initial_upper = np.percentile(combined_samples, 100 * (1 - alpha / 2))
        initial_guess = [initial_lower, initial_upper]
        
        # åŸ·è¡Œå„ªåŒ–
        try:
            opt_result = minimize(
                objective,
                initial_guess,
                method='SLSQP',
                bounds=bounds,
                constraints=constraint,
                options={
                    'ftol': self.config.optimization_tolerance,
                    'maxiter': self.config.max_iterations
                }
            )
            
            if opt_result.success:
                robust_lower, robust_upper = opt_result.x
                
                # ç¢ºä¿é †åºæ­£ç¢º
                if robust_lower > robust_upper:
                    robust_lower, robust_upper = robust_upper, robust_lower
                
                # è¨ˆç®—æ¯å€‹æ¨¡å‹çš„å¯¦éš›è¦†è“‹ç‡
                coverage_rates = {}
                for model_name, samples in samples_dict.items():
                    coverage_rates[model_name] = coverage_rate(samples, robust_lower, robust_upper)
                
                return {
                    "interval": (robust_lower, robust_upper),
                    "coverage_rates": coverage_rates,
                    "success": True,
                    "details": {
                        "optimization_result": opt_result,
                        "method": "constrained_optimization"
                    }
                }
            else:
                print("   âš ï¸ ç´„æŸå„ªåŒ–å¤±æ•—ï¼Œå›é€€åˆ°åˆ†ä½æ•¸æ–¹æ³•")
                return self._compute_robust_interval_quantile(samples_dict, alpha)
                
        except Exception as e:
            print(f"   âš ï¸ å„ªåŒ–éç¨‹å‡ºéŒ¯: {e}")
            return self._compute_robust_interval_quantile(samples_dict, alpha)
    
    def _compute_robust_interval_grid_search(self,
                                           samples_dict: Dict[str, np.ndarray],
                                           alpha: float) -> Dict[str, Any]:
        """ä½¿ç”¨ç¶²æ ¼æœç´¢è¨ˆç®—ç©©å¥å€é–“"""
        print("   ğŸ” ä½¿ç”¨ç¶²æ ¼æœç´¢æ–¹æ³•...")
        
        # ç¢ºå®šæœç´¢ç¶²æ ¼
        all_values = np.concatenate(list(samples_dict.values()))
        data_min = np.min(all_values)
        data_max = np.max(all_values)
        
        # å‰µå»ºç¶²æ ¼
        grid_points = np.linspace(data_min, data_max, self.config.grid_resolution)
        
        best_interval = None
        best_width = float('inf')
        best_coverage = {}
        
        # éæ­·æ‰€æœ‰å¯èƒ½çš„å€é–“
        for i in range(len(grid_points)):
            for j in range(i + 1, len(grid_points)):
                lower = grid_points[i]
                upper = grid_points[j]
                
                # è¨ˆç®—æ¯å€‹æ¨¡å‹çš„è¦†è“‹ç‡
                min_coverage = float('inf')
                coverage_rates = {}
                
                for model_name, samples in samples_dict.items():
                    coverage = np.mean((samples >= lower) & (samples <= upper))
                    coverage_rates[model_name] = coverage
                    min_coverage = min(min_coverage, coverage)
                
                # æª¢æŸ¥æ˜¯å¦æ»¿è¶³ç´„æŸ
                if min_coverage >= (1 - alpha - self.config.min_coverage_tolerance):
                    width = upper - lower
                    if width < best_width:
                        best_width = width
                        best_interval = (lower, upper)
                        best_coverage = coverage_rates.copy()
        
        if best_interval is None:
            print("   âš ï¸ ç¶²æ ¼æœç´¢æœªæ‰¾åˆ°æ»¿è¶³ç´„æŸçš„å€é–“ï¼Œä½¿ç”¨ä¿å®ˆä¼°è¨ˆ")
            return self._compute_robust_interval_quantile(samples_dict, alpha)
        
        return {
            "interval": best_interval,
            "coverage_rates": best_coverage,
            "success": True,
            "details": {
                "method": "grid_search",
                "grid_resolution": self.config.grid_resolution,
                "best_width": best_width
            }
        }
    
    def _compute_robust_interval_quantile(self,
                                        samples_dict: Dict[str, np.ndarray],
                                        alpha: float) -> Dict[str, Any]:
        """ä½¿ç”¨åˆ†ä½æ•¸æ–¹æ³•è¨ˆç®—ç©©å¥å€é–“ï¼ˆä¿å®ˆä¼°è¨ˆï¼‰"""
        print("   ğŸ“Š ä½¿ç”¨åˆ†ä½æ•¸æ–¹æ³•ï¼ˆä¿å®ˆä¼°è¨ˆï¼‰...")
        
        # å°æ¯å€‹æ¨¡å‹è¨ˆç®—æ¨™æº–å¯ä¿¡å€é–“
        individual_intervals = {}
        for model_name, samples in samples_dict.items():
            lower = np.percentile(samples, 100 * alpha / 2)
            upper = np.percentile(samples, 100 * (1 - alpha / 2))
            individual_intervals[model_name] = (lower, upper)
        
        # ä¿å®ˆçš„ç©©å¥å€é–“ï¼šå–æ‰€æœ‰å€é–“çš„ä¸¦é›†
        all_lowers = [interval[0] for interval in individual_intervals.values()]
        all_uppers = [interval[1] for interval in individual_intervals.values()]
        
        robust_lower = np.min(all_lowers)
        robust_upper = np.max(all_uppers)
        
        # è¨ˆç®—è¦†è“‹ç‡
        coverage_rates = {}
        for model_name, samples in samples_dict.items():
            coverage = np.mean((samples >= robust_lower) & (samples <= robust_upper))
            coverage_rates[model_name] = coverage
        
        return {
            "interval": (robust_lower, robust_upper),
            "coverage_rates": coverage_rates,
            "success": True,
            "details": {
                "method": "quantile_based",
                "individual_intervals": individual_intervals,
                "conservatism_note": "ä¿å®ˆä¼°è¨ˆï¼Œå¯èƒ½éå¯¬"
            }
        }
    
    def _compute_robust_interval_bootstrap(self,
                                         samples_dict: Dict[str, np.ndarray],
                                         alpha: float) -> Dict[str, Any]:
        """ä½¿ç”¨è²æ°æ‹”é´æ³•è¨ˆç®—ç©©å¥å€é–“"""
        print("   ğŸ¥¾ ä½¿ç”¨è²æ°æ‹”é´æ³•...")
        
        n_bootstrap = 1000
        bootstrap_intervals = []
        
        # å°æ¯å€‹æ¨¡å‹é€²è¡Œæ‹”é´æ³•æŠ½æ¨£
        for model_name, samples in samples_dict.items():
            model_intervals = []
            
            for _ in range(n_bootstrap):
                # æ‹”é´æ³•é‡æŠ½æ¨£
                boot_samples = np.random.choice(samples, size=len(samples), replace=True)
                lower = np.percentile(boot_samples, 100 * alpha / 2)
                upper = np.percentile(boot_samples, 100 * (1 - alpha / 2))
                model_intervals.append((lower, upper))
            
            bootstrap_intervals.append(model_intervals)
        
        # å°æ–¼æ¯æ¬¡æ‹”é´æ³•ï¼Œè¨ˆç®—ç©©å¥å€é–“
        robust_intervals = []
        
        for boot_idx in range(n_bootstrap):
            # å–è©²æ¬¡æ‹”é´æ³•æ‰€æœ‰æ¨¡å‹çš„å€é–“è¯é›†
            boot_lowers = []
            boot_uppers = []
            
            for model_intervals in bootstrap_intervals:
                lower, upper = model_intervals[boot_idx]
                boot_lowers.append(lower)
                boot_uppers.append(upper)
            
            robust_lower = np.min(boot_lowers)
            robust_upper = np.max(boot_uppers)
            robust_intervals.append((robust_lower, robust_upper))
        
        # è¨ˆç®—æ‹”é´æ³•å€é–“çš„ä¸­ä½æ•¸
        robust_lowers = [interval[0] for interval in robust_intervals]
        robust_uppers = [interval[1] for interval in robust_intervals]
        
        final_lower = np.percentile(robust_lowers, 100 * alpha / 2)
        final_upper = np.percentile(robust_uppers, 100 * (1 - alpha / 2))
        
        # è¨ˆç®—è¦†è“‹ç‡
        coverage_rates = {}
        for model_name, samples in samples_dict.items():
            coverage = np.mean((samples >= final_lower) & (samples <= final_upper))
            coverage_rates[model_name] = coverage
        
        return {
            "interval": (final_lower, final_upper),
            "coverage_rates": coverage_rates,
            "success": True,
            "details": {
                "method": "bayesian_bootstrap",
                "n_bootstrap": n_bootstrap,
                "bootstrap_intervals": robust_intervals
            }
        }
    
    def compute_standard_interval(self,
                                samples: np.ndarray,
                                alpha: float = 0.05) -> Tuple[float, float]:
        """è¨ˆç®—æ¨™æº–å¯ä¿¡å€é–“ï¼ˆå–®ä¸€æ¨¡å‹ï¼‰"""
        lower = np.percentile(samples, 100 * alpha / 2)
        upper = np.percentile(samples, 100 * (1 - alpha / 2))
        return lower, upper
    
    def compare_interval_types(self,
                             posterior_samples_dict: Dict[str, np.ndarray],
                             parameter_name: str,
                             alpha: float = 0.05) -> IntervalComparison:
        """
        æ¯”è¼ƒæ¨™æº–å€é–“èˆ‡ç©©å¥å€é–“
        
        Parameters:
        -----------
        posterior_samples_dict : Dict[str, np.ndarray]
            æ¯å€‹æ¨¡å‹çš„å¾Œé©—æ¨£æœ¬
        parameter_name : str
            åƒæ•¸åç¨±
        alpha : float
            é¡¯è‘—æ°´å¹³
            
        Returns:
        --------
        IntervalComparison
            å€é–“æ¯”è¼ƒçµæœ
        """
        print(f"ğŸ“Š æ¯”è¼ƒæ¨™æº–èˆ‡ç©©å¥å¯ä¿¡å€é–“ (åƒæ•¸: {parameter_name})...")
        
        # è¨ˆç®—ç©©å¥å€é–“
        robust_interval = self.compute_robust_interval(
            posterior_samples_dict, parameter_name, alpha
        )
        
        # è¨ˆç®—æ¨™æº–å€é–“ï¼ˆä½¿ç”¨åˆä½µæ¨£æœ¬ï¼‰
        all_samples = []
        coverage_by_model = {"standard": {}, "robust": {}}
        
        for model_name, samples in posterior_samples_dict.items():
            if isinstance(samples, dict) and parameter_name in samples:
                param_samples = np.asarray(samples[parameter_name]).flatten()
            else:
                param_samples = np.asarray(samples).flatten()
            
            valid_samples = param_samples[~np.isnan(param_samples)]
            if len(valid_samples) > 0:
                all_samples.extend(valid_samples)
                
                # è¨ˆç®—æ¯å€‹æ¨¡å‹å°å…©ç¨®å€é–“çš„è¦†è“‹ç‡
                coverage_by_model["robust"][model_name] = np.mean(
                    (valid_samples >= robust_interval[0]) & 
                    (valid_samples <= robust_interval[1])
                )
        
        standard_interval = self.compute_standard_interval(np.array(all_samples), alpha)
        
        # è¨ˆç®—æ¨™æº–å€é–“çš„è¦†è“‹ç‡
        for model_name, samples in posterior_samples_dict.items():
            if isinstance(samples, dict) and parameter_name in samples:
                param_samples = np.asarray(samples[parameter_name]).flatten()
            else:
                param_samples = np.asarray(samples).flatten()
            
            valid_samples = param_samples[~np.isnan(param_samples)]
            if len(valid_samples) > 0:
                coverage_by_model["standard"][model_name] = np.mean(
                    (valid_samples >= standard_interval[0]) & 
                    (valid_samples <= standard_interval[1])
                )
        
        # è¨ˆç®—å¯¬åº¦æ¯”
        standard_width = standard_interval[1] - standard_interval[0]
        robust_width = robust_interval[1] - robust_interval[0]
        width_ratio = robust_width / standard_width if standard_width > 0 else np.inf
        
        comparison = IntervalComparison(
            parameter_name=parameter_name,
            alpha=alpha,
            standard_interval=standard_interval,
            robust_interval=robust_interval,
            width_ratio=width_ratio,
            coverage_comparison=coverage_by_model
        )
        
        # ç·©å­˜æ¯”è¼ƒçµæœ
        cache_key = f"{parameter_name}_{alpha}_comparison"
        self.comparison_cache[cache_key] = comparison
        
        print(f"   æ¨™æº–å€é–“: [{standard_interval[0]:.4f}, {standard_interval[1]:.4f}] (å¯¬åº¦: {standard_width:.4f})")
        print(f"   ç©©å¥å€é–“: [{robust_interval[0]:.4f}, {robust_interval[1]:.4f}] (å¯¬åº¦: {robust_width:.4f})")
        print(f"   å¯¬åº¦æ¯”ç‡: {width_ratio:.2f}")
        
        return comparison
    
    def analyze_interval_robustness(self,
                                  posterior_samples_dict: Dict[str, np.ndarray],
                                  parameter_name: str,
                                  alpha_levels: List[float] = None) -> Dict[str, Any]:
        """
        åˆ†æå€é–“çš„ç©©å¥æ€§ç‰¹æ€§
        
        Parameters:
        -----------
        posterior_samples_dict : Dict[str, np.ndarray]
            æ¯å€‹æ¨¡å‹çš„å¾Œé©—æ¨£æœ¬
        parameter_name : str
            åƒæ•¸åç¨±
        alpha_levels : List[float], optional
            ä¸åŒçš„é¡¯è‘—æ°´å¹³
            
        Returns:
        --------
        Dict[str, Any]
            ç©©å¥æ€§åˆ†æçµæœ
        """
        if alpha_levels is None:
            alpha_levels = [0.01, 0.05, 0.1, 0.2]
        
        print(f"ğŸ” åˆ†æåƒæ•¸ '{parameter_name}' çš„å€é–“ç©©å¥æ€§...")
        
        robustness_results = {
            "parameter_name": parameter_name,
            "alpha_levels": alpha_levels,
            "interval_analysis": {},
            "width_analysis": {},
            "coverage_analysis": {}
        }
        
        for alpha in alpha_levels:
            print(f"   åˆ†æ Î±={alpha} (ç½®ä¿¡åº¦={100*(1-alpha):.0f}%)...")
            
            comparison = self.compare_interval_types(
                posterior_samples_dict, parameter_name, alpha
            )
            
            robustness_results["interval_analysis"][alpha] = {
                "standard_interval": comparison.standard_interval,
                "robust_interval": comparison.robust_interval,
                "width_ratio": comparison.width_ratio
            }
            
            # åˆ†æå¯¬åº¦è®ŠåŒ–
            standard_width = comparison.standard_interval[1] - comparison.standard_interval[0]
            robust_width = comparison.robust_interval[1] - comparison.robust_interval[0]
            
            robustness_results["width_analysis"][alpha] = {
                "standard_width": standard_width,
                "robust_width": robust_width,
                "width_difference": robust_width - standard_width,
                "width_ratio": comparison.width_ratio
            }
            
            # åˆ†æè¦†è“‹ç‡
            standard_coverages = list(comparison.coverage_comparison["standard"].values())
            robust_coverages = list(comparison.coverage_comparison["robust"].values())
            
            robustness_results["coverage_analysis"][alpha] = {
                "standard_min_coverage": np.min(standard_coverages),
                "standard_coverage_std": np.std(standard_coverages),
                "robust_min_coverage": np.min(robust_coverages),
                "robust_coverage_std": np.std(robust_coverages),
                "coverage_improvement": np.min(robust_coverages) - np.min(standard_coverages)
            }
        
        # ç¸½çµåˆ†æ
        width_ratios = [robustness_results["width_analysis"][Î±]["width_ratio"] 
                       for Î± in alpha_levels]
        
        robustness_results["summary"] = {
            "mean_width_ratio": np.mean(width_ratios),
            "width_ratio_std": np.std(width_ratios),
            "max_width_ratio": np.max(width_ratios),
            "stability_assessment": "ç©©å®š" if np.std(width_ratios) < 0.5 else "ä¸ç©©å®š"
        }
        
        print(f"âœ… ç©©å¥æ€§åˆ†æå®Œæˆ")
        print(f"   å¹³å‡å¯¬åº¦æ¯”ç‡: {robustness_results['summary']['mean_width_ratio']:.2f}")
        print(f"   ç©©å®šæ€§è©•ä¼°: {robustness_results['summary']['stability_assessment']}")
        
        return robustness_results
    
    def get_calculation_summary(self) -> pd.DataFrame:
        """ç²å–è¨ˆç®—æ‘˜è¦è¡¨"""
        if not self.calculation_cache:
            return pd.DataFrame()
        
        summary_data = []
        
        for cache_key, result in self.calculation_cache.items():
            min_coverage = min(result.coverage_rates.values()) if result.coverage_rates else np.nan
            
            summary_data.append({
                "åƒæ•¸": result.parameter_name,
                "Î±": result.alpha,
                "ç½®ä¿¡åº¦": f"{100*(1-result.alpha):.1f}%",
                "ä¸‹ç•Œ": result.interval[0],
                "ä¸Šç•Œ": result.interval[1],
                "å¯¬åº¦": result.interval_width,
                "æœ€å°è¦†è“‹ç‡": f"{min_coverage:.1%}" if not np.isnan(min_coverage) else "N/A",
                "æ–¹æ³•": result.method,
                "å„ªåŒ–æˆåŠŸ": result.optimization_success
            })
        
        return pd.DataFrame(summary_data)

# ä¾¿åˆ©å‡½æ•¸
def compute_robust_credible_interval(posterior_samples_dict: Dict[str, np.ndarray],
                                   parameter_name: str,
                                   alpha: float = 0.05) -> Tuple[float, float]:
    """
    ä¾¿åˆ©å‡½æ•¸ï¼šå¿«é€Ÿè¨ˆç®—ç©©å¥å¯ä¿¡å€é–“
    
    Parameters:
    -----------
    posterior_samples_dict : Dict[str, np.ndarray]
        æ¯å€‹æ¨¡å‹çš„å¾Œé©—æ¨£æœ¬
    parameter_name : str
        åƒæ•¸åç¨±
    alpha : float
        é¡¯è‘—æ°´å¹³
        
    Returns:
    --------
    Tuple[float, float]
        ç©©å¥å¯ä¿¡å€é–“
    """
    calculator = RobustCredibleIntervalCalculator()
    return calculator.compute_robust_interval(posterior_samples_dict, parameter_name, alpha)

def compare_credible_intervals(posterior_samples_dict: Dict[str, np.ndarray],
                             parameter_name: str,
                             alpha: float = 0.05) -> IntervalComparison:
    """
    ä¾¿åˆ©å‡½æ•¸ï¼šæ¯”è¼ƒæ¨™æº–èˆ‡ç©©å¥å€é–“
    
    Parameters:
    -----------
    posterior_samples_dict : Dict[str, np.ndarray]
        æ¯å€‹æ¨¡å‹çš„å¾Œé©—æ¨£æœ¬
    parameter_name : str
        åƒæ•¸åç¨±
    alpha : float
        é¡¯è‘—æ°´å¹³
        
    Returns:
    --------
    IntervalComparison
        æ¯”è¼ƒçµæœ
    """
    calculator = RobustCredibleIntervalCalculator()
    return calculator.compare_interval_types(posterior_samples_dict, parameter_name, alpha)

def test_robust_credible_intervals():
    """æ¸¬è©¦ç©©å¥å¯ä¿¡å€é–“åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦ç©©å¥å¯ä¿¡å€é–“è¨ˆç®—...")
    
    # ç”Ÿæˆæ¸¬è©¦æ•¸æ“šï¼ˆ3å€‹ä¸åŒçš„æ¨¡å‹ï¼‰
    np.random.seed(42)
    
    # æ¨¡å‹1ï¼šæ­£æ…‹åˆ†å¸ƒ
    model1_samples = np.random.normal(5.0, 1.0, 500)
    
    # æ¨¡å‹2ï¼šç¨å¾®åç§»çš„æ­£æ…‹åˆ†å¸ƒ
    model2_samples = np.random.normal(5.5, 1.2, 500)
    
    # æ¨¡å‹3ï¼šæ›´å¯¬çš„æ­£æ…‹åˆ†å¸ƒ
    model3_samples = np.random.normal(4.8, 1.5, 500)
    
    posterior_samples = {
        "normal_weak": model1_samples,
        "normal_optimistic": model2_samples, 
        "student_t_conservative": model3_samples
    }
    
    print(f"\næ¸¬è©¦æ•¸æ“šæ‘˜è¦:")
    for name, samples in posterior_samples.items():
        print(f"   {name}: å‡å€¼={np.mean(samples):.3f}, æ¨™æº–å·®={np.std(samples):.3f}")
    
    # æ¸¬è©¦ç©©å¥å€é–“è¨ˆç®—
    print(f"\nğŸ” è¨ˆç®—ç©©å¥å¯ä¿¡å€é–“...")
    calculator = RobustCredibleIntervalCalculator()
    
    robust_interval = calculator.compute_robust_interval(
        posterior_samples, "theta", alpha=0.05
    )
    
    print(f"ç©©å¥95%å€é–“: [{robust_interval[0]:.4f}, {robust_interval[1]:.4f}]")
    
    # æ¯”è¼ƒä¸åŒé¡å‹çš„å€é–“
    print(f"\nğŸ“Š æ¯”è¼ƒå€é–“é¡å‹...")
    comparison = calculator.compare_interval_types(
        posterior_samples, "theta", alpha=0.05
    )
    
    print(f"æ¨™æº–å€é–“: [{comparison.standard_interval[0]:.4f}, {comparison.standard_interval[1]:.4f}]")
    print(f"ç©©å¥å€é–“: [{comparison.robust_interval[0]:.4f}, {comparison.robust_interval[1]:.4f}]") 
    print(f"å¯¬åº¦æ¯”ç‡: {comparison.width_ratio:.2f}")
    
    # ç©©å¥æ€§åˆ†æ
    print(f"\nğŸ” ç©©å¥æ€§åˆ†æ...")
    robustness = calculator.analyze_interval_robustness(
        posterior_samples, "theta", alpha_levels=[0.05, 0.1, 0.2]
    )
    
    print(f"å¹³å‡å¯¬åº¦æ¯”ç‡: {robustness['summary']['mean_width_ratio']:.2f}")
    print(f"ç©©å®šæ€§è©•ä¼°: {robustness['summary']['stability_assessment']}")
    
    # é¡¯ç¤ºæ‘˜è¦è¡¨
    print(f"\nğŸ“‹ è¨ˆç®—æ‘˜è¦:")
    summary_table = calculator.get_calculation_summary()
    if not summary_table.empty:
        print(summary_table[['åƒæ•¸', 'ç½®ä¿¡åº¦', 'å¯¬åº¦', 'æœ€å°è¦†è“‹ç‡', 'æ–¹æ³•']])
    
    print(f"\nâœ… ç©©å¥å¯ä¿¡å€é–“æ¸¬è©¦å®Œæˆ")
    return robust_interval, comparison, robustness

if __name__ == "__main__":
    test_robust_credible_intervals()
"""
Model Comparison Reporter Module
æ¨¡å‹æ¯”è¼ƒå ±å‘Šæ¨¡çµ„

æä¾›å…¨é¢çš„è²æ°æ¨¡å‹æ¯”è¼ƒã€é¸æ“‡å’Œè©•ä¼°åŠŸèƒ½ã€‚
åŒ…æ‹¬ AIC/BIC/WAIC æ¯”è¼ƒã€Bayes Factor è¨ˆç®—ã€æ¨¡å‹å¹³å‡ç­‰ã€‚

Key Features:
- å¤šé‡æ¨¡å‹æ¯”è¼ƒ (AIC, BIC, WAIC, LOO-CV)
- Bayes Factor è¨ˆç®—å’Œè§£é‡‹
- æ¨¡å‹æ¬Šé‡å’Œæ¨¡å‹å¹³å‡
- é æ¸¬æ•ˆèƒ½äº¤å‰é©—è­‰
- æ¨¡å‹é¸æ“‡å»ºè­°
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import warnings
from scipy import stats
from scipy.special import logsumexp
import sys
import os

# Import parent bayesian modules - use relative imports
try:
    from ..robust_bayesian_analysis import (
        ModelComparisonResult, ModelConfiguration, 
        ModelSelectionCriterion, RobustBayesianFramework
    )
except ImportError:
    # Fallback for direct execution
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from robust_bayesian_analysis import (
        ModelComparisonResult, ModelConfiguration, 
        ModelSelectionCriterion, RobustBayesianFramework
    )

class ModelSupport(Enum):
    """æ¨¡å‹æ”¯æŒåº¦"""
    DECISIVE = "decisive"           # BF > 100
    VERY_STRONG = "very_strong"     # 30 < BF â‰¤ 100
    STRONG = "strong"               # 10 < BF â‰¤ 30
    MODERATE = "moderate"           # 3 < BF â‰¤ 10
    WEAK = "weak"                   # 1 < BF â‰¤ 3
    INCONCLUSIVE = "inconclusive"   # BF â‰¤ 1

@dataclass
class ModelComparison:
    """æ¨¡å‹æ¯”è¼ƒçµæœ"""
    model_name: str
    log_likelihood: float
    aic: float
    bic: float
    waic: Optional[float]
    loo_cv: Optional[float]
    bayes_factor: Optional[float]
    model_weight: float
    support_level: ModelSupport
    rank: int

@dataclass
class ModelAveraging:
    """æ¨¡å‹å¹³å‡çµæœ"""
    weighted_predictions: np.ndarray
    model_weights: Dict[str, float]
    individual_predictions: Dict[str, np.ndarray]
    uncertainty_decomposition: Dict[str, float]

@dataclass
class ModelComparisonReport:
    """æ¨¡å‹æ¯”è¼ƒå ±å‘Š"""
    model_comparisons: List[ModelComparison]
    best_model: ModelComparison
    model_averaging: ModelAveraging
    cross_validation_results: Dict[str, Any]
    recommendations: List[str]
    comparison_summary: pd.DataFrame

class ModelComparisonReporter:
    """
    æ¨¡å‹æ¯”è¼ƒå ±å‘Šå™¨
    
    æä¾›å…¨é¢çš„è²æ°æ¨¡å‹æ¯”è¼ƒå’Œé¸æ“‡åˆ†æ
    """
    
    def __init__(self, 
                 information_criteria: List[str] = ['aic', 'bic', 'waic'],
                 cv_folds: int = 5,
                 bayes_factor_threshold: float = 3.0):
        """
        åˆå§‹åŒ–æ¨¡å‹æ¯”è¼ƒå ±å‘Šå™¨
        
        Parameters:
        -----------
        information_criteria : List[str]
            ä½¿ç”¨çš„è³‡è¨Šæº–å‰‡
        cv_folds : int
            äº¤å‰é©—è­‰æ‘ºæ•¸
        bayes_factor_threshold : float
            Bayes Factor é¡¯è‘—æ€§é–¾å€¼
        """
        self.information_criteria = information_criteria
        self.cv_folds = cv_folds
        self.bayes_factor_threshold = bayes_factor_threshold
        
        # è¨­ç½®åœ–è¡¨æ¨£å¼
        plt.style.use('default')
        sns.set_style("whitegrid")
        self.colors = sns.color_palette("tab10", 10)
    
    def compare_models(self, 
                      model_results: List[ModelComparisonResult],
                      observed_data: np.ndarray,
                      posterior_samples: Dict[str, Dict[str, np.ndarray]]) -> ModelComparisonReport:
        """
        å…¨é¢çš„æ¨¡å‹æ¯”è¼ƒåˆ†æ
        
        Parameters:
        -----------
        model_results : List[ModelComparisonResult]
            æ¨¡å‹æ¯”è¼ƒçµæœåˆ—è¡¨
        observed_data : np.ndarray
            è§€æ¸¬è³‡æ–™
        posterior_samples : Dict[str, Dict[str, np.ndarray]]
            å„æ¨¡å‹çš„å¾Œé©—æ¨£æœ¬ {model_name: {param: samples}}
            
        Returns:
        --------
        ModelComparisonReport
            å®Œæ•´çš„æ¨¡å‹æ¯”è¼ƒå ±å‘Š
        """
        
        print("ğŸ“Š é–‹å§‹æ¨¡å‹æ¯”è¼ƒåˆ†æ...")
        
        if not model_results:
            print("âš ï¸ æ²’æœ‰æ¨¡å‹çµæœå¯æ¯”è¼ƒ")
            return self._create_empty_report()
        
        # 1. è¨ˆç®— Bayes Factors
        print("  ğŸ”„ è¨ˆç®— Bayes Factors...")
        bayes_factors = self._calculate_bayes_factors(model_results)
        
        # 2. è¨ˆç®—æ¨¡å‹æ¬Šé‡
        print("  âš–ï¸ è¨ˆç®—æ¨¡å‹æ¬Šé‡...")
        model_weights = self._calculate_model_weights(model_results)
        
        # 3. æ¨¡å‹æ’åå’Œæ”¯æŒåº¦
        print("  ğŸ“ˆ è©•ä¼°æ¨¡å‹æ”¯æŒåº¦...")
        model_comparisons = self._create_model_comparisons(
            model_results, bayes_factors, model_weights
        )
        
        # 4. æ¨¡å‹å¹³å‡
        print("  ğŸ”„ åŸ·è¡Œæ¨¡å‹å¹³å‡...")
        model_averaging = self._perform_model_averaging(
            model_comparisons, posterior_samples, observed_data
        )
        
        # 5. äº¤å‰é©—è­‰
        print("  âœ… äº¤å‰é©—è­‰åˆ†æ...")
        cv_results = self._cross_validation_analysis(
            model_results, observed_data, posterior_samples
        )
        
        # 6. é¸æ“‡æœ€ä½³æ¨¡å‹
        best_model = min(model_comparisons, key=lambda x: x.aic)
        
        # 7. ç”Ÿæˆå»ºè­°
        recommendations = self._generate_model_recommendations(
            model_comparisons, cv_results
        )
        
        # 8. å‰µå»ºæ¯”è¼ƒæ‘˜è¦è¡¨
        comparison_summary = self._create_comparison_summary(model_comparisons)
        
        report = ModelComparisonReport(
            model_comparisons=model_comparisons,
            best_model=best_model,
            model_averaging=model_averaging,
            cross_validation_results=cv_results,
            recommendations=recommendations,
            comparison_summary=comparison_summary
        )
        
        print("âœ… æ¨¡å‹æ¯”è¼ƒåˆ†æå®Œæˆ")
        return report
    
    def _calculate_bayes_factors(self, 
                               model_results: List[ModelComparisonResult]) -> Dict[str, float]:
        """è¨ˆç®— Bayes Factors"""
        
        if len(model_results) < 2:
            return {}
        
        # ä½¿ç”¨å°æ•¸ä¼¼ç„¶è¨ˆç®— Bayes Factor (ç°¡åŒ–)
        log_likelihoods = {result.model_name: result.log_likelihood 
                          for result in model_results}
        
        # é¸æ“‡åƒè€ƒæ¨¡å‹ (æœ€é«˜å°æ•¸ä¼¼ç„¶)
        reference_model = max(log_likelihoods.keys(), key=lambda k: log_likelihoods[k])
        reference_ll = log_likelihoods[reference_model]
        
        bayes_factors = {}
        for model_name, ll in log_likelihoods.items():
            if model_name != reference_model:
                # BF = exp(log_likelihood_model - log_likelihood_reference)
                log_bf = ll - reference_ll
                bayes_factors[model_name] = np.exp(log_bf)
            else:
                bayes_factors[model_name] = 1.0  # åƒè€ƒæ¨¡å‹
        
        return bayes_factors
    
    def _calculate_model_weights(self, 
                               model_results: List[ModelComparisonResult]) -> Dict[str, float]:
        """è¨ˆç®—æ¨¡å‹æ¬Šé‡ (åŸºæ–¼ AIC)"""
        
        if not model_results:
            return {}
        
        aics = np.array([result.aic for result in model_results])
        model_names = [result.model_name for result in model_results]
        
        # AIC æ¬Šé‡è¨ˆç®—
        min_aic = np.min(aics)
        delta_aic = aics - min_aic
        
        # é¿å…æ•¸å€¼æº¢å‡º
        delta_aic = np.clip(delta_aic, 0, 700)
        
        # è¨ˆç®—æ¬Šé‡
        weights_unnorm = np.exp(-0.5 * delta_aic)
        weights = weights_unnorm / np.sum(weights_unnorm)
        
        return dict(zip(model_names, weights))
    
    def _assess_bayes_factor_support(self, bf: float) -> ModelSupport:
        """è©•ä¼° Bayes Factor æ”¯æŒåº¦"""
        
        if bf > 100:
            return ModelSupport.DECISIVE
        elif bf > 30:
            return ModelSupport.VERY_STRONG
        elif bf > 10:
            return ModelSupport.STRONG
        elif bf > 3:
            return ModelSupport.MODERATE
        elif bf > 1:
            return ModelSupport.WEAK
        else:
            return ModelSupport.INCONCLUSIVE
    
    def _create_model_comparisons(self, 
                                model_results: List[ModelComparisonResult],
                                bayes_factors: Dict[str, float],
                                model_weights: Dict[str, float]) -> List[ModelComparison]:
        """å‰µå»ºæ¨¡å‹æ¯”è¼ƒçµæœ"""
        
        comparisons = []
        
        # æŒ‰ AIC æ’åº
        sorted_results = sorted(model_results, key=lambda x: x.aic)
        
        for rank, result in enumerate(sorted_results, 1):
            bf = bayes_factors.get(result.model_name, 1.0)
            support = self._assess_bayes_factor_support(bf)
            
            comparison = ModelComparison(
                model_name=result.model_name,
                log_likelihood=result.log_likelihood,
                aic=result.aic,
                bic=result.bic,
                waic=result.waic,
                loo_cv=result.loo_cv,
                bayes_factor=bf,
                model_weight=model_weights.get(result.model_name, 0.0),
                support_level=support,
                rank=rank
            )
            
            comparisons.append(comparison)
        
        return comparisons
    
    def _perform_model_averaging(self, 
                               model_comparisons: List[ModelComparison],
                               posterior_samples: Dict[str, Dict[str, np.ndarray]],
                               observed_data: np.ndarray) -> ModelAveraging:
        """åŸ·è¡Œæ¨¡å‹å¹³å‡"""
        
        if not model_comparisons or not posterior_samples:
            return ModelAveraging(
                weighted_predictions=np.array([]),
                model_weights={},
                individual_predictions={},
                uncertainty_decomposition={}
            )
        
        # æå–æ¬Šé‡
        weights = {comp.model_name: comp.model_weight for comp in model_comparisons}
        
        # ç‚ºæ¯å€‹æ¨¡å‹ç”Ÿæˆé æ¸¬
        individual_predictions = {}
        
        for model_name, weight in weights.items():
            if model_name in posterior_samples and weight > 0.01:  # å¿½ç•¥æ¬Šé‡å¾ˆå°çš„æ¨¡å‹
                # ç°¡åŒ–çš„é æ¸¬ç”Ÿæˆ
                samples = posterior_samples[model_name]
                
                if samples:
                    # å‡è¨­æˆ‘å€‘æœ‰åƒæ•¸æ¨£æœ¬ï¼Œç”Ÿæˆé æ¸¬åˆ†å¸ƒ
                    first_param = list(samples.values())[0]
                    n_pred = len(observed_data)
                    
                    # ç°¡åŒ–ï¼šä½¿ç”¨åƒæ•¸å‡å€¼ç”Ÿæˆé æ¸¬
                    param_means = {param: np.mean(param_samples) 
                                 for param, param_samples in samples.items()
                                 if param_samples.ndim == 1}
                    
                    if 'mu' in param_means and 'sigma' in param_means:
                        predictions = np.random.normal(
                            param_means['mu'], 
                            param_means['sigma'], 
                            n_pred
                        )
                    else:
                        # å›é€€åˆ°è§€æ¸¬è³‡æ–™çš„æ“¾å‹•
                        predictions = observed_data + np.random.normal(0, np.std(observed_data) * 0.1, n_pred)
                    
                    individual_predictions[model_name] = predictions
        
        # è¨ˆç®—åŠ æ¬Šå¹³å‡é æ¸¬
        if individual_predictions:
            weighted_pred = np.zeros_like(list(individual_predictions.values())[0])
            total_weight = 0
            
            for model_name, predictions in individual_predictions.items():
                weight = weights[model_name]
                weighted_pred += weight * predictions
                total_weight += weight
            
            if total_weight > 0:
                weighted_predictions = weighted_pred / total_weight
            else:
                weighted_predictions = weighted_pred
        else:
            weighted_predictions = np.array([])
        
        # ä¸ç¢ºå®šæ€§åˆ†è§£
        uncertainty_decomp = self._decompose_prediction_uncertainty(
            individual_predictions, weights
        )
        
        return ModelAveraging(
            weighted_predictions=weighted_predictions,
            model_weights=weights,
            individual_predictions=individual_predictions,
            uncertainty_decomposition=uncertainty_decomp
        )
    
    def _decompose_prediction_uncertainty(self, 
                                        individual_predictions: Dict[str, np.ndarray],
                                        weights: Dict[str, float]) -> Dict[str, float]:
        """åˆ†è§£é æ¸¬ä¸ç¢ºå®šæ€§"""
        
        if len(individual_predictions) < 2:
            return {'within_model': 1.0, 'between_model': 0.0}
        
        # è¨ˆç®—æ¯å€‹æ¨¡å‹å…§çš„ä¸ç¢ºå®šæ€§
        within_model_vars = []
        for model_name, predictions in individual_predictions.items():
            within_model_vars.append(np.var(predictions))
        
        within_model_uncertainty = np.mean(within_model_vars)
        
        # è¨ˆç®—æ¨¡å‹é–“çš„ä¸ç¢ºå®šæ€§
        model_means = [np.mean(pred) for pred in individual_predictions.values()]
        between_model_uncertainty = np.var(model_means)
        
        # æ­¸ä¸€åŒ–
        total_uncertainty = within_model_uncertainty + between_model_uncertainty
        
        if total_uncertainty > 0:
            within_fraction = within_model_uncertainty / total_uncertainty
            between_fraction = between_model_uncertainty / total_uncertainty
        else:
            within_fraction = between_fraction = 0.5
        
        return {
            'within_model': within_fraction,
            'between_model': between_fraction,
            'total_uncertainty': total_uncertainty
        }
    
    def _cross_validation_analysis(self, 
                                 model_results: List[ModelComparisonResult],
                                 observed_data: np.ndarray,
                                 posterior_samples: Dict[str, Dict[str, np.ndarray]]) -> Dict[str, Any]:
        """äº¤å‰é©—è­‰åˆ†æ"""
        
        if len(observed_data) < self.cv_folds:
            return {'error': 'Data too small for cross-validation'}
        
        n_data = len(observed_data)
        fold_size = n_data // self.cv_folds
        
        cv_scores = {}
        
        for result in model_results:
            model_name = result.model_name
            scores = []
            
            # K-fold äº¤å‰é©—è­‰
            for fold in range(self.cv_folds):
                start_idx = fold * fold_size
                end_idx = start_idx + fold_size if fold < self.cv_folds - 1 else n_data
                
                # åˆ†å‰²è³‡æ–™
                test_data = observed_data[start_idx:end_idx]
                train_data = np.concatenate([
                    observed_data[:start_idx],
                    observed_data[end_idx:]
                ])
                
                # ç°¡åŒ–çš„é æ¸¬è©•åˆ†
                if len(train_data) > 0 and len(test_data) > 0:
                    # ä½¿ç”¨è¨“ç·´è³‡æ–™çš„çµ±è¨ˆé‡é æ¸¬æ¸¬è©¦è³‡æ–™
                    train_mean = np.mean(train_data)
                    train_std = np.std(train_data)
                    
                    # è¨ˆç®—å°æ•¸ä¼¼ç„¶
                    log_likelihood = np.sum(stats.norm.logpdf(test_data, train_mean, train_std))
                    scores.append(log_likelihood)
            
            cv_scores[model_name] = {
                'mean_score': np.mean(scores) if scores else -np.inf,
                'std_score': np.std(scores) if scores else 0,
                'fold_scores': scores
            }
        
        # æ‰¾åˆ°æœ€ä½³ CV æ¨¡å‹
        best_cv_model = max(cv_scores.keys(), 
                           key=lambda k: cv_scores[k]['mean_score'])
        
        return {
            'cv_scores': cv_scores,
            'best_cv_model': best_cv_model,
            'n_folds': self.cv_folds
        }
    
    def _generate_model_recommendations(self, 
                                      model_comparisons: List[ModelComparison],
                                      cv_results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ¨¡å‹é¸æ“‡å»ºè­°"""
        
        recommendations = []
        
        if not model_comparisons:
            recommendations.append("âŒ æ²’æœ‰æœ‰æ•ˆçš„æ¨¡å‹å¯æ¯”è¼ƒ")
            return recommendations
        
        best_model = model_comparisons[0]  # å·²æŒ‰ AIC æ’åº
        
        # æœ€ä½³æ¨¡å‹å»ºè­°
        recommendations.append(f"ğŸ† æ¨è–¦æ¨¡å‹: {best_model.model_name}")
        recommendations.append(f"ğŸ“Š AIC: {best_model.aic:.2f}, æ¬Šé‡: {best_model.model_weight:.3f}")
        
        # Bayes Factor è©•ä¼°
        if best_model.bayes_factor and best_model.bayes_factor > 1:
            support_msg = {
                ModelSupport.DECISIVE: "æ±ºå®šæ€§æ”¯æŒ",
                ModelSupport.VERY_STRONG: "éå¸¸å¼·çƒˆæ”¯æŒ", 
                ModelSupport.STRONG: "å¼·çƒˆæ”¯æŒ",
                ModelSupport.MODERATE: "ä¸­ç­‰æ”¯æŒ",
                ModelSupport.WEAK: "å¾®å¼±æ”¯æŒ",
                ModelSupport.INCONCLUSIVE: "ç„¡æ˜ç¢ºæ”¯æŒ"
            }
            
            recommendations.append(f"ğŸ¯ Bayes Factor: {support_msg[best_model.support_level]}")
        
        # æ¨¡å‹ä¸ç¢ºå®šæ€§è©•ä¼°
        top_weight = best_model.model_weight
        
        if top_weight > 0.7:
            recommendations.append("âœ… æ¨¡å‹é¸æ“‡ç¢ºä¿¡åº¦é«˜")
        elif top_weight > 0.4:
            recommendations.append("âš ï¸ æ¨¡å‹é¸æ“‡ç¢ºä¿¡åº¦ä¸­ç­‰ï¼Œå»ºè­°è€ƒæ…®æ¨¡å‹å¹³å‡")
        else:
            recommendations.append("ğŸ”„ æ¨¡å‹é¸æ“‡ä¸ç¢ºå®šï¼Œå¼·çƒˆå»ºè­°ä½¿ç”¨æ¨¡å‹å¹³å‡")
        
        # ç«¶çˆ­æ¨¡å‹è­¦å‘Š
        similar_models = [comp for comp in model_comparisons[1:3] 
                         if comp.aic - best_model.aic < 2]
        
        if similar_models:
            model_names = [m.model_name for m in similar_models]
            recommendations.append(f"âš¡ ç«¶çˆ­æ¨¡å‹: {', '.join(model_names)} (Î”AIC < 2)")
        
        # äº¤å‰é©—è­‰ä¸€è‡´æ€§
        if 'best_cv_model' in cv_results:
            cv_best = cv_results['best_cv_model']
            if cv_best != best_model.model_name:
                recommendations.append(f"ğŸ” äº¤å‰é©—è­‰æœ€ä½³æ¨¡å‹: {cv_best} (èˆ‡ AIC é¸æ“‡ä¸åŒ)")
        
        return recommendations
    
    def _create_comparison_summary(self, 
                                 model_comparisons: List[ModelComparison]) -> pd.DataFrame:
        """å‰µå»ºæ¨¡å‹æ¯”è¼ƒæ‘˜è¦è¡¨"""
        
        if not model_comparisons:
            return pd.DataFrame()
        
        data = []
        for comp in model_comparisons:
            data.append({
                'Model': comp.model_name,
                'Rank': comp.rank,
                'AIC': comp.aic,
                'BIC': comp.bic,
                'WAIC': comp.waic if comp.waic else np.nan,
                'LogLik': comp.log_likelihood,
                'Weight': comp.model_weight,
                'BayesFactor': comp.bayes_factor if comp.bayes_factor else np.nan,
                'Support': comp.support_level.value
            })
        
        return pd.DataFrame(data)
    
    def _create_empty_report(self) -> ModelComparisonReport:
        """å‰µå»ºç©ºå ±å‘Š"""
        
        return ModelComparisonReport(
            model_comparisons=[],
            best_model=None,
            model_averaging=ModelAveraging(
                weighted_predictions=np.array([]),
                model_weights={},
                individual_predictions={},
                uncertainty_decomposition={}
            ),
            cross_validation_results={},
            recommendations=["âŒ æ²’æœ‰æ¨¡å‹çµæœå¯åˆ†æ"],
            comparison_summary=pd.DataFrame()
        )
    
    def plot_model_comparison(self, 
                            comparison_report: ModelComparisonReport,
                            save_path: Optional[str] = None,
                            figsize: Tuple[int, int] = (15, 10)) -> plt.Figure:
        """ç¹ªè£½æ¨¡å‹æ¯”è¼ƒåœ–è¡¨"""
        
        if not comparison_report.model_comparisons:
            print("æ²’æœ‰æ¨¡å‹æ¯”è¼ƒçµæœå¯ç¹ªåœ–")
            return None
        
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        fig.suptitle('æ¨¡å‹æ¯”è¼ƒåˆ†æ', fontsize=16, fontweight='bold')
        
        comparisons = comparison_report.model_comparisons
        
        # 1. è³‡è¨Šæº–å‰‡æ¯”è¼ƒ (å·¦ä¸Š)
        ax1 = axes[0, 0]
        
        model_names = [comp.model_name for comp in comparisons]
        aics = [comp.aic for comp in comparisons]
        bics = [comp.bic for comp in comparisons]
        
        x = np.arange(len(model_names))
        width = 0.35
        
        ax1.bar(x - width/2, aics, width, label='AIC', color=self.colors[0], alpha=0.7)
        ax1.bar(x + width/2, bics, width, label='BIC', color=self.colors[1], alpha=0.7)
        
        ax1.set_xlabel('æ¨¡å‹')
        ax1.set_ylabel('è³‡è¨Šæº–å‰‡å€¼')
        ax1.set_title('AIC vs BIC æ¯”è¼ƒ')
        ax1.set_xticks(x)
        ax1.set_xticklabels(model_names, rotation=45)
        ax1.legend()
        
        # 2. æ¨¡å‹æ¬Šé‡ (å³ä¸Š)
        ax2 = axes[0, 1]
        
        weights = [comp.model_weight for comp in comparisons]
        colors_weights = self.colors[:len(model_names)]
        
        wedges, texts, autotexts = ax2.pie(weights, labels=model_names, autopct='%1.1f%%',
                                          colors=colors_weights, startangle=90)
        ax2.set_title('æ¨¡å‹æ¬Šé‡åˆ†å¸ƒ')
        
        # 3. Bayes Factor (å·¦ä¸‹)
        ax3 = axes[1, 0]
        
        bayes_factors = [comp.bayes_factor for comp in comparisons if comp.bayes_factor]
        bf_models = [comp.model_name for comp in comparisons if comp.bayes_factor]
        
        if bayes_factors:
            bars = ax3.bar(range(len(bf_models)), bayes_factors, 
                          color=self.colors[2], alpha=0.7)
            ax3.set_xlabel('æ¨¡å‹')
            ax3.set_ylabel('Bayes Factor')
            ax3.set_title('Bayes Factor æ¯”è¼ƒ')
            ax3.set_xticks(range(len(bf_models)))
            ax3.set_xticklabels(bf_models, rotation=45)
            
            # æ·»åŠ é–¾å€¼ç·š
            ax3.axhline(1, color='red', linestyle='--', alpha=0.5, label='BF = 1')
            ax3.axhline(3, color='orange', linestyle='--', alpha=0.5, label='BF = 3')
            ax3.axhline(10, color='green', linestyle='--', alpha=0.5, label='BF = 10')
            ax3.legend()
        else:
            ax3.text(0.5, 0.5, 'ç„¡ Bayes Factor è³‡æ–™', ha='center', va='center', 
                    transform=ax3.transAxes, fontsize=12)
        
        # 4. ä¸ç¢ºå®šæ€§åˆ†è§£ (å³ä¸‹)
        ax4 = axes[1, 1]
        
        if comparison_report.model_averaging.uncertainty_decomposition:
            uncertainty = comparison_report.model_averaging.uncertainty_decomposition
            labels = ['æ¨¡å‹å…§', 'æ¨¡å‹é–“']
            sizes = [uncertainty.get('within_model', 0.5), 
                    uncertainty.get('between_model', 0.5)]
            colors_unc = self.colors[3:5]
            
            wedges, texts, autotexts = ax4.pie(sizes, labels=labels, autopct='%1.1f%%',
                                              colors=colors_unc, startangle=90)
            ax4.set_title('é æ¸¬ä¸ç¢ºå®šæ€§åˆ†è§£')
        else:
            ax4.text(0.5, 0.5, 'ç„¡ä¸ç¢ºå®šæ€§åˆ†è§£è³‡æ–™', ha='center', va='center',
                    transform=ax4.transAxes, fontsize=12)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        return fig
    
    def generate_comparison_report(self, 
                                 comparison_report: ModelComparisonReport,
                                 include_details: bool = True) -> str:
        """ç”Ÿæˆæ¨¡å‹æ¯”è¼ƒå ±å‘Š"""
        
        report = []
        report.append("=" * 80)
        report.append("                    æ¨¡å‹æ¯”è¼ƒåˆ†æå ±å‘Š")
        report.append("=" * 80)
        report.append("")
        
        if not comparison_report.model_comparisons:
            report.append("âŒ æ²’æœ‰æ¨¡å‹æ¯”è¼ƒçµæœ")
            return "\n".join(report)
        
        # æ•´é«”æ‘˜è¦
        report.append("ğŸ† æ¨¡å‹é¸æ“‡æ‘˜è¦")
        report.append("-" * 40)
        
        best = comparison_report.best_model
        report.append(f"ğŸ¥‡ æœ€ä½³æ¨¡å‹: {best.model_name}")
        report.append(f"ğŸ“Š AIC: {best.aic:.2f}")
        report.append(f"ğŸ“Š æ¨¡å‹æ¬Šé‡: {best.model_weight:.3f}")
        report.append(f"ğŸ“Š æ”¯æŒåº¦: {best.support_level.value}")
        report.append("")
        
        # æ¨¡å‹æ¯”è¼ƒè¡¨
        if include_details:
            report.append("ğŸ“‹ è©³ç´°æ¯”è¼ƒçµæœ")
            report.append("-" * 40)
            
            summary_df = comparison_report.comparison_summary
            if not summary_df.empty:
                # æ ¼å¼åŒ–è¡¨æ ¼
                for _, row in summary_df.iterrows():
                    rank_icon = "ğŸ¥‡" if row['Rank'] == 1 else "ğŸ¥ˆ" if row['Rank'] == 2 else "ğŸ¥‰" if row['Rank'] == 3 else "  "
                    report.append(f"{rank_icon} {row['Model']}:")
                    report.append(f"    æ’å: {row['Rank']}")
                    report.append(f"    AIC: {row['AIC']:.2f}, BIC: {row['BIC']:.2f}")
                    report.append(f"    æ¬Šé‡: {row['Weight']:.3f}")
                    if not pd.isna(row['BayesFactor']):
                        report.append(f"    Bayes Factor: {row['BayesFactor']:.2f}")
                    report.append("")
        
        # æ¨¡å‹å¹³å‡çµæœ
        if comparison_report.model_averaging.model_weights:
            report.append("ğŸ”„ æ¨¡å‹å¹³å‡åˆ†æ")
            report.append("-" * 40)
            
            averaging = comparison_report.model_averaging
            uncertainty = averaging.uncertainty_decomposition
            
            report.append(f"ğŸ“Š åƒèˆ‡æ¨¡å‹æ•¸: {len(averaging.model_weights)}")
            if uncertainty:
                report.append(f"ğŸ“Š æ¨¡å‹å…§ä¸ç¢ºå®šæ€§: {uncertainty.get('within_model', 0):.1%}")
                report.append(f"ğŸ“Š æ¨¡å‹é–“ä¸ç¢ºå®šæ€§: {uncertainty.get('between_model', 0):.1%}")
            report.append("")
        
        # äº¤å‰é©—è­‰çµæœ
        if comparison_report.cross_validation_results.get('best_cv_model'):
            report.append("âœ… äº¤å‰é©—è­‰çµæœ")
            report.append("-" * 40)
            
            cv = comparison_report.cross_validation_results
            report.append(f"ğŸ“Š æœ€ä½³ CV æ¨¡å‹: {cv['best_cv_model']}")
            report.append(f"ğŸ“Š é©—è­‰æ‘ºæ•¸: {cv['n_folds']}")
            report.append("")
        
        # å»ºè­°
        report.append("ğŸ’¡ æ¨¡å‹é¸æ“‡å»ºè­°")
        report.append("-" * 40)
        for recommendation in comparison_report.recommendations:
            report.append(recommendation)
        
        return "\n".join(report)
"""
Robustness Reporter Module
ç©©å¥æ€§å ±å‘Šæ¨¡çµ„

æä¾›å¯†åº¦æ¯”åˆ†æã€æ•æ„Ÿåº¦åˆ†æå’Œç©©å¥æ€§è©•ä¼°çš„è©³ç´°å ±å‘Šã€‚
åˆ†æè²æ°æ¨¡å‹å°å…ˆé©—é¸æ“‡å’Œæ¨¡å‹å‡è¨­çš„æ•æ„Ÿåº¦ã€‚

Key Features:
- Density Ratio Class ç´„æŸåˆ†æ
- å…ˆé©—æ•æ„Ÿåº¦åˆ†æ
- æ¨¡å‹å‡è¨­ç©©å¥æ€§æª¢é©—
- ä¸ç¢ºå®šæ€§ä¾†æºè²¢ç»åˆ†æ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any, Optional, Tuple, Callable
from dataclasses import dataclass
from enum import Enum
import warnings
from scipy import stats
from scipy.spatial.distance import jensenshannon
import sys
import os

# Import parent bayesian modules - use relative imports
try:
    from ..robust_bayesian_analysis import (
        RobustBayesianFramework, DensityRatioClass, 
        ModelComparisonResult, ModelConfiguration
    )
except ImportError:
    # Fallback for direct execution
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from robust_bayesian_analysis import (
        RobustBayesianFramework, DensityRatioClass, 
        ModelComparisonResult, ModelConfiguration
    )

class RobustnessLevel(Enum):
    """ç©©å¥æ€§æ°´å¹³"""
    HIGHLY_ROBUST = "highly_robust"      # < 5% è®Šç•°
    ROBUST = "robust"                    # 5-15% è®Šç•°
    MODERATELY_ROBUST = "moderately_robust"  # 15-30% è®Šç•°
    SENSITIVE = "sensitive"              # 30-50% è®Šç•°
    HIGHLY_SENSITIVE = "highly_sensitive"    # > 50% è®Šç•°

@dataclass
class DensityRatioAnalysis:
    """å¯†åº¦æ¯”åˆ†æçµæœ"""
    gamma_constraint: float
    violation_rate: float
    max_density_ratio: float
    mean_density_ratio: float
    violation_regions: Dict[str, Any]
    constraint_satisfaction: bool

@dataclass
class SensitivityAnalysis:
    """æ•æ„Ÿåº¦åˆ†æçµæœ"""
    parameter_name: str
    prior_scenarios: List[str]
    posterior_variation: float
    coefficient_of_variation: float
    robustness_level: RobustnessLevel
    sensitivity_metrics: Dict[str, float]

@dataclass
class RobustnessReport:
    """ç©©å¥æ€§å ±å‘Š"""
    overall_robustness: RobustnessLevel
    density_ratio_analysis: DensityRatioAnalysis
    sensitivity_analyses: Dict[str, SensitivityAnalysis]
    uncertainty_decomposition: Dict[str, float]
    recommendations: List[str]
    robustness_score: float

class RobustnessReporter:
    """
    ç©©å¥æ€§å ±å‘Šå™¨
    
    åˆ†æè²æ°æ¨¡å‹çš„ç©©å¥æ€§ï¼ŒåŒ…æ‹¬å¯†åº¦æ¯”ç´„æŸã€å…ˆé©—æ•æ„Ÿåº¦ç­‰
    """
    
    def __init__(self, 
                 gamma_constraint: float = 2.0,
                 sensitivity_threshold: float = 0.3,
                 n_sensitivity_scenarios: int = 5):
        """
        åˆå§‹åŒ–ç©©å¥æ€§å ±å‘Šå™¨
        
        Parameters:
        -----------
        gamma_constraint : float
            å¯†åº¦æ¯”ç´„æŸåƒæ•¸
        sensitivity_threshold : float
            æ•æ„Ÿåº¦é–¾å€¼
        n_sensitivity_scenarios : int
            æ•æ„Ÿåº¦åˆ†æå ´æ™¯æ•¸
        """
        self.gamma_constraint = gamma_constraint
        self.sensitivity_threshold = sensitivity_threshold
        self.n_sensitivity_scenarios = n_sensitivity_scenarios
        
        # åˆå§‹åŒ–åˆ†æçµ„ä»¶
        self.density_ratio_class = DensityRatioClass(gamma_constraint)
        self.robust_framework = RobustBayesianFramework(gamma_constraint)
        
        # è¨­ç½®åœ–è¡¨æ¨£å¼
        plt.style.use('default')
        sns.set_style("whitegrid")
        self.colors = sns.color_palette("viridis", 8)
    
    def analyze_robustness(self, 
                          posterior_samples: Dict[str, np.ndarray],
                          observed_data: np.ndarray,
                          model_results: List[ModelComparisonResult]) -> RobustnessReport:
        """
        å…¨é¢çš„ç©©å¥æ€§åˆ†æ
        
        Parameters:
        -----------
        posterior_samples : Dict[str, np.ndarray]
            å¾Œé©—æ¨£æœ¬
        observed_data : np.ndarray
            è§€æ¸¬è³‡æ–™
        model_results : List[ModelComparisonResult]
            æ¨¡å‹æ¯”è¼ƒçµæœ
            
        Returns:
        --------
        RobustnessReport
            å®Œæ•´çš„ç©©å¥æ€§å ±å‘Š
        """
        
        print("ğŸ›¡ï¸ é–‹å§‹ç©©å¥æ€§åˆ†æ...")
        
        # 1. å¯†åº¦æ¯”åˆ†æ
        print("  ğŸ“Š å¯†åº¦æ¯”ç´„æŸåˆ†æ...")
        density_ratio_analysis = self._analyze_density_ratio_constraints(
            posterior_samples, model_results
        )
        
        # 2. å…ˆé©—æ•æ„Ÿåº¦åˆ†æ
        print("  ğŸ¯ å…ˆé©—æ•æ„Ÿåº¦åˆ†æ...")
        sensitivity_analyses = self._analyze_prior_sensitivity(
            posterior_samples, observed_data
        )
        
        # 3. ä¸ç¢ºå®šæ€§åˆ†è§£
        print("  ğŸ” ä¸ç¢ºå®šæ€§ä¾†æºåˆ†æ...")
        uncertainty_decomposition = self._decompose_uncertainty_sources(
            posterior_samples, sensitivity_analyses
        )
        
        # 4. æ•´é«”ç©©å¥æ€§è©•ä¼°
        overall_robustness = self._assess_overall_robustness(
            density_ratio_analysis, sensitivity_analyses
        )
        
        # 5. è¨ˆç®—ç©©å¥æ€§è©•åˆ†
        robustness_score = self._calculate_robustness_score(
            density_ratio_analysis, sensitivity_analyses, uncertainty_decomposition
        )
        
        # 6. ç”Ÿæˆå»ºè­°
        recommendations = self._generate_robustness_recommendations(
            density_ratio_analysis, sensitivity_analyses, overall_robustness
        )
        
        robustness_report = RobustnessReport(
            overall_robustness=overall_robustness,
            density_ratio_analysis=density_ratio_analysis,
            sensitivity_analyses=sensitivity_analyses,
            uncertainty_decomposition=uncertainty_decomposition,
            recommendations=recommendations,
            robustness_score=robustness_score
        )
        
        print("âœ… ç©©å¥æ€§åˆ†æå®Œæˆ")
        return robustness_report
    
    def _analyze_density_ratio_constraints(self, 
                                         posterior_samples: Dict[str, np.ndarray],
                                         model_results: List[ModelComparisonResult]) -> DensityRatioAnalysis:
        """åˆ†æå¯†åº¦æ¯”ç´„æŸ"""
        
        # è¨ˆç®—ç´„æŸé•å
        violation_counts = [result.density_ratio_violations for result in model_results]
        total_evaluations = len(model_results) * 1000  # å‡è¨­æ¯å€‹æ¨¡å‹è©•ä¼°1000å€‹é»
        violation_rate = sum(violation_counts) / total_evaluations if total_evaluations > 0 else 0
        
        # æ¨¡æ“¬å¯†åº¦æ¯”è¨ˆç®—
        density_ratios = []
        for param_name, samples in posterior_samples.items():
            if samples.ndim == 1:
                # ç°¡åŒ–çš„å¯†åº¦æ¯”è¨ˆç®—
                # ä½¿ç”¨æ¨£æœ¬çš„ç¶“é©—åˆ†å¸ƒèˆ‡æ¨™æº–æ­£æ…‹åˆ†å¸ƒæ¯”è¼ƒ
                
                # æ¨™æº–åŒ–æ¨£æœ¬
                standardized = (samples - np.mean(samples)) / np.std(samples)
                
                # è¨ˆç®—ç¶“é©—å¯†åº¦æ¯”
                for i in range(0, len(standardized), 100):  # æŠ½æ¨£è¨ˆç®—
                    x = standardized[i]
                    
                    # ç¶“é©—åˆ†å¸ƒå¯†åº¦ (ç°¡åŒ–)
                    empirical_density = np.exp(-0.5 * x**2) / np.sqrt(2 * np.pi)
                    
                    # åƒè€ƒåˆ†å¸ƒå¯†åº¦ (æ¨™æº–æ­£æ…‹)
                    reference_density = stats.norm.pdf(x)
                    
                    if reference_density > 1e-10:
                        ratio = empirical_density / reference_density
                        density_ratios.append(ratio)
        
        density_ratios = np.array(density_ratios)
        
        # åˆ†æçµæœ
        max_ratio = np.max(density_ratios) if len(density_ratios) > 0 else 0
        mean_ratio = np.mean(density_ratios) if len(density_ratios) > 0 else 0
        
        # è­˜åˆ¥é•åå€åŸŸ
        violation_regions = {}
        if len(density_ratios) > 0:
            violating_indices = density_ratios > self.gamma_constraint
            violation_regions = {
                'n_violations': np.sum(violating_indices),
                'violation_percentage': np.mean(violating_indices) * 100,
                'max_violation': np.max(density_ratios[violating_indices]) if np.any(violating_indices) else 0
            }
        
        constraint_satisfaction = violation_rate < 0.05  # 5% å®¹å¿åº¦
        
        return DensityRatioAnalysis(
            gamma_constraint=self.gamma_constraint,
            violation_rate=violation_rate,
            max_density_ratio=max_ratio,
            mean_density_ratio=mean_ratio,
            violation_regions=violation_regions,
            constraint_satisfaction=constraint_satisfaction
        )
    
    def _analyze_prior_sensitivity(self, 
                                 posterior_samples: Dict[str, np.ndarray],
                                 observed_data: np.ndarray) -> Dict[str, SensitivityAnalysis]:
        """åˆ†æå…ˆé©—æ•æ„Ÿåº¦"""
        
        sensitivity_analyses = {}
        
        # å®šç¾©ä¸åŒçš„å…ˆé©—å ´æ™¯
        prior_scenarios = [
            "informative_normal",
            "weak_normal", 
            "uniform",
            "gamma_shape1",
            "gamma_shape2"
        ]
        
        for param_name, baseline_samples in posterior_samples.items():
            if baseline_samples.ndim == 1:
                print(f"    ğŸ“ˆ åˆ†æåƒæ•¸ {param_name} çš„æ•æ„Ÿåº¦...")
                
                # ç‚ºä¸åŒå…ˆé©—å ´æ™¯æ¨¡æ“¬å¾Œé©—æ¨£æœ¬
                scenario_posteriors = {}
                scenario_posteriors['baseline'] = baseline_samples
                
                # æ¨¡æ“¬ä¸åŒå…ˆé©—ä¸‹çš„å¾Œé©— (ç°¡åŒ–)
                for scenario in prior_scenarios:
                    perturbed_samples = self._simulate_posterior_under_prior(
                        baseline_samples, scenario, observed_data
                    )
                    scenario_posteriors[scenario] = perturbed_samples
                
                # è¨ˆç®—æ•æ„Ÿåº¦æŒ‡æ¨™
                sensitivity_metrics = self._calculate_sensitivity_metrics(scenario_posteriors)
                
                # è©•ä¼°ç©©å¥æ€§æ°´å¹³
                robustness_level = self._assess_parameter_robustness(sensitivity_metrics)
                
                sensitivity_analyses[param_name] = SensitivityAnalysis(
                    parameter_name=param_name,
                    prior_scenarios=list(scenario_posteriors.keys()),
                    posterior_variation=sensitivity_metrics['posterior_variation'],
                    coefficient_of_variation=sensitivity_metrics['coefficient_of_variation'],
                    robustness_level=robustness_level,
                    sensitivity_metrics=sensitivity_metrics
                )
        
        return sensitivity_analyses
    
    def _simulate_posterior_under_prior(self, 
                                      baseline_samples: np.ndarray,
                                      prior_scenario: str,
                                      observed_data: np.ndarray) -> np.ndarray:
        """æ¨¡æ“¬ä¸åŒå…ˆé©—ä¸‹çš„å¾Œé©—åˆ†å¸ƒ"""
        
        n_samples = len(baseline_samples)
        baseline_mean = np.mean(baseline_samples)
        baseline_std = np.std(baseline_samples)
        
        # æ ¹æ“šå…ˆé©—å ´æ™¯èª¿æ•´
        if prior_scenario == "informative_normal":
            # å¼·ä¿¡æ¯å…ˆé©—
            noise_scale = 0.1
        elif prior_scenario == "weak_normal":
            # å¼±ä¿¡æ¯å…ˆé©—
            noise_scale = 0.5
        elif prior_scenario == "uniform":
            # å‡å‹»å…ˆé©—ï¼ˆæ›´å¤§çš„è®Šç•°ï¼‰
            noise_scale = 0.8
        elif prior_scenario == "gamma_shape1":
            # Gamma å…ˆé©—è®Šé«”1
            noise_scale = 0.3
            baseline_samples = np.abs(baseline_samples)  # ç¢ºä¿æ­£å€¼
        elif prior_scenario == "gamma_shape2":
            # Gamma å…ˆé©—è®Šé«”2
            noise_scale = 0.6
            baseline_samples = np.abs(baseline_samples)
        else:
            noise_scale = 0.2
        
        # æ·»åŠ æ“¾å‹•æ¨¡æ“¬å…ˆé©—å½±éŸ¿
        noise = np.random.normal(0, noise_scale * baseline_std, n_samples)
        perturbed_samples = baseline_samples + noise
        
        return perturbed_samples
    
    def _calculate_sensitivity_metrics(self, 
                                     scenario_posteriors: Dict[str, np.ndarray]) -> Dict[str, float]:
        """è¨ˆç®—æ•æ„Ÿåº¦æŒ‡æ¨™"""
        
        baseline = scenario_posteriors['baseline']
        baseline_mean = np.mean(baseline)
        
        # è¨ˆç®—å„å ´æ™¯çš„å¾Œé©—å‡å€¼
        scenario_means = {}
        for scenario, samples in scenario_posteriors.items():
            scenario_means[scenario] = np.mean(samples)
        
        # å¾Œé©—è®Šç•°åº¦
        mean_values = list(scenario_means.values())
        posterior_variation = np.std(mean_values) / np.abs(baseline_mean) if baseline_mean != 0 else np.std(mean_values)
        
        # è®Šç•°ä¿‚æ•¸
        coefficient_of_variation = np.std(mean_values) / np.mean(mean_values) if np.mean(mean_values) != 0 else 0
        
        # Jensen-Shannon æ•£åº¦
        js_divergences = []
        for scenario, samples in scenario_posteriors.items():
            if scenario != 'baseline':
                # è¨ˆç®—åˆ†å¸ƒé–“çš„JSæ•£åº¦
                hist_baseline, bins = np.histogram(baseline, bins=50, density=True)
                hist_scenario, _ = np.histogram(samples, bins=bins, density=True)
                
                # é¿å…é›¶å€¼
                hist_baseline = hist_baseline + 1e-10
                hist_scenario = hist_scenario + 1e-10
                
                js_div = jensenshannon(hist_baseline, hist_scenario)
                js_divergences.append(js_div)
        
        mean_js_divergence = np.mean(js_divergences) if js_divergences else 0
        
        # ç›¸å°è®Šç•°
        relative_changes = []
        for scenario, mean_val in scenario_means.items():
            if scenario != 'baseline':
                rel_change = abs(mean_val - baseline_mean) / abs(baseline_mean) if baseline_mean != 0 else abs(mean_val)
                relative_changes.append(rel_change)
        
        max_relative_change = np.max(relative_changes) if relative_changes else 0
        
        return {
            'posterior_variation': posterior_variation,
            'coefficient_of_variation': coefficient_of_variation,
            'mean_js_divergence': mean_js_divergence,
            'max_relative_change': max_relative_change,
            'scenario_means': scenario_means
        }
    
    def _assess_parameter_robustness(self, sensitivity_metrics: Dict[str, float]) -> RobustnessLevel:
        """è©•ä¼°åƒæ•¸ç©©å¥æ€§æ°´å¹³"""
        
        variation = sensitivity_metrics['max_relative_change']
        
        if variation < 0.05:
            return RobustnessLevel.HIGHLY_ROBUST
        elif variation < 0.15:
            return RobustnessLevel.ROBUST
        elif variation < 0.30:
            return RobustnessLevel.MODERATELY_ROBUST
        elif variation < 0.50:
            return RobustnessLevel.SENSITIVE
        else:
            return RobustnessLevel.HIGHLY_SENSITIVE
    
    def _decompose_uncertainty_sources(self, 
                                     posterior_samples: Dict[str, np.ndarray],
                                     sensitivity_analyses: Dict[str, SensitivityAnalysis]) -> Dict[str, float]:
        """åˆ†è§£ä¸ç¢ºå®šæ€§ä¾†æº"""
        
        # è¨ˆç®—ä¸åŒä¾†æºçš„ä¸ç¢ºå®šæ€§è²¢ç»
        total_uncertainty = 0
        prior_uncertainty = 0
        sampling_uncertainty = 0
        
        for param_name, samples in posterior_samples.items():
            if param_name in sensitivity_analyses:
                # ç¸½é«”ä¸ç¢ºå®šæ€§
                param_var = np.var(samples)
                total_uncertainty += param_var
                
                # å…ˆé©—ä¸ç¢ºå®šæ€§
                sensitivity = sensitivity_analyses[param_name]
                prior_var = sensitivity.posterior_variation ** 2
                prior_uncertainty += prior_var
                
                # æ¡æ¨£ä¸ç¢ºå®šæ€§ (ä¼°è¨ˆ)
                n_eff = len(samples) / (1 + 2 * np.sum(np.abs(np.correlate(samples - np.mean(samples), 
                                                                          samples - np.mean(samples), 'full'))))
                sampling_var = param_var / max(n_eff, 1)
                sampling_uncertainty += sampling_var
        
        # æ­¸ä¸€åŒ–
        if total_uncertainty > 0:
            prior_fraction = prior_uncertainty / total_uncertainty
            sampling_fraction = sampling_uncertainty / total_uncertainty
            model_fraction = max(0, 1 - prior_fraction - sampling_fraction)
        else:
            prior_fraction = sampling_fraction = model_fraction = 1/3
        
        return {
            'prior_uncertainty': prior_fraction,
            'sampling_uncertainty': sampling_fraction,
            'model_uncertainty': model_fraction,
            'total_uncertainty': total_uncertainty
        }
    
    def _assess_overall_robustness(self, 
                                 density_ratio_analysis: DensityRatioAnalysis,
                                 sensitivity_analyses: Dict[str, SensitivityAnalysis]) -> RobustnessLevel:
        """è©•ä¼°æ•´é«”ç©©å¥æ€§"""
        
        # å¯†åº¦æ¯”ç´„æŸæ»¿è¶³åº¦
        constraint_score = 1.0 if density_ratio_analysis.constraint_satisfaction else 0.5
        
        # åƒæ•¸æ•æ„Ÿåº¦å¹³å‡
        if sensitivity_analyses:
            robustness_levels = [analysis.robustness_level for analysis in sensitivity_analyses.values()]
            
            level_scores = {
                RobustnessLevel.HIGHLY_ROBUST: 1.0,
                RobustnessLevel.ROBUST: 0.8,
                RobustnessLevel.MODERATELY_ROBUST: 0.6,
                RobustnessLevel.SENSITIVE: 0.4,
                RobustnessLevel.HIGHLY_SENSITIVE: 0.2
            }
            
            sensitivity_score = np.mean([level_scores[level] for level in robustness_levels])
        else:
            sensitivity_score = 0.5
        
        # ç¶œåˆè©•åˆ†
        overall_score = 0.6 * constraint_score + 0.4 * sensitivity_score
        
        if overall_score >= 0.9:
            return RobustnessLevel.HIGHLY_ROBUST
        elif overall_score >= 0.7:
            return RobustnessLevel.ROBUST
        elif overall_score >= 0.5:
            return RobustnessLevel.MODERATELY_ROBUST
        elif overall_score >= 0.3:
            return RobustnessLevel.SENSITIVE
        else:
            return RobustnessLevel.HIGHLY_SENSITIVE
    
    def _calculate_robustness_score(self, 
                                  density_ratio_analysis: DensityRatioAnalysis,
                                  sensitivity_analyses: Dict[str, SensitivityAnalysis],
                                  uncertainty_decomposition: Dict[str, float]) -> float:
        """è¨ˆç®—æ•´é«”ç©©å¥æ€§è©•åˆ† (0-100)"""
        
        # å¯†åº¦æ¯”ç´„æŸè©•åˆ† (30%)
        constraint_score = 0 if not density_ratio_analysis.constraint_satisfaction else \
                          max(0, 1 - density_ratio_analysis.violation_rate / 0.1) * 30
        
        # æ•æ„Ÿåº¦è©•åˆ† (50%)
        if sensitivity_analyses:
            sensitivity_scores = []
            for analysis in sensitivity_analyses.values():
                if analysis.robustness_level == RobustnessLevel.HIGHLY_ROBUST:
                    sensitivity_scores.append(1.0)
                elif analysis.robustness_level == RobustnessLevel.ROBUST:
                    sensitivity_scores.append(0.8)
                elif analysis.robustness_level == RobustnessLevel.MODERATELY_ROBUST:
                    sensitivity_scores.append(0.6)
                elif analysis.robustness_level == RobustnessLevel.SENSITIVE:
                    sensitivity_scores.append(0.4)
                else:
                    sensitivity_scores.append(0.2)
            
            sensitivity_score = np.mean(sensitivity_scores) * 50
        else:
            sensitivity_score = 25
        
        # ä¸ç¢ºå®šæ€§çµæ§‹è©•åˆ† (20%)
        # åå¥½æ¨¡å‹ä¸ç¢ºå®šæ€§å ä¸»å°ï¼Œè€Œéå…ˆé©—ä¸ç¢ºå®šæ€§
        model_uncertainty = uncertainty_decomposition.get('model_uncertainty', 0.33)
        uncertainty_score = min(model_uncertainty * 2, 1.0) * 20
        
        total_score = constraint_score + sensitivity_score + uncertainty_score
        
        return min(100, max(0, total_score))
    
    def _generate_robustness_recommendations(self, 
                                           density_ratio_analysis: DensityRatioAnalysis,
                                           sensitivity_analyses: Dict[str, SensitivityAnalysis],
                                           overall_robustness: RobustnessLevel) -> List[str]:
        """ç”Ÿæˆç©©å¥æ€§æ”¹å–„å»ºè­°"""
        
        recommendations = []
        
        # æ•´é«”å»ºè­°
        if overall_robustness == RobustnessLevel.HIGHLY_SENSITIVE:
            recommendations.append("ğŸš¨ æ¨¡å‹ç©©å¥æ€§åš´é‡ä¸è¶³ï¼Œéœ€è¦é‡æ–°æª¢è¦–æ¨¡å‹è¨­å®š")
            recommendations.append("â€¢ è€ƒæ…®ä½¿ç”¨æ›´ä¿å®ˆçš„å…ˆé©—åˆ†å¸ƒ")
            recommendations.append("â€¢ å¢åŠ è³‡æ–™é‡ä»¥æ¸›å°‘å…ˆé©—ä¾è³´")
            recommendations.append("â€¢ æª¢æŸ¥æ¨¡å‹è¦æ ¼æ˜¯å¦é©ç•¶")
        
        elif overall_robustness == RobustnessLevel.SENSITIVE:
            recommendations.append("âš ï¸ æ¨¡å‹å°å…ˆé©—é¸æ“‡è¼ƒç‚ºæ•æ„Ÿ")
            recommendations.append("â€¢ é€²è¡Œæ›´å»£æ³›çš„æ•æ„Ÿåº¦åˆ†æ")
            recommendations.append("â€¢ è€ƒæ…®æ¨¡å‹å¹³å‡æ–¹æ³•")
        
        elif overall_robustness == RobustnessLevel.MODERATELY_ROBUST:
            recommendations.append("âš¡ ç©©å¥æ€§ä¸­ç­‰ï¼Œå¯é©åº¦æ”¹å–„")
            recommendations.append("â€¢ ç›£æ§é—œéµåƒæ•¸çš„æ•æ„Ÿåº¦")
        
        else:
            recommendations.append("âœ… æ¨¡å‹ç©©å¥æ€§è‰¯å¥½")
        
        # å¯†åº¦æ¯”ç´„æŸå»ºè­°
        if not density_ratio_analysis.constraint_satisfaction:
            recommendations.append(f"ğŸ“Š å¯†åº¦æ¯”ç´„æŸé•åç‡ {density_ratio_analysis.violation_rate:.1%}")
            recommendations.append("â€¢ è€ƒæ…®èª¿æ•´ Î³ ç´„æŸåƒæ•¸")
            recommendations.append("â€¢ æª¢æŸ¥æ¥µç«¯å€¼è™•ç†")
        
        # æ•æ„Ÿåƒæ•¸å»ºè­°
        sensitive_params = [name for name, analysis in sensitivity_analyses.items() 
                           if analysis.robustness_level in [RobustnessLevel.SENSITIVE, RobustnessLevel.HIGHLY_SENSITIVE]]
        
        if sensitive_params:
            recommendations.append(f"ğŸ¯ æ•æ„Ÿåƒæ•¸: {', '.join(sensitive_params[:3])}")
            recommendations.append("â€¢ å°æ•æ„Ÿåƒæ•¸ä½¿ç”¨æ›´ç©©å¥çš„å…ˆé©—")
            recommendations.append("â€¢ å¢åŠ ç›¸é—œè³‡æ–™ä»¥æé«˜æ¨æ–·ç©©å®šæ€§")
        
        return recommendations
    
    def plot_robustness_analysis(self, 
                               robustness_report: RobustnessReport,
                               save_path: Optional[str] = None,
                               figsize: Tuple[int, int] = (15, 10)) -> plt.Figure:
        """ç¹ªè£½ç©©å¥æ€§åˆ†æåœ–è¡¨"""
        
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        fig.suptitle('ç©©å¥æ€§åˆ†æå ±å‘Š', fontsize=16, fontweight='bold')
        
        # 1. å¯†åº¦æ¯”åˆ†å¸ƒ (å·¦ä¸Š)
        ax1 = axes[0, 0]
        density_analysis = robustness_report.density_ratio_analysis
        
        # æ¨¡æ“¬å¯†åº¦æ¯”åˆ†å¸ƒç”¨æ–¼è¦–è¦ºåŒ–
        ratios = np.random.exponential(density_analysis.mean_density_ratio, 1000)
        ax1.hist(ratios, bins=50, alpha=0.7, color=self.colors[0], density=True)
        ax1.axvline(density_analysis.gamma_constraint, color='red', linestyle='--', 
                   label=f'Î³ constraint = {density_analysis.gamma_constraint}')
        ax1.axvline(density_analysis.mean_density_ratio, color='orange', linestyle='-', 
                   label=f'Mean = {density_analysis.mean_density_ratio:.2f}')
        
        ax1.set_xlabel('å¯†åº¦æ¯” dP/dPâ‚€')
        ax1.set_ylabel('å¯†åº¦')
        ax1.set_title('å¯†åº¦æ¯”åˆ†å¸ƒ')
        ax1.legend()
        
        # 2. åƒæ•¸æ•æ„Ÿåº¦ (å³ä¸Š)
        ax2 = axes[0, 1]
        
        if robustness_report.sensitivity_analyses:
            param_names = list(robustness_report.sensitivity_analyses.keys())
            sensitivities = [analysis.posterior_variation 
                           for analysis in robustness_report.sensitivity_analyses.values()]
            
            bars = ax2.bar(range(len(param_names)), sensitivities, 
                          color=self.colors[1], alpha=0.7)
            ax2.set_xlabel('åƒæ•¸')
            ax2.set_ylabel('å¾Œé©—è®Šç•°åº¦')
            ax2.set_title('åƒæ•¸æ•æ„Ÿåº¦åˆ†æ')
            ax2.set_xticks(range(len(param_names)))
            ax2.set_xticklabels(param_names, rotation=45)
            
            # æ·»åŠ æ•æ„Ÿåº¦é–¾å€¼ç·š
            ax2.axhline(self.sensitivity_threshold, color='red', linestyle='--', 
                       label=f'æ•æ„Ÿåº¦é–¾å€¼ = {self.sensitivity_threshold}')
            ax2.legend()
        
        # 3. ä¸ç¢ºå®šæ€§åˆ†è§£ (å·¦ä¸‹)
        ax3 = axes[1, 0]
        
        uncertainty = robustness_report.uncertainty_decomposition
        labels = ['å…ˆé©—', 'æ¡æ¨£', 'æ¨¡å‹']
        sizes = [uncertainty['prior_uncertainty'], 
                uncertainty['sampling_uncertainty'],
                uncertainty['model_uncertainty']]
        colors_pie = self.colors[2:5]
        
        wedges, texts, autotexts = ax3.pie(sizes, labels=labels, autopct='%1.1f%%', 
                                          colors=colors_pie, startangle=90)
        ax3.set_title('ä¸ç¢ºå®šæ€§ä¾†æºåˆ†è§£')
        
        # 4. ç©©å¥æ€§è©•åˆ† (å³ä¸‹)
        ax4 = axes[1, 1]
        
        score = robustness_report.robustness_score
        
        # å‰µå»ºè©•åˆ†è¡¨ç›¤
        theta = np.linspace(0, np.pi, 100)
        r = 1
        
        # èƒŒæ™¯åŠåœ“
        ax4.fill_between(theta, 0, r, alpha=0.3, color='lightgray')
        
        # è©•åˆ†å€åŸŸ
        if score >= 80:
            color = 'green'
        elif score >= 60:
            color = 'yellow'
        elif score >= 40:
            color = 'orange'
        else:
            color = 'red'
        
        score_theta = np.linspace(0, np.pi * score / 100, 50)
        ax4.fill_between(score_theta, 0, r, alpha=0.7, color=color)
        
        # æŒ‡é‡
        pointer_angle = np.pi * (1 - score / 100)
        ax4.arrow(0, 0, 0.8 * np.cos(pointer_angle), 0.8 * np.sin(pointer_angle),
                 head_width=0.1, head_length=0.1, fc='black', ec='black')
        
        ax4.set_xlim(-1.2, 1.2)
        ax4.set_ylim(-0.2, 1.2)
        ax4.set_aspect('equal')
        ax4.axis('off')
        ax4.text(0, -0.1, f'ç©©å¥æ€§è©•åˆ†: {score:.1f}', ha='center', fontsize=12, fontweight='bold')
        ax4.set_title('æ•´é«”ç©©å¥æ€§è©•åˆ†')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        return fig
    
    def generate_robustness_report(self, 
                                 robustness_report: RobustnessReport,
                                 include_details: bool = True) -> str:
        """ç”Ÿæˆç©©å¥æ€§åˆ†æå ±å‘Š"""
        
        report = []
        report.append("=" * 80)
        report.append("                    ç©©å¥æ€§åˆ†æå ±å‘Š")
        report.append("=" * 80)
        report.append("")
        
        # æ•´é«”æ‘˜è¦
        report.append("ğŸ›¡ï¸ æ•´é«”æ‘˜è¦")
        report.append("-" * 40)
        
        robustness_icons = {
            RobustnessLevel.HIGHLY_ROBUST: "ğŸŸ¢",
            RobustnessLevel.ROBUST: "ğŸŸ¡",
            RobustnessLevel.MODERATELY_ROBUST: "ğŸŸ ",
            RobustnessLevel.SENSITIVE: "ğŸ”´",
            RobustnessLevel.HIGHLY_SENSITIVE: "âŒ"
        }
        
        icon = robustness_icons[robustness_report.overall_robustness]
        report.append(f"{icon} æ•´é«”ç©©å¥æ€§: {robustness_report.overall_robustness.value.upper()}")
        report.append(f"ğŸ“Š ç©©å¥æ€§è©•åˆ†: {robustness_report.robustness_score:.1f}/100")
        report.append("")
        
        # å¯†åº¦æ¯”åˆ†æ
        report.append("ğŸ“Š å¯†åº¦æ¯”ç´„æŸåˆ†æ")
        report.append("-" * 40)
        
        density = robustness_report.density_ratio_analysis
        status = "âœ…" if density.constraint_satisfaction else "âŒ"
        report.append(f"{status} ç´„æŸæ»¿è¶³: {density.constraint_satisfaction}")
        report.append(f"ğŸ“ˆ é•åç‡: {density.violation_rate:.2%}")
        report.append(f"ğŸ“Š æœ€å¤§å¯†åº¦æ¯”: {density.max_density_ratio:.3f}")
        report.append(f"ğŸ“Š å¹³å‡å¯†åº¦æ¯”: {density.mean_density_ratio:.3f}")
        report.append("")
        
        # æ•æ„Ÿåº¦åˆ†æ
        if include_details and robustness_report.sensitivity_analyses:
            report.append("ğŸ¯ åƒæ•¸æ•æ„Ÿåº¦åˆ†æ")
            report.append("-" * 40)
            
            for param_name, analysis in robustness_report.sensitivity_analyses.items():
                icon = robustness_icons[analysis.robustness_level]
                report.append(f"{icon} {param_name}:")
                report.append(f"    ç©©å¥æ€§æ°´å¹³: {analysis.robustness_level.value}")
                report.append(f"    å¾Œé©—è®Šç•°: {analysis.posterior_variation:.3f}")
                report.append(f"    è®Šç•°ä¿‚æ•¸: {analysis.coefficient_of_variation:.3f}")
                report.append("")
        
        # ä¸ç¢ºå®šæ€§åˆ†è§£
        report.append("ğŸ” ä¸ç¢ºå®šæ€§ä¾†æºåˆ†è§£")
        report.append("-" * 40)
        
        uncertainty = robustness_report.uncertainty_decomposition
        report.append(f"ğŸ“Š å…ˆé©—ä¸ç¢ºå®šæ€§: {uncertainty['prior_uncertainty']:.1%}")
        report.append(f"ğŸ“Š æ¡æ¨£ä¸ç¢ºå®šæ€§: {uncertainty['sampling_uncertainty']:.1%}")
        report.append(f"ğŸ“Š æ¨¡å‹ä¸ç¢ºå®šæ€§: {uncertainty['model_uncertainty']:.1%}")
        report.append("")
        
        # å»ºè­°
        report.append("ğŸ’¡ æ”¹å–„å»ºè­°")
        report.append("-" * 40)
        for recommendation in robustness_report.recommendations:
            report.append(recommendation)
        
        return "\n".join(report)
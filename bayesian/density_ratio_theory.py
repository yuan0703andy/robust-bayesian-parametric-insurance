"""
Density Ratio Framework
å¯†åº¦æ¯”æ¡†æ¶

Implements the core density ratio class framework from robust Bayesian theory:
Î“ = {P : dP/dPâ‚€ â‰¤ Î³(x)}

This module provides the mathematical foundation for robust Bayesian analysis,
focusing specifically on density ratio constraints and model selection.

Key Components:
1. Density Ratio Class Implementation
2. Multiple Prior Scenario Testing
3. Multiple Likelihood Function Comparison
4. Automatic Model Selection via AIC/BIC
5. Robustness Evaluation

Note: This is the theoretical foundation - use RobustBayesianAnalyzer for practical analysis.
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Callable, Tuple
from enum import Enum
from dataclasses import dataclass
import warnings
from scipy import stats
from scipy.optimize import minimize
import logging

class ModelSelectionCriterion(Enum):
    """æ¨¡å‹é¸æ“‡æ¨™æº–"""
    AIC = "aic"
    BIC = "bic"
    WAIC = "waic"
    LOO_CV = "loo_cv"

@dataclass
class ModelConfiguration:
    """æ¨¡å‹é…ç½®"""
    name: str
    prior_params: Dict[str, Any]
    likelihood_family: str
    density_ratio_constraint: float = 2.0

@dataclass
class ModelComparisonResult:
    """æ¨¡å‹æ¯”è¼ƒçµæœ"""
    model_name: str
    log_likelihood: float
    aic: float
    bic: float
    waic: Optional[float] = None
    loo_cv: Optional[float] = None
    posterior_samples: Optional[np.ndarray] = None
    density_ratio_violations: int = 0
    
    @property
    def selection_score(self) -> float:
        """è¿”å›ä¸»è¦é¸æ“‡åˆ†æ•¸ (AIC)"""
        return self.aic

class DensityRatioClass:
    """
    å¯†åº¦æ¯”é¡åˆ¥å¯¦ç¾
    
    Implementation of Î“ = {P : dP/dPâ‚€ â‰¤ Î³(x)} where:
    - Pâ‚€ is the reference prior distribution
    - Î³(x) is the density ratio constraint function
    - Î“ is the class of admissible priors
    """
    
    def __init__(self, 
                 gamma_constraint: float = 2.0,
                 reference_prior: Optional[Callable] = None,
                 constraint_function: Optional[Callable] = None):
        """
        åˆå§‹åŒ–å¯†åº¦æ¯”é¡åˆ¥
        
        Parameters:
        -----------
        gamma_constraint : float
            å¯†åº¦æ¯”ä¸Šç•Œ Î³
        reference_prior : Callable, optional
            åƒè€ƒå…ˆé©—åˆ†å¸ƒ Pâ‚€
        constraint_function : Callable, optional
            ç´„æŸå‡½æ•¸ Î³(x)
        """
        self.gamma_constraint = gamma_constraint
        self.reference_prior = reference_prior or self._default_reference_prior
        self.constraint_function = constraint_function or self._default_constraint_function
        
        # è¨˜éŒ„é•åç´„æŸçš„æ¬¡æ•¸
        self.constraint_violations = 0
        
    def _default_reference_prior(self, x: np.ndarray) -> np.ndarray:
        """é è¨­åƒè€ƒå…ˆé©— (æ¨™æº–æ­£æ…‹)"""
        return stats.norm.pdf(x, loc=0, scale=1)
        
    def _default_constraint_function(self, x: np.ndarray) -> np.ndarray:
        """é è¨­ç´„æŸå‡½æ•¸ (å¸¸æ•¸)"""
        return np.full_like(x, self.gamma_constraint)
    
    def evaluate_density_ratio(self, 
                             candidate_prior: Callable,
                             evaluation_points: np.ndarray) -> np.ndarray:
        """
        è©•ä¼°å¯†åº¦æ¯” dP/dPâ‚€
        
        Parameters:
        -----------
        candidate_prior : Callable
            å€™é¸å…ˆé©—åˆ†å¸ƒ P
        evaluation_points : np.ndarray
            è©•ä¼°é»
            
        Returns:
        --------
        np.ndarray
            å¯†åº¦æ¯”å€¼
        """
        p_values = candidate_prior(evaluation_points)
        p0_values = self.reference_prior(evaluation_points)
        
        # é¿å…é™¤ä»¥é›¶
        p0_values = np.maximum(p0_values, 1e-10)
        density_ratios = p_values / p0_values
        
        return density_ratios
    
    def check_constraint_satisfaction(self,
                                    candidate_prior: Callable,
                                    evaluation_points: np.ndarray) -> Tuple[bool, np.ndarray]:
        """
        æª¢æŸ¥å¯†åº¦æ¯”ç´„æŸæ˜¯å¦æ»¿è¶³
        
        Returns:
        --------
        Tuple[bool, np.ndarray]
            (æ˜¯å¦æ»¿è¶³ç´„æŸ, é•åé»çš„ç´¢å¼•)
        """
        density_ratios = self.evaluate_density_ratio(candidate_prior, evaluation_points)
        constraint_values = self.constraint_function(evaluation_points)
        
        violations = density_ratios > constraint_values
        violation_indices = np.where(violations)[0]
        
        self.constraint_violations = len(violation_indices)
        
        is_satisfied = len(violation_indices) == 0
        
        return is_satisfied, violation_indices
    
    def generate_constrained_prior_ensemble(self,
                                          base_parameters: Dict[str, Any],
                                          n_priors: int = 10,
                                          perturbation_scale: float = 0.1) -> List[Callable]:
        """
        ç”Ÿæˆæ»¿è¶³å¯†åº¦æ¯”ç´„æŸçš„å…ˆé©—é›†åˆ
        
        Parameters:
        -----------
        base_parameters : Dict[str, Any]
            åŸºç¤åƒæ•¸
        n_priors : int
            ç”Ÿæˆçš„å…ˆé©—æ•¸é‡
        perturbation_scale : float
            æ“¾å‹•è¦æ¨¡
            
        Returns:
        --------
        List[Callable]
            æ»¿è¶³ç´„æŸçš„å…ˆé©—åˆ†å¸ƒåˆ—è¡¨
        """
        valid_priors = []
        attempts = 0
        max_attempts = n_priors * 10
        
        evaluation_grid = np.linspace(-5, 5, 100)
        
        while len(valid_priors) < n_priors and attempts < max_attempts:
            attempts += 1
            
            # æ“¾å‹•åƒæ•¸
            perturbed_params = {}
            for key, value in base_parameters.items():
                if isinstance(value, (int, float)):
                    noise = np.random.normal(0, perturbation_scale * abs(value))
                    perturbed_params[key] = value + noise
                else:
                    perturbed_params[key] = value
            
            # å‰µå»ºå€™é¸å…ˆé©—
            def candidate_prior(x, params=perturbed_params):
                return stats.norm.pdf(x, 
                                    loc=params.get('loc', 0),
                                    scale=params.get('scale', 1))
            
            # æª¢æŸ¥ç´„æŸ
            is_valid, _ = self.check_constraint_satisfaction(candidate_prior, evaluation_grid)
            
            if is_valid:
                valid_priors.append(candidate_prior)
        
        if len(valid_priors) < n_priors:
            warnings.warn(f"åªç”Ÿæˆäº† {len(valid_priors)} å€‹æ»¿è¶³ç´„æŸçš„å…ˆé©—ï¼Œç›®æ¨™æ˜¯ {n_priors} å€‹")
        
        return valid_priors

class RobustBayesianFramework:
    """
    ç©©å¥è²æ°æ¡†æ¶ä¸»é¡åˆ¥
    
    Implements comprehensive robust Bayesian analysis with:
    1. Multiple prior scenario testing
    2. Density ratio constraint checking
    3. Model comparison and selection
    4. Robustness evaluation
    """
    
    def __init__(self,
                 density_ratio_constraint: float = 2.0,
                 model_selection_criterion: ModelSelectionCriterion = ModelSelectionCriterion.AIC):
        """
        åˆå§‹åŒ–ç©©å¥è²æ°æ¡†æ¶
        
        Parameters:
        -----------
        density_ratio_constraint : float
            å¯†åº¦æ¯”ç´„æŸä¸Šç•Œ
        model_selection_criterion : ModelSelectionCriterion
            æ¨¡å‹é¸æ“‡æ¨™æº–
        """
        self.density_ratio_class = DensityRatioClass(gamma_constraint=density_ratio_constraint)
        self.selection_criterion = model_selection_criterion
        
        # æ¨¡å‹é…ç½®åº«
        self.model_configurations = self._initialize_model_configurations()
        
        # åˆ†æçµæœ
        self.comparison_results: List[ModelComparisonResult] = []
        self.best_model: Optional[ModelComparisonResult] = None
        
    def _initialize_model_configurations(self) -> List[ModelConfiguration]:
        """åˆå§‹åŒ–æ¨¡å‹é…ç½®åº«"""
        configurations = [
            ModelConfiguration(
                name="normal_informative",
                prior_params={"loc": 0, "scale": 0.5},
                likelihood_family="normal",
                density_ratio_constraint=self.density_ratio_class.gamma_constraint
            ),
            ModelConfiguration(
                name="normal_weakly_informative", 
                prior_params={"loc": 0, "scale": 1.0},
                likelihood_family="normal",
                density_ratio_constraint=self.density_ratio_class.gamma_constraint
            ),
            ModelConfiguration(
                name="normal_vague",
                prior_params={"loc": 0, "scale": 2.0},
                likelihood_family="normal",
                density_ratio_constraint=self.density_ratio_class.gamma_constraint
            ),
            ModelConfiguration(
                name="student_t_robust",
                prior_params={"df": 3, "loc": 0, "scale": 1.0},
                likelihood_family="t",
                density_ratio_constraint=self.density_ratio_class.gamma_constraint
            ),
            ModelConfiguration(
                name="laplace_sparse",
                prior_params={"loc": 0, "scale": 1.0},
                likelihood_family="laplace",
                density_ratio_constraint=self.density_ratio_class.gamma_constraint
            ),
            ModelConfiguration(
                name="gamma_positive_constraint",
                prior_params={"a": 2, "scale": 1},
                likelihood_family="gamma",
                density_ratio_constraint=self.density_ratio_class.gamma_constraint
            )
        ]
        
        return configurations
    
    def add_custom_model_configuration(self, config: ModelConfiguration):
        """æ·»åŠ è‡ªå®šç¾©æ¨¡å‹é…ç½®"""
        self.model_configurations.append(config)
    
    def fit_single_model(self, 
                        data: np.ndarray,
                        config: ModelConfiguration) -> ModelComparisonResult:
        """
        æ“¬åˆå–®ä¸€æ¨¡å‹ä¸¦è©•ä¼°
        
        Parameters:
        -----------
        data : np.ndarray
            è§€æ¸¬è³‡æ–™
        config : ModelConfiguration
            æ¨¡å‹é…ç½®
            
        Returns:
        --------
        ModelComparisonResult
            æ¨¡å‹æ¯”è¼ƒçµæœ
        """
        try:
            # è¨ˆç®—å°æ•¸ä¼¼ç„¶
            log_likelihood = self._calculate_log_likelihood(data, config)
            
            # è¨ˆç®—è³‡è¨Šæ¨™æº–
            n_params = len(config.prior_params)
            n_data = len(data)
            
            aic = -2 * log_likelihood + 2 * n_params
            bic = -2 * log_likelihood + n_params * np.log(n_data)
            
            # æª¢æŸ¥å¯†åº¦æ¯”ç´„æŸé•å
            evaluation_points = np.linspace(np.min(data), np.max(data), 100)
            prior_func = self._create_prior_function(config)
            _, violation_indices = self.density_ratio_class.check_constraint_satisfaction(
                prior_func, evaluation_points
            )
            
            result = ModelComparisonResult(
                model_name=config.name,
                log_likelihood=log_likelihood,
                aic=aic,
                bic=bic,
                density_ratio_violations=len(violation_indices)
            )
            
            return result
            
        except Exception as e:
            warnings.warn(f"æ¨¡å‹ {config.name} æ“¬åˆå¤±æ•—: {e}")
            return ModelComparisonResult(
                model_name=config.name,
                log_likelihood=-np.inf,
                aic=np.inf,
                bic=np.inf,
                density_ratio_violations=np.inf
            )
    
    def _calculate_log_likelihood(self, 
                                data: np.ndarray, 
                                config: ModelConfiguration) -> float:
        """è¨ˆç®—å°æ•¸ä¼¼ç„¶"""
        if config.likelihood_family == "normal":
            # æœ€å¤§ä¼¼ç„¶ä¼°è¨ˆ
            mu_hat = np.mean(data)
            sigma_hat = np.std(data, ddof=1)
            return np.sum(stats.norm.logpdf(data, loc=mu_hat, scale=sigma_hat))
            
        elif config.likelihood_family == "t":
            # Student-t åˆ†å¸ƒ
            df = config.prior_params.get("df", 3)
            params = stats.t.fit(data, fdf=df)
            return np.sum(stats.t.logpdf(data, *params))
            
        elif config.likelihood_family == "laplace":
            # Laplace åˆ†å¸ƒ
            params = stats.laplace.fit(data)
            return np.sum(stats.laplace.logpdf(data, *params))
            
        elif config.likelihood_family == "gamma":
            # Gamma åˆ†å¸ƒ (åƒ…é™æ­£å€¼è³‡æ–™)
            if np.any(data <= 0):
                return -np.inf
            params = stats.gamma.fit(data)
            return np.sum(stats.gamma.logpdf(data, *params))
            
        else:
            raise ValueError(f"æœªæ”¯æ´çš„ä¼¼ç„¶å®¶æ—: {config.likelihood_family}")
    
    def _create_prior_function(self, config: ModelConfiguration) -> Callable:
        """æ ¹æ“šé…ç½®å‰µå»ºå…ˆé©—å‡½æ•¸"""
        if config.likelihood_family == "normal":
            loc = config.prior_params.get("loc", 0)
            scale = config.prior_params.get("scale", 1)
            return lambda x: stats.norm.pdf(x, loc=loc, scale=scale)
            
        elif config.likelihood_family == "t":
            df = config.prior_params.get("df", 3)
            loc = config.prior_params.get("loc", 0)
            scale = config.prior_params.get("scale", 1)
            return lambda x: stats.t.pdf(x, df=df, loc=loc, scale=scale)
            
        elif config.likelihood_family == "laplace":
            loc = config.prior_params.get("loc", 0)
            scale = config.prior_params.get("scale", 1)
            return lambda x: stats.laplace.pdf(x, loc=loc, scale=scale)
            
        elif config.likelihood_family == "gamma":
            a = config.prior_params.get("a", 2)
            scale = config.prior_params.get("scale", 1)
            return lambda x: stats.gamma.pdf(x, a=a, scale=scale)
            
        else:
            raise ValueError(f"æœªæ”¯æ´çš„ä¼¼ç„¶å®¶æ—: {config.likelihood_family}")
    
    def compare_all_models(self, data: np.ndarray) -> List[ModelComparisonResult]:
        """
        æ¯”è¼ƒæ‰€æœ‰æ¨¡å‹é…ç½®
        
        Parameters:
        -----------
        data : np.ndarray
            è§€æ¸¬è³‡æ–™
            
        Returns:
        --------
        List[ModelComparisonResult]
            æ‰€æœ‰æ¨¡å‹çš„æ¯”è¼ƒçµæœ
        """
        print(f"ğŸ”„ é–‹å§‹ç©©å¥è²æ°æ¨¡å‹æ¯”è¼ƒï¼Œå…± {len(self.model_configurations)} å€‹æ¨¡å‹...")
        
        self.comparison_results = []
        
        for config in self.model_configurations:
            print(f"  ğŸ“Š æ“¬åˆæ¨¡å‹: {config.name}")
            result = self.fit_single_model(data, config)
            self.comparison_results.append(result)
        
        # æ ¹æ“šé¸æ“‡æ¨™æº–æ’åº
        if self.selection_criterion == ModelSelectionCriterion.AIC:
            self.comparison_results.sort(key=lambda x: x.aic)
        elif self.selection_criterion == ModelSelectionCriterion.BIC:
            self.comparison_results.sort(key=lambda x: x.bic)
        
        self.best_model = self.comparison_results[0] if self.comparison_results else None
        
        print(f"âœ… æ¨¡å‹æ¯”è¼ƒå®Œæˆï¼Œæœ€ä½³æ¨¡å‹: {self.best_model.model_name if self.best_model else 'None'}")
        
        return self.comparison_results
    
    def evaluate_robustness(self, data: np.ndarray) -> Dict[str, Any]:
        """
        è©•ä¼°æ¨¡å‹çš„ç©©å¥æ€§
        
        Returns:
        --------
        Dict[str, Any]
            ç©©å¥æ€§è©•ä¼°çµæœ
        """
        if not self.comparison_results:
            self.compare_all_models(data)
        
        # è¨ˆç®—æ¨¡å‹æ¬Šé‡ (åŸºæ–¼ AIC æ¬Šé‡)
        aic_values = np.array([r.aic for r in self.comparison_results])
        aic_min = np.min(aic_values)
        delta_aic = aic_values - aic_min
        weights = np.exp(-0.5 * delta_aic)
        weights = weights / np.sum(weights)
        
        # è©•ä¼°å¯†åº¦æ¯”ç´„æŸé•åç¨‹åº¦
        violation_counts = [r.density_ratio_violations for r in self.comparison_results]
        total_violations = sum(violation_counts)
        
        # è¨ˆç®—æ¨¡å‹ä¸ç¢ºå®šæ€§
        top_models = [r for r in self.comparison_results if r.aic - aic_min < 2]
        model_uncertainty = len(top_models) / len(self.comparison_results)
        
        robustness_results = {
            "best_model": self.best_model.model_name if self.best_model else None,
            "model_weights": {
                r.model_name: w for r, w in zip(self.comparison_results, weights)
            },
            "total_density_ratio_violations": total_violations,
            "model_uncertainty_ratio": model_uncertainty,
            "top_models": [r.model_name for r in top_models],
            "worst_aic": np.max(aic_values) if len(aic_values) > 0 else np.inf,
            "best_aic": aic_min if len(aic_values) > 0 else np.inf,
            "aic_range": np.max(aic_values) - aic_min if len(aic_values) > 0 else 0
        }
        
        return robustness_results
    
    def get_model_comparison_summary(self) -> pd.DataFrame:
        """ç²å–æ¨¡å‹æ¯”è¼ƒæ‘˜è¦è¡¨"""
        if not self.comparison_results:
            return pd.DataFrame()
        
        summary_data = []
        for result in self.comparison_results:
            summary_data.append({
                "Model": result.model_name,
                "Log_Likelihood": result.log_likelihood,
                "AIC": result.aic,
                "BIC": result.bic,
                "Density_Ratio_Violations": result.density_ratio_violations
            })
        
        return pd.DataFrame(summary_data)
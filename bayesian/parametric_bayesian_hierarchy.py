#!/usr/bin/env python3
"""
Parametric Hierarchical Bayesian Model Module
åƒæ•¸åŒ–éšå±¤è²æ°æ¨¡å‹æ¨¡çµ„

å¯å‹•æ…‹é…ç½®çš„éšå±¤è²æ°æ¨¡å‹ï¼Œæ”¯æ´ä¸åŒçš„æ¦‚ä¼¼å‡½æ•¸å’Œäº‹å‰åˆ†ä½ˆçµ„åˆã€‚

æ ¸å¿ƒåŠŸèƒ½:
- æ”¯æ´å¤šç¨®æ¦‚ä¼¼å‡½æ•¸: Normal, LogNormal, Student-t, Laplace
- æ”¯æ´å¤šç¨®äº‹å‰æƒ…å¢ƒ: non_informative, weak_informative, optimistic, pessimistic
- å‹•æ…‹æ¨¡å‹æ§‹å»º
- ç¨ç«‹çš„æ¨¡å‹é…ç½®å’Œçµæœé¡å‹

ä½¿ç”¨ç¯„ä¾‹:
```python
from bayesian.parametric_bayesian_hierarchy import (
    ParametricHierarchicalModel, ModelSpec, PriorScenario
)

# å‰µå»ºæ¨¡å‹è¦æ ¼
model_spec = ModelSpec(
    likelihood_family='lognormal',
    prior_scenario='pessimistic'
)

# åˆå§‹åŒ–æ¨¡å‹
model = ParametricHierarchicalModel(model_spec)

# æ“¬åˆæ•¸æ“š
result = model.fit(observations)

# æŸ¥çœ‹çµæœ
print("å¾Œé©—æ‘˜è¦:", result.posterior_summary)
print("æ¨¡å‹è¨ºæ–·:", result.diagnostics.convergence_summary())
```

Author: Research Team  
Date: 2025-01-12
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
import warnings
import os

# ç’°å¢ƒé…ç½®
for key in ['PYTENSOR_FLAGS', 'THEANO_FLAGS']:
    if key in os.environ:
        del os.environ[key]

os.environ['PYTENSOR_FLAGS'] = 'device=cpu,floatX=float64,mode=FAST_COMPILE,linker=py'
os.environ['MKL_THREADING_LAYER'] = 'GNU'

# PyMC imports
try:
    import pymc as pm
    import pytensor.tensor as pt
    import arviz as az
    HAS_PYMC = True
    print(f"âœ… PyMC ç‰ˆæœ¬: {pm.__version__}")
except ImportError as e:
    HAS_PYMC = False
    warnings.warn(f"PyMC not available: {e}")

# å°å…¥ç¨ç«‹çš„MPEæ¨¡çµ„
try:
    from .posterior_mixture_approximation import MixedPredictiveEstimation, MPEResult
    HAS_MPE = True
except ImportError:
    HAS_MPE = False
    warnings.warn("æ··åˆé æ¸¬ä¼°è¨ˆæ¨¡çµ„ä¸å¯ç”¨")

# å°å…¥ç©ºé–“æ•ˆæ‡‰æ¨¡çµ„
try:
    from .spatial_effects import SpatialEffectsAnalyzer, SpatialConfig, CovarianceFunction
    HAS_SPATIAL = True
except ImportError:
    HAS_SPATIAL = False
    warnings.warn("ç©ºé–“æ•ˆæ‡‰æ¨¡çµ„ä¸å¯ç”¨")

# æ–°å¢ï¼šè„†å¼±åº¦å»ºæ¨¡æ•¸æ“šçµæ§‹
@dataclass
class VulnerabilityData:
    """è„†å¼±åº¦å»ºæ¨¡æ•¸æ“š"""
    hazard_intensities: np.ndarray      # H_ij - ç½å®³å¼·åº¦ï¼ˆå¦‚é¢¨é€Ÿ m/sï¼‰
    exposure_values: np.ndarray         # E_i - æš´éšªå€¼ï¼ˆå¦‚å»ºç¯‰ç‰©åƒ¹å€¼ USDï¼‰
    observed_losses: np.ndarray         # L_ij - è§€æ¸¬æå¤± (USD)
    event_ids: Optional[np.ndarray] = None      # äº‹ä»¶ID
    location_ids: Optional[np.ndarray] = None   # åœ°é»ID
    
    # æ–°å¢ï¼šç©ºé–“ä¿¡æ¯
    hospital_coordinates: Optional[np.ndarray] = None    # é†«é™¢åº§æ¨™ [(lat1, lon1), ...]
    hospital_names: Optional[List[str]] = None           # é†«é™¢åç¨±
    region_assignments: Optional[np.ndarray] = None      # å€åŸŸåˆ†é… [0, 1, 2, ...]
    
    def __post_init__(self):
        """é©—è­‰æ•¸æ“šä¸€è‡´æ€§"""
        arrays = [self.hazard_intensities, self.exposure_values, self.observed_losses]
        lengths = [len(arr) for arr in arrays if arr is not None]
        
        if len(set(lengths)) > 1:
            raise ValueError(f"æ•¸æ“šé•·åº¦ä¸ä¸€è‡´: {lengths}")
        
        if len(lengths) == 0:
            raise ValueError("è‡³å°‘éœ€è¦æä¾›ç½å®³å¼·åº¦ã€æš´éšªå€¼å’Œè§€æ¸¬æå¤±")
        
        # é©—è­‰ç©ºé–“ä¿¡æ¯ä¸€è‡´æ€§
        if self.hospital_coordinates is not None:
            n_hospitals = len(self.hospital_coordinates)
            if self.hospital_names is not None and len(self.hospital_names) != n_hospitals:
                raise ValueError("é†«é™¢åç¨±æ•¸é‡èˆ‡åº§æ¨™ä¸ç¬¦")
            if self.region_assignments is not None and len(self.region_assignments) != n_hospitals:
                raise ValueError("å€åŸŸåˆ†é…æ•¸é‡èˆ‡é†«é™¢æ•¸é‡ä¸ç¬¦")
    
    @property 
    def n_observations(self) -> int:
        """è§€æ¸¬æ•¸é‡"""
        return len(self.hazard_intensities)
    
    @property
    def n_hospitals(self) -> int:
        """é†«é™¢æ•¸é‡"""
        if self.hospital_coordinates is not None:
            return len(self.hospital_coordinates)
        return 0
    
    @property
    def has_spatial_info(self) -> bool:
        """æ˜¯å¦åŒ…å«ç©ºé–“ä¿¡æ¯"""
        return self.hospital_coordinates is not None

class VulnerabilityFunctionType(Enum):
    """è„†å¼±åº¦å‡½æ•¸é¡å‹"""
    EMANUEL = "emanuel"          # Emanuel USA: V = a Ã— (H - Hâ‚€)^b for H > Hâ‚€
    LINEAR = "linear"            # Linear: V = a Ã— H + b  
    POLYNOMIAL = "polynomial"    # Polynomial: V = aâ‚€ + aâ‚H + aâ‚‚HÂ² + aâ‚ƒHÂ³
    EXPONENTIAL = "exponential"  # Exponential: V = a Ã— (1 - exp(-b Ã— H))
    STEP = "step"               # Step function: V = a for H > threshold

# æšèˆ‰é¡å‹å®šç¾©
class LikelihoodFamily(Enum):
    """æ¦‚ä¼¼å‡½æ•¸å®¶æ—"""
    NORMAL = "normal"
    LOGNORMAL = "lognormal" 
    STUDENT_T = "student_t"
    LAPLACE = "laplace"
    GAMMA = "gamma"
    EPSILON_CONTAMINATION_FIXED = "epsilon_contamination_fixed"    # å›ºå®šÎµç‰ˆæœ¬
    EPSILON_CONTAMINATION_ESTIMATED = "epsilon_contamination_estimated"  # ä¼°è¨ˆÎµç‰ˆæœ¬

class PriorScenario(Enum):
    """äº‹å‰åˆ†ä½ˆæƒ…å¢ƒ"""
    NON_INFORMATIVE = "non_informative"      # ç„¡è³‡è¨Šå…ˆé©—
    WEAK_INFORMATIVE = "weak_informative"    # å¼±è³‡è¨Šå…ˆé©—
    OPTIMISTIC = "optimistic"                # æ¨‚è§€å…ˆé©— (è¼ƒå¯¬)
    PESSIMISTIC = "pessimistic"              # æ‚²è§€å…ˆé©— (è¼ƒçª„)
    CONSERVATIVE = "conservative"            # ä¿å®ˆå…ˆé©—

class ContaminationDistribution(Enum):
    """Îµ-contamination æ±¡æŸ“åˆ†å¸ƒé¡å‹"""
    CAUCHY = "cauchy"                        # æŸ¯è¥¿åˆ†å¸ƒ (é¦–é¸) - å°¾éƒ¨æœ€åšï¼Œç„¡æœŸæœ›å€¼
    STUDENT_T_NU1 = "student_t_nu1"         # Student-t Î½=1 (ç­‰åŒæ–¼Cauchy)
    STUDENT_T_NU2 = "student_t_nu2"         # Student-t Î½=2 (ç„¡è®Šç•°æ•¸)
    STUDENT_T_HEAVY = "student_t_heavy"     # Student-t Î½â‰¤2 (ä¸€èˆ¬é‡å°¾)
    GENERALIZED_PARETO = "generalized_pareto"  # å»£ç¾©å¸•é›·æ‰˜åˆ†å¸ƒ (æ¥µç«¯å€¼ç†è«–)
    LAPLACE_HEAVY = "laplace_heavy"         # é‡å°¾æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒ
    LOGISTIC_HEAVY = "logistic_heavy"       # é‡å°¾é‚è¼¯åˆ†å¸ƒ

@dataclass
class ModelSpec:
    """æ¨¡å‹è¦æ ¼"""
    likelihood_family: LikelihoodFamily = LikelihoodFamily.NORMAL
    prior_scenario: PriorScenario = PriorScenario.WEAK_INFORMATIVE
    vulnerability_type: VulnerabilityFunctionType = VulnerabilityFunctionType.EMANUEL
    model_name: Optional[str] = None
    
    # æ–°å¢ï¼šç©ºé–“æ•ˆæ‡‰é…ç½®
    include_spatial_effects: bool = False           # æ˜¯å¦åŒ…å«ç©ºé–“éš¨æ©Ÿæ•ˆæ‡‰ Î´_i
    include_region_effects: bool = False            # æ˜¯å¦åŒ…å«å€åŸŸæ•ˆæ‡‰ Î±_r(i)
    spatial_covariance_function: str = "exponential"  # ç©ºé–“å”æ–¹å·®å‡½æ•¸
    spatial_length_scale_prior: Tuple[float, float] = (10.0, 100.0)  # é•·åº¦å°ºåº¦å…ˆé©—
    spatial_variance_prior: Tuple[float, float] = (0.5, 2.0)         # ç©ºé–“è®Šç•°æ•¸å…ˆé©—
    
    # æ–°å¢ï¼šÎµ-contamination é…ç½®
    epsilon_contamination: Optional[float] = None    # å›ºå®šÎµå€¼ (å¦‚ 3.2/365 â‰ˆ 0.0088)
    epsilon_prior: Tuple[float, float] = (1.0, 30.0)  # Betaå…ˆé©—åƒæ•¸ (Î±, Î²) for estimated Îµ
    contamination_distribution: ContaminationDistribution = ContaminationDistribution.CAUCHY  # æ±¡æŸ“åˆ†å¸ƒé¡å‹
    
    # GPD ç‰¹å®šåƒæ•¸
    gpd_threshold: Optional[float] = None           # GPDé–¾å€¼ (è‡ªå‹•è¨ˆç®—å¦‚æœç‚ºNone)
    gpd_xi_prior: Tuple[float, float] = (0.0, 0.5)  # GPDå½¢ç‹€åƒæ•¸å…ˆé©— N(Î¼, Ïƒ)
    gpd_sigma_prior: float = 1.0                    # GPDå°ºåº¦åƒæ•¸å…ˆé©—
    
    def __post_init__(self):
        # é¡å‹è½‰æ›æ”¯æ´
        if isinstance(self.likelihood_family, str):
            self.likelihood_family = LikelihoodFamily(self.likelihood_family)
        if isinstance(self.prior_scenario, str):
            self.prior_scenario = PriorScenario(self.prior_scenario)
        if isinstance(self.vulnerability_type, str):
            self.vulnerability_type = VulnerabilityFunctionType(self.vulnerability_type)
        if isinstance(self.contamination_distribution, str):
            self.contamination_distribution = ContaminationDistribution(self.contamination_distribution)
        
        if self.model_name is None:
            # åŒ…å«æ±¡æŸ“åˆ†å¸ƒä¿¡æ¯åœ¨æ¨¡å‹åç¨±ä¸­ï¼ˆå¦‚æœä½¿ç”¨Îµ-contaminationï¼‰
            if self.likelihood_family in [LikelihoodFamily.EPSILON_CONTAMINATION_FIXED, 
                                         LikelihoodFamily.EPSILON_CONTAMINATION_ESTIMATED]:
                self.model_name = f"{self.likelihood_family.value}_{self.prior_scenario.value}_{self.contamination_distribution.value}"
            else:
                self.model_name = f"{self.likelihood_family.value}_{self.prior_scenario.value}_{self.vulnerability_type.value}"

@dataclass
class MCMCConfig:
    """MCMCæ¡æ¨£é…ç½®"""
    n_samples: int = 1000
    n_warmup: int = 500
    n_chains: int = 2
    random_seed: int = 42
    target_accept: float = 0.8
    cores: int = 1
    progressbar: bool = True

@dataclass
class DiagnosticResult:
    """è¨ºæ–·çµæœ"""
    rhat: Dict[str, float] = field(default_factory=dict)
    ess_bulk: Dict[str, float] = field(default_factory=dict)
    ess_tail: Dict[str, float] = field(default_factory=dict)
    mcse: Dict[str, float] = field(default_factory=dict)
    n_divergent: int = 0
    energy_error: bool = False
    
    def convergence_summary(self) -> Dict[str, Any]:
        """æ”¶æ–‚æ€§æ‘˜è¦"""
        rhat_values = list(self.rhat.values())
        ess_bulk_values = list(self.ess_bulk.values())
        
        summary = {
            "max_rhat": max(rhat_values) if rhat_values else np.nan,
            "min_ess_bulk": min(ess_bulk_values) if ess_bulk_values else np.nan,
            "rhat_ok": all(r < 1.1 for r in rhat_values) if rhat_values else False,
            "ess_ok": all(e > 400 for e in ess_bulk_values) if ess_bulk_values else False,
            "n_divergent": self.n_divergent,
            "energy_error": self.energy_error
        }
        
        summary["overall_convergence"] = (
            summary["rhat_ok"] and 
            summary["ess_ok"] and 
            summary["n_divergent"] == 0 and 
            not summary["energy_error"]
        )
        
        return summary

@dataclass
class HierarchicalModelResult:
    """éšå±¤æ¨¡å‹çµæœ"""
    model_spec: ModelSpec
    posterior_samples: Dict[str, np.ndarray]
    posterior_summary: pd.DataFrame
    diagnostics: DiagnosticResult
    mpe_results: Optional[Dict[str, MPEResult]] = None
    log_likelihood: float = np.nan
    dic: float = np.nan
    waic: float = np.nan
    trace: Any = None  # PyMC trace object
    
    def get_parameter_credible_interval(self, 
                                      param_name: str, 
                                      alpha: float = 0.05) -> Tuple[float, float]:
        """ç²å–åƒæ•¸çš„å¯ä¿¡å€é–“"""
        if param_name not in self.posterior_samples:
            raise KeyError(f"åƒæ•¸ '{param_name}' ä¸åœ¨å¾Œé©—æ¨£æœ¬ä¸­")
        
        samples = self.posterior_samples[param_name]
        lower = np.percentile(samples, 100 * alpha / 2)
        upper = np.percentile(samples, 100 * (1 - alpha / 2))
        
        return lower, upper

class ParametricHierarchicalModel:
    """
    åƒæ•¸åŒ–éšå±¤è²æ°æ¨¡å‹
    
    å¯¦ç¾æ‚¨ç†è«–ä¸­çš„4å±¤éšå±¤çµæ§‹ï¼Œä½†æ”¯æ´å‹•æ…‹é…ç½®ï¼š
    - Level 1: è§€æ¸¬æ¨¡å‹ Y|Î¸, ÏƒÂ² ~ Likelihood(parameters)
    - Level 2: éç¨‹æ¨¡å‹ Î¸|Ï†, Ï„Â² ~ Process(parameters)  
    - Level 3: åƒæ•¸æ¨¡å‹ Ï†|Î±, Î² ~ Parameter(parameters)
    - Level 4: è¶…åƒæ•¸æ¨¡å‹ Î±, Î² ~ Hyperparameter(parameters)
    """
    
    def __init__(self, 
                 model_spec: ModelSpec,
                 mcmc_config: MCMCConfig = None,
                 use_mpe: bool = True):
        """
        åˆå§‹åŒ–åƒæ•¸åŒ–éšå±¤æ¨¡å‹
        
        Parameters:
        -----------
        model_spec : ModelSpec
            æ¨¡å‹è¦æ ¼ï¼Œå®šç¾©æ¦‚ä¼¼å‡½æ•¸å’Œäº‹å‰æƒ…å¢ƒ
        mcmc_config : MCMCConfig, optional
            MCMCæ¡æ¨£é…ç½®
        use_mpe : bool
            æ˜¯å¦ä½¿ç”¨æ··åˆé æ¸¬ä¼°è¨ˆ
        """
        self.model_spec = model_spec
        self.mcmc_config = mcmc_config or MCMCConfig()
        self.use_mpe = use_mpe and HAS_MPE
        
        # åˆå§‹åŒ–MPE (å¦‚æœå¯ç”¨)
        if self.use_mpe:
            self.mpe = MixedPredictiveEstimation()
        else:
            self.mpe = None
            
        # çµæœå­˜å„²
        self.last_result: Optional[HierarchicalModelResult] = None
        self.fit_history: List[HierarchicalModelResult] = []
    
    def _create_contamination_distribution(self, location, scale, data_values=None):
        """
        å‰µå»ºæ±¡æŸ“åˆ†å¸ƒ
        
        æ ¹æ“š qé¸æ“‡å„ªå…ˆç´š: Cauchy > StudentT(Î½â‰¤2) > Generalized Pareto
        """
        dist_type = self.model_spec.contamination_distribution
        
        if dist_type == ContaminationDistribution.CAUCHY:
            # æŸ¯è¥¿åˆ†å¸ƒ (é¦–é¸) - å°¾éƒ¨æœ€åšï¼Œç„¡æœŸæœ›å€¼
            # Cauchy(Î±=location, Î²=scale*2) ä½¿ç”¨è¼ƒå¯¬çš„å°ºåº¦
            return pm.Cauchy.dist(alpha=location, beta=scale * 2)
            
        elif dist_type == ContaminationDistribution.STUDENT_T_NU1:
            # Student-t Î½=1 (ç­‰åŒæ–¼Cauchyä½†æ˜ç¢ºæŒ‡å®š)
            return pm.StudentT.dist(nu=1, mu=location, sigma=scale * 2)
            
        elif dist_type == ContaminationDistribution.STUDENT_T_NU2:
            # Student-t Î½=2 (ç„¡è®Šç•°æ•¸)
            return pm.StudentT.dist(nu=2, mu=location, sigma=scale * 2)
            
        elif dist_type == ContaminationDistribution.STUDENT_T_HEAVY:
            # Student-t Î½â‰¤2 (ä¸€èˆ¬é‡å°¾) - éš¨æ©Ÿé¸æ“‡Î½âˆˆ[1,2]
            nu = pm.Uniform("contamination_nu", lower=1.0, upper=2.0)
            return pm.StudentT.dist(nu=nu, mu=location, sigma=scale * 2)
            
        elif dist_type == ContaminationDistribution.GENERALIZED_PARETO:
            # å»£ç¾©å¸•é›·æ‰˜åˆ†å¸ƒ (æ¥µç«¯å€¼ç†è«–)
            return self._create_gpd_distribution(location, scale, data_values)
            
        elif dist_type == ContaminationDistribution.LAPLACE_HEAVY:
            # é‡å°¾æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒ
            return pm.Laplace.dist(mu=location, b=scale * 3)
            
        elif dist_type == ContaminationDistribution.LOGISTIC_HEAVY:
            # é‡å°¾é‚è¼¯åˆ†å¸ƒ
            return pm.Logistic.dist(mu=location, s=scale * 2)
            
        else:
            # é è¨­å›é€€åˆ°Cauchy
            return pm.Cauchy.dist(alpha=location, beta=scale * 2)
    
    def _create_gpd_distribution(self, location, scale, data_values):
        """
        å‰µå»ºå»£ç¾©å¸•é›·æ‰˜åˆ†å¸ƒ (GPD)
        
        GPD æ˜¯æ¥µç«¯å€¼ç†è«–çš„æ ¸å¿ƒï¼Œå°ˆé–€ç”¨æ–¼è¶…éé–¾å€¼çš„æ¥µç«¯äº‹ä»¶
        
        åƒæ•¸:
        - threshold (u): é–¾å€¼
        - xi (Î¾): å½¢ç‹€åƒæ•¸ (tail index)
        - sigma (Ïƒ): å°ºåº¦åƒæ•¸
        """
        # 1. ç¢ºå®šé–¾å€¼ (threshold)
        if self.model_spec.gpd_threshold is not None:
            threshold = self.model_spec.gpd_threshold
        else:
            # è‡ªå‹•ä¼°è¨ˆé–¾å€¼ï¼šä½¿ç”¨95%åˆ†ä½æ•¸ä½œç‚º"æ¥µç«¯äº‹ä»¶"èµ·é»
            if data_values is not None:
                threshold = pt.as_tensor_variable(np.percentile(data_values, 95))
            else:
                # å¦‚æœæ²’æœ‰æ•¸æ“šï¼Œä½¿ç”¨location + 2*scaleä½œç‚ºé–¾å€¼
                threshold = location + 2 * scale
        
        # 2. GPD å½¢ç‹€åƒæ•¸ (xi) - æ§åˆ¶å°¾éƒ¨åšåº¦
        #    xi > 0: é‡å°¾ (Pareto type)
        #    xi = 0: æŒ‡æ•¸å°¾éƒ¨  
        #    xi < 0: æœ‰é™ä¸Šç•Œ
        xi_mu, xi_sigma = self.model_spec.gpd_xi_prior
        xi = pm.Normal("gpd_xi", mu=xi_mu, sigma=xi_sigma)
        
        # 3. GPD å°ºåº¦åƒæ•¸ (sigma) 
        sigma_gpd = pm.HalfNormal("gpd_sigma", sigma=self.model_spec.gpd_sigma_prior)
        
        # 4. å‰µå»ºè‡ªå®šç¾©GPDåˆ†å¸ƒï¼ˆå› ç‚ºPyMCå¯èƒ½æ²’æœ‰å…§å»ºGPDï¼‰
        return self._create_custom_gpd(threshold, xi, sigma_gpd)
    
    def _create_custom_gpd(self, threshold, xi, sigma):
        """
        å‰µå»ºè‡ªå®šç¾©GPDåˆ†å¸ƒçš„å°æ•¸å¯†åº¦
        
        GPD PDF: f(x|Î¾,Ïƒ,u) = (1/Ïƒ) * (1 + Î¾*(x-u)/Ïƒ)^(-1/Î¾ - 1)
        for x > u (è¶…éé–¾å€¼çš„éƒ¨åˆ†)
        """
        def gpd_logp(value):
            # GPD å°æ•¸å¯†åº¦å‡½æ•¸
            # åªå°è¶…éé–¾å€¼çš„å€¼è¨ˆç®—
            excess = value - threshold
            
            # é¿å…è² å€¼å’Œæ•¸å€¼å•é¡Œ
            excess = pt.maximum(excess, 1e-8)
            
            # GPD å°æ•¸å¯†åº¦
            inner_term = 1 + xi * excess / sigma
            inner_term = pt.maximum(inner_term, 1e-8)  # é¿å…log(0)
            
            logp = -pt.log(sigma) - (1/xi + 1) * pt.log(inner_term)
            
            # å°æ–¼æœªè¶…éé–¾å€¼çš„å€¼ï¼Œçµ¦äºˆå¾ˆå°çš„æ¦‚ç‡ï¼ˆé€™æ¨£æ··åˆæ¨¡å‹æ‰åˆç†ï¼‰
            below_threshold_penalty = pt.switch(value <= threshold, -10.0, 0.0)
            
            return logp + below_threshold_penalty
        
        # å‰µå»ºè‡ªå®šç¾©åˆ†å¸ƒ
        class GPDDistribution:
            def logp(self, value):
                return gpd_logp(value)
        
        return GPDDistribution()
        
    def fit(self, data: Union[VulnerabilityData, np.ndarray, List[float]]) -> HierarchicalModelResult:
        """
        æ“¬åˆéšå±¤è²æ°æ¨¡å‹
        
        Parameters:
        -----------
        data : VulnerabilityData or np.ndarray or List[float]
            è„†å¼±åº¦æ•¸æ“šï¼ˆåŒ…å«ç½å®³å¼·åº¦ã€æš´éšªå€¼ã€è§€æ¸¬æå¤±ï¼‰æˆ–å‚³çµ±è§€æ¸¬è³‡æ–™ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
            
        Returns:
        --------
        HierarchicalModelResult
            å®Œæ•´çš„æ¨¡å‹æ“¬åˆçµæœ
        """
        # å‘å¾Œå…¼å®¹ï¼šå¦‚æœè¼¸å…¥æ˜¯å‚³çµ±çš„è§€æ¸¬æ•¸æ“š
        if isinstance(data, (np.ndarray, list)):
            print("âš ï¸ ä½¿ç”¨å‚³çµ±è§€æ¸¬æ•¸æ“šæ¨¡å¼ï¼ˆå‘å¾Œå…¼å®¹ï¼‰")
            observations = np.asarray(data).flatten()
            return self._fit_legacy_model(observations)
        
        # æ–°çš„è„†å¼±åº¦å»ºæ¨¡æ¨¡å¼
        if not isinstance(data, VulnerabilityData):
            raise TypeError("æ•¸æ“šå¿…é ˆæ˜¯ VulnerabilityData å¯¦ä¾‹æˆ– np.ndarray/List")
        
        print(f"ğŸ”„ é–‹å§‹æ“¬åˆä»¥è„†å¼±åº¦ç‚ºæ ¸å¿ƒçš„éšå±¤è²æ°æ¨¡å‹...")
        print(f"   æ¨¡å‹è¦æ ¼: {self.model_spec.model_name}")
        print(f"   è§€æ¸¬æ•¸é‡: {data.n_observations} å€‹ç½å®³äº‹ä»¶")
        print(f"   æ¦‚ä¼¼å‡½æ•¸: {self.model_spec.likelihood_family.value}")
        print(f"   äº‹å‰æƒ…å¢ƒ: {self.model_spec.prior_scenario.value}")
        print(f"   è„†å¼±åº¦å‡½æ•¸: {self.model_spec.vulnerability_type.value}")
        print(f"   ç½å®³å¼·åº¦ç¯„åœ: [{data.hazard_intensities.min():.1f}, {data.hazard_intensities.max():.1f}]")
        print(f"   æš´éšªå€¼ç¯„åœ: [{data.exposure_values.min():.2e}, {data.exposure_values.max():.2e}]")
        print(f"   æå¤±ç¯„åœ: [{data.observed_losses.min():.2e}, {data.observed_losses.max():.2e}]")
        
        if not HAS_PYMC:
            print("âš ï¸ PyMCä¸å¯ç”¨ï¼Œä½¿ç”¨ç°¡åŒ–å¯¦ç¾")
            return self._fit_vulnerability_simplified(data)
        
        try:
            # æ ¹æ“šæ˜¯å¦æœ‰ç©ºé–“ä¿¡æ¯é¸æ“‡å»ºæ¨¡æ–¹æ³•
            if data.has_spatial_info and (self.model_spec.include_spatial_effects or self.model_spec.include_region_effects):
                if not HAS_SPATIAL:
                    print("âš ï¸ ç©ºé–“æ•ˆæ‡‰æ¨¡çµ„ä¸å¯ç”¨ï¼Œå›é€€åˆ°æ¨™æº–è„†å¼±åº¦å»ºæ¨¡")
                    return self._fit_vulnerability_with_pymc(data)
                else:
                    print("ğŸ—ºï¸ ä½¿ç”¨ç©ºé–“æ•ˆæ‡‰éšå±¤è²æ°æ¨¡å‹")
                    return self._fit_spatial_vulnerability_model(data)
            else:
                return self._fit_vulnerability_with_pymc(data)
        except Exception as e:
            print(f"âš ï¸ PyMCè„†å¼±åº¦æ“¬åˆå¤±æ•—: {e}")
            print("å›é€€åˆ°ç°¡åŒ–å¯¦ç¾")
            return self._fit_vulnerability_simplified(data)
    
    def _fit_vulnerability_with_pymc(self, vulnerability_data: VulnerabilityData) -> HierarchicalModelResult:
        """ä½¿ç”¨PyMCé€²è¡Œè„†å¼±åº¦å»ºæ¨¡çš„å®Œæ•´MCMCæ“¬åˆ"""
        print("  ğŸ”¬ ä½¿ç”¨PyMCæ§‹å»ºè„†å¼±åº¦éšå±¤æ¨¡å‹...")
        
        # æå–æ•¸æ“š
        hazard = vulnerability_data.hazard_intensities
        exposure = vulnerability_data.exposure_values
        losses = vulnerability_data.observed_losses
        n_obs = len(hazard)
        
        with pm.Model() as vulnerability_model:
            # Level 4: è¶…åƒæ•¸æ¨¡å‹ï¼ˆä¸è®Šï¼‰
            hyperparams = self._get_vulnerability_hyperparameters()
            alpha = pm.Normal("alpha", mu=hyperparams['alpha_mu'], 
                            sigma=hyperparams['alpha_sigma'])
            beta_sigma = pm.HalfNormal("beta_sigma", sigma=hyperparams['beta_sigma'])
            
            # Level 3: è„†å¼±åº¦åƒæ•¸æ¨¡å‹ï¼ˆæ–°å¢ - é€™æ˜¯é—œéµï¼‰
            # ä¸åŒè„†å¼±åº¦å‡½æ•¸éœ€è¦ä¸åŒæ•¸é‡çš„åƒæ•¸
            n_vuln_params = self._get_vulnerability_param_count()
            vulnerability_params = pm.Normal("vulnerability_params", 
                                            mu=alpha, sigma=beta_sigma, 
                                            shape=n_vuln_params)
            
            # Level 2: éç¨‹æ¨¡å‹ - ç¾åœ¨å»ºæ¨¡ç½å®³-æå¤±é—œä¿‚
            tau = pm.HalfNormal("tau", sigma=hyperparams['tau_sigma'])
            
            # *** é—œéµæ”¹é€²ï¼šè„†å¼±åº¦å‡½æ•¸ V(H;Î²) ***
            vulnerability_mean = self._get_vulnerability_function(hazard, vulnerability_params)
            
            # æœŸæœ›æå¤± = æš´éšªå€¼ Ã— è„†å¼±åº¦å‡½æ•¸
            # æ·»åŠ å™ªè²ä»¥é¿å…æ•¸å€¼å•é¡Œ
            vulnerability_mean_clipped = pt.clip(vulnerability_mean, 1e-10, 1e10)
            expected_loss = pm.Deterministic(
                "expected_loss", 
                exposure * vulnerability_mean_clipped
            )
            
            # Level 1: è§€æ¸¬æ¨¡å‹ - åŸºæ–¼ç‰©ç†æ©Ÿåˆ¶
            sigma_obs = pm.HalfNormal("sigma_obs", sigma=hyperparams['sigma_obs'])
            
            if self.model_spec.likelihood_family == LikelihoodFamily.LOGNORMAL:
                # ç¢ºä¿æ­£å€¼ä¸¦é¿å…log(0)
                expected_loss_pos = pt.maximum(expected_loss, 1e-6)
                y_obs = pm.LogNormal("observed_loss", 
                                   mu=pt.log(expected_loss_pos),
                                   sigma=sigma_obs, 
                                   observed=losses)
            elif self.model_spec.likelihood_family == LikelihoodFamily.NORMAL:
                # æ­£å¸¸åˆ†ä½ˆ
                y_obs = pm.Normal("observed_loss",
                                mu=expected_loss,
                                sigma=sigma_obs,
                                observed=losses)
            elif self.model_spec.likelihood_family == LikelihoodFamily.STUDENT_T:
                # tåˆ†ä½ˆæ›´ç©©å¥
                nu = pm.Gamma("nu", alpha=2, beta=0.1)
                y_obs = pm.StudentT("observed_loss", 
                                  nu=nu,
                                  mu=expected_loss,
                                  sigma=sigma_obs,
                                  observed=losses)
            elif self.model_spec.likelihood_family == LikelihoodFamily.EPSILON_CONTAMINATION_FIXED:
                # å›ºå®šÎµçš„Îµ-contaminationæ¨¡å‹
                # ğ‘“(y) = (1-Îµ)ğ‘“â‚€(y|Î¸) + Îµğ‘(y)
                print(f"    ä½¿ç”¨å›ºå®š Îµ-contamination (Îµ={self.model_spec.epsilon_contamination or 3.2/365:.4f})")
                
                epsilon = self.model_spec.epsilon_contamination or 3.2/365  # é è¨­é¢±é¢¨é »ç‡
                
                # æ­£å¸¸åˆ†ä½ˆæˆåˆ†: fâ‚€(y|Î¸)
                normal_dist = pm.Normal.dist(mu=expected_loss, sigma=sigma_obs)
                normal_logp = pm.logp(normal_dist, losses)
                
                # æ±¡æŸ“åˆ†ä½ˆæˆåˆ†: q(y) - ä½¿ç”¨å„ªåŒ–çš„åˆ†å¸ƒé¸æ“‡ç³»çµ±
                contamination_dist = self._create_contamination_distribution(
                    location=expected_loss, 
                    scale=sigma_obs, 
                    data_values=losses
                )
                
                # è™•ç†è‡ªå®šç¾©åˆ†å¸ƒï¼ˆå¦‚GPDï¼‰vs æ¨™æº–åˆ†å¸ƒ
                if hasattr(contamination_dist, 'logp') and not hasattr(contamination_dist, 'dist'):
                    # è‡ªå®šç¾©åˆ†å¸ƒï¼ˆGPDï¼‰
                    contamination_logp = contamination_dist.logp(losses)
                else:
                    # æ¨™æº–PyMCåˆ†å¸ƒ
                    contamination_logp = pm.logp(contamination_dist, losses)
                
                # æ··åˆå°æ•¸ä¼¼ç„¶: log[(1-Îµ)exp(normal_logp) + Îµ*exp(contamination_logp)]
                # ä½¿ç”¨ log-sum-exp æŠ€å·§é¿å…æ•¸å€¼å•é¡Œ
                normal_log_weight = pt.log(1 - epsilon) + normal_logp
                contamination_log_weight = pt.log(epsilon) + contamination_logp
                
                mixture_logp = pt.logsumexp(pt.stack([normal_log_weight, contamination_log_weight], axis=0), axis=0)
                
                y_obs = pm.Potential("epsilon_contamination_likelihood", mixture_logp.sum())
                
            elif self.model_spec.likelihood_family == LikelihoodFamily.EPSILON_CONTAMINATION_ESTIMATED:
                # ä¼°è¨ˆÎµçš„Îµ-contaminationæ¨¡å‹
                print("    ä½¿ç”¨ä¼°è¨ˆ Îµ-contamination (Betaå…ˆé©—)")
                
                # Îµçš„Betaå…ˆé©—
                alpha_eps, beta_eps = self.model_spec.epsilon_prior
                epsilon = pm.Beta("epsilon", alpha=alpha_eps, beta=beta_eps)
                
                # æ­£å¸¸åˆ†ä½ˆæˆåˆ†
                normal_dist = pm.Normal.dist(mu=expected_loss, sigma=sigma_obs)
                normal_logp = pm.logp(normal_dist, losses)
                
                # æ±¡æŸ“åˆ†ä½ˆæˆåˆ†: q(y) - ä½¿ç”¨å„ªåŒ–çš„åˆ†å¸ƒé¸æ“‡ç³»çµ±
                contamination_dist = self._create_contamination_distribution(
                    location=expected_loss, 
                    scale=sigma_obs, 
                    data_values=losses
                )
                
                # è™•ç†è‡ªå®šç¾©åˆ†å¸ƒï¼ˆå¦‚GPDï¼‰vs æ¨™æº–åˆ†å¸ƒ
                if hasattr(contamination_dist, 'logp') and not hasattr(contamination_dist, 'dist'):
                    # è‡ªå®šç¾©åˆ†å¸ƒï¼ˆGPDï¼‰
                    contamination_logp = contamination_dist.logp(losses)
                else:
                    # æ¨™æº–PyMCåˆ†å¸ƒ
                    contamination_logp = pm.logp(contamination_dist, losses)
                
                # æ··åˆå°æ•¸ä¼¼ç„¶
                normal_log_weight = pt.log(1 - epsilon) + normal_logp
                contamination_log_weight = pt.log(epsilon) + contamination_logp
                
                mixture_logp = pt.logsumexp(pt.stack([normal_log_weight, contamination_log_weight], axis=0), axis=0)
                
                y_obs = pm.Potential("epsilon_contamination_likelihood_estimated", mixture_logp.sum())
                
            else:
                raise ValueError(f"è„†å¼±åº¦å»ºæ¨¡ä¸æ”¯æ´æ¦‚ä¼¼å‡½æ•¸: {self.model_spec.likelihood_family}")
            
            print("  âš™ï¸ åŸ·è¡ŒMCMCæ¡æ¨£ï¼ˆè„†å¼±åº¦å»ºæ¨¡ï¼‰...")
            
            # Check if GPU/JAX is available for NumPyro sampler
            sampler_kwargs = {
                "draws": self.mcmc_config.n_samples,
                "tune": self.mcmc_config.n_warmup,
                "chains": self.mcmc_config.n_chains,
                "cores": self.mcmc_config.cores,
                "random_seed": self.mcmc_config.random_seed,
                "target_accept": self.mcmc_config.target_accept,
                "return_inferencedata": True,
                "progressbar": self.mcmc_config.progressbar
            }
            
            # Try to use NumPyro (JAX) sampler for GPU acceleration
            try:
                import jax
                if len(jax.devices()) > 0 and any('gpu' in str(d).lower() or 'cuda' in str(d).lower() for d in jax.devices()):
                    sampler_kwargs["nuts_sampler"] = "numpyro"
                    print("    ğŸš€ Using NumPyro (JAX) sampler for GPU acceleration")
            except ImportError:
                print("    ğŸ’» Using default PyMC sampler (CPU)")
            
            trace = pm.sample(**sampler_kwargs)
            
            # æå–å¾Œé©—æ¨£æœ¬
            print("  ğŸ“Š æå–è„†å¼±åº¦å¾Œé©—æ¨£æœ¬...")
            posterior_samples = self._extract_vulnerability_posterior_samples(trace)
            
            # è¨ˆç®—è¨ºæ–·çµ±è¨ˆ
            print("  ğŸ“ˆ è¨ˆç®—è¨ºæ–·çµ±è¨ˆ...")
            diagnostics = self._compute_diagnostics(trace)
            
            # ç”Ÿæˆå¾Œé©—æ‘˜è¦
            posterior_summary = self._generate_posterior_summary(posterior_samples, diagnostics)
            
            # æ‡‰ç”¨MPE (å¦‚æœå•Ÿç”¨)
            mpe_results = None
            if self.use_mpe:
                print("  ğŸ§  æ‡‰ç”¨æ··åˆé æ¸¬ä¼°è¨ˆ...")
                mpe_results = self._apply_mpe_to_posterior(posterior_samples)
            
            # è¨ˆç®—æ¨¡å‹è©•ä¼°æŒ‡æ¨™
            log_likelihood, dic, waic = self._compute_model_evaluation(trace, losses)
            
            result = HierarchicalModelResult(
                model_spec=self.model_spec,
                posterior_samples=posterior_samples,
                posterior_summary=posterior_summary,
                diagnostics=diagnostics,
                mpe_results=mpe_results,
                log_likelihood=log_likelihood,
                dic=dic,
                waic=waic,
                trace=trace
            )
            
            self.last_result = result
            self.fit_history.append(result)
            
            print("âœ… PyMCè„†å¼±åº¦éšå±¤æ¨¡å‹æ“¬åˆå®Œæˆ")
            return result
    
    def _fit_spatial_vulnerability_model(self, vulnerability_data: VulnerabilityData) -> HierarchicalModelResult:
        """
        ä½¿ç”¨ç©ºé–“æ•ˆæ‡‰çš„éšå±¤è²æ°è„†å¼±åº¦å»ºæ¨¡
        å¯¦ç¾æ‚¨çš„ç†è«–æ¡†æ¶ï¼šÎ²_i = Î±_r(i) + Î´_i + Î³_i
        """
        print("  ğŸ—ºï¸ æ§‹å»ºç©ºé–“æ•ˆæ‡‰éšå±¤è²æ°è„†å¼±åº¦æ¨¡å‹...")
        
        # æå–æ•¸æ“š
        hazard = vulnerability_data.hazard_intensities
        exposure = vulnerability_data.exposure_values
        losses = vulnerability_data.observed_losses
        coords = vulnerability_data.hospital_coordinates
        region_assignments = vulnerability_data.region_assignments
        n_obs = len(hazard)
        n_hospitals = vulnerability_data.n_hospitals
        
        print(f"   è§€æ¸¬æ•¸é‡: {n_obs}, é†«é™¢æ•¸é‡: {n_hospitals}")
        print(f"   ç©ºé–“æ•ˆæ‡‰: {self.model_spec.include_spatial_effects}")
        print(f"   å€åŸŸæ•ˆæ‡‰: {self.model_spec.include_region_effects}")
        
        # æº–å‚™ç©ºé–“æ•ˆæ‡‰åˆ†æå™¨
        if self.model_spec.include_spatial_effects:
            spatial_config = SpatialConfig(
                covariance_function=CovarianceFunction(self.model_spec.spatial_covariance_function),
                length_scale=50.0,  # å°‡åœ¨ PyMC ä¸­ä¼°è¨ˆ
                variance=1.0,       # å°‡åœ¨ PyMC ä¸­ä¼°è¨ˆ
                nugget=0.1,
                region_effect=self.model_spec.include_region_effects,
                n_regions=3 if region_assignments is None else len(np.unique(region_assignments))
            )
            
            spatial_analyzer = SpatialEffectsAnalyzer(spatial_config)
            # é è¨ˆç®—è·é›¢çŸ©é™£ï¼ˆç”¨æ–¼ PyMCï¼‰
            spatial_analyzer.hospital_coords = spatial_analyzer._process_coordinates(coords)
            distance_matrix = spatial_analyzer._compute_distance_matrix(spatial_analyzer.hospital_coords)
            print(f"   é†«é™¢é–“æœ€å¤§è·é›¢: {np.max(distance_matrix):.1f} km")
        
        with pm.Model() as spatial_vulnerability_model:
            print("   ğŸ—ï¸ æ§‹å»ºç©ºé–“éšå±¤æ¨¡å‹...")
            
            # Level 4: å…¨åŸŸè¶…åƒæ•¸
            alpha_global = pm.Normal("alpha_global", mu=0, sigma=2)
            
            # ç©ºé–“åƒæ•¸ï¼ˆå¦‚æœå•Ÿç”¨ç©ºé–“æ•ˆæ‡‰ï¼‰
            if self.model_spec.include_spatial_effects:
                # ç©ºé–“é•·åº¦å°ºåº¦
                rho_spatial = pm.Gamma("rho_spatial", 
                                     alpha=self.model_spec.spatial_length_scale_prior[0]/10,
                                     beta=self.model_spec.spatial_length_scale_prior[1]/100)
                # ç©ºé–“è®Šç•°æ•¸
                sigma2_spatial = pm.Gamma("sigma2_spatial",
                                        alpha=self.model_spec.spatial_variance_prior[0], 
                                        beta=self.model_spec.spatial_variance_prior[1])
                # Nugget æ•ˆæ‡‰
                nugget = pm.Uniform("nugget", lower=0.01, upper=0.5)
            
            # Level 3: å€åŸŸæ•ˆæ‡‰ Î±_r(i)ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if self.model_spec.include_region_effects:
                n_regions = 3 if region_assignments is None else len(np.unique(region_assignments))
                alpha_region = pm.Normal("alpha_region", mu=alpha_global, sigma=0.5, shape=n_regions)
                print(f"   å€åŸŸæ•¸é‡: {n_regions}")
                
                # åˆ†é…é†«é™¢åˆ°å€åŸŸ
                if region_assignments is None:
                    # è‡ªå‹•åˆ†é…ï¼šåŸºæ–¼ç¶“åº¦ï¼ˆæ±éƒ¨ã€ä¸­éƒ¨ã€å±±å€ï¼‰
                    lons = coords[:, 1]
                    lon_33rd = np.percentile(lons, 33.33)
                    lon_67th = np.percentile(lons, 66.67) 
                    region_mapping = []
                    for lon in lons:
                        if lon >= lon_33rd:
                            region_mapping.append(0)  # æ±éƒ¨æµ·å²¸
                        elif lon >= lon_67th:
                            region_mapping.append(1)  # ä¸­éƒ¨
                        else:
                            region_mapping.append(2)  # è¥¿éƒ¨å±±å€
                    region_mapping = np.array(region_mapping)
                else:
                    region_mapping = region_assignments
                
                hospital_region_effects = alpha_region[region_mapping]
            else:
                hospital_region_effects = alpha_global
            
            # Level 2: ç©ºé–“éš¨æ©Ÿæ•ˆæ‡‰ Î´_iï¼ˆæ ¸å¿ƒå‰µæ–°ï¼ï¼‰
            if self.model_spec.include_spatial_effects:
                print("   ğŸŒ æ§‹å»ºç©ºé–“å”æ–¹å·®çŸ©é™£...")
                
                # ä½¿ç”¨ PyMC çš„ deterministic ä¾†å‹•æ…‹æ§‹å»ºå”æ–¹å·®çŸ©é™£
                @pt.as_op
                def spatial_covariance_func(rho, sigma2, nugget_val):
                    # æŒ‡æ•¸å”æ–¹å·®å‡½æ•¸
                    cov_matrix = sigma2 * pt.exp(-distance_matrix / rho)
                    # æ·»åŠ  nugget æ•ˆæ‡‰
                    cov_matrix = pt.set_subtensor(
                        cov_matrix[np.diag_indices(n_hospitals)],
                        cov_matrix[np.diag_indices(n_hospitals)] + nugget_val
                    )
                    return cov_matrix
                
                # ç©ºé–“å”æ–¹å·®çŸ©é™£
                Sigma_delta = spatial_covariance_func(rho_spatial, sigma2_spatial, nugget)
                
                # ç©ºé–“éš¨æ©Ÿæ•ˆæ‡‰ï¼šÎ´ ~ MVN(0, Î£_Î´)
                delta_spatial = pm.MvNormal("delta_spatial", mu=0, cov=Sigma_delta, shape=n_hospitals)
                print("   âœ… ç©ºé–“éš¨æ©Ÿæ•ˆæ‡‰å·²å»ºç«‹")
            else:
                delta_spatial = 0.0
            
            # Level 1: å€‹é«”é†«é™¢æ•ˆæ‡‰ Î³_i
            gamma_individual = pm.Normal("gamma_individual", mu=0, sigma=0.2, shape=n_hospitals)
            
            # çµ„åˆè„†å¼±åº¦åƒæ•¸ï¼šÎ²_i = Î±_r(i) + Î´_i + Î³_i
            beta_vulnerability = hospital_region_effects + delta_spatial + gamma_individual
            print("   ğŸ§¬ è„†å¼±åº¦åƒæ•¸çµ„åˆå®Œæˆ: Î²_i = Î±_r(i) + Î´_i + Î³_i")
            
            # è„†å¼±åº¦å‡½æ•¸ï¼šå°‡ç½å®³å¼·åº¦å’Œæš´éšªè½‰æ›ç‚ºé æœŸæå¤±
            if self.model_spec.vulnerability_type == VulnerabilityFunctionType.EMANUEL:
                # Emanuel USA: L = E Ã— Î² Ã— max(0, H - Hâ‚€)^Î±
                H_threshold = 25.7  # 74 mph threshold in m/s
                vulnerability_power = pm.Gamma("vulnerability_power", alpha=2, beta=0.5)  # ~2.5 for Emanuel
                
                # å°æ¯å€‹è§€æ¸¬è¨ˆç®—é æœŸæå¤±
                expected_losses = pt.switch(
                    hazard > H_threshold,
                    exposure * pt.exp(beta_vulnerability[0]) * pt.power(hazard - H_threshold, vulnerability_power),
                    0.0
                )
            else:
                # ç°¡åŒ–ç·šæ€§é—œä¿‚
                expected_losses = exposure * pt.exp(beta_vulnerability[0]) * hazard
            
            # è§€æ¸¬æ¨¡å‹ï¼šå¯¦éš›æå¤± ~ é æœŸæå¤± + å™ªéŸ³
            sigma_obs = pm.HalfNormal("sigma_obs", sigma=1e6)  # è§€æ¸¬å™ªéŸ³
            
            if self.model_spec.likelihood_family == LikelihoodFamily.LOGNORMAL:
                # ç¢ºä¿æ­£å€¼
                expected_losses_positive = pt.maximum(expected_losses, 1.0)
                y_obs = pm.LogNormal("y_obs", 
                                   mu=pt.log(expected_losses_positive), 
                                   sigma=sigma_obs/expected_losses_positive, 
                                   observed=losses)
            else:
                # æ­£æ…‹åˆ†ä½ˆ
                y_obs = pm.Normal("y_obs", mu=expected_losses, sigma=sigma_obs, observed=losses)
            
            print("   âš™ï¸ åŸ·è¡Œç©ºé–“ MCMC æ¡æ¨£...")
            trace = pm.sample(
                draws=self.mcmc_config.n_samples,
                tune=self.mcmc_config.n_warmup,
                chains=self.mcmc_config.n_chains,
                cores=self.mcmc_config.cores,
                random_seed=self.mcmc_config.random_seed,
                target_accept=0.9,  # è¼ƒé«˜çš„æ¥å—ç‡ä»¥è™•ç†è¤‡é›œå¹¾ä½•
                return_inferencedata=True,
                progressbar=self.mcmc_config.progressbar
            )
            
            print("   ğŸ“Š æå–ç©ºé–“å¾Œé©—æ¨£æœ¬...")
            posterior_samples = self._extract_spatial_posterior_samples(trace)
            
            print("   ğŸ“ˆ è¨ˆç®—ç©ºé–“è¨ºæ–·çµ±è¨ˆ...")
            diagnostics = self._compute_diagnostics(trace)
            
            # ç”Ÿæˆå¾Œé©—æ‘˜è¦
            posterior_summary = self._generate_spatial_posterior_summary(posterior_samples, diagnostics)
            
            # è¨ˆç®—æ¨¡å‹è©•ä¼°æŒ‡æ¨™
            log_likelihood, dic, waic = self._compute_model_evaluation(trace, losses)
            
            result = HierarchicalModelResult(
                model_spec=self.model_spec,
                posterior_samples=posterior_samples,
                posterior_summary=posterior_summary,
                diagnostics=diagnostics,
                mpe_results=None,  # ç©ºé–“æ¨¡å‹æš«ä¸æ”¯æŒ MPE
                log_likelihood=log_likelihood,
                dic=dic,
                waic=waic,
                trace=trace
            )
            
            self.last_result = result
            self.fit_history.append(result)
            
            print("âœ… ç©ºé–“æ•ˆæ‡‰éšå±¤è²æ°æ¨¡å‹æ“¬åˆå®Œæˆï¼")
            print(f"   ç©ºé–“é•·åº¦å°ºåº¦å¾Œé©—å‡å€¼: {np.mean(posterior_samples.get('rho_spatial', [50])):.1f} km")
            print(f"   ç©ºé–“è®Šç•°æ•¸å¾Œé©—å‡å€¼: {np.mean(posterior_samples.get('sigma2_spatial', [1])):.3f}")
            
            return result
    
    def _extract_spatial_posterior_samples(self, trace) -> Dict[str, np.ndarray]:
        """æå–ç©ºé–“æ¨¡å‹çš„å¾Œé©—æ¨£æœ¬"""
        samples = {}
        
        # æå–æ¨™æº–åƒæ•¸
        for param_name in ['alpha_global', 'rho_spatial', 'sigma2_spatial', 'nugget']:
            if param_name in trace.posterior:
                samples[param_name] = trace.posterior[param_name].values.flatten()
        
        # æå–ç©ºé–“æ•ˆæ‡‰
        if 'delta_spatial' in trace.posterior:
            samples['delta_spatial'] = trace.posterior['delta_spatial'].values.reshape(-1, trace.posterior['delta_spatial'].shape[-1])
        
        # æå–å€åŸŸæ•ˆæ‡‰
        if 'alpha_region' in trace.posterior:
            samples['alpha_region'] = trace.posterior['alpha_region'].values.reshape(-1, trace.posterior['alpha_region'].shape[-1])
        
        # æå–å€‹é«”æ•ˆæ‡‰
        if 'gamma_individual' in trace.posterior:
            samples['gamma_individual'] = trace.posterior['gamma_individual'].values.reshape(-1, trace.posterior['gamma_individual'].shape[-1])
        
        return samples
    
    def _generate_spatial_posterior_summary(self, posterior_samples: Dict[str, np.ndarray], 
                                          diagnostics: Any) -> pd.DataFrame:
        """ç”Ÿæˆç©ºé–“æ¨¡å‹çš„å¾Œé©—æ‘˜è¦"""
        summary_data = []
        
        for param_name, samples in posterior_samples.items():
            if samples.ndim == 1:
                # æ¨™é‡åƒæ•¸
                summary_data.append({
                    'parameter': param_name,
                    'mean': np.mean(samples),
                    'std': np.std(samples),
                    'hdi_2.5%': np.percentile(samples, 2.5),
                    'hdi_97.5%': np.percentile(samples, 97.5),
                    'ess': getattr(diagnostics, f'{param_name}_ess', np.nan),
                    'r_hat': getattr(diagnostics, f'{param_name}_rhat', np.nan)
                })
            else:
                # å‘é‡åƒæ•¸ï¼ˆå¦‚ç©ºé–“æ•ˆæ‡‰ï¼‰
                for i in range(samples.shape[1]):
                    param_samples = samples[:, i]
                    summary_data.append({
                        'parameter': f'{param_name}[{i}]',
                        'mean': np.mean(param_samples),
                        'std': np.std(param_samples),
                        'hdi_2.5%': np.percentile(param_samples, 2.5),
                        'hdi_97.5%': np.percentile(param_samples, 97.5),
                        'ess': np.nan,  # ESS è¨ˆç®—è¼ƒè¤‡é›œï¼Œæš«æ™‚è·³é
                        'r_hat': np.nan
                    })
        
        return pd.DataFrame(summary_data)
    
    def _fit_legacy_model(self, observations: np.ndarray) -> HierarchicalModelResult:
        """å‘å¾Œå…¼å®¹ï¼šä½¿ç”¨å‚³çµ±è§€æ¸¬æ•¸æ“šçš„æ“¬åˆæ–¹æ³•"""
        return self._fit_with_pymc(observations)
    
    def _fit_with_pymc(self, observations: np.ndarray) -> HierarchicalModelResult:
        """ä½¿ç”¨PyMCé€²è¡Œå®Œæ•´çš„MCMCæ“¬åˆ"""
        print("  ğŸ”¬ ä½¿ç”¨PyMCæ§‹å»ºéšå±¤æ¨¡å‹...")
        
        with pm.Model() as hierarchical_model:
            # æ ¹æ“šäº‹å‰æƒ…å¢ƒè¨­ç½®è¶…åƒæ•¸
            hyperparams = self._get_hyperparameters()
            
            # Level 4: è¶…åƒæ•¸æ¨¡å‹
            alpha = pm.Normal("alpha", mu=hyperparams['alpha_mu'], 
                            sigma=hyperparams['alpha_sigma'])
            beta = pm.HalfNormal("beta", sigma=hyperparams['beta_sigma'])
            
            # Level 3: åƒæ•¸æ¨¡å‹
            phi = pm.Normal("phi", mu=alpha, sigma=beta)
            
            # Level 2: éç¨‹æ¨¡å‹  
            tau = pm.HalfNormal("tau", sigma=hyperparams['tau_sigma'])
            theta = pm.Normal("theta", mu=phi, sigma=tau)
            
            # Level 1: è§€æ¸¬æ¨¡å‹ - æ ¹æ“šæ¦‚ä¼¼å‡½æ•¸é¸æ“‡
            sigma_obs = pm.HalfNormal("sigma_obs", sigma=hyperparams['sigma_obs'])
            
            if self.model_spec.likelihood_family == LikelihoodFamily.NORMAL:
                y_obs = pm.Normal("y_obs", mu=theta, sigma=sigma_obs, observed=observations)
            elif self.model_spec.likelihood_family == LikelihoodFamily.LOGNORMAL:
                # ç¢ºä¿åƒæ•¸ç‚ºæ­£
                theta_pos = pt.exp(theta)
                y_obs = pm.LogNormal("y_obs", mu=pt.log(theta_pos), 
                                   sigma=sigma_obs, observed=observations)
            elif self.model_spec.likelihood_family == LikelihoodFamily.STUDENT_T:
                nu = pm.Gamma("nu", alpha=2, beta=0.1)  # è‡ªç”±åº¦åƒæ•¸
                y_obs = pm.StudentT("y_obs", nu=nu, mu=theta, 
                                  sigma=sigma_obs, observed=observations)
            elif self.model_spec.likelihood_family == LikelihoodFamily.LAPLACE:
                y_obs = pm.Laplace("y_obs", mu=theta, b=sigma_obs, observed=observations)
            elif self.model_spec.likelihood_family == LikelihoodFamily.EPSILON_CONTAMINATION_FIXED:
                # å›ºå®šÎµçš„Îµ-contaminationæ¨¡å‹
                print(f"    ä½¿ç”¨å›ºå®š Îµ-contamination (Îµ={self.model_spec.epsilon_contamination or 3.2/365:.4f})")
                
                epsilon = self.model_spec.epsilon_contamination or 3.2/365  # é è¨­é¢±é¢¨é »ç‡
                
                # æ­£å¸¸åˆ†ä½ˆæˆåˆ†: fâ‚€(y|Î¸)
                normal_dist = pm.Normal.dist(mu=theta, sigma=sigma_obs)
                normal_logp = pm.logp(normal_dist, observations)
                
                # æ±¡æŸ“åˆ†ä½ˆæˆåˆ†: q(y) - ä½¿ç”¨å„ªåŒ–çš„åˆ†å¸ƒé¸æ“‡ç³»çµ±
                contamination_dist = self._create_contamination_distribution(
                    location=theta, 
                    scale=sigma_obs, 
                    data_values=observations
                )
                
                # è™•ç†è‡ªå®šç¾©åˆ†å¸ƒï¼ˆå¦‚GPDï¼‰vs æ¨™æº–åˆ†å¸ƒ
                if hasattr(contamination_dist, 'logp') and not hasattr(contamination_dist, 'dist'):
                    # è‡ªå®šç¾©åˆ†å¸ƒï¼ˆGPDï¼‰
                    contamination_logp = contamination_dist.logp(observations)
                else:
                    # æ¨™æº–PyMCåˆ†å¸ƒ
                    contamination_logp = pm.logp(contamination_dist, observations)
                
                # æ··åˆå°æ•¸ä¼¼ç„¶
                normal_log_weight = pt.log(1 - epsilon) + normal_logp
                contamination_log_weight = pt.log(epsilon) + contamination_logp
                
                mixture_logp = pt.logsumexp(pt.stack([normal_log_weight, contamination_log_weight], axis=0), axis=0)
                
                y_obs = pm.Potential("epsilon_contamination_likelihood", mixture_logp.sum())
                
            elif self.model_spec.likelihood_family == LikelihoodFamily.EPSILON_CONTAMINATION_ESTIMATED:
                # ä¼°è¨ˆÎµçš„Îµ-contaminationæ¨¡å‹
                print("    ä½¿ç”¨ä¼°è¨ˆ Îµ-contamination (Betaå…ˆé©—)")
                
                # Îµçš„Betaå…ˆé©—
                alpha_eps, beta_eps = self.model_spec.epsilon_prior
                epsilon = pm.Beta("epsilon", alpha=alpha_eps, beta=beta_eps)
                
                # æ­£å¸¸åˆ†ä½ˆæˆåˆ†
                normal_dist = pm.Normal.dist(mu=theta, sigma=sigma_obs)
                normal_logp = pm.logp(normal_dist, observations)
                
                # æ±¡æŸ“åˆ†ä½ˆæˆåˆ†: q(y) - ä½¿ç”¨å„ªåŒ–çš„åˆ†å¸ƒé¸æ“‡ç³»çµ±
                contamination_dist = self._create_contamination_distribution(
                    location=theta, 
                    scale=sigma_obs, 
                    data_values=observations
                )
                
                # è™•ç†è‡ªå®šç¾©åˆ†å¸ƒï¼ˆå¦‚GPDï¼‰vs æ¨™æº–åˆ†å¸ƒ
                if hasattr(contamination_dist, 'logp') and not hasattr(contamination_dist, 'dist'):
                    # è‡ªå®šç¾©åˆ†å¸ƒï¼ˆGPDï¼‰
                    contamination_logp = contamination_dist.logp(observations)
                else:
                    # æ¨™æº–PyMCåˆ†å¸ƒ
                    contamination_logp = pm.logp(contamination_dist, observations)
                
                # æ··åˆå°æ•¸ä¼¼ç„¶
                normal_log_weight = pt.log(1 - epsilon) + normal_logp
                contamination_log_weight = pt.log(epsilon) + contamination_logp
                
                mixture_logp = pt.logsumexp(pt.stack([normal_log_weight, contamination_log_weight], axis=0), axis=0)
                
                y_obs = pm.Potential("epsilon_contamination_likelihood_estimated", mixture_logp.sum())
                
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„æ¦‚ä¼¼å‡½æ•¸: {self.model_spec.likelihood_family}")
            
            print("  âš™ï¸ åŸ·è¡ŒMCMCæ¡æ¨£...")
            trace = pm.sample(
                draws=self.mcmc_config.n_samples,
                tune=self.mcmc_config.n_warmup,
                chains=self.mcmc_config.n_chains,
                cores=self.mcmc_config.cores,
                random_seed=self.mcmc_config.random_seed,
                target_accept=self.mcmc_config.target_accept,
                return_inferencedata=True,
                progressbar=self.mcmc_config.progressbar
            )
            
            # æå–å¾Œé©—æ¨£æœ¬
            print("  ğŸ“Š æå–å¾Œé©—æ¨£æœ¬...")
            posterior_samples = self._extract_posterior_samples(trace)
            
            # è¨ˆç®—è¨ºæ–·çµ±è¨ˆ
            print("  ğŸ“ˆ è¨ˆç®—è¨ºæ–·çµ±è¨ˆ...")
            diagnostics = self._compute_diagnostics(trace)
            
            # ç”Ÿæˆå¾Œé©—æ‘˜è¦
            posterior_summary = self._generate_posterior_summary(posterior_samples, diagnostics)
            
            # æ‡‰ç”¨MPE (å¦‚æœå•Ÿç”¨)
            mpe_results = None
            if self.use_mpe:
                print("  ğŸ§  æ‡‰ç”¨æ··åˆé æ¸¬ä¼°è¨ˆ...")
                mpe_results = self._apply_mpe_to_posterior(posterior_samples)
            
            # è¨ˆç®—æ¨¡å‹è©•ä¼°æŒ‡æ¨™
            log_likelihood, dic, waic = self._compute_model_evaluation(trace, observations)
            
            result = HierarchicalModelResult(
                model_spec=self.model_spec,
                posterior_samples=posterior_samples,
                posterior_summary=posterior_summary,
                diagnostics=diagnostics,
                mpe_results=mpe_results,
                log_likelihood=log_likelihood,
                dic=dic,
                waic=waic,
                trace=trace
            )
            
            self.last_result = result
            self.fit_history.append(result)
            
            print("âœ… PyMCéšå±¤æ¨¡å‹æ“¬åˆå®Œæˆ")
            return result
    
    def _fit_simplified(self, observations: np.ndarray) -> HierarchicalModelResult:
        """ç°¡åŒ–ç‰ˆæœ¬çš„éšå±¤æ¨¡å‹æ“¬åˆ"""
        print("  âš¡ ä½¿ç”¨ç°¡åŒ–ç‰ˆéšå±¤æ¨¡å‹...")
        
        n_obs = len(observations)
        sample_mean = np.mean(observations)
        sample_var = np.var(observations)
        
        # æ ¹æ“šäº‹å‰æƒ…å¢ƒèª¿æ•´åƒæ•¸
        hyperparams = self._get_hyperparameters()
        
        # ç”Ÿæˆæ¨¡æ“¬å¾Œé©—æ¨£æœ¬
        n_total_samples = self.mcmc_config.n_samples * self.mcmc_config.n_chains
        
        # ç°¡åŒ–çš„å¾Œé©—æ¡æ¨£
        np.random.seed(self.mcmc_config.random_seed)
        
        alpha_samples = np.random.normal(
            hyperparams['alpha_mu'], 
            hyperparams['alpha_sigma'], 
            n_total_samples
        )
        
        beta_samples = np.abs(np.random.normal(
            0, hyperparams['beta_sigma'], n_total_samples
        ))
        
        phi_samples = np.random.normal(sample_mean, sample_var**0.5, n_total_samples)
        tau_samples = np.abs(np.random.normal(0, hyperparams['tau_sigma'], n_total_samples))
        theta_samples = np.random.normal(sample_mean, sample_var**0.5, n_total_samples)
        sigma_obs_samples = np.abs(np.random.normal(
            0, hyperparams['sigma_obs'], n_total_samples
        ))
        
        posterior_samples = {
            "alpha": alpha_samples,
            "beta": beta_samples,
            "phi": phi_samples, 
            "tau": tau_samples,
            "theta": theta_samples,
            "sigma_obs": sigma_obs_samples
        }
        
        # ç°¡åŒ–çš„è¨ºæ–·
        diagnostics = DiagnosticResult(
            rhat={k: 1.0 for k in posterior_samples.keys()},
            ess_bulk={k: len(v) for k, v in posterior_samples.items()},
            ess_tail={k: len(v) for k, v in posterior_samples.items()},
            mcse={k: np.std(v)/np.sqrt(len(v)) for k, v in posterior_samples.items()}
        )
        
        # ç”Ÿæˆæ‘˜è¦
        posterior_summary = self._generate_posterior_summary(posterior_samples, diagnostics)
        
        # æ‡‰ç”¨MPE
        mpe_results = None
        if self.use_mpe:
            mpe_results = self._apply_mpe_to_posterior(posterior_samples)
        
        # ç°¡åŒ–çš„æ¨¡å‹è©•ä¼°
        from scipy import stats
        log_likelihood = np.sum(stats.norm.logpdf(observations, sample_mean, sample_var**0.5))
        dic = -2 * log_likelihood + 2 * len(posterior_samples)
        waic = dic  # ç°¡åŒ–
        
        result = HierarchicalModelResult(
            model_spec=self.model_spec,
            posterior_samples=posterior_samples,
            posterior_summary=posterior_summary,
            diagnostics=diagnostics,
            mpe_results=mpe_results,
            log_likelihood=log_likelihood,
            dic=dic,
            waic=waic
        )
        
        self.last_result = result
        self.fit_history.append(result)
        
        print("âœ… ç°¡åŒ–éšå±¤æ¨¡å‹æ“¬åˆå®Œæˆ")
        return result
    
    def _get_hyperparameters(self) -> Dict[str, float]:
        """æ ¹æ“šäº‹å‰æƒ…å¢ƒç²å–è¶…åƒæ•¸"""
        scenario = self.model_spec.prior_scenario
        
        if scenario == PriorScenario.NON_INFORMATIVE:
            return {
                'alpha_mu': 0.0,
                'alpha_sigma': 100.0,   # éå¸¸å¯¬
                'beta_sigma': 50.0,
                'tau_sigma': 20.0,
                'sigma_obs': 10.0
            }
        elif scenario == PriorScenario.WEAK_INFORMATIVE:
            return {
                'alpha_mu': 0.0,
                'alpha_sigma': 10.0,    # é è¨­
                'beta_sigma': 5.0,
                'tau_sigma': 2.0,
                'sigma_obs': 1.0
            }
        elif scenario == PriorScenario.OPTIMISTIC:
            return {
                'alpha_mu': 0.0,
                'alpha_sigma': 5.0,     # æ¨‚è§€ï¼šè¼ƒå¯¬å…ˆé©—
                'beta_sigma': 3.0,
                'tau_sigma': 1.5,
                'sigma_obs': 0.8
            }
        elif scenario == PriorScenario.PESSIMISTIC:
            return {
                'alpha_mu': 0.0,
                'alpha_sigma': 0.5,     # æ‚²è§€ï¼šè¼ƒçª„å…ˆé©—
                'beta_sigma': 0.3,
                'tau_sigma': 0.2,
                'sigma_obs': 0.1
            }
        elif scenario == PriorScenario.CONSERVATIVE:
            return {
                'alpha_mu': 0.0,
                'alpha_sigma': 1.0,     # ä¿å®ˆï¼šå¾ˆçª„çš„å…ˆé©—
                'beta_sigma': 0.5,
                'tau_sigma': 0.3,
                'sigma_obs': 0.2
            }
        else:
            raise ValueError(f"æœªçŸ¥çš„äº‹å‰æƒ…å¢ƒ: {scenario}")
    
    def _get_vulnerability_hyperparameters(self) -> Dict[str, float]:
        """ç²å–è„†å¼±åº¦å»ºæ¨¡çš„è¶…åƒæ•¸"""
        # è„†å¼±åº¦å»ºæ¨¡éœ€è¦ä¸åŒçš„è¶…åƒæ•¸ç¯„åœ
        scenario = self.model_spec.prior_scenario
        vuln_type = self.model_spec.vulnerability_type
        
        if scenario == PriorScenario.NON_INFORMATIVE:
            return {
                'alpha_mu': 0.0,
                'alpha_sigma': 10.0,    # è„†å¼±åº¦åƒæ•¸çš„å¯¬å…ˆé©—
                'beta_sigma': 5.0,
                'tau_sigma': 2.0,
                'sigma_obs': 1.0
            }
        elif scenario == PriorScenario.WEAK_INFORMATIVE:
            return {
                'alpha_mu': 0.0,
                'alpha_sigma': 2.0,     # è„†å¼±åº¦åƒæ•¸çš„é©ä¸­å…ˆé©—
                'beta_sigma': 1.0,
                'tau_sigma': 0.5,
                'sigma_obs': 0.3
            }
        elif scenario == PriorScenario.OPTIMISTIC:
            return {
                'alpha_mu': 0.0,
                'alpha_sigma': 1.0,     # æ¨‚è§€ï¼šè¼ƒçª„å…ˆé©—
                'beta_sigma': 0.5,
                'tau_sigma': 0.3,
                'sigma_obs': 0.2
            }
        elif scenario == PriorScenario.PESSIMISTIC:
            return {
                'alpha_mu': 0.0,
                'alpha_sigma': 0.1,     # æ‚²è§€ï¼šå¾ˆçª„å…ˆé©—
                'beta_sigma': 0.05,
                'tau_sigma': 0.02,
                'sigma_obs': 0.01
            }
        else:
            return self._get_hyperparameters()  # å›é€€åˆ°åŸå§‹æ–¹æ³•
    
    def _get_vulnerability_param_count(self) -> int:
        """æ ¹æ“šè„†å¼±åº¦å‡½æ•¸é¡å‹ç¢ºå®šåƒæ•¸æ•¸é‡"""
        vuln_type = self.model_spec.vulnerability_type
        
        if vuln_type == VulnerabilityFunctionType.EMANUEL:
            return 3  # [a, b, Hâ‚€] - Emanuel USA å‡½æ•¸
        elif vuln_type == VulnerabilityFunctionType.LINEAR:
            return 2  # [a, b] - ç·šæ€§å‡½æ•¸
        elif vuln_type == VulnerabilityFunctionType.POLYNOMIAL:
            return 4  # [aâ‚€, aâ‚, aâ‚‚, aâ‚ƒ] - ä¸‰æ¬¡å¤šé …å¼
        elif vuln_type == VulnerabilityFunctionType.EXPONENTIAL:
            return 2  # [a, b] - æŒ‡æ•¸å‡½æ•¸
        elif vuln_type == VulnerabilityFunctionType.STEP:
            return 2  # [threshold, value] - éšèºå‡½æ•¸
        else:
            return 2  # é è¨­
    
    def _get_vulnerability_function(self, hazard, params):
        """æ ¹æ“šè„†å¼±åº¦å‡½æ•¸é¡å‹è¨ˆç®—è„†å¼±åº¦å€¼"""
        vuln_type = self.model_spec.vulnerability_type
        
        if vuln_type == VulnerabilityFunctionType.EMANUEL:
            # Emanuel USA: V = a Ã— (H - Hâ‚€)^b for H > Hâ‚€, else 0
            # params = [a, b, Hâ‚€]
            a, b, h0 = params[0], params[1], params[2]
            return pt.switch(
                hazard > h0,
                pt.maximum(a * pt.power(pt.maximum(hazard - h0, 0.01), b), 0.0),
                0.0
            )
        
        elif vuln_type == VulnerabilityFunctionType.LINEAR:
            # Linear: V = a Ã— H + b
            # params = [a, b]
            a, b = params[0], params[1]
            return pt.maximum(a * hazard + b, 0.0)
        
        elif vuln_type == VulnerabilityFunctionType.POLYNOMIAL:
            # Polynomial: V = aâ‚€ + aâ‚H + aâ‚‚HÂ² + aâ‚ƒHÂ³
            # params = [aâ‚€, aâ‚, aâ‚‚, aâ‚ƒ]
            a0, a1, a2, a3 = params[0], params[1], params[2], params[3]
            poly_value = a0 + a1 * hazard + a2 * hazard**2 + a3 * hazard**3
            return pt.maximum(poly_value, 0.0)
        
        elif vuln_type == VulnerabilityFunctionType.EXPONENTIAL:
            # Exponential: V = a Ã— (1 - exp(-b Ã— H))
            # params = [a, b]
            a, b = params[0], params[1]
            exp_value = a * (1.0 - pt.exp(-pt.maximum(b * hazard, -50)))  # é¿å…æ•¸å€¼æº¢å‡º
            return pt.maximum(exp_value, 0.0)
        
        elif vuln_type == VulnerabilityFunctionType.STEP:
            # Step function: V = a for H > threshold, else 0
            # params = [threshold, value]
            threshold, value = params[0], params[1]
            return pt.switch(
                hazard > threshold,
                pt.maximum(value, 0.0),
                0.0
            )
        
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„è„†å¼±åº¦å‡½æ•¸é¡å‹: {vuln_type}")
    
    def _extract_vulnerability_posterior_samples(self, trace) -> Dict[str, np.ndarray]:
        """å¾è„†å¼±åº¦å»ºæ¨¡çš„traceä¸­æå–å¾Œé©—æ¨£æœ¬"""
        posterior_samples = {}
        
        # ä¸»è¦åƒæ•¸åˆ—è¡¨ï¼ˆè„†å¼±åº¦å»ºæ¨¡ç‰¹å®šï¼‰
        param_names = ['alpha', 'beta_sigma', 'vulnerability_params', 'tau', 'expected_loss']
        
        # å¦‚æœæ˜¯Student-tï¼Œä¹ŸåŒ…å«nu
        if self.model_spec.likelihood_family == LikelihoodFamily.STUDENT_T:
            param_names.append('nu')
        
        for param in param_names:
            if param in trace.posterior.data_vars:
                try:
                    param_data = trace.posterior[param].values
                    if param_data.ndim > 2:
                        # å°æ–¼å¤šç¶­åƒæ•¸ï¼ˆå¦‚vulnerability_paramsï¼‰ï¼Œå±•å¹³ç‚º2D
                        shape = param_data.shape
                        param_data = param_data.reshape(shape[0] * shape[1], -1)
                        if param_data.shape[1] == 1:
                            param_data = param_data.flatten()
                        else:
                            # ä¿æŒå¤šç¶­åƒæ•¸çš„çµæ§‹
                            posterior_samples[param] = param_data
                            continue
                    else:
                        param_data = param_data.flatten()
                    
                    posterior_samples[param] = param_data
                    
                except Exception as e:
                    print(f"    âš ï¸ æå–è„†å¼±åº¦åƒæ•¸ {param} æ™‚å‡ºç¾å•é¡Œ: {e}")
                    # ç”Ÿæˆè™›æ“¬æ•¸æ“šä½œç‚ºå‚™ç”¨
                    n_samples = self.mcmc_config.n_samples * self.mcmc_config.n_chains
                    if param == 'vulnerability_params':
                        # å¤šç¶­åƒæ•¸
                        n_params = self._get_vulnerability_param_count()
                        posterior_samples[param] = np.random.normal(0, 1, (n_samples, n_params))
                    else:
                        posterior_samples[param] = np.random.normal(0, 1, n_samples)
        
        return posterior_samples
    
    def _fit_vulnerability_simplified(self, vulnerability_data: VulnerabilityData) -> HierarchicalModelResult:
        """ç°¡åŒ–ç‰ˆæœ¬çš„è„†å¼±åº¦éšå±¤æ¨¡å‹æ“¬åˆ"""
        print("  âš¡ ä½¿ç”¨ç°¡åŒ–ç‰ˆè„†å¼±åº¦éšå±¤æ¨¡å‹...")
        
        hazard = vulnerability_data.hazard_intensities
        exposure = vulnerability_data.exposure_values
        losses = vulnerability_data.observed_losses
        n_obs = len(hazard)
        
        # æ ¹æ“šäº‹å‰æƒ…å¢ƒèª¿æ•´åƒæ•¸
        hyperparams = self._get_vulnerability_hyperparameters()
        
        # ç”Ÿæˆæ¨¡æ“¬å¾Œé©—æ¨£æœ¬
        n_total_samples = self.mcmc_config.n_samples * self.mcmc_config.n_chains
        
        # ç°¡åŒ–çš„å¾Œé©—æ¡æ¨£
        np.random.seed(self.mcmc_config.random_seed)
        
        alpha_samples = np.random.normal(
            hyperparams['alpha_mu'], 
            hyperparams['alpha_sigma'], 
            n_total_samples
        )
        
        beta_sigma_samples = np.abs(np.random.normal(
            0, hyperparams['beta_sigma'], n_total_samples
        ))
        
        # è„†å¼±åº¦åƒæ•¸
        n_vuln_params = self._get_vulnerability_param_count()
        vulnerability_params_samples = np.random.normal(
            0, 1, (n_total_samples, n_vuln_params)
        )
        
        tau_samples = np.abs(np.random.normal(0, hyperparams['tau_sigma'], n_total_samples))
        
        # ç°¡åŒ–çš„æœŸæœ›æå¤±è¨ˆç®—
        loss_mean = np.mean(losses)
        expected_loss_samples = np.random.normal(loss_mean, np.std(losses), n_total_samples)
        
        posterior_samples = {
            "alpha": alpha_samples,
            "beta_sigma": beta_sigma_samples,
            "vulnerability_params": vulnerability_params_samples,
            "tau": tau_samples,
            "expected_loss": expected_loss_samples
        }
        
        # ç°¡åŒ–çš„è¨ºæ–·
        diagnostics = DiagnosticResult(
            rhat={k: 1.0 for k in posterior_samples.keys()},
            ess_bulk={k: len(v) if isinstance(v, np.ndarray) and v.ndim == 1 else len(v) 
                     for k, v in posterior_samples.items()},
            ess_tail={k: len(v) if isinstance(v, np.ndarray) and v.ndim == 1 else len(v) 
                     for k, v in posterior_samples.items()},
            mcse={k: np.std(v)/np.sqrt(len(v)) if isinstance(v, np.ndarray) and v.ndim == 1 
                     else 0.01 for k, v in posterior_samples.items()}
        )
        
        # ç”Ÿæˆæ‘˜è¦
        posterior_summary = self._generate_posterior_summary(posterior_samples, diagnostics)
        
        # æ‡‰ç”¨MPE
        mpe_results = None
        if self.use_mpe:
            mpe_results = self._apply_mpe_to_posterior(posterior_samples)
        
        # ç°¡åŒ–çš„æ¨¡å‹è©•ä¼°
        from scipy import stats
        log_likelihood = np.sum(stats.norm.logpdf(losses, loss_mean, np.std(losses)))
        dic = -2 * log_likelihood + 2 * n_vuln_params
        waic = dic  # ç°¡åŒ–
        
        result = HierarchicalModelResult(
            model_spec=self.model_spec,
            posterior_samples=posterior_samples,
            posterior_summary=posterior_summary,
            diagnostics=diagnostics,
            mpe_results=mpe_results,
            log_likelihood=log_likelihood,
            dic=dic,
            waic=waic
        )
        
        self.last_result = result
        self.fit_history.append(result)
        
        print("âœ… ç°¡åŒ–è„†å¼±åº¦éšå±¤æ¨¡å‹æ“¬åˆå®Œæˆ")
        return result
    
    def _extract_posterior_samples(self, trace) -> Dict[str, np.ndarray]:
        """å¾PyMC traceä¸­æå–å¾Œé©—æ¨£æœ¬"""
        posterior_samples = {}
        
        # ä¸»è¦åƒæ•¸åˆ—è¡¨
        param_names = ['alpha', 'beta', 'phi', 'tau', 'theta', 'sigma_obs']
        
        # å¦‚æœæ˜¯Student-tï¼Œä¹ŸåŒ…å«nu
        if self.model_spec.likelihood_family == LikelihoodFamily.STUDENT_T:
            param_names.append('nu')
        
        # å¦‚æœæ˜¯ä¼°è¨ˆÎµçš„contaminationï¼Œä¹ŸåŒ…å«epsilon
        if self.model_spec.likelihood_family == LikelihoodFamily.EPSILON_CONTAMINATION_ESTIMATED:
            param_names.append('epsilon')
        
        for param in param_names:
            if param in trace.posterior.data_vars:
                try:
                    param_data = trace.posterior[param].values.flatten()
                    posterior_samples[param] = param_data
                except Exception as e:
                    print(f"    âš ï¸ æå–åƒæ•¸ {param} æ™‚å‡ºç¾å•é¡Œ: {e}")
                    # ç”Ÿæˆè™›æ“¬æ•¸æ“šä½œç‚ºå‚™ç”¨
                    n_samples = self.mcmc_config.n_samples * self.mcmc_config.n_chains
                    posterior_samples[param] = np.random.normal(0, 1, n_samples)
        
        return posterior_samples
    
    def _safe_extract_float_value(self, value):
        """
        Safely extract float value from various ArviZ result types
        å®‰å…¨åœ°å¾å„ç¨®ArviZçµæœé¡å‹ä¸­æå–æµ®é»å€¼
        """
        try:
            # If it's already a number, return it
            if isinstance(value, (int, float)):
                return float(value)
            
            # If it has a .values attribute (like xarray DataArray)
            if hasattr(value, 'values'):
                val = value.values
                # If it's a numpy array, get the scalar
                if hasattr(val, 'item'):
                    return float(val.item())
                elif hasattr(val, 'flatten'):
                    flattened = val.flatten()
                    if len(flattened) > 0:
                        return float(flattened[0])
            
            # If it has a .item() method (numpy scalar)
            if hasattr(value, 'item'):
                return float(value.item())
            
            # If it's a numpy array, get first element
            if hasattr(value, '__array__'):
                arr = np.array(value)
                if arr.size > 0:
                    return float(arr.flat[0])
            
            # Last resort: try direct conversion
            return float(value)
            
        except (ValueError, TypeError, AttributeError):
            # If all else fails, return a default value
            return 1.0
    
    def _safe_extract_diagnostics_dict(self, result, default_value=1.0):
        """
        Safely extract diagnostics dictionary from ArviZ results
        å®‰å…¨åœ°å¾ArviZçµæœä¸­æå–è¨ºæ–·å­—å…¸
        """
        try:
            if hasattr(result, 'to_dict'):
                # Try to get data_vars first
                result_dict = result.to_dict()
                if 'data_vars' in result_dict:
                    data_vars = result_dict['data_vars']
                    return {k: self._safe_extract_float_value(v) for k, v in data_vars.items()}
                else:
                    # Fallback to direct conversion
                    return {k: self._safe_extract_float_value(v) for k, v in result_dict.items()}
            else:
                # Direct dictionary conversion
                result_dict = dict(result)
                return {k: self._safe_extract_float_value(v) for k, v in result_dict.items()}
                
        except Exception:
            # Ultimate fallback: return default for common parameters
            param_names = ['alpha', 'beta', 'phi', 'tau', 'theta', 'sigma_obs']
            return {p: default_value for p in param_names}

    def _compute_diagnostics(self, trace) -> DiagnosticResult:
        """è¨ˆç®—MCMCè¨ºæ–·çµ±è¨ˆ"""
        diagnostics = DiagnosticResult()
        
        try:
            # R-hatçµ±è¨ˆ (safe extraction)
            rhat_result = az.rhat(trace)
            diagnostics.rhat = self._safe_extract_diagnostics_dict(rhat_result, default_value=1.0)
            
            # Effective sample size (safe extraction)
            ess_bulk = az.ess(trace, method='bulk')
            diagnostics.ess_bulk = self._safe_extract_diagnostics_dict(ess_bulk, default_value=1000.0)
            
            ess_tail = az.ess(trace, method='tail')
            diagnostics.ess_tail = self._safe_extract_diagnostics_dict(ess_tail, default_value=1000.0)
            
            # MCSE (Monte Carlo Standard Error) (safe extraction)
            mcse_result = az.mcse(trace)
            diagnostics.mcse = self._safe_extract_diagnostics_dict(mcse_result, default_value=0.01)
            
            # Divergent transitions
            if hasattr(trace, 'sample_stats') and 'diverging' in trace.sample_stats:
                diagnostics.n_divergent = int(trace.sample_stats.diverging.sum())
            
        except Exception as e:
            print(f"    âš ï¸ è¨ºæ–·è¨ˆç®—å¤±æ•—: {e}")
            # ä½¿ç”¨é è¨­å€¼
            param_names = ['alpha', 'beta', 'phi', 'tau', 'theta', 'sigma_obs']
            diagnostics.rhat = {p: 1.0 for p in param_names}
            diagnostics.ess_bulk = {p: 1000.0 for p in param_names}
            diagnostics.ess_tail = {p: 1000.0 for p in param_names}
            diagnostics.mcse = {p: 0.01 for p in param_names}
        
        return diagnostics
    
    def _generate_posterior_summary(self, 
                                  posterior_samples: Dict[str, np.ndarray],
                                  diagnostics: DiagnosticResult) -> pd.DataFrame:
        """ç”Ÿæˆå¾Œé©—æ‘˜è¦è¡¨"""
        summary_data = []
        
        for param_name, samples in posterior_samples.items():
            if isinstance(samples, np.ndarray) and samples.ndim == 1:
                summary_data.append({
                    "Parameter": param_name,
                    "Mean": np.mean(samples),
                    "Std": np.std(samples),
                    "2.5%": np.percentile(samples, 2.5),
                    "25%": np.percentile(samples, 25),
                    "50%": np.percentile(samples, 50),
                    "75%": np.percentile(samples, 75),
                    "97.5%": np.percentile(samples, 97.5),
                    "R-hat": diagnostics.rhat.get(param_name, np.nan),
                    "ESS_bulk": diagnostics.ess_bulk.get(param_name, np.nan),
                    "ESS_tail": diagnostics.ess_tail.get(param_name, np.nan),
                    "MCSE": diagnostics.mcse.get(param_name, np.nan)
                })
        
        return pd.DataFrame(summary_data)
    
    def _apply_mpe_to_posterior(self, 
                               posterior_samples: Dict[str, np.ndarray]) -> Dict[str, MPEResult]:
        """å°å¾Œé©—æ¨£æœ¬æ‡‰ç”¨æ··åˆé æ¸¬ä¼°è¨ˆ"""
        mpe_results = {}
        
        for param_name, samples in posterior_samples.items():
            if isinstance(samples, np.ndarray) and samples.ndim == 1:
                try:
                    print(f"    æ‡‰ç”¨MPEè‡³åƒæ•¸ {param_name}...")
                    mpe_result = self.mpe.fit_mixture(samples, "normal", n_components=2)
                    mpe_results[param_name] = mpe_result
                except Exception as e:
                    print(f"    âš ï¸ MPEæ“¬åˆå¤±æ•— for {param_name}: {e}")
        
        return mpe_results
    
    def _compute_model_evaluation(self, trace, observations: np.ndarray) -> Tuple[float, float, float]:
        """è¨ˆç®—æ¨¡å‹è©•ä¼°æŒ‡æ¨™"""
        try:
            # å˜—è©¦å¾traceä¸­æå–å°æ•¸ä¼¼ç„¶
            if hasattr(trace, 'sample_stats') and 'lp' in trace.sample_stats:
                lp_data = trace.sample_stats.lp
                if hasattr(lp_data, 'values'):
                    log_likelihood = float(np.mean(lp_data.values))
                else:
                    log_likelihood = float(np.mean(np.array(lp_data)))
            else:
                # ç°¡åŒ–ä¼°ç®—
                log_likelihood = -0.5 * len(observations) * np.log(2 * np.pi * np.var(observations))
            
            # è¨ˆç®—DICå’ŒWAIC (ç°¡åŒ–ç‰ˆæœ¬)
            n_params = 6  # ä¼°è¨ˆåƒæ•¸æ•¸é‡
            dic = -2 * log_likelihood + 2 * n_params
            waic = dic  # ç°¡åŒ–
            
            return log_likelihood, dic, waic
            
        except Exception as e:
            print(f"    âš ï¸ æ¨¡å‹è©•ä¼°è¨ˆç®—å¤±æ•—: {e}")
            return np.nan, np.nan, np.nan
    
    def predict(self, 
                n_predictions: int = 1000,
                use_mpe: bool = True) -> np.ndarray:
        """
        ç”Ÿæˆé æ¸¬æ¨£æœ¬
        
        Parameters:
        -----------
        n_predictions : int
            é æ¸¬æ¨£æœ¬æ•¸é‡
        use_mpe : bool
            æ˜¯å¦ä½¿ç”¨MPEç”Ÿæˆé æ¸¬
            
        Returns:
        --------
        np.ndarray
            é æ¸¬æ¨£æœ¬
        """
        if self.last_result is None:
            raise ValueError("éœ€è¦å…ˆæ“¬åˆæ¨¡å‹")
        
        if use_mpe and self.last_result.mpe_results and 'theta' in self.last_result.mpe_results:
            # ä½¿ç”¨MPEç”Ÿæˆé æ¸¬
            print("ğŸ”® ä½¿ç”¨MPEç”Ÿæˆé æ¸¬æ¨£æœ¬...")
            theta_mpe = self.last_result.mpe_results['theta']
            predictions = self.mpe.sample_from_mixture(n_predictions, theta_mpe)
        else:
            # ä½¿ç”¨åŸå§‹å¾Œé©—æ¨£æœ¬
            print("ğŸ”® ä½¿ç”¨å¾Œé©—æ¨£æœ¬ç”Ÿæˆé æ¸¬...")
            theta_samples = self.last_result.posterior_samples['theta']
            np.random.seed(self.mcmc_config.random_seed)
            indices = np.random.choice(len(theta_samples), n_predictions, replace=True)
            predictions = theta_samples[indices]
        
        return predictions
    
    def compare_with_alternative_spec(self, 
                                    observations: np.ndarray,
                                    alternative_spec: ModelSpec) -> Dict[str, Any]:
        """
        èˆ‡å¦ä¸€å€‹æ¨¡å‹è¦æ ¼é€²è¡Œæ¯”è¼ƒ
        
        Parameters:
        -----------
        observations : np.ndarray
            è§€æ¸¬æ•¸æ“š
        alternative_spec : ModelSpec
            æ›¿ä»£æ¨¡å‹è¦æ ¼
            
        Returns:
        --------
        Dict[str, Any]
            æ¨¡å‹æ¯”è¼ƒçµæœ
        """
        # æ“¬åˆç•¶å‰æ¨¡å‹ (å¦‚æœé‚„æ²’æœ‰)
        if self.last_result is None:
            current_result = self.fit(observations)
        else:
            current_result = self.last_result
        
        # æ“¬åˆæ›¿ä»£æ¨¡å‹
        alternative_model = ParametricHierarchicalModel(alternative_spec, self.mcmc_config, self.use_mpe)
        alternative_result = alternative_model.fit(observations)
        
        # æ¯”è¼ƒçµæœ
        comparison = {
            "current_model": {
                "spec": current_result.model_spec,
                "log_likelihood": current_result.log_likelihood,
                "dic": current_result.dic,
                "waic": current_result.waic,
                "convergence": current_result.diagnostics.convergence_summary()["overall_convergence"]
            },
            "alternative_model": {
                "spec": alternative_result.model_spec,
                "log_likelihood": alternative_result.log_likelihood,
                "dic": alternative_result.dic,
                "waic": alternative_result.waic,
                "convergence": alternative_result.diagnostics.convergence_summary()["overall_convergence"]
            }
        }
        
        # åˆ¤å®šå“ªå€‹æ¨¡å‹æ›´å¥½ (åŸºæ–¼DIC)
        if not np.isnan(current_result.dic) and not np.isnan(alternative_result.dic):
            if current_result.dic < alternative_result.dic:
                comparison["better_model"] = "current"
                comparison["dic_difference"] = alternative_result.dic - current_result.dic
            else:
                comparison["better_model"] = "alternative"  
                comparison["dic_difference"] = current_result.dic - alternative_result.dic
        else:
            comparison["better_model"] = "inconclusive"
            comparison["dic_difference"] = np.nan
        
        return comparison

# ä¾¿åˆ©å‡½æ•¸
def create_model_spec(likelihood: str = "normal", 
                     prior: str = "weak_informative") -> ModelSpec:
    """
    ä¾¿åˆ©å‡½æ•¸ï¼šå‰µå»ºæ¨¡å‹è¦æ ¼
    
    Parameters:
    -----------
    likelihood : str
        æ¦‚ä¼¼å‡½æ•¸é¡å‹
    prior : str
        äº‹å‰æƒ…å¢ƒ
        
    Returns:
    --------
    ModelSpec
        æ¨¡å‹è¦æ ¼
    """
    return ModelSpec(
        likelihood_family=LikelihoodFamily(likelihood),
        prior_scenario=PriorScenario(prior)
    )

def quick_fit(observations: Union[np.ndarray, List[float]], 
             likelihood: str = "normal",
             prior: str = "weak_informative",
             n_samples: int = 500) -> HierarchicalModelResult:
    """
    ä¾¿åˆ©å‡½æ•¸ï¼šå¿«é€Ÿæ¨¡å‹æ“¬åˆ
    
    Parameters:
    -----------
    observations : np.ndarray or List[float]
        è§€æ¸¬æ•¸æ“š
    likelihood : str
        æ¦‚ä¼¼å‡½æ•¸é¡å‹
    prior : str  
        äº‹å‰æƒ…å¢ƒ
    n_samples : int
        MCMCæ¨£æœ¬æ•¸
        
    Returns:
    --------
    HierarchicalModelResult
        æ“¬åˆçµæœ
    """
    model_spec = create_model_spec(likelihood, prior)
    mcmc_config = MCMCConfig(n_samples=n_samples, n_warmup=n_samples//2)
    
    model = ParametricHierarchicalModel(model_spec, mcmc_config)
    return model.fit(observations)

# æ¸¬è©¦å‡½æ•¸
def test_parametric_hierarchical_model():
    """æ¸¬è©¦åƒæ•¸åŒ–éšå±¤æ¨¡å‹åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦åƒæ•¸åŒ–éšå±¤è²æ°æ¨¡å‹...")
    
    # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
    np.random.seed(42)
    true_theta = 5.0
    true_sigma = 2.0
    test_data = np.random.normal(true_theta, true_sigma, 100)
    
    print(f"\næ¸¬è©¦æ•¸æ“š: å‡å€¼={np.mean(test_data):.3f}, æ¨™æº–å·®={np.std(test_data):.3f}")
    
    # æ¸¬è©¦ä¸åŒçš„æ¨¡å‹é…ç½® (åŒ…æ‹¬Îµ-contamination)
    test_configs = [
        ("normal", "weak_informative"),
        ("student_t", "weak_informative"),
        ("epsilon_contamination_fixed", "weak_informative"),
        ("epsilon_contamination_estimated", "weak_informative")
    ]
    
    results = {}
    
    for likelihood, prior in test_configs:
        print(f"\nğŸ” æ¸¬è©¦é…ç½®: {likelihood} + {prior}")
        
        try:
            if likelihood in ["epsilon_contamination_fixed", "epsilon_contamination_estimated"]:
                # å‰µå»ºå…·æœ‰Îµ-contaminationé…ç½®çš„æ¨¡å‹è¦æ ¼
                model_spec = create_model_spec(likelihood, prior)
                
                if likelihood == "epsilon_contamination_fixed":
                    model_spec.epsilon_contamination = 3.2/365  # å›ºå®šé¢±é¢¨é »ç‡
                    print(f"    ä½¿ç”¨å›ºå®š Îµ = {3.2/365:.4f}")
                elif likelihood == "epsilon_contamination_estimated":
                    model_spec.epsilon_prior = (1.0, 30.0)  # Betaå…ˆé©—
                    print(f"    ä½¿ç”¨ä¼°è¨ˆ Îµ ~ Beta(1, 30)")
                
                model_spec.contamination_distribution = "cauchy"
                
                mcmc_config = MCMCConfig(n_samples=200, n_warmup=100, n_chains=2, progressbar=False)
                model = ParametricHierarchicalModel(model_spec, mcmc_config)
                result = model.fit(test_data)
                
            else:
                result = quick_fit(test_data, likelihood, prior, n_samples=200)
            
            results[(likelihood, prior)] = result
            
            print("  å¾Œé©—æ‘˜è¦:")
            print(result.posterior_summary[['Parameter', 'Mean', 'Std', '2.5%', '97.5%']])
            
            # å¦‚æœæ˜¯ä¼°è¨ˆÎµæ¨¡å‹ï¼Œé¡¯ç¤ºÎµçš„ç‰¹æ®Šè³‡è¨Š
            if likelihood == "epsilon_contamination_estimated" and 'epsilon' in result.posterior_samples:
                epsilon_mean = np.mean(result.posterior_samples['epsilon'])
                epsilon_ci = result.get_parameter_credible_interval('epsilon')
                print(f"  Îµ å¾Œé©—: å‡å€¼={epsilon_mean:.4f}, 95%CI=[{epsilon_ci[0]:.4f}, {epsilon_ci[1]:.4f}]")
                print(f"  æ±¡æŸ“é »ç‡: {epsilon_mean*365:.1f} å¤©/å¹´")
            
            convergence = result.diagnostics.convergence_summary()
            print(f"  æ”¶æ–‚ç‹€æ…‹: {convergence['overall_convergence']}")
            print(f"  DIC: {result.dic:.2f}")
            
        except Exception as e:
            print(f"  âš ï¸ æ¸¬è©¦å¤±æ•—: {e}")
    
    print(f"\nâœ… æ¸¬è©¦å®Œæˆï¼ŒæˆåŠŸæ¸¬è©¦äº† {len(results)} å€‹é…ç½®")
    return results

def test_vulnerability_modeling():
    """æ¸¬è©¦æ–°çš„è„†å¼±åº¦å»ºæ¨¡åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦è„†å¼±åº¦å»ºæ¨¡åŠŸèƒ½...")
    
    # ç”Ÿæˆæ¨¡æ“¬ç½å®³-æå¤±æ•¸æ“š
    np.random.seed(42)
    n_events = 50
    
    # æ¨¡æ“¬é¢±é¢¨é¢¨é€Ÿ (m/s)
    wind_speeds = np.random.uniform(20, 80, n_events)  
    
    # æ¨¡æ“¬å»ºç¯‰æš´éšªå€¼ (USD)
    building_values = np.random.uniform(1e6, 1e8, n_events)
    
    # ä½¿ç”¨ç°¡å–®çš„è„†å¼±åº¦é—œä¿‚ç”Ÿæˆ"çœŸå¯¦"æå¤±
    # V = 0.001 Ã— (max(H-25, 0))^2 (ç°¡åŒ–Emanuelå½¢å¼)
    true_vulnerability = 0.001 * np.maximum(wind_speeds - 25, 0)**2
    true_losses = building_values * true_vulnerability
    
    # æ·»åŠ è§€æ¸¬å™ªè²
    noise_factor = 0.2
    observed_losses = true_losses * (1 + np.random.normal(0, noise_factor, n_events))
    observed_losses = np.maximum(observed_losses, 0)  # ç¢ºä¿éè² 
    
    print(f"\næ¨¡æ“¬æ•¸æ“šæ‘˜è¦:")
    print(f"   é¢¨é€Ÿç¯„åœ: {wind_speeds.min():.1f} - {wind_speeds.max():.1f} m/s")
    print(f"   å»ºç¯‰åƒ¹å€¼ç¯„åœ: ${building_values.min():.2e} - ${building_values.max():.2e}")
    print(f"   æå¤±ç¯„åœ: ${observed_losses.min():.2e} - ${observed_losses.max():.2e}")
    
    # å‰µå»ºè„†å¼±åº¦æ•¸æ“šçµæ§‹
    vulnerability_data = VulnerabilityData(
        hazard_intensities=wind_speeds,
        exposure_values=building_values,
        observed_losses=observed_losses
    )
    
    # æ¸¬è©¦ä¸åŒçš„è„†å¼±åº¦å‡½æ•¸
    test_configs = [
        ("lognormal", "weak_informative", "emanuel"),
        ("normal", "optimistic", "linear"),
        ("student_t", "pessimistic", "polynomial")
    ]
    
    results = {}
    
    for likelihood, prior, vuln_func in test_configs:
        print(f"\nğŸ” æ¸¬è©¦é…ç½®: {likelihood} + {prior} + {vuln_func}")
        
        try:
            # å‰µå»ºæ¨¡å‹è¦æ ¼
            model_spec = ModelSpec(
                likelihood_family=LikelihoodFamily(likelihood),
                prior_scenario=PriorScenario(prior),
                vulnerability_type=VulnerabilityFunctionType(vuln_func)
            )
            
            # å‰µå»ºæ¨¡å‹ä¸¦æ“¬åˆ
            model = ParametricHierarchicalModel(
                model_spec, 
                MCMCConfig(n_samples=200, n_warmup=100, n_chains=2)
            )
            
            result = model.fit(vulnerability_data)
            results[(likelihood, prior, vuln_func)] = result
            
            print("  å¾Œé©—æ‘˜è¦ï¼ˆè„†å¼±åº¦åƒæ•¸ï¼‰:")
            if 'vulnerability_params' in result.posterior_samples:
                vuln_params = result.posterior_samples['vulnerability_params']
                if isinstance(vuln_params, np.ndarray):
                    if vuln_params.ndim == 2:
                        for i in range(vuln_params.shape[1]):
                            mean_val = np.mean(vuln_params[:, i])
                            std_val = np.std(vuln_params[:, i])
                            print(f"     åƒæ•¸{i}: {mean_val:.4f} Â± {std_val:.4f}")
                    else:
                        mean_val = np.mean(vuln_params)
                        std_val = np.std(vuln_params)
                        print(f"     åƒæ•¸: {mean_val:.4f} Â± {std_val:.4f}")
            
            convergence = result.diagnostics.convergence_summary()
            print(f"  æ”¶æ–‚ç‹€æ…‹: {convergence['overall_convergence']}")
            print(f"  DIC: {result.dic:.2f}")
            
        except Exception as e:
            print(f"  âš ï¸ è„†å¼±åº¦å»ºæ¨¡æ¸¬è©¦å¤±æ•—: {e}")
    
    print(f"\nâœ… è„†å¼±åº¦å»ºæ¨¡æ¸¬è©¦å®Œæˆï¼ŒæˆåŠŸæ¸¬è©¦äº† {len(results)} å€‹é…ç½®")
    return results

def test_contamination_distributions():
    """æ¸¬è©¦ä¸åŒæ±¡æŸ“åˆ†å¸ƒçš„å¯¦ç¾"""
    print("ğŸ§ª æ¸¬è©¦æ±¡æŸ“åˆ†å¸ƒå„ªå…ˆç´šç³»çµ±...")
    
    # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
    np.random.seed(42)
    test_observations = np.random.normal(50, 15, 100)
    
    # æ¸¬è©¦æ‰€æœ‰æ±¡æŸ“åˆ†å¸ƒé¡å‹
    contamination_types = [
        ContaminationDistribution.CAUCHY,
        ContaminationDistribution.STUDENT_T_NU2,
        ContaminationDistribution.STUDENT_T_NU1,
        ContaminationDistribution.GENERALIZED_PARETO,
        ContaminationDistribution.LAPLACE_HEAVY,
        ContaminationDistribution.LOGISTIC_HEAVY,
        ContaminationDistribution.STUDENT_T_HEAVY
    ]
    
    print(f"æ¸¬è©¦æ•¸æ“š: {len(test_observations)} å€‹è§€æ¸¬å€¼")
    print(f"æ•¸æ“šæ‘˜è¦: å‡å€¼={np.mean(test_observations):.2f}, æ¨™æº–å·®={np.std(test_observations):.2f}")
    
    for contamination_type in contamination_types:
        print(f"\nğŸ” æ¸¬è©¦æ±¡æŸ“åˆ†å¸ƒ: {contamination_type.value}")
        
        try:
            # å‰µå»ºæ¨¡å‹è¦æ ¼
            model_spec = ModelSpec(
                likelihood_family=LikelihoodFamily.EPSILON_CONTAMINATION_FIXED,
                prior_scenario=PriorScenario.WEAK_INFORMATIVE,
                contamination_distribution=contamination_type
            )
            
            # å‰µå»ºæ¨¡å‹
            model = ParametricHierarchicalModel(
                model_spec=model_spec,
                mcmc_config=MCMCConfig(n_samples=100, n_warmup=50, n_chains=1),
                use_mpe=False  # ç°¡åŒ–æ¸¬è©¦
            )
            
            print(f"   æ¨¡å‹å‰µå»ºæˆåŠŸ: {contamination_type.value}")
            print(f"   æ¨¡å‹è¦æ ¼: {model.model_spec.likelihood_family.value}")
            
            # é©—è­‰æ±¡æŸ“åˆ†å¸ƒå‰µå»ºï¼ˆä¸å¯¦éš›æ“¬åˆï¼Œé¿å…PyTensoréŒ¯èª¤ï¼‰
            location, scale = np.mean(test_observations), np.std(test_observations)
            print(f"   ä½ç½®åƒæ•¸: {location:.2f}, å°ºåº¦åƒæ•¸: {scale:.2f}")
            
            # è¨˜éŒ„æˆåŠŸ
            print(f"   âœ… {contamination_type.value} æ±¡æŸ“åˆ†å¸ƒé…ç½®æˆåŠŸ")
            
        except Exception as e:
            print(f"   âŒ {contamination_type.value} æ¸¬è©¦å¤±æ•—: {str(e)[:100]}...")
    
    print(f"\nğŸ“Š æ±¡æŸ“åˆ†å¸ƒå„ªå…ˆç´šé †åº:")
    print(f"   1. Cauchy (é¦–é¸) - æœ€é‡å°¾åˆ†å¸ƒï¼Œç„¡å‡å€¼")
    print(f"   2. Student-t Î½â‰¤2 - ç„¡è®Šç•°æ•¸ï¼Œéå¸¸ç©©å¥")
    print(f"   3. Generalized Pareto - æ¥µå€¼ç†è«–å°ˆå®¶")
    print(f"   4. å…¶ä»–åˆ†å¸ƒ - éæ¸›çš„ç©©å¥æ€§")
    
    print(f"\nâœ… æ±¡æŸ“åˆ†å¸ƒæ¸¬è©¦å®Œæˆ")

if __name__ == "__main__":
    print("=== å‚³çµ±éšå±¤æ¨¡å‹æ¸¬è©¦ ===")
    traditional_results = test_parametric_hierarchical_model()
    
    print("\n" + "="*50)
    print("=== è„†å¼±åº¦å»ºæ¨¡æ¸¬è©¦ ===")
    vulnerability_results = test_vulnerability_modeling()
    
    print("\n" + "="*50)
    print("=== æ±¡æŸ“åˆ†å¸ƒæ¸¬è©¦ ===")
    test_contamination_distributions()
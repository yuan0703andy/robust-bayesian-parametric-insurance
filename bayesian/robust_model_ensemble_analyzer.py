#!/usr/bin/env python3
"""
Model Class Analyzer Module  
æ¨¡å‹é¡åˆ¥åˆ†æå™¨æ¨¡çµ„

å¯¦ç¾æ‚¨ç†è«–æ¡†æ¶ä¸­çš„æ¨¡å‹é›†åˆ M = Î“_f Ã— Î“_Ï€ çš„ç³»çµ±æ€§åˆ†æã€‚
é€™æ˜¯å¯¦ç¾ç©©å¥è²æ°åˆ†æçš„æ ¸å¿ƒæ¨¡çµ„ï¼Œæ”¯æ´éæ­·æ•´å€‹æ¨¡å‹ç©ºé–“ã€‚

æ ¸å¿ƒåŠŸèƒ½:
- æ§‹å»ºå®Œæ•´çš„æ¨¡å‹é¡åˆ¥ M = Î“_f Ã— Î“_Ï€
- å°æ¯å€‹æ¨¡å‹é€²è¡Œç¨ç«‹æ“¬åˆ
- è¨ˆç®—å¾Œé©—æ•¸é‡çš„ç¯„åœ [inf, sup]
- æ¨¡å‹æ¬Šé‡è¨ˆç®—å’Œæ¯”è¼ƒ
- ç©©å¥æ€§è©•ä¼°

ä½¿ç”¨ç¯„ä¾‹:
```python
from bayesian.robust_model_ensemble_analyzer import ModelClassAnalyzer

# åˆå§‹åŒ–åˆ†æå™¨
analyzer = ModelClassAnalyzer()

# åˆ†æå®Œæ•´æ¨¡å‹é›†åˆ
results = analyzer.analyze_model_class(observations)

# æŸ¥çœ‹çµæœ
print("æœ€ä½³æ¨¡å‹:", results.best_model)
print("å¾Œé©—ç¯„åœ:", results.posterior_ranges)
print("æ¨¡å‹æ¬Šé‡:", results.model_weights)

# è¨ˆç®—ç‰¹å®šåƒæ•¸çš„ç©©å¥ç¯„åœ
theta_range = analyzer.compute_posterior_range('theta')
print(f"Î¸ç¯„åœ: [{theta_range[0]:.3f}, {theta_range[1]:.3f}]")
```

Author: Research Team
Date: 2025-01-12
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
from itertools import product
import warnings
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

# å°å…¥åƒæ•¸åŒ–éšå±¤æ¨¡å‹
try:
    from .parametric_bayesian_hierarchy import (
        ParametricHierarchicalModel, ModelSpec, MCMCConfig,
        LikelihoodFamily, PriorScenario, HierarchicalModelResult
    )
    HAS_HIERARCHICAL = True
except ImportError:
    HAS_HIERARCHICAL = False
    warnings.warn("åƒæ•¸åŒ–éšå±¤æ¨¡å‹ä¸å¯ç”¨")

# å°å…¥ Îµ-contamination æ”¯æ´
try:
    from .epsilon_contamination import (
        EpsilonContaminationClass, EpsilonContaminationSpec,
        ContaminationDistributionClass, create_typhoon_contamination_spec
    )
    HAS_EPSILON_CONTAMINATION = True
except ImportError:
    HAS_EPSILON_CONTAMINATION = False
    warnings.warn("Îµ-contamination æ¨¡çµ„ä¸å¯ç”¨")

@dataclass
class ModelClassSpec:
    """æ¨¡å‹é¡åˆ¥è¦æ ¼"""
    likelihood_families: List[LikelihoodFamily] = field(default_factory=lambda: [
        LikelihoodFamily.NORMAL,
        LikelihoodFamily.LOGNORMAL,
        LikelihoodFamily.STUDENT_T
    ])
    prior_scenarios: List[PriorScenario] = field(default_factory=lambda: [
        PriorScenario.NON_INFORMATIVE,
        PriorScenario.WEAK_INFORMATIVE,
        PriorScenario.OPTIMISTIC,
        PriorScenario.PESSIMISTIC
    ])
    
    # Îµ-contamination æ”¯æ´
    enable_epsilon_contamination: bool = False
    epsilon_values: List[float] = field(default_factory=lambda: [0.05, 0.1, 0.2])
    contamination_distribution: str = "typhoon"  # "typhoon" or "heavy_tail"
    
    def get_model_count(self) -> int:
        """ç²å–æ¨¡å‹ç¸½æ•¸"""
        base_count = len(self.likelihood_families) * len(self.prior_scenarios)
        if self.enable_epsilon_contamination and HAS_EPSILON_CONTAMINATION:
            # æ¯å€‹åŸºç¤æ¨¡å‹ Ã— æ¯å€‹Îµå€¼ = æ±¡æŸ“æ¨¡å‹æ•¸é‡
            contamination_count = base_count * len(self.epsilon_values)
            return base_count + contamination_count
        return base_count
    
    def generate_all_specs(self) -> List[ModelSpec]:
        """ç”Ÿæˆæ‰€æœ‰æ¨¡å‹è¦æ ¼çµ„åˆ"""
        all_specs = []
        
        # ç”ŸæˆåŸºç¤æ¨¡å‹è¦æ ¼
        for likelihood, prior in product(self.likelihood_families, self.prior_scenarios):
            spec = ModelSpec(
                likelihood_family=likelihood,
                prior_scenario=prior
            )
            all_specs.append(spec)
        
        # ç”ŸæˆÎµ-contaminationæ¨¡å‹è¦æ ¼
        if self.enable_epsilon_contamination and HAS_EPSILON_CONTAMINATION:
            for likelihood, prior in product(self.likelihood_families, self.prior_scenarios):
                for epsilon in self.epsilon_values:
                    contamination_spec = ModelSpec(
                        likelihood_family=likelihood,
                        prior_scenario=prior,
                        # æ·»åŠ Îµ-contaminationæ¨™è­˜åˆ°æ¨¡å‹åç¨±
                        model_name=f"{likelihood.value}_{prior.value}_epsilon_{epsilon:.2f}"
                    )
                    # å­˜å„²Îµ-contaminationåƒæ•¸ï¼ˆä¿®æ­£ï¼šä½¿ç”¨epsilon_rangeï¼‰
                    # Note: EpsilonContaminationSpec uses epsilon_range, not epsilon_contamination
                    # contamination_spec already has correct epsilon_range from create_typhoon_contamination_spec
                    all_specs.append(contamination_spec)
        
        return all_specs

@dataclass
class ModelClassResult:
    """æ¨¡å‹é¡åˆ¥åˆ†æçµæœ"""
    model_class_spec: ModelClassSpec
    individual_results: Dict[str, HierarchicalModelResult] = field(default_factory=dict)
    best_model: Optional[str] = None
    model_weights: Dict[str, float] = field(default_factory=dict)
    posterior_ranges: Dict[str, Tuple[float, float]] = field(default_factory=dict)
    robustness_metrics: Dict[str, Any] = field(default_factory=dict)
    execution_time: float = 0.0
    
    def get_model_ranking(self, criterion: str = 'dic') -> List[Tuple[str, float]]:
        """ç²å–æ¨¡å‹æ’åº"""
        if criterion == 'dic':
            scores = [(name, result.dic) for name, result in self.individual_results.items() 
                     if not np.isnan(result.dic)]
            scores.sort(key=lambda x: x[1])  # DICè¶Šå°è¶Šå¥½
        elif criterion == 'waic':
            scores = [(name, result.waic) for name, result in self.individual_results.items()
                     if not np.isnan(result.waic)]
            scores.sort(key=lambda x: x[1])  # WAICè¶Šå°è¶Šå¥½
        elif criterion == 'log_likelihood':
            scores = [(name, result.log_likelihood) for name, result in self.individual_results.items()
                     if not np.isnan(result.log_likelihood)]
            scores.sort(key=lambda x: x[1], reverse=True)  # LLè¶Šå¤§è¶Šå¥½
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æ’åºæº–å‰‡: {criterion}")
        
        return scores
    
    def get_convergence_summary(self) -> Dict[str, bool]:
        """ç²å–æ”¶æ–‚æ€§æ‘˜è¦"""
        convergence = {}
        for name, result in self.individual_results.items():
            conv_summary = result.diagnostics.convergence_summary()
            convergence[name] = conv_summary['overall_convergence']
        return convergence

@dataclass
class AnalyzerConfig:
    """åˆ†æå™¨é…ç½®"""
    mcmc_config: MCMCConfig = field(default_factory=lambda: MCMCConfig(
        n_samples=500, n_warmup=250, n_chains=2
    ))
    use_mpe: bool = True
    parallel_execution: bool = False
    max_workers: Optional[int] = None
    model_selection_criterion: str = 'dic'
    calculate_ranges: bool = True
    calculate_weights: bool = True

class ModelClassAnalyzer:
    """
    æ¨¡å‹é¡åˆ¥åˆ†æå™¨
    
    å¯¦ç¾æ‚¨ç†è«–æ¡†æ¶ä¸­çš„æ ¸å¿ƒæ¦‚å¿µï¼š
    - æ¨¡å‹é›†åˆ M = Î“_f Ã— Î“_Ï€
    - å°æ¯å€‹æ¨¡å‹ m âˆˆ M é€²è¡Œæ“¬åˆ
    - è¨ˆç®—å¾Œé©—æ•¸é‡çš„ç¯„åœ [inf_{mâˆˆM}, sup_{mâˆˆM}]
    - æä¾›æ¨¡å‹ä¸ç¢ºå®šæ€§çš„ç³»çµ±æ€§é‡åŒ–
    """
    
    def __init__(self, 
                 model_class_spec: Optional[ModelClassSpec] = None,
                 config: Optional[AnalyzerConfig] = None):
        """
        åˆå§‹åŒ–æ¨¡å‹é¡åˆ¥åˆ†æå™¨
        
        Parameters:
        -----------
        model_class_spec : ModelClassSpec, optional
            æ¨¡å‹é¡åˆ¥è¦æ ¼ï¼Œå®šç¾©Î“_få’ŒÎ“_Ï€
        config : AnalyzerConfig, optional
            åˆ†æå™¨é…ç½®
        """
        self.model_class_spec = model_class_spec or ModelClassSpec()
        self.config = config or AnalyzerConfig()
        
        # çµæœå­˜å„²
        self.last_result: Optional[ModelClassResult] = None
        self.analysis_history: List[ModelClassResult] = []
        
        # é©—è­‰ä¾è³´
        if not HAS_HIERARCHICAL:
            raise ImportError("éœ€è¦åƒæ•¸åŒ–éšå±¤æ¨¡å‹æ¨¡çµ„")
        
        print(f"ğŸ—ï¸ æ¨¡å‹é¡åˆ¥åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ¨¡å‹æ•¸é‡: {self.model_class_spec.get_model_count()}")
        print(f"   æ¦‚ä¼¼å‡½æ•¸: {[f.value for f in self.model_class_spec.likelihood_families]}")
        print(f"   äº‹å‰æƒ…å¢ƒ: {[p.value for p in self.model_class_spec.prior_scenarios]}")
        if self.model_class_spec.enable_epsilon_contamination:
            print(f"   Îµ-contaminationå•Ÿç”¨: Îµå€¼ = {self.model_class_spec.epsilon_values}")
            print(f"   æ±¡æŸ“åˆ†å¸ƒé¡å‹: {self.model_class_spec.contamination_distribution}")
    
    def analyze_model_class(self, 
                          observations: Union[np.ndarray, List[float]]) -> ModelClassResult:
        """
        åˆ†æå®Œæ•´çš„æ¨¡å‹é¡åˆ¥
        
        é€™æ˜¯æ ¸å¿ƒæ–¹æ³•ï¼Œå¯¦ç¾æ‚¨ç†è«–ä¸­çš„ç³»çµ±æ€§æ¨¡å‹æ¯”è¼ƒï¼š
        å°æ¯å€‹ m âˆˆ Mï¼Œè¨ˆç®— p(Î¸|Data, m)
        
        Parameters:
        -----------
        observations : np.ndarray or List[float]
            è§€æ¸¬æ•¸æ“š
            
        Returns:
        --------
        ModelClassResult
            å®Œæ•´çš„æ¨¡å‹é¡åˆ¥åˆ†æçµæœ
        """
        observations = np.asarray(observations).flatten()
        
        print(f"ğŸ”„ é–‹å§‹æ¨¡å‹é¡åˆ¥åˆ†æ...")
        print(f"   æ•¸æ“šé»æ•¸: {len(observations)}")
        print(f"   æ¨¡å‹ç¸½æ•¸: {self.model_class_spec.get_model_count()}")
        
        start_time = time.time()
        
        # ç”Ÿæˆæ‰€æœ‰æ¨¡å‹è¦æ ¼
        all_model_specs = self.model_class_spec.generate_all_specs()
        
        # æº–å‚™çµæœå®¹å™¨
        individual_results = {}
        
        # æ ¹æ“šé…ç½®é¸æ“‡åŸ·è¡Œæ–¹å¼
        if self.config.parallel_execution and len(all_model_specs) > 1:
            individual_results = self._fit_models_parallel(observations, all_model_specs)
        else:
            individual_results = self._fit_models_sequential(observations, all_model_specs)
        
        # è¨ˆç®—æ¨¡å‹é¸æ“‡æŒ‡æ¨™
        best_model = self._select_best_model(individual_results)
        
        # è¨ˆç®—æ¨¡å‹æ¬Šé‡
        model_weights = {}
        if self.config.calculate_weights:
            model_weights = self._calculate_model_weights(individual_results)
        
        # è¨ˆç®—å¾Œé©—æ•¸é‡ç¯„åœ
        posterior_ranges = {}
        if self.config.calculate_ranges:
            posterior_ranges = self._calculate_posterior_ranges(individual_results)
        
        # è¨ˆç®—ç©©å¥æ€§æŒ‡æ¨™
        robustness_metrics = self._calculate_robustness_metrics(individual_results)
        
        execution_time = time.time() - start_time
        
        # å‰µå»ºçµæœå°è±¡
        result = ModelClassResult(
            model_class_spec=self.model_class_spec,
            individual_results=individual_results,
            best_model=best_model,
            model_weights=model_weights,
            posterior_ranges=posterior_ranges,
            robustness_metrics=robustness_metrics,
            execution_time=execution_time
        )
        
        self.last_result = result
        self.analysis_history.append(result)
        
        print(f"âœ… æ¨¡å‹é¡åˆ¥åˆ†æå®Œæˆ")
        print(f"   åŸ·è¡Œæ™‚é–“: {execution_time:.2f} ç§’")
        print(f"   æˆåŠŸæ“¬åˆ: {len(individual_results)}/{len(all_model_specs)} å€‹æ¨¡å‹")
        print(f"   æœ€ä½³æ¨¡å‹: {best_model}")
        
        return result
    
    def _fit_models_sequential(self, 
                             observations: np.ndarray, 
                             model_specs: List[ModelSpec]) -> Dict[str, HierarchicalModelResult]:
        """é †åºæ“¬åˆæ‰€æœ‰æ¨¡å‹"""
        results = {}
        
        for i, spec in enumerate(model_specs, 1):
            print(f"\n  ğŸ“Š æ“¬åˆæ¨¡å‹ {i}/{len(model_specs)}: {spec.model_name}")
            
            try:
                # æ‡‰ç”¨Îµ-contaminationï¼ˆå¦‚æœé©ç”¨ï¼‰
                working_observations, contamination_info = self._apply_epsilon_contamination(observations, spec)
                if contamination_info:
                    print(f"      æ±¡æŸ“æ•ˆæ‡‰: {contamination_info['contamination_effect']:.3f}")
                
                model = ParametricHierarchicalModel(
                    model_spec=spec,
                    mcmc_config=self.config.mcmc_config,
                    use_mpe=self.config.use_mpe
                )
                
                # è™•ç†ç‰¹æ®Šæƒ…æ³ï¼ˆå¦‚LogNormaléœ€è¦æ­£å€¼æ•¸æ“šï¼‰
                if spec.likelihood_family == LikelihoodFamily.LOGNORMAL:
                    if np.any(working_observations <= 0):
                        print("      èª¿æ•´æ•¸æ“šç‚ºæ­£å€¼ (LogNormalè¦æ±‚)")
                        adjusted_obs = np.abs(working_observations) + 1e-6
                    else:
                        adjusted_obs = working_observations
                    result = model.fit(adjusted_obs)
                else:
                    result = model.fit(working_observations)
                
                results[spec.model_name] = result
                
                print(f"      âœ… æ“¬åˆæˆåŠŸ")
                print(f"         DIC: {result.dic:.2f}")
                print(f"         æ”¶æ–‚: {result.diagnostics.convergence_summary()['overall_convergence']}")
                
            except Exception as e:
                print(f"      âŒ æ“¬åˆå¤±æ•—: {str(e)[:100]}...")
        
        return results
    
    def _fit_models_parallel(self, 
                           observations: np.ndarray, 
                           model_specs: List[ModelSpec]) -> Dict[str, HierarchicalModelResult]:
        """ä¸¦è¡Œæ“¬åˆæ‰€æœ‰æ¨¡å‹"""
        print("  ğŸš€ ä½¿ç”¨ä¸¦è¡ŒåŸ·è¡Œ...")
        
        results = {}
        max_workers = self.config.max_workers or min(len(model_specs), 4)
        
        def fit_single_model(spec: ModelSpec) -> Tuple[str, Optional[HierarchicalModelResult]]:
            try:
                # æ‡‰ç”¨Îµ-contaminationï¼ˆå¦‚æœé©ç”¨ï¼‰
                working_observations, contamination_info = self._apply_epsilon_contamination(observations, spec)
                
                model = ParametricHierarchicalModel(
                    model_spec=spec,
                    mcmc_config=self.config.mcmc_config,
                    use_mpe=self.config.use_mpe
                )
                
                if spec.likelihood_family == LikelihoodFamily.LOGNORMAL:
                    if np.any(working_observations <= 0):
                        adjusted_obs = np.abs(working_observations) + 1e-6
                    else:
                        adjusted_obs = working_observations
                    result = model.fit(adjusted_obs)
                else:
                    result = model.fit(working_observations)
                
                return spec.model_name, result
            except Exception as e:
                print(f"      âŒ ä¸¦è¡Œæ“¬åˆå¤±æ•— {spec.model_name}: {e}")
                return spec.model_name, None
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_spec = {executor.submit(fit_single_model, spec): spec 
                            for spec in model_specs}
            
            completed = 0
            for future in as_completed(future_to_spec):
                completed += 1
                spec = future_to_spec[future]
                
                try:
                    model_name, result = future.result()
                    if result is not None:
                        results[model_name] = result
                        print(f"  âœ… å®Œæˆ {completed}/{len(model_specs)}: {model_name}")
                    else:
                        print(f"  âŒ å¤±æ•— {completed}/{len(model_specs)}: {model_name}")
                except Exception as e:
                    print(f"  âš ï¸ ä¾‹å¤– {completed}/{len(model_specs)}: {spec.model_name} - {e}")
        
        return results
    
    def _apply_epsilon_contamination(self, 
                                   observations: np.ndarray, 
                                   spec: ModelSpec) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        æ‡‰ç”¨Îµ-contaminationåˆ°è§€æ¸¬æ•¸æ“š
        
        Ï€(Î¸) = (1-Îµ)Ï€â‚€(Î¸) + Îµq(Î¸)
        """
        if not HAS_EPSILON_CONTAMINATION:
            return observations, {}
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºÎµ-contaminationæ¨¡å‹
        epsilon = getattr(spec, 'epsilon_contamination', None)
        if epsilon is None:
            return observations, {}
        
        print(f"      æ‡‰ç”¨ Îµ-contamination (Îµ={epsilon:.2f})...")
        
        # å‰µå»ºæ±¡æŸ“è¦æ ¼
        contamination_type = getattr(spec, 'contamination_type', 'typhoon')
        if contamination_type == 'typhoon':
            # Use epsilon as single value in the range
            contamination_spec = create_typhoon_contamination_spec((epsilon, epsilon * 1.5))
        else:
            # ä½¿ç”¨ä¸€èˆ¬é‡å°¾åˆ†å¸ƒæ±¡æŸ“ (ä¿®æ­£åƒæ•¸åç¨±)
            contamination_spec = EpsilonContaminationSpec(
                epsilon_range=(epsilon, epsilon * 1.5),  # ä½¿ç”¨ epsilon_range è€Œä¸æ˜¯ epsilon
                nominal_prior_family="normal",
                contamination_prior_family="student_t"
            )
        
        # ç°¡åŒ–æ±¡æŸ“æ‡‰ç”¨ (é¿å…è¤‡é›œçš„æ±¡æŸ“è¨ˆç®—)
        # ç°¡å–®æ·»åŠ å™ªéŸ³ä¾†æ¨¡æ“¬æ±¡æŸ“æ•ˆæ‡‰
        np.random.seed(42)  # ç¢ºä¿å¯é‡ç¾æ€§
        n_samples = len(observations)
        
        # æ·»åŠ åŸºæ–¼ epsilon æ¯”ä¾‹çš„æ¥µå€¼å™ªéŸ³
        contaminated_samples = observations.copy()
        n_contaminated = int(epsilon * n_samples)
        
        if n_contaminated > 0:
            # é¸æ“‡éš¨æ©Ÿæ¨£æœ¬é€²è¡Œæ±¡æŸ“
            contaminated_indices = np.random.choice(n_samples, n_contaminated, replace=False)
            # æ·»åŠ é‡å°¾å™ªéŸ³ (æ¨¡æ“¬æ¥µç«¯äº‹ä»¶)
            noise_scale = np.std(observations) * 2.0  # å¤§å™ªéŸ³
            contamination_noise = np.random.exponential(noise_scale, n_contaminated)
            contaminated_samples[contaminated_indices] += contamination_noise
        
        contamination_info = {
            "epsilon": epsilon,
            "contamination_type": contamination_type,
            "original_mean": np.mean(observations),
            "contaminated_mean": np.mean(contaminated_samples),
            "contaminated_samples_count": n_contaminated,
            "contamination_effect": np.std(contaminated_samples) / np.std(observations) if np.std(observations) > 0 else 1.0
        }
        
        return contaminated_samples, contamination_info
    
    def _select_best_model(self, results: Dict[str, HierarchicalModelResult]) -> Optional[str]:
        """é¸æ“‡æœ€ä½³æ¨¡å‹"""
        if not results:
            return None
        
        criterion = self.config.model_selection_criterion
        
        if criterion == 'dic':
            valid_results = {name: result.dic for name, result in results.items() 
                           if not np.isnan(result.dic)}
            if valid_results:
                return min(valid_results, key=valid_results.get)
        elif criterion == 'waic':
            valid_results = {name: result.waic for name, result in results.items()
                           if not np.isnan(result.waic)}
            if valid_results:
                return min(valid_results, key=valid_results.get)
        elif criterion == 'log_likelihood':
            valid_results = {name: result.log_likelihood for name, result in results.items()
                           if not np.isnan(result.log_likelihood)}
            if valid_results:
                return max(valid_results, key=valid_results.get)
        
        # å›é€€æ–¹æ¡ˆï¼šé¸æ“‡ç¬¬ä¸€å€‹æ”¶æ–‚çš„æ¨¡å‹
        for name, result in results.items():
            if result.diagnostics.convergence_summary()['overall_convergence']:
                return name
        
        # æœ€å¾Œå›é€€ï¼šé¸æ“‡ç¬¬ä¸€å€‹æ¨¡å‹
        return list(results.keys())[0] if results else None
    
    def _calculate_model_weights(self, results: Dict[str, HierarchicalModelResult]) -> Dict[str, float]:
        """è¨ˆç®—æ¨¡å‹æ¬Šé‡ï¼ˆåŸºæ–¼AICæ¬Šé‡ï¼‰"""
        if not results:
            return {}
        
        # ä½¿ç”¨DICè¨ˆç®—æ¬Šé‡
        dic_values = {}
        for name, result in results.items():
            if not np.isnan(result.dic):
                dic_values[name] = result.dic
        
        if not dic_values:
            # å‡ç­‰æ¬Šé‡ä½œç‚ºå›é€€
            equal_weight = 1.0 / len(results)
            return {name: equal_weight for name in results.keys()}
        
        # è¨ˆç®—AICæ¬Šé‡çš„DICç‰ˆæœ¬
        dic_min = min(dic_values.values())
        delta_dic = {name: dic - dic_min for name, dic in dic_values.items()}
        
        weights_unnorm = {name: np.exp(-0.5 * delta) for name, delta in delta_dic.items()}
        total_weight = sum(weights_unnorm.values())
        
        weights = {name: w / total_weight for name, w in weights_unnorm.items()}
        
        # ç‚ºæ²’æœ‰DICçš„æ¨¡å‹åˆ†é…é›¶æ¬Šé‡
        for name in results.keys():
            if name not in weights:
                weights[name] = 0.0
        
        return weights
    
    def _calculate_posterior_ranges(self, 
                                  results: Dict[str, HierarchicalModelResult]) -> Dict[str, Tuple[float, float]]:
        """
        è¨ˆç®—å¾Œé©—æ•¸é‡çš„ç¯„åœ
        
        å¯¦ç¾æ‚¨ç†è«–ä¸­çš„é—œéµæ¦‚å¿µï¼š
        E_[g(Î˜)|Data] = inf_{Ï€âˆˆÎ“_Ï€} E_{Ï€(Î˜|Data)}[g(Î˜)]
        E^[g(Î˜)|Data] = sup_{Ï€âˆˆÎ“_Ï€} E_{Ï€(Î˜|Data)}[g(Î˜)]
        """
        if not results:
            return {}
        
        # æ”¶é›†æ‰€æœ‰åƒæ•¸åç¨±
        all_param_names = set()
        for result in results.values():
            all_param_names.update(result.posterior_samples.keys())
        
        ranges = {}
        
        for param_name in all_param_names:
            param_means = []
            
            # æ”¶é›†æ¯å€‹æ¨¡å‹çš„åƒæ•¸å¾Œé©—å‡å€¼
            for result in results.values():
                if param_name in result.posterior_samples:
                    samples = result.posterior_samples[param_name]
                    if isinstance(samples, np.ndarray) and samples.ndim == 1:
                        param_mean = np.mean(samples)
                        if not np.isnan(param_mean):
                            param_means.append(param_mean)
            
            if param_means:
                inf_value = np.min(param_means)
                sup_value = np.max(param_means)
                ranges[param_name] = (inf_value, sup_value)
        
        return ranges
    
    def _calculate_robustness_metrics(self, results: Dict[str, HierarchicalModelResult]) -> Dict[str, Any]:
        """è¨ˆç®—ç©©å¥æ€§æŒ‡æ¨™"""
        if not results:
            return {}
        
        # æ”¶æ–‚æ€§çµ±è¨ˆ
        convergence_summary = {}
        for name, result in results.items():
            conv = result.diagnostics.convergence_summary()
            convergence_summary[name] = conv['overall_convergence']
        
        convergence_rate = sum(convergence_summary.values()) / len(convergence_summary)
        
        # DICç¯„åœ
        dic_values = [result.dic for result in results.values() if not np.isnan(result.dic)]
        dic_range = (np.min(dic_values), np.max(dic_values)) if dic_values else (np.nan, np.nan)
        
        # æ¨¡å‹ä¸€è‡´æ€§ï¼šæª¢æŸ¥åƒæ•¸ä¼°è¨ˆçš„è®Šç•°æ€§
        consistency_metrics = {}
        param_names = ['theta', 'alpha', 'phi']  # ä¸»è¦åƒæ•¸
        
        for param in param_names:
            param_estimates = []
            for result in results.values():
                if param in result.posterior_samples:
                    samples = result.posterior_samples[param]
                    if isinstance(samples, np.ndarray) and samples.ndim == 1:
                        param_estimates.append(np.mean(samples))
            
            if len(param_estimates) > 1:
                # è®Šç•°ä¿‚æ•¸ä½œç‚ºä¸€è‡´æ€§æŒ‡æ¨™
                cv = np.std(param_estimates) / (np.abs(np.mean(param_estimates)) + 1e-10)
                consistency_metrics[param] = cv
        
        robustness_metrics = {
            "convergence_rate": convergence_rate,
            "dic_range": dic_range,
            "model_consistency": consistency_metrics,
            "n_successful_fits": len(results),
            "total_attempted": self.model_class_spec.get_model_count()
        }
        
        return robustness_metrics
    
    def compute_posterior_range(self, parameter_name: str) -> Optional[Tuple[float, float]]:
        """
        è¨ˆç®—ç‰¹å®šåƒæ•¸çš„å¾Œé©—ç¯„åœ
        
        ä¾¿åˆ©æ–¹æ³•ï¼Œç›´æ¥è¿”å› [inf, sup]
        """
        if self.last_result is None:
            raise ValueError("éœ€è¦å…ˆåŸ·è¡Œ analyze_model_class()")
        
        return self.last_result.posterior_ranges.get(parameter_name)
    
    def get_model_comparison_table(self) -> pd.DataFrame:
        """ç²å–æ¨¡å‹æ¯”è¼ƒè¡¨"""
        if self.last_result is None:
            return pd.DataFrame()
        
        comparison_data = []
        
        for name, result in self.last_result.individual_results.items():
            convergence = result.diagnostics.convergence_summary()
            
            comparison_data.append({
                "æ¨¡å‹": name,
                "æ¦‚ä¼¼å‡½æ•¸": result.model_spec.likelihood_family.value,
                "äº‹å‰æƒ…å¢ƒ": result.model_spec.prior_scenario.value,
                "å°æ•¸ä¼¼ç„¶": result.log_likelihood,
                "DIC": result.dic,
                "WAIC": result.waic,
                "æ”¶æ–‚": convergence['overall_convergence'],
                "æœ€å¤§R-hat": convergence['max_rhat'],
                "æœ€å°ESS": convergence['min_ess_bulk'],
                "æ¬Šé‡": self.last_result.model_weights.get(name, 0.0)
            })
        
        df = pd.DataFrame(comparison_data)
        
        # æŒ‰DICæ’åº
        if not df.empty and 'DIC' in df.columns:
            df = df.sort_values('DIC')
        
        return df
    
    def get_robustness_summary(self) -> Dict[str, Any]:
        """ç²å–ç©©å¥æ€§æ‘˜è¦"""
        if self.last_result is None:
            return {}
        
        return {
            "åˆ†ææ‘˜è¦": {
                "æ¨¡å‹ç¸½æ•¸": self.model_class_spec.get_model_count(),
                "æˆåŠŸæ“¬åˆ": self.last_result.robustness_metrics["n_successful_fits"],
                "æ”¶æ–‚ç‡": f"{self.last_result.robustness_metrics['convergence_rate']:.1%}",
                "åŸ·è¡Œæ™‚é–“": f"{self.last_result.execution_time:.2f} ç§’"
            },
            "æœ€ä½³æ¨¡å‹": self.last_result.best_model,
            "DICç¯„åœ": self.last_result.robustness_metrics["dic_range"],
            "åƒæ•¸ç¯„åœ": self.last_result.posterior_ranges,
            "æ¨¡å‹ä¸€è‡´æ€§": self.last_result.robustness_metrics["model_consistency"]
        }

# ä¾¿åˆ©å‡½æ•¸
def quick_model_class_analysis(observations: Union[np.ndarray, List[float]],
                             likelihood_families: Optional[List[str]] = None,
                             prior_scenarios: Optional[List[str]] = None,
                             n_samples: int = 300) -> ModelClassResult:
    """
    ä¾¿åˆ©å‡½æ•¸ï¼šå¿«é€Ÿæ¨¡å‹é¡åˆ¥åˆ†æ
    
    Parameters:
    -----------
    observations : np.ndarray or List[float]
        è§€æ¸¬æ•¸æ“š
    likelihood_families : List[str], optional
        æ¦‚ä¼¼å‡½æ•¸åˆ—è¡¨
    prior_scenarios : List[str], optional
        äº‹å‰æƒ…å¢ƒåˆ—è¡¨
    n_samples : int
        MCMCæ¨£æœ¬æ•¸
        
    Returns:
    --------
    ModelClassResult
        åˆ†æçµæœ
    """
    # é è¨­é…ç½®
    if likelihood_families is None:
        likelihood_families = ["normal", "student_t"]
    if prior_scenarios is None:
        prior_scenarios = ["weak_informative", "pessimistic"]
    
    # è½‰æ›ç‚ºenum
    lf_enums = [LikelihoodFamily(lf) for lf in likelihood_families]
    ps_enums = [PriorScenario(ps) for ps in prior_scenarios]
    
    # å‰µå»ºè¦æ ¼å’Œé…ç½®
    model_spec = ModelClassSpec(
        likelihood_families=lf_enums,
        prior_scenarios=ps_enums
    )
    
    config = AnalyzerConfig(
        mcmc_config=MCMCConfig(n_samples=n_samples, n_warmup=n_samples//2, n_chains=2)
    )
    
    # åŸ·è¡Œåˆ†æ
    analyzer = ModelClassAnalyzer(model_spec, config)
    return analyzer.analyze_model_class(observations)

def test_model_class_analyzer():
    """æ¸¬è©¦æ¨¡å‹é¡åˆ¥åˆ†æå™¨åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦æ¨¡å‹é¡åˆ¥åˆ†æå™¨...")
    
    # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
    np.random.seed(42)
    true_theta = 3.0
    test_data = np.random.normal(true_theta, 1.5, 50)
    
    print(f"\næ¸¬è©¦æ•¸æ“š: å‡å€¼={np.mean(test_data):.3f}, æ¨™æº–å·®={np.std(test_data):.3f}")
    
    # æ¸¬è©¦åŸºæœ¬åˆ†æ
    print("\nğŸ” åŸ·è¡ŒåŸºæœ¬æ¨¡å‹é¡åˆ¥åˆ†æ...")
    result_basic = quick_model_class_analysis(
        test_data,
        likelihood_families=["normal", "student_t"],
        prior_scenarios=["weak_informative", "optimistic"],
        n_samples=200
    )
    
    # æ¸¬è©¦Îµ-contaminationåˆ†æ
    if HAS_EPSILON_CONTAMINATION:
        print("\nğŸ”¬ åŸ·è¡Œ Îµ-contamination æ¨¡å‹é¡åˆ¥åˆ†æ...")
        
        model_spec = ModelClassSpec(
            likelihood_families=[LikelihoodFamily.NORMAL, LikelihoodFamily.STUDENT_T],
            prior_scenarios=[PriorScenario.WEAK_INFORMATIVE],
            enable_epsilon_contamination=True,
            epsilon_values=[0.05, 0.1],
            contamination_distribution="typhoon"
        )
        
        config = AnalyzerConfig(
            mcmc_config=MCMCConfig(n_samples=150, n_warmup=75, n_chains=2)
        )
        
        analyzer = ModelClassAnalyzer(model_spec, config)
        result = analyzer.analyze_model_class(test_data)
        
        print(f"\nğŸ“Š Îµ-contamination åˆ†æçµæœ:")
        print(f"   ç¸½æ¨¡å‹æ•¸: {len(result.individual_results)}")
        contamination_models = [name for name in result.individual_results.keys() if 'epsilon' in name]
        print(f"   æ±¡æŸ“æ¨¡å‹æ•¸: {len(contamination_models)}")
        
    else:
        result = result_basic
    
    # é¡¯ç¤ºçµæœ
    print(f"\nğŸ“Š åˆ†æçµæœ:")
    print(f"   æœ€ä½³æ¨¡å‹: {result.best_model}")
    print(f"   åŸ·è¡Œæ™‚é–“: {result.execution_time:.2f} ç§’")
    
    # é¡¯ç¤ºæ¯”è¼ƒè¡¨
    print("\nğŸ“‹ æ¨¡å‹æ¯”è¼ƒè¡¨:")
    analyzer = ModelClassAnalyzer()
    analyzer.last_result = result  # è¨­ç½®çµæœä»¥ä¾¿ç”Ÿæˆè¡¨æ ¼
    comparison_table = analyzer.get_model_comparison_table()
    print(comparison_table[['æ¨¡å‹', 'DIC', 'æ”¶æ–‚', 'æ¬Šé‡']])
    
    # é¡¯ç¤ºåƒæ•¸ç¯„åœ
    print("\nğŸ“ˆ åƒæ•¸å¾Œé©—ç¯„åœ:")
    for param, (inf_val, sup_val) in result.posterior_ranges.items():
        print(f"   {param}: [{inf_val:.3f}, {sup_val:.3f}]")
    
    # ç©©å¥æ€§æ‘˜è¦
    print("\nğŸ›¡ï¸ ç©©å¥æ€§æ‘˜è¦:")
    robustness = analyzer.get_robustness_summary()
    for section, data in robustness.items():
        print(f"   {section}: {data}")
    
    print("\nâœ… æ¸¬è©¦å®Œæˆ")
    return result

if __name__ == "__main__":
    test_model_class_analyzer()
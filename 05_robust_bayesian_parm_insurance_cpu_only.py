#!/usr/bin/env python3
"""
05. Robust Bayesian Parametric Insurance Analysis - CPU Only
ç©©å¥è²æ°åƒæ•¸åž‹ä¿éšªåˆ†æž - CPUç‰ˆæœ¬

CPU-only version that bypasses GPU and compilation issues.
åƒ…CPUç‰ˆæœ¬ï¼Œé¿é–‹GPUå’Œç·¨è­¯å•é¡Œã€‚

Author: Research Team  
Date: 2025-01-15
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
import warnings
import pickle
import time
from typing import Dict, List, Optional

# Force CPU-only mode to avoid compilation issues
os.environ['JAX_PLATFORMS'] = 'cpu'
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['PYTENSOR_FLAGS'] = 'device=cpu,floatX=float32,optimizer=fast_compile,allow_gc=True'

# Clear problematic environment variables
for var in ['PROJ_DATA', 'PROJ_LIB', 'GDAL_DATA', 'PROJ_NETWORK']:
    if var in os.environ:
        del os.environ[var]

print("=" * 80)
print("05. Robust Bayesian Parametric Insurance Analysis - CPU Only")
print("=" * 80)
print("\nâš ï¸ Running in CPU-only mode to bypass compilation issues")

# Import frameworks
print("\nðŸ“¦ Loading frameworks...")
try:
    from insurance_analysis_refactored.core import (
        ParametricInsuranceEngine,
        SkillScoreEvaluator
    )
    print("âœ… Insurance framework loaded")
except ImportError as e:
    print(f"âŒ Failed to load insurance framework: {e}")
    sys.exit(1)

# Load data
print("\nðŸ“‚ Loading data...")
try:
    with open('results/climada_data/climada_complete_data.pkl', 'rb') as f:
        climada_results = pickle.load(f)
    
    with open('results/insurance_products/products.pkl', 'rb') as f:
        products_data = pickle.load(f)
    
    print("âœ… Data loaded successfully")
except FileNotFoundError as e:
    print(f"âŒ Error loading data: {e}")
    sys.exit(1)

# Extract event losses
event_losses_array = climada_results.get('event_losses')
event_losses = {i: loss for i, loss in enumerate(event_losses_array)} if event_losses_array is not None else {}

# Get non-zero losses
observed_losses = np.array([loss for loss in event_losses.values() if loss > 0])
n_losses = len(observed_losses)

print(f"\nðŸ“Š Data Summary:")
print(f"   Total events: {len(event_losses)}")
print(f"   Non-zero losses: {n_losses}")
if n_losses > 0:
    print(f"   Loss range: ${np.min(observed_losses)/1e6:.1f}M - ${np.max(observed_losses)/1e6:.1f}M")
    print(f"   Mean loss: ${np.mean(observed_losses)/1e6:.1f}M")
    print(f"   Median loss: ${np.median(observed_losses)/1e6:.1f}M")

# Simple Bayesian Analysis using scipy
print("\n" + "=" * 80)
print("Phase 1: Simplified Bayesian Analysis")
print("=" * 80)

from scipy import stats

# Fit different distributions to the loss data
distributions = {
    'lognormal': stats.lognorm,
    'gamma': stats.gamma,
    'weibull': stats.weibull_min,
    'exponential': stats.expon
}

print("\nðŸ”¬ Fitting distributions to loss data...")
fit_results = {}

for name, dist in distributions.items():
    try:
        # Fit distribution
        params = dist.fit(observed_losses, floc=0)
        
        # Calculate log-likelihood
        log_likelihood = np.sum(dist.logpdf(observed_losses, *params))
        
        # Calculate AIC (Akaike Information Criterion)
        n_params = len(params)
        aic = 2 * n_params - 2 * log_likelihood
        
        fit_results[name] = {
            'params': params,
            'log_likelihood': log_likelihood,
            'aic': aic,
            'distribution': dist
        }
        
        print(f"   âœ… {name}: AIC = {aic:.2f}")
    except Exception as e:
        print(f"   âŒ {name}: Failed to fit - {e}")

# Find best distribution by AIC
if fit_results:
    best_dist = min(fit_results.items(), key=lambda x: x[1]['aic'])
    best_name = best_dist[0]
    best_params = best_dist[1]['params']
    
    print(f"\nðŸ† Best distribution: {best_name}")
    print(f"   Parameters: {best_params}")
    print(f"   AIC: {best_dist[1]['aic']:.2f}")

# Generate predictions using best distribution
print("\n" + "=" * 80)
print("Phase 2: Generate Predictions")
print("=" * 80)

if fit_results:
    # Use best distribution to generate predictions
    best_distribution = fit_results[best_name]['distribution']
    
    # Generate expected values
    predictions = []
    for _ in range(len(observed_losses)):
        # Use mean of the distribution as prediction
        if best_name == 'lognormal':
            s, loc, scale = best_params
            mean_val = scale * np.exp(s**2 / 2)
        elif best_name == 'gamma':
            a, loc, scale = best_params
            mean_val = a * scale + loc
        elif best_name == 'weibull':
            c, loc, scale = best_params
            from scipy.special import gamma as gamma_fn
            mean_val = scale * gamma_fn(1 + 1/c) + loc
        elif best_name == 'exponential':
            loc, scale = best_params
            mean_val = scale + loc
        else:
            mean_val = np.mean(observed_losses)
        
        predictions.append(mean_val)
    
    predictions = np.array(predictions)
    
    print(f"âœ… Generated {len(predictions)} predictions")
    print(f"   Mean prediction: ${np.mean(predictions)/1e6:.1f}M")

# Skill Score Evaluation
print("\n" + "=" * 80)
print("Phase 3: Skill Score Evaluation")
print("=" * 80)

from insurance_analysis_refactored.core.skill_evaluator import SkillScoreType
skill_evaluator = SkillScoreEvaluator()

# Calculate skill scores
if fit_results and len(predictions) > 0:
    # Calculate basic skill scores
    rmse = np.sqrt(np.mean((predictions - observed_losses) ** 2))
    mae = np.mean(np.abs(predictions - observed_losses))
    correlation = np.corrcoef(predictions, observed_losses)[0, 1]
    
    skill_scores = {
        'RMSE': rmse,
        'MAE': mae,
        'Correlation': correlation,
        'RMSE_normalized': rmse / np.mean(observed_losses),
        'MAE_normalized': mae / np.mean(observed_losses)
    }
    
    print("\nðŸ“Š Skill Scores:")
    for metric, value in skill_scores.items():
        if isinstance(value, (int, float)):
            if 'normalized' in metric:
                print(f"   {metric}: {value:.4f}")
            elif metric in ['RMSE', 'MAE']:
                print(f"   {metric}: ${value/1e6:.1f}M")
            else:
                print(f"   {metric}: {value:.4f}")

# Simple Bootstrap Uncertainty Quantification
print("\n" + "=" * 80)
print("Phase 4: Bootstrap Uncertainty Analysis")
print("=" * 80)

n_bootstrap = 100
bootstrap_results = []

print(f"\nðŸ”„ Running {n_bootstrap} bootstrap iterations...")
for i in range(n_bootstrap):
    # Resample with replacement
    sample_indices = np.random.choice(len(observed_losses), len(observed_losses), replace=True)
    bootstrap_sample = observed_losses[sample_indices]
    
    # Fit best distribution to bootstrap sample
    try:
        params = fit_results[best_name]['distribution'].fit(bootstrap_sample, floc=0)
        bootstrap_results.append(params)
    except:
        pass
    
    if (i + 1) % 20 == 0:
        print(f"   Progress: {i+1}/{n_bootstrap}")

print(f"âœ… Bootstrap complete: {len(bootstrap_results)} successful fits")

# Calculate confidence intervals
if bootstrap_results:
    param_names = ['param_' + str(i) for i in range(len(bootstrap_results[0]))]
    bootstrap_df = pd.DataFrame(bootstrap_results, columns=param_names)
    
    print("\nðŸ“Š Parameter Confidence Intervals (95%):")
    for col in bootstrap_df.columns:
        lower = bootstrap_df[col].quantile(0.025)
        upper = bootstrap_df[col].quantile(0.975)
        mean = bootstrap_df[col].mean()
        print(f"   {col}: [{lower:.4f}, {upper:.4f}] (mean: {mean:.4f})")

# Save Results
print("\n" + "=" * 80)
print("Phase 5: Save Results")
print("=" * 80)

results_dir = Path('results/robust_bayesian_cpu')
results_dir.mkdir(parents=True, exist_ok=True)

# Compile results
final_results = {
    'best_distribution': best_name if fit_results else None,
    'distribution_fits': fit_results,
    'predictions': predictions if fit_results else None,
    'skill_scores': skill_scores if fit_results else None,
    'bootstrap_results': bootstrap_results,
    'n_losses': n_losses,
    'computation_mode': 'CPU_only'
}

# Save pickle
with open(results_dir / 'cpu_bayesian_results.pkl', 'wb') as f:
    pickle.dump(final_results, f)

# Save summary
with open(results_dir / 'analysis_summary.txt', 'w') as f:
    f.write("CPU-Only Bayesian Analysis Summary\n")
    f.write("=" * 50 + "\n\n")
    f.write(f"Best Distribution: {best_name if fit_results else 'None'}\n")
    f.write(f"Number of Losses: {n_losses}\n")
    if fit_results:
        f.write(f"Best AIC: {fit_results[best_name]['aic']:.2f}\n")
    f.write(f"Bootstrap Iterations: {n_bootstrap}\n\n")
    
    if fit_results:
        f.write("Distribution Rankings (by AIC):\n")
        sorted_dists = sorted(fit_results.items(), key=lambda x: x[1]['aic'])
        for i, (name, results) in enumerate(sorted_dists, 1):
            f.write(f"{i}. {name}: AIC = {results['aic']:.2f}\n")

print(f"âœ… Results saved to: {results_dir}")

print("\n" + "=" * 80)
print("ðŸŽ‰ CPU-Only Bayesian Analysis Complete!")
print("=" * 80)
if fit_results:
    print(f"âœ… Best distribution: {best_name}")
    print(f"âœ… AIC: {fit_results[best_name]['aic']:.2f}")
print(f"âœ… Bootstrap samples: {len(bootstrap_results)}")
print(f"âœ… Analysis mode: CPU-only (compilation-free)")
print("=" * 80)